<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO is a zero-shot video restoration method that uses Video Consistency Models (VCMs) instead of frame-by-frame image diffusion models to achieve temporally consistent high-definition video reconstruction with high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot image inverse solvers using latent diffusion models (LDMs) achieve excellent results for images but fail when applied to video restoration due to temporal inconsistencies when processing frames individually.

Method: Proposes LVTINO, which leverages Video Consistency Models (VCMs) that distill video latent diffusion models into fast generators capturing temporal causality. Uses a conditioning mechanism that bypasses automatic differentiation and requires only a few neural function evaluations.

Result: Achieves state-of-the-art video reconstruction quality with strong measurement consistency and smooth temporal transitions. Shows significant perceptual improvements over frame-by-frame LDM methods across diverse video inverse problems.

Conclusion: LVTINO establishes a new benchmark in both reconstruction fidelity and computational efficiency for high-definition video restoration, demonstrating the superiority of video-specific priors over image-based approaches for temporal consistency.

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: A method for fine-grained stylized image generation using style extraction from reference images and alignment with text representations, without modifying the downstream generative model's structure.


<details>
  <summary>Details</summary>
Motivation: Fine-grained styles cannot be precisely described in natural language, and stylized reference images are difficult to align with textual conditions in traditional text-guided generation.

Method: Three-stage training style extraction method using a style encoder and style projection layer to align style representations with text representations for fine-grained text-guided style generation.

Result: Proposed method enables fine-grained controlled stylized image generation by extracting and injecting stylistic representations from reference images.

Conclusion: The approach maximizes pretrained generative model capabilities for fine-grained stylized image generation through style extraction and alignment with textual conditions.

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: This paper introduces a dataset for temporal struggle determination in skill acquisition, featuring 61.68 hours of video recordings from 76 participants performing 18 tasks across four activities, with participants repeating tasks to capture skill evolution.


<details>
  <summary>Details</summary>
Motivation: Existing manipulation datasets have not focused on how struggle evolves over time during skill acquisition, which is crucial for optimizing human learning and developing effective assistive systems.

Method: Collected a dataset with 2,793 videos and 5,385 annotated temporal struggle segments, defining struggle determination as a temporal action localization task to identify and precisely localize struggle segments with start and end times.

Result: Temporal Action Localization models successfully learned to detect struggle cues, achieving 34.56% average mAP when generalizing across tasks and 19.24% across activities, showing struggle is transferable across skill-based tasks.

Conclusion: Struggle is a transferable concept across various skill-based tasks, though further improvement in struggle detection remains challenging. The dataset enables research on temporal struggle determination in skill acquisition.

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS is a compact foundation model using lightweight residual U-Net architecture for solving diverse PDEs, achieving SOTA performance with fewer parameters and minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To create an efficient foundation model for PDE solving that avoids the computational overhead of large transformer architectures while maintaining strong generalization capabilities.

Method: Uses lightweight residual U-Net architecture with auto-regressive pretraining strategy that mimics numerical solver behavior, pretrained on diverse fluid dynamics PDEs.

Result: Achieves state-of-the-art generalization on 6 challenging unseen PDEs while requiring significantly fewer parameters and minimal fine-tuning data.

Conclusion: SPUS demonstrates the potential of U-Net based architectures as highly parameter-efficient foundation models for solving diverse PDE systems.

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo is a reinforcement learning framework that optimizes identity diversity in multi-human image generation, solving face duplication and identity merging issues in text-to-image models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models fail at multi-human prompts by duplicating faces, merging identities, and miscounting individuals, creating an identity crisis in generative models.

Method: Uses Group-Relative Policy Optimization (GRPO) with a compositional reward that penalizes facial similarity, discourages identity repetition, enforces accurate person counts, and preserves visual fidelity through human preference scores.

Result: Achieves 98.6% Unique Face Accuracy and near-perfect Global Identity Spread on DiverseHumans Testset, surpassing both open-source and proprietary methods while maintaining competitive perceptual quality.

Conclusion: DisCo establishes a scalable, annotation-free solution that resolves the identity crisis in generative models and sets a new benchmark for compositional multi-human generation.

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: The paper proposes a hierarchical geographic embedding approach for visual geo-localization that combines visual appearance features with semantic segmentation maps, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve visual geo-localization by developing better learned representations of geography that can determine image locations anywhere on Earth using only visual content.

Method: Formulates geo-localization as aligning visual representations with learned geographic representations, using hierarchical geographic embeddings and fusing appearance features with semantic segmentation maps.

Result: Achieved improved all-time bests in 22 out of 25 metrics across five benchmark datasets, outperforming prior state-of-the-art methods and recent Large Vision-Language Models.

Conclusion: The performance gains are primarily driven by the combination of geographic and visual representations, demonstrating the effectiveness of the proposed hierarchical geographic embedding approach.

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS is a novel data selection method for LVLMs that clusters examples based on cross-modal attention matrix trajectories to remove redundancy, achieving significant data reduction while preserving model performance.


<details>
  <summary>Details</summary>
Motivation: Data-efficient learning for LVLMs is underexplored, with existing methods failing to outperform random selection. The need for principled data selection methods to eliminate redundancy in large training datasets.

Method: XMAS clusters examples based on trajectories of top singular values of cross-modal attention matrices from fine-tuning a small proxy LVLM, then samples balanced subsets from these clusters.

Result: XMAS discards 50% of LLaVA-665k and 85% of Vision-Flan datasets while fully preserving LLaVA-1.5-7B performance on 10 benchmarks, speeding up training by 1.2x with 30% more data reduction than best baseline.

Conclusion: XMAS provides the first principled data selection method for LVLMs, effectively eliminating redundancy in training data while maintaining model performance and accelerating training.

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception is a variational flow matching method for vector-quantized image generation that combines continuous transport dynamics with explicit categorical supervision over codebook indices.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between continuous flow matching methods and discrete categorical approaches, enabling geometric awareness from continuous methods while maintaining explicit discrete supervision for better training efficiency.

Method: Adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space.

Result: Training converges faster than both continuous and discrete flow matching baselines while achieving competitive FID scores on ImageNet-1k 256x256 generation, comparable to state-of-the-art models.

Conclusion: Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: Proposes unified deep learning framework using conditional diffusion models for simultaneous synthetic CECT generation from NCCT scans and aortic lumen/thrombus segmentation, outperforming single-task and multi-stage approaches.


<details>
  <summary>Details</summary>
Motivation: Reduce contrast agent use in abdominal aortic aneurysm assessment by generating synthetic CECT from NCCT, avoiding risks of nephrotoxicity, allergies, and environmental harm from iodinated contrast agents.

Method: Integrated conditional diffusion models with multi-task learning for end-to-end joint optimization of image synthesis and segmentation, sharing encoder/decoder parameters across tasks with semi-supervised training for missing labels.

Result: Achieved PSNR of 25.61 dB for image synthesis (vs 23.80 dB single-task CDM), improved lumen Dice to 0.89 (from 0.87) and thrombus Dice to 0.53 (from 0.48), reduced lumen diameter MAE to 4.19 mm (from 5.78 mm) and thrombus area error to 33.85% (from 41.45%).

Conclusion: Unified framework successfully generates synthetic CECT while improving anatomical segmentation accuracy, demonstrating clinical value for reducing contrast agent dependency in AAA assessment.

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: A framework for efficiently prototyping multi-modal video analysis pipelines using pre-trained models to create temporal semi-structured data and knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Multi-modal content analysis is complex, computationally expensive, and requires significant engineering effort. Existing pre-trained models on static data need better integration methods for complex data like videos.

Method: Developed a framework that creates candidate pipelines by combining pre-trained models to convert videos into temporal semi-structured data, then translates this into queryable frame-level indexed knowledge graphs.

Result: The framework enables efficient prototyping of multi-modal analysis pipelines and supports continual learning through dynamic incorporation of new domain-specific knowledge.

Conclusion: The presented framework successfully addresses the challenges of multi-modal video analysis by providing an efficient prototyping approach with queryable knowledge graph representations that support continual learning.

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT is a framework that reverse-engineers website functionality into reusable tools, enabling web agents to perform high-level operations like search and filter instead of fragile step-by-step UI interactions.


<details>
  <summary>Details</summary>
Motivation: Current web agents are brittle and rely on step-by-step UI interactions that break under dynamic layouts and long horizons, unlike humans who use high-level website functionality.

Method: WALT reverse-engineers latent website functionality into reusable invocable tools that abstract away low-level execution, spanning discovery, communication, and content management operations.

Result: On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning compared to traditional methods.

Conclusion: WALT establishes a robust and generalizable paradigm for browser automation by shifting computational burden from fragile step-by-step reasoning to reliable tool invocation.

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: A semi-supervised segmentation framework that uses topological consistency across multiple predictions to preserve meaningful structures in histopathology images, with a novel matching strategy for feature alignment.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of capturing meaningful semantic structures from unlabeled data in histopathology image analysis, where objects are densely distributed and distinguishing biological structures from artifacts is difficult.

Method: Leverages multiple perturbed predictions through stochastic dropouts and temporal training snapshots, enforcing topological consistency across outputs. Introduces a novel matching strategy that integrates spatial overlap with global structural alignment to match topological features without ground truth.

Result: Extensive experiments show the approach effectively reduces topological errors and produces more robust and accurate segmentations essential for reliable downstream analysis.

Conclusion: The proposed framework successfully preserves biologically meaningful structures while filtering out transient artifacts, improving segmentation reliability in histopathology image analysis.

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Diffusion-LPO is a listwise preference optimization framework for diffusion models that extends DPO to handle ranked image preferences, outperforming pairwise methods on visual quality and alignment.


<details>
  <summary>Details</summary>
Motivation: Current DPO applications for diffusion models rely on pairwise preferences, but human feedback often contains implicit ranked information that conveys more precise preferences than pairwise comparisons.

Method: Proposes Diffusion-LPO framework that aggregates user feedback into ranked image lists and derives a listwise extension of DPO objective under the Plackett-Luce model, enforcing consistency across entire rankings.

Result: Empirically demonstrates effectiveness across text-to-image generation, image editing, and personalized preference alignment, consistently outperforming pairwise DPO baselines.

Conclusion: Diffusion-LPO provides a simple and effective framework for listwise preference optimization in diffusion models, better leveraging ranked human feedback for improved alignment.

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge is a pure autoregressive unified multimodal large language model that enables both image understanding and generation within a single next-token prediction framework using a Mixture-of-Transformers architecture and semantic-to-pixel discrete representation.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in building unified MLLMs that support both understanding and generation, where hybrid approaches break the autoregressive paradigm and pure autoregressive approaches face trade-offs between semantic alignment and pixel-level fidelity.

Method: Augments pre-trained visual understanding models with generative ability through Mixture-of-Transformers architecture, and proposes semantic-to-pixel discrete representation integrating compact semantic tokens with fine-grained pixel tokens.

Result: Achieves competitive or superior results in both understanding and generation benchmarks across diverse multimodal benchmarks, with only 7.9% increase in sequence length, requiring less training data and reduced training time compared to prior unified MLLMs.

Conclusion: Bridge successfully demonstrates that pure autoregressive unified MLLMs can achieve strong performance in both understanding and generation tasks while maintaining efficiency in training requirements.

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: A hybrid CNN-Bayesian deep learning model for oral cancer classification using small datasets, achieving 94% accuracy on similar data and 88% on diverse real-world data with superior generalizability compared to traditional CNNs.


<details>
  <summary>Details</summary>
Motivation: Oral cancer has high mortality rates, especially in regions with limited healthcare access. Early diagnosis is crucial but challenging due to limited infrastructure and data. Traditional deep learning models are overconfident and require large datasets, which are unavailable in resource-limited settings.

Method: Proposed a hybrid model combining CNN with Bayesian deep learning using variational inference for uncertainty quantification. Trained on smartphone-captured photographic color images with small datasets and evaluated on three distinct test datasets.

Result: Achieved 94% accuracy on similar distribution test data (comparable to traditional CNN). On diverse real-world data, achieved 88% accuracy vs 72.94% for traditional CNNs. Model showed low uncertainty for correct classifications and high uncertainty for misclassifications.

Conclusion: Bayesian inference effectively enhances model reliability and generalizability in data-scarce environments, improving early oral cancer diagnosis through better uncertainty quantification and performance on limited datasets.

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: CADTrans is a source-free domain adaptation method that constructs invariable feature representations through assistant domains and multiple consistency strategies to handle hard samples and domain bias.


<details>
  <summary>Details</summary>
Motivation: Current SFDA methods struggle with hard samples and domain bias because they can't access source domain data to obtain deterministic invariable features.

Method: Uses assistant domain module for diversified representations, multiple consistent strategies to obtain invariable features distinguishing easy/hard samples, and CMK-MMD for aligning hard samples to easy ones.

Result: Extensive experiments on Office-31, Office-Home, VISDA-C, and DomainNet-126 benchmarks show significant performance improvements.

Conclusion: CADTrans effectively addresses SFDA challenges by constructing domain-consistent invariable feature representations and handling hard samples through assistant domains and conditional alignment strategies.

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: A system that uses historical BLV users' questions from VizWiz-LF dataset to guide MLLMs in generating more contextually-relevant image descriptions for blind and low vision users, reducing irrelevant details.


<details>
  <summary>Details</summary>
Motivation: Current MLLM applications provide comprehensive but often irrelevant descriptions for BLV users, leading to inefficient information exchange. There's a need for more contextually-relevant visual interpretations.

Method: Developed a system that identifies similar visual contexts from VizWiz-LF dataset and uses associated historical BLV user questions to guide MLLM descriptions, making them more relevant to users' likely information needs.

Result: Context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70/92) and were preferred in 54.4% of comparisons (50/92) over context-free descriptions.

Conclusion: Using historical BLV user questions to guide MLLM descriptions significantly improves contextual relevance and user preference, addressing the inefficiency of comprehensive but irrelevant visual interpretations.

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think is a multimodal reasoning dataset built on 250,000 ImageNet21k images, featuring structured thinking tokens and answers generated by state-of-the-art VLMs to train and evaluate reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To develop Vision Language Models (VLMs) with explicit reasoning capabilities and contribute to understanding multimodal reasoning mechanisms.

Method: Synthetic dataset generation using GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506 VLMs, capturing step-by-step reasoning processes and final answers for each image with two thinking-answer pairs.

Result: Creation of a comprehensive multimodal reasoning dataset with structured thinking tokens and corresponding answers for 250,000 images.

Conclusion: ImageNet-Think enables development of more robust VLMs and will be publicly available to advance research in reasoning multimodal models.

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: NPN is a novel regularization method that promotes solutions in low-dimensional projections of the sensing matrix's null-space using neural networks, improving reconstruction in imaging inverse problems.


<details>
  <summary>Details</summary>
Motivation: Existing priors ignore the task-specific structure of the null-space in imaging inverse problems, leading to suboptimal reconstructions from undersampled, noisy measurements.

Method: Proposes Non-Linear Projections of the Null-Space (NPN) - a regularization approach that uses neural networks to project solutions into low-dimensional subspaces of the sensing matrix's null-space, rather than enforcing image-domain constraints.

Result: NPN priors consistently enhance reconstruction fidelity across various imaging inverse problems including compressive sensing, deblurring, super-resolution, CT, and MRI, working with multiple reconstruction frameworks.

Conclusion: NPN provides interpretable and flexible regularization by focusing on null-space structure, offering theoretical guarantees and practical improvements across diverse inverse problems and reconstruction methods.

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: An automated genomic interpretation system that transforms DNA sequences into interpretable decisions using Chaos Game Representation and Concept Bottleneck Model, with applications in HIV subtype classification and clinical automation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretable genomic modeling and automated decision-making for medical automation and robotic systems, providing actionable and biologically meaningful genomic interpretations.

Method: Combines Chaos Game Representation (CGR) with Concept Bottleneck Model (CBM) using biologically meaningful concepts (GC content, CpG density, k-mer motifs), with concept fidelity supervision, prior consistency alignment, KL distribution matching, and uncertainty calibration. Includes cost-aware recommendation layer.

Result: Achieves state-of-the-art HIV subtype classification across in-house and LANL datasets, superior concept prediction fidelity, and favorable cost-benefit trade-offs compared to baselines. Reduces unnecessary retests and improves efficiency.

Conclusion: Establishes a reliable foundation for robotic and clinical automation in genomic medicine by providing interpretable evidence that can be validated against biological priors and balancing accuracy with clinical utility.

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1 enhances Vision-Language-Action models by integrating reinforcement learning with verifiable rewards and chain-of-thought reasoning to improve step-by-step reasoning and execution accuracy.


<details>
  <summary>Details</summary>
Motivation: Current VLA models lack explicit step-by-step reasoning, emit final actions without considering affordance constraints or geometric relations, and have weak post-training reward designs.

Method: Integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO), uses RLVR-based post-training with verifiable rewards for region alignment, trajectory consistency, and output formatting, and develops VLA-CoT-13K dataset with chain-of-thought supervision aligned with affordance and trajectory annotations.

Result: Achieves superior generalization and real-world performance compared to prior VLA methods across in-domain, out-of-domain, simulation, and real-robot platforms.

Conclusion: VLA-R1 demonstrates that systematic optimization of both reasoning and execution through verifiable rewards and chain-of-thought supervision significantly improves VLA model performance and generalization capabilities.

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: A joint deblurring and 3D reconstruction method for macrophotography that simultaneously optimizes clear 3D models and defocus blur kernels from multi-view blurry images using differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: Defocus blur in macrophotography hinders clear imaging and high-quality 3D reconstruction of small objects, while traditional methods require extensive data and annotations.

Method: Joint optimization of clear 3D model and defocus blur kernels from multi-view blurry images using differentiable rendering for self-supervised learning.

Result: The method achieves high-quality image deblurring and recovers high-fidelity 3D appearance from a small number of multi-view images.

Conclusion: The proposed framework successfully addresses defocus blur in macrophotography and enables high-quality 3D reconstruction without requiring extensive data or annotations.

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff is a novel single-step diffusion model for high-fidelity motion deblurring that reformulates deblurring as a diffusion process and trains a consistency model to enable accurate one-step deblurring.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current diffusion models for image deblurring, including unbearable inference time and compromised fidelity, while leveraging the strong generative capabilities of pre-trained diffusion models.

Method: Reformulates motion deblurring as a diffusion-like process with progressively blurred images, trains a consistency model aligning all timesteps to clean images, uses Kernel ControlNet for blur kernel estimation, and introduces adaptive timestep prediction.

Result: Achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching other state-of-the-art models.

Conclusion: FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks and establishes a robust baseline for advancing diffusion models in real-world industrial applications.

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: The paper presents a large-scale bronze inscription dataset and a two-stage detection-recognition pipeline with LadderMoE to address challenges in automatic BI recognition including visual degradation, multi-domain variability, and long-tailed character distribution.


<details>
  <summary>Details</summary>
Motivation: Bronze inscriptions are crucial for archaeological and historical studies but automatic recognition is difficult due to severe visual degradation, multi-domain variability across photographs, rubbings, and tracings, and extremely long-tailed character distribution.

Method: A two-stage detection-recognition pipeline that first localizes inscriptions and then transcribes individual characters, equipped with LadderMoE which augments a pretrained CLIP encoder with ladder-style MoE adapters for dynamic expert specialization and stronger robustness.

Result: The method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities (photographs, rubbings, tracings).

Conclusion: The approach establishes a strong foundation for bronze inscription recognition and downstream archaeological analysis, demonstrating robust cross-domain performance on a curated large-scale dataset of 22,454 full-page images and 198,598 annotated characters spanning 6,658 unique categories.

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA proposes a parameter-efficient UDA method using visual reprogramming layers instead of full backbone fine-tuning, achieving competitive accuracy with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods require fine-tuning backbone parameters for each source-target pair, leading to linear growth in parameters and storage, and preventing backbone reuse.

Method: Prepend domain-specific visual reprogramming layers to the backbone that produce visual prompts acting as textural bias, optimizing with multiple objective functions for intra- and inter-domain distribution differences without modifying backbone parameters.

Result: Achieves 92.8% mean accuracy on Office-31 with only 1.5M trainable parameters, surpassing PDA by +1.6% accuracy using 46% of its parameters, and outperforms full-backbone methods while using only 1.7-2.8% of their parameters.

Conclusion: VirDA enables efficient domain adaptation through visual reprogramming, allowing backbone reuse across domains while maintaining competitive performance with dramatically reduced parameter requirements.

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: Discrete Facial Encoding (DFE) is an unsupervised method that creates an interpretable dictionary of facial expressions using 3D mesh sequences and RVQ-VAE, outperforming FACS in psychological tasks.


<details>
  <summary>Details</summary>
Motivation: Existing facial expression coding systems like FACS have limited coverage and require costly manual annotation, creating a need for scalable, data-driven alternatives.

Method: Extract identity-invariant expression features using 3DMM, then encode them with Residual Vector Quantized Variational Autoencoder (RVQ-VAE) to produce discrete tokens from a shared codebook.

Result: DFE captures more precise facial behaviors than FACS and outperforms both FACS-based pipelines and modern representation learning models in stress detection, personality prediction, and depression detection tasks.

Conclusion: DFE serves as a scalable and effective alternative to FACS for psychological and affective computing applications, covering a wider variety of facial displays.

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM is a novel method for non-rigid structure-from-motion under conformal deformations that performs point-wise reconstruction using optimized 2D image warps in a graph-based framework, accurately computing local conformal scale and enabling precise depth estimation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing NRSfM methods that rely on strict assumptions like locally planar surfaces or locally linear deformations, and fail to recover conformal scale, while improving monocular visual deformable SLAM.

Method: Uses point-wise reconstruction with 2D selected image warps optimized through graph-based framework, employs parallel separable iterative optimization strategy, and incorporates self-supervised learning with encoder-decoder network for dense 3D point cloud generation.

Result: Outperforms existing approaches in reconstruction accuracy and robustness on both synthetic and real datasets, successfully computes local conformal scale and enables precise depth estimation.

Conclusion: The proposed Con-NRSfM method effectively handles conformal deformations, eliminates restrictive assumptions, and provides superior performance compared to existing methods, with code to be made publicly available.

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse is a unified framework that decouples robust 3D reconstruction from inconsistent multi-view images into restoration and reconstruction subtasks, using a video diffusion model to restore consistency before reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust reconstruction from inconsistent multi-view images rely heavily on dense observations and struggle with optimization. The paper aims to address this limitation by simplifying the optimization process.

Method: Proposes UniVerse framework that first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs 3D scenes from the restored images.

Result: Extensive experiments on synthetic and real-world datasets demonstrate strong generalization capability and superior performance in robust reconstruction. The method can also control the style of reconstructed 3D scenes.

Conclusion: UniVerse effectively addresses the challenge of robust reconstruction from inconsistent multi-view images by leveraging diffusion models to learn general scene priors, making it applicable to diverse image inconsistencies and outperforming existing methods.

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: A lightweight end-to-end framework for template matching that jointly estimates position, rotation, and scaling through geometric regression, achieving high precision and fast inference for industrial applications.


<details>
  <summary>Details</summary>
Motivation: Traditional template matching methods are inefficient due to exhaustive enumeration of transformations, while deep learning approaches lack explicit geometric pose modeling, making them unsuitable for real-world deployment.

Method: Proposes a framework with Template-Aware Dynamic Convolution Module (TDCM) that injects template features, uses depthwise separable convolutions and pixel shuffle for efficiency, and employs rotation-shear augmentation with pseudo labels for geometric-annotation-free training.

Result: The 3.07M parameter model achieves high precision with 14ms inference under compound transformations and shows strong robustness in small-template and multi-object scenarios.

Conclusion: The proposed method is highly suitable for real-time industrial applications due to its efficiency, precision, and robustness in handling geometric transformations.

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: A framework for adaptive pixel reasoning in Vision-Language Models that dynamically determines when to use pixel-level operations based on query difficulty, achieving superior performance while significantly reducing unnecessary visual operations.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with fine-grained visual understanding due to information loss during image encoding and insufficient attention to critical regions. Existing pixel-level approaches are inefficient and get distracted by irrelevant details.

Method: Operation-aware supervised fine-tuning followed by rollout-guided reinforcement learning that uses the model's own response feedback to determine when pixel operations should be invoked based on query difficulty.

Result: Achieves 73.4% accuracy on HR-Bench 4K with only 20.1% tool usage ratio, improving accuracy while reducing tool usage by 66.5% compared to previous methods.

Conclusion: The proposed adaptive pixel reasoning framework enables VLMs to achieve better performance on fine-grained visual tasks while being significantly more efficient by dynamically controlling pixel-level operations.

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: ASRS framework uses rotation augmentations and embedding shifts to detect error-prone chest X-ray cases, identifying hidden failures that standard metrics miss.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for chest X-ray interpretation show uneven performance across patient subgroups, with hidden failures not captured by aggregate metrics. Existing error detection methods struggle with subtle within-distribution errors.

Method: Augmentation-sensitivity risk scoring (ASRS) applies clinically plausible rotations (±15°/±30°) and measures embedding shifts using the RAD-DINO encoder. Cases are stratified into stability quartiles based on sensitivity scores.

Result: Highly sensitive cases show substantially lower recall (-0.2 to -0.3) despite high AUROC and confidence scores. ASRS effectively identifies error-prone cases that standard metrics would miss.

Conclusion: ASRS provides a label-free method for selective prediction and clinician review, improving fairness and safety in medical AI by detecting hidden failures in chest X-ray interpretation models.

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS is a training-free video stylization framework that generates stylized videos with rich style details and strong temporal coherence by integrating multiple stylized references to a pretrained image-to-video model.


<details>
  <summary>Details</summary>
Motivation: Video stylization is challenging because frame-by-frame image stylization hurts temporal consistency and reduces style richness, while training dedicated video models requires paired video data and is computationally expensive.

Method: Integrates multiple stylized references to a pretrained I2V model, uses high-frequency compensation to constrain content layout and motion, and employs flow-based motion cues to preserve style textures in low-saliency regions.

Result: FreeViS delivers higher stylization fidelity and superior temporal consistency, outperforming recent baselines and achieving strong human preference without introducing flickers and stutters.

Conclusion: The training-free pipeline offers a practical and economic solution for high-quality, temporally coherent video stylization.

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench is a new benchmark for medical image quality assessment using multi-modal LLMs, featuring perception and reasoning tasks across 5 imaging modalities with 2,600+ queries, showing current MLLMs have preliminary but unreliable capabilities for clinical use.


<details>
  <summary>Details</summary>
Motivation: Existing medical IQA approaches use scalar score-based metrics that don't capture the descriptive, human-like reasoning process used by clinical experts, creating a gap in AI evaluation.

Method: Created MedQ-Bench with two tasks: MedQ-Perception (low-level visual attributes) and MedQ-Reasoning (no-reference and comparison tasks). Used 5 imaging modalities, 40+ quality attributes, 2,600 perceptual queries, and 708 reasoning assessments from clinical, simulated, and AI-generated images. Proposed multi-dimensional judging protocol and validated with radiologists.

Result: Evaluation of 14 state-of-the-art MLLMs revealed they exhibit preliminary but unstable perceptual and reasoning skills, with insufficient accuracy for reliable clinical deployment.

Conclusion: Current MLLMs need targeted optimization for medical IQA, and MedQ-Bench provides a foundation to catalyze further development and unlock MLLMs' potential for medical image quality evaluation.

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer is a network that predicts full occlusion and depth orderings for all instances in a scene from a single RGB image input, eliminating the need for expensive input formats and quadratic inference costs.


<details>
  <summary>Details</summary>
Motivation: Understanding instance-wise geometries is challenging for visual models, and existing methods rely on expensive input formats (category labels, segmentation masks) and high inference costs (quadratic forward passes).

Method: InstaFormer uses interactions between object queries and latent mask descriptors that semantically represent the same objects while carrying complementary information, enabling holistic order prediction in a single forward pass.

Result: The approach is comprehensively benchmarked and ablated to demonstrate its effectiveness in predicting occlusion and depth orderings.

Conclusion: InstaFormer provides an efficient solution for instance-wise geometry understanding with reduced input requirements and computational costs, with open-source code and models available.

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler is a transformer framework with Pyramidal Positional Encoding that improves neural style transfer by capturing multi-scale details while reducing computation, achieving faster inference and better style/content preservation.


<details>
  <summary>Details</summary>
Motivation: Existing CNN and transformer-based NST models struggle with scaling to complex styles and high-resolution inputs efficiently.

Method: Transformer framework with Pyramidal Positional Encoding (hierarchical multi-scale encoding) and reinforcement learning for dynamic optimization.

Result: Reduced content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs with 1.39s inference; further improved to content 2.03 and style 0.75 with RL while maintaining 1.40s speed.

Conclusion: Achieves real-time, high-quality artistic rendering suitable for media and design applications.

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS is a load-balanced and efficient 3D Gaussian Splatting framework that addresses scalability issues in large scenes through depth-aware partitioning, optimization-based load balancing, and lightweight optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Scaling 3D Gaussian Splatting to large unbounded scenes like city blocks is challenging due to memory constraints and inefficient partitioning in existing divide-and-conquer methods, which suffer from load imbalance and high overhead.

Method: Introduces depth-aware partitioning for fast preprocessing, optimization-based strategy to balance visible Gaussians across blocks, and lightweight techniques including visibility cropping and selective densification.

Result: Achieves up to 2x faster end-to-end training time than state-of-the-art baselines while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.

Conclusion: LoBE-GS successfully addresses the scalability limitations of 3D Gaussian Splatting for large-scale scenes through efficient load balancing and optimization techniques.

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: Proposes MemoryPack for dynamic context modeling and Direct Forcing to mitigate error accumulation in long-form video generation, achieving minute-level temporal consistency with linear complexity.


<details>
  <summary>Details</summary>
Motivation: Long-form video generation faces challenges in capturing long-range dependencies while preventing error accumulation in autoregressive decoding.

Method: MemoryPack: learnable context-retrieval mechanism using textual and image information as global guidance; Direct Forcing: single-step approximating strategy for better training-inference alignment.

Result: Achieves minute-level temporal consistency, scales gracefully with video length, maintains computational efficiency with linear complexity, and reduces error propagation.

Conclusion: MemoryPack and Direct Forcing substantially enhance context consistency and reliability of long-form video generation, advancing practical usability of autoregressive video models.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: This paper proposes methods for confidence calibration in 3D object detectors, introducing new metrics and loss terms to improve calibration of both dominant and secondary class predictions.


<details>
  <summary>Details</summary>
Motivation: Precise object detection and uncertainty estimation are critical for autonomous systems' self-aware and safe operation, particularly for 3D object detectors' classification task.

Method: Proposed two auxiliary regularizing loss terms for calibration of dominant prediction or full prediction vector, evaluated on CenterPoint, PillarNet and DSVT-Pillar detectors using post-hoc and train-time methods.

Result: Combining the full class prediction calibration loss term with isotonic regression achieved best calibration for CenterPoint and PillarNet, but DSVT-Pillar could not be jointly calibrated for both dominant and secondary predictions.

Conclusion: Different 3D object detectors require tailored calibration approaches, with the proposed full prediction vector calibration method being effective for some architectures but not universally applicable.

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS is a novel person search framework that leverages pre-trained diffusion models to address optimization conflicts between detection and re-identification tasks, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods use ImageNet pre-trained backbones that are suboptimal for person search due to complex spatial context requirements and conflicting optimization objectives between detection and re-identification tasks.

Method: Proposes three specialized modules: Diffusion-Guided Region Proposal Network (DGRPN) for person localization, Multi-Scale Frequency Refinement Network (MSFRN) to reduce shape bias, and Semantic-Adaptive Feature Aggregation Network (SFAN) to utilize text-aligned diffusion features.

Result: DiffPS achieves new state-of-the-art performance on CUHK-SYSU and PRW datasets.

Conclusion: Leveraging diffusion model priors effectively addresses the optimization conflict in person search and improves both localization and identification performance.

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU integrates flow matching with deep unfolding for hyperspectral image reconstruction, achieving superior results through generative priors and global consistency enforcement.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging is costly and challenging to reconstruct from compressed measurements, with existing methods suffering from degradation and loss of spectral details.

Method: Proposes Flow-Matching-guided Unfolding network (FMU) that embeds flow matching generative prior within deep unfolding framework, with mean velocity loss for global consistency.

Result: Extensive experiments on simulated and real datasets show FMU significantly outperforms existing approaches in reconstruction quality.

Conclusion: FMU successfully combines interpretability of optimization methods with generative capacity of flow matching for robust HSI reconstruction.

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: Automated defect detection system for DIP components using deep learning and ConSinGAN data augmentation, achieving 95.50% accuracy with YOLOv7.


<details>
  <summary>Details</summary>
Motivation: Conventional defect detection is time-consuming and labor-intensive, creating burden on quality inspection personnel and making quality management difficult.

Method: Used ConSinGAN to generate dataset for training, investigated four YOLO models (v3, v4, v7, v9) with and without ConSinGAN augmentation, and developed SCADA system with sensor architecture.

Result: YOLOv7 with ConSinGAN achieved superior performance: 95.50% accuracy and 285 ms detection time, significantly outperforming threshold-based approaches.

Conclusion: The proposed automated defect detection system can be easily established for various defect types and works well even with insufficient defect data.

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD is a few-shot anomaly detection method that uses foundation visual encoders and learns a nonlinear projection operator to identify anomalies by detecting out-of-distribution regions in images.


<details>
  <summary>Details</summary>
Motivation: Few-shot anomaly detection faces challenges in differentiating normal and abnormal features with limited samples, especially under category-agnostic conditions. Foundation visual encoders' pre-training helps learn general normal image distributions.

Method: Learns a nonlinear projection operator onto the natural image manifold. The operator characterizes and identifies out-of-distribution regions by utilizing the correlation between anomaly amount and embedding differences from foundation encoders.

Result: Achieves competitive performance in multi-class detection while using substantially fewer parameters than prior methods. Validated with multiple foundation encoders including DINOv3.

Conclusion: The approach broadens perspective on foundation features and advances few-shot anomaly detection field by effectively leveraging pre-trained encoders for anomaly identification.

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT reduces Vision Transformer computational complexity for semantic segmentation by clustering similar tokens and regenerating fine details, achieving significant speedup with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers have high accuracy but quadratic attention complexity limits their practical use in robotics. Token merging works for classification but is less suitable for dense prediction tasks like semantic segmentation.

Method: Extends Vision Transformer with a trainable Cluster module that merges similar tokens using pseudo-clusters from segmentation masks, and a Regenerator module that restores fine details for downstream processing.

Result: Achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three datasets while maintaining comparable segmentation accuracy to standard approaches.

Conclusion: ClustViT provides an efficient solution for deploying Vision Transformers in real-world robotic systems by significantly reducing computational complexity without sacrificing segmentation performance.

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT introduces a unified paradigm for MLLMs to directly generate both textual and visual outputs using Visual Reference Tokens (VRTs) interleaved with text tokens, enabling dense prediction tasks like detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs rely on indirect representations like generating coordinates as text, which limits performance and prevents dense prediction tasks such as segmentation.

Method: Uses Visual Reference Tokens (VRTs) derived from visual patch embeddings, interleaved with LLM's textual tokens. A lightweight decoder transforms outputs into visual predictions. Features independent VRT processing and dynamic embedding table expansion.

Result: Achieves state-of-the-art performance across four visual perception and understanding tasks, outperforming significantly larger MLLM models.

Conclusion: PaDT provides an effective unified framework for MLLMs to handle both textual and diverse visual outputs, overcoming limitations of previous indirect representation approaches.

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: The paper addresses trust issues in online produce e-commerce by proposing a 'Trust Pyramid' model and 'Triangular Trust Index' (TTI) to quantify quality trade-offs. It introduces TriAlignXA, an explainable AI framework that transforms algorithms from decision-makers to transparent decision-support tools through multi-objective optimization.


<details>
  <summary>Details</summary>
Motivation: The 'trust deficit' in online fruit and vegetable e-commerce arises from the inability of digital transactions to provide direct sensory perception of product quality, creating a fundamental barrier to consumer trust in online produce purchases.

Method: The study constructs a 'Trust Pyramid' model through 'dual-source verification' and proposes the 'Triangular Trust Index' (TTI) to quantify trade-offs. It designs TriAlignXA framework with three engines: Bio-Adaptive Engine for quality description, Timeliness Optimization Engine for efficiency, and Economic Optimization Engine for cost control, plus a 'Pre-Mapping Mechanism' using QR codes for transparency.

Result: Experiments on grading tasks demonstrate significantly higher accuracy than baseline models. Empirical evidence and theoretical analysis verify the framework's capability to balance the 'impossible triangle' of biological characteristics, timeliness, and economic viability in agricultural product grading.

Conclusion: The research provides comprehensive support from theory to practice for building trustworthy online produce ecosystems, establishing a critical pathway from algorithmic decision-making to consumer trust through transparent, explainable AI systems that acknowledge and manage fundamental trade-offs in agricultural product quality assessment.

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft is a consistent and interactive 4D Gaussian Splatting editing framework that addresses view, temporal, and non-editing region consistency issues while handling complex text instructions through LLM-based intent understanding.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in 4D Gaussian Splatting editing including view, temporal, and non-editing region consistency problems, as well as difficulties in handling complex text instructions.

Method: Uses a 4D-aware InstructPix2Pix model with 4D VGGT geometry features, multi-view grid module for consistency refinement, Gaussian selection mechanism for non-edited region preservation, and LLM-based module for user intent understanding and instruction decomposition.

Result: The approach enables more consistent and controllable 4D scene editing compared to related works, with improved handling of complex user commands.

Conclusion: 4DGS-Craft provides an effective framework for consistent and interactive 4D Gaussian Splatting editing that can interpret complex user instructions and maintain editing consistency across views, time, and non-edited regions.

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP) is a pixel-level masking mechanism that identifies pure pixels to exempt them from expensive computations, integrated into ATD-light model to achieve superior SR performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing lightweight SR methods like CAMixer have limitations including poor adaptability, coarse-grained masking and spatial inflexibility. There's a need for more efficient computation routing in image super-resolution.

Method: PP uses fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking. It identifies pure pixels and exempts them from expensive computations.

Result: PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency while saving similar computation.

Conclusion: Pure-Pass provides an effective pixel-level masking mechanism that enables fine-grained, spatially flexible computation routing for improved SR performance with minimal computational overhead.

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: The study proposes a Self-correction Loop with Structured Output (SLSO) framework using GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs, showing improved accuracy over conventional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of automated jaw cyst findings generation on dental radiographs by addressing inconsistencies and hallucinations in AI-generated outputs.

Method: Implemented a 10-step SLSO framework with iterative regeneration when inconsistencies are detected, comparing it against conventional Chain-of-Thought (CoT) method across seven evaluation criteria.

Result: SLSO improved output accuracy with 66.9%, 33.3%, and 28.6% improvement rates for tooth number, tooth movement, and root resorption respectively. Successful cases achieved consistent output after up to five regenerations.

Conclusion: The SLSO framework effectively reduces hallucinations and improves accuracy, though limitations exist for extensive lesions spanning multiple teeth. Further refinement is needed for practical implementation.

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net is a 3D autoencoder that efficiently encodes LiDAR point clouds from traffic environments using simplified skip connections and fewer encoder layers, achieving good reconstruction quality and generalization.


<details>
  <summary>Details</summary>
Motivation: To create an efficient 3D autoencoder for LiDAR point clouds from real traffic environments that uses fewer resources than state-of-the-art architectures while maintaining performance.

Method: Proposed LiLa-Net architecture with reduced encoder layers and simplified skip connections to encode efficient features from LiDAR point clouds, balancing skip connection information with latent encoding.

Result: The model produces an efficient latent space that accurately reconstructs original point clouds, achieves improved reconstruction quality without compromising performance, and demonstrates strong generalization to objects outside the original traffic environment.

Conclusion: LiLa-Net successfully creates an efficient 3D autoencoder for LiDAR point clouds with simplified architecture that maintains reconstruction accuracy and generalization capabilities while using fewer computational resources.

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools is an open-source package for automated multi-species behavioral monitoring using drone video and machine learning to extract behavioral, social, and spatial metrics from wildlife footage.


<details>
  <summary>Details</summary>
Motivation: Traditional field observations are limited in scope, time-consuming, and labor-intensive, hindering comprehensive assessment of behavioral responses across landscapes.

Method: Integrates drone-based video with machine learning systems including object detection, tracking, and behavioral classification to generate metrics like time budgets, behavioral transitions, social interactions, habitat associations, and group composition dynamics.

Result: Drone-based observations improved behavioral granularity by 15% visibility loss reduction and captured more transitions with higher accuracy. Case studies analyzed 969 behavioral sequences, revealing species-specific behavioral patterns and spatial segregation in mixed-species herds.

Conclusion: kabr-tools enables automated behavioral monitoring at scale, offering a powerful tool for ecosystem-wide studies that advances conservation, biodiversity research, and ecological monitoring.

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing is a novel framework for semantic-aware 3D shape and texture morphing from multi-view images that uses mesh-guided 3D Gaussian Splatting for high-fidelity geometry and appearance modeling with unified deformation strategy and unsupervised semantic correspondence.


<details>
  <summary>Details</summary>
Motivation: Previous approaches for 3D morphing rely on point clouds or require pre-defined homeomorphic mappings for untextured data, which have limitations in handling textured 3D data with semantic awareness.

Method: Leverages mesh-guided 3D Gaussian Splatting (3DGS) with unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. Establishes unsupervised semantic correspondence using mesh topology as geometric prior.

Result: Outperforms prior 2D/3D methods on TexMorph benchmark, reducing color consistency error (ΔE) by 22.2% and EI by 26.2%.

Conclusion: The framework enables semantic-aware 3D shape and texture morphing without requiring labeled data, preserving both local detail and global semantic coherence throughout the morphing process.

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose: A diffusion-based pose estimation method that uses only rotational measurements from sparse sensors and achieves zero-shot generalization across users by treating pose estimation as an inverse problem.


<details>
  <summary>Details</summary>
Motivation: Existing pose estimation methods using conditional diffusion models generalize poorly across users because location measurements are highly influenced by body size variations.

Method: Formulates pose estimation as an inverse problem using a pre-trained diffusion model conditioned only on rotational measurements, guided by a likelihood term derived from measured locations.

Result: The proposed InPose method can generatively estimate pose sequences that best explain sparse on-body measurements for any user.

Conclusion: InPose enables zero-shot generalization in pose estimation by decoupling body size dependencies through rotational-only conditioning and location-based likelihood guidance.

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation - a transformer-driven diffusion framework that improves brain tumor segmentation accuracy by combining global contextual reasoning with iterative denoising.


<details>
  <summary>Details</summary>
Motivation: Convolutional architectures like U-Net have limited capacity to capture long-range dependencies, constraining performance on complex brain tumor structures. Recent diffusion models show potential for generating high-fidelity medical images and refining segmentation boundaries.

Method: Embed a vision transformer at the core of the diffusion process to leverage global contextual reasoning with iterative denoising. The transformer backbone models spatial relationships across entire MRI volumes, while diffusion refinement mitigates voxel-level errors and recovers fine-grained tumor details.

Result: Experimental validation on MRI brain tumor datasets demonstrates consistent gains in Dice similarity and Hausdorff distance metrics.

Conclusion: The hybrid transformer-diffusion design provides improved robustness and scalability in neuro-oncology, advancing beyond conventional U-Net baselines and showing potential to advance the state of the art in tumor segmentation.

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: A deep learning pipeline using dual-pass U-Net to extract urban footprints from historical French maps (1925-1950), creating the first national-scale urban dataset for this period with 73% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of nationwide digital urban footprint data for historical urban sprawl analysis in France before the 1970s, using the Scan Histo map series as a valuable but complex source.

Method: Scalable deep learning pipeline with dual-pass U-Net approach: first pass identifies confusing areas for targeted data augmentation, second pass uses refined dataset and binarized output to reduce radiometric noise. Processed 941 high-resolution tiles on HPC cluster.

Result: Created first open-access, national-scale urban footprint dataset for 1925-1950 France with 73% overall accuracy, effectively capturing diverse urban patterns while minimizing artifacts like labels and contour lines.

Conclusion: Successfully bridged the historical data gap by developing an effective method for extracting urban areas from complex historical maps, providing valuable resources for long-term urbanization research through open release of code, datasets, and nationwide urban raster.

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: Point-based tracking in surgical videos is competitive for tools but underperforms for anatomical targets due to tissue similarity and ambiguous boundaries.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze the failure modes of point-based tracking in laparoscopic cholecystectomy videos and understand its reliability in complex surgical environments.

Method: Compared point-based tracking with segmentation mask initialization for three surgical targets (gallbladder, grasper, L-hook electrocautery) in laparoscopic cholecystectomy videos.

Result: Point-based tracking performs well for surgical tools but consistently underperforms for anatomical targets due to tissue similarity and ambiguous boundaries.

Conclusion: The study provides actionable recommendations for selecting and placing tracking points to improve performance in surgical video analysis, highlighting the limitations of point-based tracking for anatomical structures.

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: FRIEREN is a federated learning framework that addresses domain shifts in semantic segmentation using vision foundation models and unlabeled client data, without re-accessing source data.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with domain shifts when client data is unlabeled, and existing methods either require labeled client data or don't leverage modern vision foundation models effectively.

Method: Uses vision-language decoder guided by CLIP text embeddings for semantic disambiguation, and weak-to-strong consistency learning for robust training on pseudo-labels from unlabeled client data.

Result: Achieves competitive performance on synthetic-to-real and clear-to-adverse-weather benchmarks against established domain generalization and adaptation methods.

Conclusion: FRIEREN effectively tackles the challenging FFREEDG task and sets a strong baseline for future research in federated learning with domain shifts.

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint is a structured prompting framework that uses action-centric knowledge to improve video anomaly detection with frozen vision-language models, achieving state-of-the-art performance without training.


<details>
  <summary>Details</summary>
Motivation: Existing prompting methods for video anomaly detection are too abstract and overlook fine-grained human-object interactions and action semantics that define complex anomalies in surveillance videos.

Method: Organizes prompts into semantically coherent groups (violence, property crimes, public safety) and formulates fine-grained guiding questions that align model predictions with discriminative visual cues.

Result: Extensive experiments on UCF-Crime and XD-Violence show consistent AUC improvements over prior baselines, achieving state-of-the-art performance compared to both fine-tuned and training-free methods.

Conclusion: ASK-Hint establishes the critical role of prompt granularity and provides a training-free, generalizable solution for explainable video anomaly detection with interpretable reasoning traces.

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify is a method that uses geometric priors from 3D self-supervised models to purify noisy 2D VLM features for 3D semantic segmentation, achieving state-of-the-art performance with only 1.5% of training data.


<details>
  <summary>Details</summary>
Motivation: Current approaches for transferring 2D VLM features to 3D segmentation face a trade-off: direct projection yields noisy results while geometric coherence requires expensive training and large annotated datasets. The limitation comes from failing to reconcile 2D semantics with 3D geometric structure.

Method: Uses a Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors distilled from a 3D self-supervised teacher model. Includes Geometry-Guided Pooling during inference for denoising and ensuring semantic/structural consistency.

Result: Achieves or surpasses state-of-the-art performance on major 3D benchmarks while utilizing only about 1.5% of training data. Effectively mitigates the trade-off between feature quality and data efficiency.

Conclusion: GeoPurify demonstrates that latent geometric information in noisy 2D-to-3D features can be effectively exploited through geometric priors, enabling superior 3D semantic segmentation with minimal training data requirements.

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: A noninvasive pig identification system using auricular vein patterns achieves 98.12% accuracy with SVM classification, providing a cost-effective alternative to physical identifiers for small-scale farmers.


<details>
  <summary>Details</summary>
Motivation: Traditional pig identification methods (ear tags, microchips) are unreliable, costly, target pure breeds, and impractical for small-scale farmers, creating a need for better solutions.

Method: Collected 800 ear images from 20 mixed-breed pigs using smartphone and back lighting, developed multistage computer vision pipeline for vein enhancement and feature extraction, used machine learning models for classification.

Result: Support Vector Machines achieved 98.12% precision in pig identification across mixed-breed populations, with entire process taking average 8.3 seconds for real-time deployment.

Conclusion: Auricular vein biometrics provide a cost-effective, stress-free alternative to physical identifiers, enabling precision farming benefits for resource-constrained agricultural communities.

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: A multi-category counting framework using Twins pyramid vision-transformer backbone with multi-class counting head and Category Focus Module for suppressing inter-category cross-talk, achieving superior performance on dense scene counting benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address object counting in dense and occluded scenes where discrete counting-by-detection methods fail, particularly for multi-category scenarios.

Method: Uses Twins pyramid vision-transformer backbone with multi-class counting head based on multiscale decoding approach, plus a Category Focus Module using segmentation to suppress inter-category cross-talk during training.

Result: Achieved 33%, 43% and 64% reduction in MAE compared to prior multi-category crowd-counting approaches on VisDrone and iSAID benchmarks, and demonstrated superiority over YOLOv11 in dense scenes.

Conclusion: The method's regional loss enables multi-class crowd counting in new domains, demonstrated through biodiversity monitoring applications for conservation and ecological insights.

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl enables fine-grained temporal control in text-to-video generation by using cross-attention maps to align visual concepts with timing signals during inference, without retraining.


<details>
  <summary>Details</summary>
Motivation: Current generative video models lack fine-grained temporal control, preventing users from specifying when specific visual elements should appear in generated sequences.

Method: Uses cross-attention maps from text-to-video diffusion models with a novel optimization approach that aligns temporal shape via correlation, amplifies visibility via energy, and maintains spatial focus via entropy.

Result: Enables precise timing control while maintaining high video quality and diversity, demonstrated in temporal reordering for single/multiple objects, action, and audio-aligned generation.

Conclusion: TempoControl provides effective temporal alignment of visual concepts during video generation without requiring retraining or additional supervision.

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: RewardMap is a multi-stage RL framework that addresses sparse rewards in fine-grained visual reasoning tasks by introducing difficulty-aware rewards and progressive training from perception to reasoning, achieving consistent performance gains across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Fine-grained visual reasoning remains challenging for MLLMs, with sparse rewards and unstable optimization impeding standard RL approaches on tasks like transit map reasoning.

Method: Constructed ReasonMap-Plus dataset with dense VQA rewards, then developed RewardMap with difficulty-aware reward design and multi-stage RL scheme that progresses from simple perception to complex reasoning tasks.

Result: Each component of RewardMap contributes to performance gains, with combined approach achieving best results - average 3.47% improvement across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks.

Conclusion: RewardMap effectively enhances MLLMs' visual understanding and reasoning capabilities through its multi-stage RL framework and dense reward signals, outperforming conventional supervised fine-tuning approaches.

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow is a novel drag-based image editing framework that leverages FLUX's strong generative priors through region-based editing, overcoming limitations of point-based approaches in DiT architectures.


<details>
  <summary>Details</summary>
Motivation: Existing drag-based editing suffers from distortions due to insufficient priors from earlier models like Stable Diffusion. While newer models like FLUX have stronger priors, they haven't been effectively utilized for drag editing.

Method: Proposes region-based editing with affine transformations for consistent feature supervision, integrates IP-Adapter for subject consistency, uses gradient masks for background preservation, and employs MLLMs for task disambiguation.

Result: DragFlow significantly outperforms both point-based and region-based baselines on DragBench-DR and the novel ReD Bench benchmark, achieving state-of-the-art performance in drag-based image editing.

Conclusion: The work demonstrates that region-based editing with FLUX's strong priors enables high-quality drag-based editing, overcoming limitations of previous approaches and setting new standards in the field.

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: F2C proposes using key clips instead of key frames for video understanding, with adaptive resolution to maintain fixed token count, achieving significant improvements on long-form video benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current Video LLMs face the 'needle in a haystack' problem where massive visual tokens from raw video frames exhaust context windows, and existing frame-wise selection methods discard essential temporal dynamics.

Method: Extends selection from isolated key frames to key clips (short temporally coherent segments) with adaptive resolution strategy that dynamically balances spatial resolution and clip length to maintain constant token count.

Result: Outperforms uniform sampling by 8.1% on Video-MME, 5.6% on LongVideoBench, and 10.3% on MLVU benchmarks. Training-free approach demonstrates importance of temporal coherence preservation.

Conclusion: Preserving temporal coherence through key clips provides practical pathway for scaling Video LLMs to real-world video understanding applications, with significant performance gains over frame-based methods.

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: This study compares video-based 3D human pose estimation models with IMU sensors for movement assessment, finding MotionAGFormer performs best among video methods, with both technologies showing viability for out-of-lab kinematic analysis but with different trade-offs.


<details>
  <summary>Details</summary>
Motivation: To enable accurate human movement assessment outside specialized labs for telemedicine, sports science, and rehabilitation by comparing the performance of video-based pose estimation versus IMU sensors.

Method: Used the VIDIMU dataset with 13 daily activities captured by commodity cameras and 5 IMUs. Evaluated joint angles from deep learning frameworks (MotionAGFormer, MotionBERT, MMPose, NVIDIA BodyTrack) against IMU data processed through OpenSim inverse kinematics, following Human3.6M format with 17 keypoints.

Result: MotionAGFormer achieved best performance with lowest RMSE (9.27°±4.80°), lowest MAE (7.86°±4.18°), highest Pearson correlation (0.86±0.15), and highest R² (0.67±0.28). Both video and IMU approaches proved viable for out-of-lab assessment.

Conclusion: Video models provide clinically promising kinematics in healthy adults, with trade-offs between video- and sensor-based approaches in cost, accessibility, and precision. Study provides guidelines for developing telehealth solutions.

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift is a diffusion-based model that integrates AutoKL and CLIP adapters for cross-subject visual stimulus reconstruction from fMRI data, achieving state-of-the-art performance with minimal training time.


<details>
  <summary>Details</summary>
Motivation: To address challenges in cross-subject visual stimulus reconstruction from fMRI data, including inter-subject variability and the brain's abstract encoding of semantic features in complex visual inputs.

Method: Integrates complementary adapters via diffusion: AutoKL for low-level features and CLIP for semantics. Uses CLIP Adapter trained on Stable Diffusion generated images with COCO captions. Employs pretraining on one subject followed by fine-tuning only 17% of parameters (fully connected layers) for new subjects.

Result: Achieves state-of-the-art performance with only one hour of training per subject on lightweight GPUs (three RTX 4090), outperforming existing methods.

Conclusion: NeuroSwift enables efficient and accurate cross-subject visual stimulus reconstruction from fMRI data with minimal computational requirements and training time.

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP is a self-training framework that enhances CLIP's fine-grained classification by fusing saliency-guided local features with global features and using LLM-derived classifiers for stable adaptation.


<details>
  <summary>Details</summary>
Motivation: CLIP's reliance on coarse global features limits performance on fine-grained classification tasks, and existing methods that align LLM descriptions with CLIP's [CLS] token lack spatial precision.

Method: Proposes Saliency-Oriented Attention Pooling (SOAP) to create saliency-guided [FG] tokens, fuses them with global [CLS] tokens, and uses a two-headed LLM-derived classifier with Dynamic Knowledge Aggregation for stable pseudo-label refinement.

Result: Achieves a consistent 2.90% average accuracy gain across 13 fine-grained benchmarks with only light adaptation.

Conclusion: microCLIP effectively uncovers latent fine-grained signals in CLIP, improving performance on fine-grained classification tasks through joint refinement of visual and textual representations.

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1 is a video authenticity detector that fine-tunes a multi-modal large language model using group relative policy optimization to provide accurate AI-generated video detection with interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Address the urgent need for effective AI-generated video detection tools to mitigate societal risks like misinformation and reputational harm, while ensuring transparency through interpretable explanations for regulators and end users.

Method: Fine-tunes Qwen-VL MLLM using GRPO with two specialized reward models targeting temporal artifacts and generation complexity, trained on a challenging dataset of 140k real and AI-generated videos.

Result: Achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Produces precise and interpretable rationales for predictions.

Conclusion: VidGuard-R1 successfully provides both highly accurate video authenticity judgments and insightful reasoning, making it a valuable tool for combating AI-generated video risks with transparency.

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: Proposes a method to mitigate quality degradation in long-horizon video generation by using teacher models to guide student models through self-generated long videos, achieving up to 20x length scaling without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for video generation face computational costs with transformers and quality degradation when extending to long videos due to error compounding in autoregressive approaches.

Method: Uses teacher model knowledge to guide student models through sampled segments from self-generated long videos, maintaining temporal consistency without recomputing overlapping frames.

Result: Generates videos up to 4 minutes 15 seconds (99.9% of base model's maximum span), 50x longer than baseline, with substantial improvements in fidelity and consistency on benchmarks.

Conclusion: The approach effectively mitigates quality degradation in long-horizon video generation without requiring long-video supervision or retraining, achieving significant length scaling while maintaining quality.

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask is a physics-guided video generation method that enables realistic rigid body control and interactions using a two-stage training strategy with object masks.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with physically plausible object interactions and lack physics-grounded control mechanisms, limiting their use as world simulators for robotics and embodied decision making.

Method: Two-stage training strategy that gradually removes future motion supervision via object masks, training video diffusion models on synthetic scenes and integrating low-level motion control with high-level textual conditioning.

Result: Significant improvements in object interactions in real scenes, strong improvements over recent models of comparable size, and effective synthesis of complex dynamical phenomena.

Conclusion: KineMask successfully addresses limitations in physical plausibility of object interactions and provides physics-grounded control for video generation, with complementary roles of low- and high-level conditioning demonstrated through ablation studies.

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: The paper introduces fine-grained multimodal actions for household robots, incorporating proprioception, kinesthesia, force haptics, and muscle activation to enable precise real-time control that current video models lack.


<details>
  <summary>Details</summary>
Motivation: Current video models fail as world models because they lack fine-grained control needed for delicate household tasks and urgent situations requiring real-time motor control.

Method: Developed a feature learning paradigm that aligns multimodal senses (proprioception, kinesthesia, force haptics, muscle activation) while preserving unique information from each modality, with a regularization scheme to enhance causality of action trajectory features.

Result: Experiments show incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Ablation studies and downstream applications demonstrate effectiveness and practicality.

Conclusion: The proposed multimodal approach enables fine-grained interactions that are difficult to simulate with text-conditioned generative models, providing better simulation of intricate interaction dynamics for household robots.

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA adapts Native Sparse Attention to video-language models, enabling reliable scaling to 128K tokens and improved performance on long-video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Video understanding in multimodal models is limited by context length constraints, causing models to miss key transition frames and struggle with coherence across long time scales.

Method: Adapt Native Sparse Attention (NSA) to video-language models through end-to-end training on a 216K video instruction dataset, using hardware-aware hybrid attention (dense for text, sparse for video).

Result: Achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks compared to token-compression and training-free sparse baselines.

Conclusion: VideoNSA enables reliable scaling to 128K tokens with optimal global-local attention allocation, task-dependent branch usage patterns, and learnable combined sparse attention inducing dynamic attention sinks.

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift is a training-free method that recalibrates noise levels in diffusion models based on resolution size to improve low-resolution image generation quality without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models trained on fixed resolutions fail to generalize to lower resolutions, preventing budget-efficient alternatives for users who don't need high-resolution images.

Method: Identifies that noise schedulers have unequal perceptual effects across resolutions, then recalibrates noise levels of the denoiser conditioned on resolution size without changing model architecture or sampling schedule.

Result: Significantly improved quality at low resolutions: SD3.5 improved by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on LAION-COCO; SD3.5 improved by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on CelebA.

Conclusion: NoiseShift effectively mitigates resolution-dependent artifacts and enhances low-resolution image generation quality, being compatible with existing models without requiring retraining.

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: This paper studies predicting dynamic physical properties (elasticity, viscosity, friction) from videos using temporal information, introducing new datasets and comparing oracle methods, video foundation models, and MLLMs.


<details>
  <summary>Details</summary>
Motivation: To enable prediction of dynamic physical properties that require temporal information from video data, addressing the challenge of inferring properties like elasticity, viscosity, and friction through visual observation.

Method: Collected new video datasets for each physical property with synthetic training/testing splits and real-world evaluation. Explored three approaches: oracle methods using classical computer vision, visual prompts with pre-trained video models, and prompt strategies for Multi-modal Large Language Models (MLLMs).

Result: Video foundation models trained generatively or self-supervised achieved similar performance (though behind oracle methods), while MLLMs were currently inferior but could be improved through better prompting strategies.

Conclusion: Video foundation models show promise for predicting dynamic physical properties, with MLLMs having potential for improvement through optimized prompting, though classical computer vision methods currently provide the best performance.

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: The paper introduces sounding object detection - a task to identify objects involved in sound-producing interactions. A multimodal framework uses egocentric videos with automatic object segmentation masks and slot attention to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To develop models that can link everyday sounds to the specific objects involved in producing them, similar to human perception of object-sound relationships.

Method: Multimodal object-aware framework using in-the-wild egocentric videos. Automatic pipeline computes object segmentation masks to guide model focus. Slot attention visual encoder enforces object prior.

Result: Achieves state-of-the-art performance on the new sounding object detection task and existing multimodal action understanding tasks.

Conclusion: The proposed framework successfully links sounds to objects involved in interactions, demonstrating improved object-centric multimodal understanding.

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: The paper analyzes 3D Gaussian Splatting (3DGS) vulnerabilities to poisoning attacks and proposes a novel density-guided attack method that injects Gaussian points into low-density regions using KDE, creating viewpoint-dependent illusory objects while maintaining stealth.


<details>
  <summary>Details</summary>
Motivation: As 3D scene representation methods like NeRF and 3DGS become widely used, understanding and addressing their security vulnerabilities becomes critical. The paper aims to analyze 3DGS robustness against poisoning attacks.

Method: Proposes a density-guided poisoning method using Kernel Density Estimation (KDE) to identify low-density regions for strategic Gaussian point injection. Also introduces an adaptive noise strategy to disrupt multi-view consistency and a KDE-based evaluation protocol for systematic assessment.

Result: Extensive experiments demonstrate superior performance compared to state-of-the-art techniques, with the method effectively embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views.

Conclusion: The proposed density-guided poisoning attack successfully exploits 3DGS vulnerabilities, and the KDE-based evaluation protocol provides an objective benchmarking framework for future research on 3D scene representation security.

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a theoretical framework and algorithms for improving multi-subject fidelity in text-to-image models by addressing attribute leakage, identity entanglement, and subject omissions through stochastic optimal control applied to flow matching.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models perform well on single-entity prompts but struggle with multi-subject descriptions, showing issues like attribute leakage between subjects, identity entanglement, and subject omissions.

Method: The approach views flow matching through stochastic optimal control, formulating subject disentanglement as control over trained flow matching samplers. It provides two architecture-agnostic algorithms: a training-free test-time controller that perturbs base velocity, and Adjoint Matching for lightweight fine-tuning that regresses a control network to backward adjoint signals.

Result: Both algorithms consistently improve multi-subject alignment while maintaining base-model style across Stable Diffusion 3.5, FLUX, and Stable Diffusion XL. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers generalize to unseen prompts. FOCUS achieves state-of-the-art multi-subject fidelity.

Conclusion: The proposed framework provides the first principled, optimizable objective for multi-subject fidelity in text-to-image generation, unifying prior attention heuristics and extending to diffusion models through flow-diffusion correspondence.

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>
