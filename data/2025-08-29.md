<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: CHAIR-DPO method uses CHAIR metric to identify hallucinated vs non-hallucinated answers for Direct Preference Optimization, effectively reducing hallucinations in Multimodal LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the persistent problem of hallucinations in Multimodal Large Language Models (MLLMs) where models generate answers not reflected in visual inputs, treating it as an alignment problem.

Method: Leverage CHAIR metric to distinguish hallucinated (loser) and non-hallucinated (winner) answer pairs, then fine-tune off-the-shelf MLLMs using Direct Preference Optimization (DPO) with this preference data.

Result: CHAIR-DPO effectively diminishes hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of using CHAIR-based reward for fine-tuning.

Conclusion: The approach provides a simpler alternative to complex synthetic data pipelines, successfully aligning MLLMs to reduce hallucinations without relying on proprietary models.

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: A novel image forgery localization framework that integrates Stable Diffusion 3's multimodal capabilities to detect image manipulations without requiring extensive annotated data, achieving 12% performance improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing image forgery localization methods struggle to keep pace with advanced manipulation technologies like Stable Diffusion, requiring labor-intensive annotated data. The rapid advancement of multimodal large models poses significant challenges to traditional image forensics approaches.

Method: Leverages Stable Diffusion 3's multimodal framework by treating image forgery residuals (high-frequency signals from highpass filters) as an explicit modality. These residuals are fused into SD3's latent space during training while preserving the original semantic features. The approach conditions SD's architecture on forgery-related information to inherently output localization results.

Result: Achieves up to 12% performance improvement on standard benchmarking datasets compared to state-of-the-art methods. Demonstrates strong generalization on real-world document forgery and natural scene forging images, even when such data was completely unseen during training.

Conclusion: The integration of SD3's multimodal capabilities provides an efficient and accurate solution for image forgery localization that doesn't require extensive annotated data, showing excellent generalization to unseen manipulation types and real-world forensic scenarios.

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: Combining MLLMs with quantitative lesion attributes improves interpretability of AI skin cancer diagnosis models by grounding predictions in measurable clinical features.


<details>
  <summary>Details</summary>
Motivation: AI models show promise in skin disease diagnosis but lack interpretability needed for clinical use. MLLMs offer natural language reasoning while quantitative attributes provide measurable grounding for predictions.

Method: Fine-tune Multimodal Large Language Models to predict quantitative lesion attributes (e.g., lesion area) from images and evaluate grounding through attribute-specific content-based image retrieval using SLICE-3D dataset.

Result: Evidence shows MLLM embedding spaces can be successfully grounded in quantitative lesion attributes through fine-tuning.

Conclusion: Combining MLLMs with quantitative attributes provides a promising approach for improving interpretability of AI skin cancer diagnosis models, making them more suitable for clinical practice.

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: A unified Vision Transformer framework for automatic modulation recognition that combines supervised, self-supervised, and reconstruction objectives to achieve robust performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing AMR solutions require large labeled datasets or complex multi-stage training pipelines, limiting scalability and generalization in practical applications.

Method: Uses a ViT encoder with lightweight convolutional decoder and linear classifier; reconstruction branch maps augmented signals back to originals to preserve fine-grained I/Q structure; combines supervised, self-supervised, and reconstruction objectives.

Result: Outperforms supervised CNN and ViT baselines in low-label regimes on RML2018.01A dataset; achieves ResNet-level accuracy with only 15-20% labeled data; maintains strong performance across varying SNR levels.

Conclusion: Provides a simple, generalizable, and label-efficient solution for automatic modulation recognition that addresses the limitations of existing approaches.

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityHuman is a coarse-to-fine framework that generates high-resolution, long-duration audio-driven human videos with stable appearance and natural hand motions, addressing issues of identity drift and hand distortion in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven human animation methods suffer from error accumulation leading to identity drift, color shifts, scene instability, and poor hand modeling with noticeable distortions and audio misalignment.

Method: A coarse-to-fine framework that first generates audio-synchronized representations, then refines them using a pose-guided refiner with stable poses and initial frame as visual anchor, plus a hand-specific reward mechanism trained with high-quality hand motion data.

Result: State-of-the-art performance on EMTD and HDTF datasets in video quality, identity preservation, hand accuracy, and lip-sync, with ablation studies confirming each module's effectiveness.

Conclusion: InfinityHuman successfully addresses key challenges in audio-driven human animation by leveraging pose-guided refinement and hand-specific rewards to produce high-quality, stable, and natural-looking long-duration videos.

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: This paper introduces SalViT360 and SalViT360-AV models for 360-degree audio-visual saliency prediction, along with a new dataset YT360-EyeTracking, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive datasets for 360-degree audio-visual saliency prediction and the need to address spherical distortion and spatial audio integration in omnidirectional videos motivated this research.

Method: Proposed two novel models: SalViT360 (vision-transformer-based with spherical geometry-aware attention) and SalViT360-AV (incorporates transformer adapters conditioned on audio input), using a new dataset of 81 ODVs with varying audio-visual conditions.

Result: Both SalViT360 and SalViT360-AV significantly outperform existing methods on benchmark datasets, demonstrating that integrating spatial audio cues is crucial for accurate saliency prediction in 360-degree videos.

Conclusion: The integration of spatial audio cues in model architecture is essential for effective visual saliency prediction in omnidirectional videos, and the proposed models and dataset advance the field of 360-degree audio-visual attention prediction.

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: Proposes a pipeline using Vision-Language Models to explain vision models at both sample and dataset levels, addressing the gap in understanding general model behavior beyond individual samples.


<details>
  <summary>Details</summary>
Motivation: Current vision model development focuses on performance metrics (accuracy, IoU, mAP) but lacks explainability methods that can capture general model behavior across large datasets, which is crucial for preventing biased judgments and identifying model patterns.

Method: Leverages Vision-Language Models to create an explainability pipeline that works at both sample-level and dataset-level, enabling discovery of failure cases and model insights with minimal effort.

Result: The proposed pipeline integrates vision model development with xAI analysis, providing meaningful explanations of trained models and advancing image analysis capabilities.

Conclusion: The approach bridges the gap between vision model performance optimization and explainability, offering a comprehensive solution to understand model behavior at multiple levels and prevent biased outcomes.

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: ATMS-KD framework combines adaptive temperature scheduling with mixed-sample augmentation to create lightweight CNN models for agricultural applications, achieving 97.11% accuracy with compact models while maintaining low inference latency.


<details>
  <summary>Details</summary>
Motivation: To develop lightweight CNN models suitable for resource-constrained agricultural environments by effectively transferring knowledge from larger teacher models to smaller student models.

Method: Proposes ATMS-KD framework combining adaptive temperature scheduling and mixed-sample augmentation for knowledge distillation from MobileNetV3 Large teacher to lightweight residual CNN students with three configurations (Compact, Standard, Enhanced).

Result: All student models achieved validation accuracies exceeding 96.7% (vs 95-96% with direct training), with compact model reaching 97.11% accuracy - 1.60 percentage points improvement over second-best approach while maintaining lowest inference latency of 72.19ms.

Conclusion: ATMS-KD effectively transfers knowledge regardless of student model capacity, demonstrating superior performance over 11 established knowledge distillation methods and making it suitable for real-world agricultural computer vision applications.

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: A novel framework using hybrid vision-language representations (VLRs) to link microstructure informatics with expert knowledge for zero-shot classification of materials, enabling human-in-the-loop decision making without retraining.


<details>
  <summary>Details</summary>
Motivation: Rapid and reliable qualification of advanced materials from non-conventional additive manufacturing processes remains a bottleneck in industrial manufacturing, particularly for heterogeneous structures.

Method: Integrates deep semantic segmentation with pre-trained multi-modal models (CLIP and FLAVA) to encode visual microstructural data and textual expert assessments into shared representations. Develops customized similarity-based representation with positive/negative references from expert-annotated images and descriptions. Uses net similarity scoring and Z-score normalization for zero-shot classification.

Result: Validation on additively manufactured metal matrix composite dataset shows framework can distinguish acceptable vs defective samples across characterization criteria. FLAVA offers higher visual sensitivity, CLIP provides consistent textual alignment. Z-score normalization enables effective alignment and classification.

Conclusion: The method enhances traceability and interpretability in qualification pipelines, enables human-in-the-loop decision-making without retraining, and advances semantic interoperability between raw data and expert knowledge for scalable domain-adaptable qualification strategies.

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: MedNeXt-L-k5, a Transformer-inspired 3D CNN, was adapted for automated perivascular spaces (PVS) segmentation in MRI, achieving high performance on T2-weighted images comparable to human inter-rater reliability but lower performance on T1-weighted images and across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Manual PVS segmentation is time-consuming with moderate inter-rater reliability, while existing automated deep learning models have moderate performance and poor generalization across diverse MRI datasets.

Method: Adapted MedNeXt-L-k5 (Transformer-inspired 3D encoder-decoder CNN) for PVS segmentation. Trained two models: one on homogeneous T2w MRI from HCP-Aging (200 scans), another on heterogeneous T1w MRI from seven studies across six scanners (40 volumes). Evaluated using 5-fold cross-validation and leave-one-site-out cross-validation.

Result: On T2w images: Dice score 0.88±0.06 (WM), comparable to human inter-rater reliability. On T1w images: Dice score 0.58±0.09 (WM). Under LOSOCV: voxel-level Dice 0.38±0.16 (WM) and 0.35±0.12 (BG); cluster-level Dice 0.61±0.19 (WM) and 0.62±0.21 (BG). Did not outperform nnU-Net.

Conclusion: MedNeXt-L-k5 provides efficient automated PVS segmentation across diverse T1w and T2w MRI datasets, but attention-based mechanisms in transformer-inspired models are not required for high accuracy in PVS segmentation as it did not outperform nnU-Net.

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: A training-free feedback framework that adapts output-based patch correspondences back to intermediate attention to improve CLIP's spatial coherence for open-vocabulary segmentation.


<details>
  <summary>Details</summary>
Motivation: CLIP has strong visual-textual alignment but poor localization for segmentation. Prior methods modify intermediate attention but coherence isn't consistently propagated to final outputs due to subsequent operations, and intermediate attention lacks direct text interaction.

Method: Feedback-driven self-adaptive framework that uses output predictions as spatial coherence prior. Includes attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble modules. Works as plug-in for various backbones and attention types.

Result: Consistently improves performance across eight benchmarks when integrated into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H) and multiple attention types.

Conclusion: The proposed framework effectively enhances semantic consistency between internal representations and final predictions by leveraging output coherence cues, addressing CLIP's localization limitations for open-vocabulary segmentation.

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: A probing framework to analyze layer-wise processing in Multimodal Large Language Models (MLLMs) reveals consistent stage-wise structure across different models, with early layers handling visual grounding, middle layers supporting semantic reasoning, and final layers preparing task-specific outputs.


<details>
  <summary>Details</summary>
Motivation: Despite strong performance of MLLMs on vision-language tasks, their internal processing dynamics remain underexplored, particularly how they integrate visual and textual information across different layers.

Method: Trained linear classifiers to predict fine-grained visual categories from token embeddings at each layer using standardized anchor questions. Evaluated under three prompt variations: lexical variants, semantic negation variants, and output format variants. Applied to LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL models.

Result: Identified consistent stage-wise structure: early layers perform visual grounding, middle layers support lexical integration and semantic reasoning, final layers prepare task-specific outputs. Stage structure remains stable across visual tokenization and training variations, but layer allocation shifts with base LLM architecture changes.

Conclusion: Provides unified perspective on MLLM layer organization and offers lightweight, model-agnostic approach for analyzing multimodal representation dynamics, revealing consistent functional stages across different model architectures.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: Proposes SLiCS - a supervised dictionary learning method to disentangle vision-language embedding spaces into concept-specific subspaces for improved concept-filtered image retrieval and conditional generation.


<details>
  <summary>Details</summary>
Motivation: To decompose complex scene embeddings into interpretable concept-specific components by separating semantic information into different subspaces, enabling more precise concept-based operations.

Method: Supervised dictionary learning with sparse non-negative combinations of grouped vectors (atoms), using alternating optimization with convergence guarantees. Leverages text co-embeddings for semantic descriptions and zero-shot classification.

Result: SLiCS enables more precise concept-filtered image retrieval and conditional generation across various embeddings (CLIP, TiTok autoencoder, DINOv2), with improved quantitative and qualitative performance.

Conclusion: The proposed sparse linear concept subspaces successfully disentangle embedding spaces, providing interpretable concept-specific representations that enhance downstream vision-language tasks with better precision and semantic control.

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MedFoundationHub is a GUI toolkit that enables secure deployment of medical vision-language models for clinical applications while addressing privacy and security concerns.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs introduce serious security risks including PHI exposure and data leakage, which are critical concerns in hospital environments that need to be addressed.

Method: Developed a graphical user interface toolkit with Docker-orchestrated deployment that supports plug-and-play integration of Hugging Face models and requires only a local workstation with single GPU.

Result: Evaluated 5 state-of-the-art VLMs with board-certified pathologists, generating 1015 clinician-model scoring events that revealed limitations including off-target answers and inconsistent terminology.

Conclusion: MedFoundationHub provides a secure, accessible solution for deploying medical VLMs while expert evaluation highlights current model limitations that need addressing for clinical adoption.

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: Bidirectional Interaction Mamba (BIM) uses novel scanning mechanisms to enable efficient cross-task interaction for multi-task dense prediction without computational trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing multi-task dense prediction methods face a trade-off between interaction completeness and computational efficiency - sufficient cross-task interaction leads to high complexity.

Method: Proposes BIM with two novel scanning mechanisms: 1) Bidirectional Interaction Scan (BI-Scan) that constructs task-specific representations as bidirectional sequences, 2) Multi-Scale Scan (MS-Scan) for multi-granularity scene modeling. Both operate with linear complexity.

Result: Extensive experiments on NYUD-V2 and PASCAL-Context benchmarks show BIM outperforms state-of-the-art competitors.

Conclusion: BIM successfully addresses the computational efficiency vs interaction completeness trade-off in multi-task dense prediction through innovative bidirectional and multi-scale scanning mechanisms.

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: A novel audio-guided visual editing framework that uses multi-modal prompts without additional training, handling complex scenarios where text-only methods fail.


<details>
  <summary>Details</summary>
Motivation: Textual guidance alone is insufficient for complex visual editing scenarios, and existing audio-guided methods require specific training that limits generalization to real-world situations.

Method: Leverages a pre-trained multi-modal encoder with zero-shot capabilities, integrates diverse audio into visual editing by aligning audio encoder space with diffusion model's prompt encoder space, and uses separate noise branching with adaptive patch selection for multi-modal prompts.

Result: The framework successfully handles diverse complex editing tasks by incorporating rich audio information, outperforming text-only approaches.

Conclusion: The proposed audio-guided visual editing framework effectively addresses complex editing scenarios through multi-modal integration without requiring additional training, demonstrating superior performance over text-only methods.

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: Proposes AEVLP framework with GPR Loss and DAMP technique for single positive multi-label learning, achieving state-of-the-art results by effectively handling noisy pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: Fully annotating large-scale multi-label datasets is costly and impractical. Traditional SPML methods treating missing labels as unknown/negative cause inaccuracies and false negatives, while pseudo-labeling strategies introduce additional noise.

Method: Introduces Generalized Pseudo-Label Robust Loss (GPR Loss) to learn from diverse pseudo-labels while mitigating noise, and Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together they form the AEVLP framework.

Result: Extensive experiments on four benchmark datasets demonstrate significant advancements in multi-label classification, achieving state-of-the-art results.

Conclusion: The proposed AEVLP framework effectively addresses challenges in single positive multi-label learning by combining robust loss functions with dynamic pseudo-labeling techniques, outperforming existing methods.

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: Proposed delay-spike approach and temporal-dependent Integrate-and-Fire (tdIF) neuron for SNNs to improve visual detection tasks with ultra-low latency (5 time-steps), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current ANN-SNN conversion methods perform well in classification tasks but show suboptimal performance in visual detection tasks due to residual membrane potential issues from heterogeneous spiking patterns.

Method: Introduces delay-spike approach to mitigate residual membrane potential and proposes tdIF neuron architecture that enables IF neurons to dynamically adjust accumulation and firing behaviors based on temporal order of time-steps.

Result: Achieves state-of-the-art performance in object detection and lane line detection with ultra-low latency (within 5 time-steps), surpassing current ANN-SNN conversion approaches while maintaining energy consumption equivalent to traditional IF neurons.

Conclusion: The tdIF method enables more precise feature representation with lower time-steps, allowing high performance and ultra-low latency in visual detection tasks while maintaining energy efficiency.

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: Proposes DUP-MCRNet for salient object detection with dynamic uncertainty propagation and multimodal fusion to address detail loss and edge blurring in complex scenes.


<details>
  <summary>Details</summary>
Motivation: Existing SOD methods suffer from losing details, blurring edges, and insufficient fusion of single-modal information in complex scenes, leading to poor performance in challenging conditions.

Method: Uses dynamic uncertainty graph convolution (DUGC) for uncertainty propagation, multimodal collaborative fusion (MCF) with learnable gating weights for RGB/depth/edge features, and multi-scale BCE/IoU loss with cross-scale consistency constraints.

Result: Outperforms various SOD methods on most benchmark datasets, particularly excelling in edge clarity and robustness to complex backgrounds.

Conclusion: DUP-MCRNet effectively addresses detail preservation and edge clarity issues through uncertainty propagation and adaptive multimodal fusion, demonstrating superior performance in complex scenarios.

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: MSMVD improves multi-view pedestrian detection by generating multi-scale BEV features from multi-scale image features and using feature pyramid networks to handle varying pedestrian scales across different views.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end MVPD methods struggle with detecting pedestrians that have consistently small/large scales within views or vastly different scales between views, as they don't properly exploit multi-scale image features.

Method: Proposes MSMVD which generates multi-scale BEV features by projecting multi-scale image features into BEV space scale-by-scale, then processes them using a feature pyramid network to combine information across different scales and views.

Result: Extensive experiments show MSMVD outperforms previous methods by 4.5 MODA points on GMVD dataset, demonstrating significant improvement in detection performance.

Conclusion: Exploiting multi-scale image features through multi-scale BEV features greatly enhances pedestrian detection performance in multi-view scenarios, particularly for challenging scale variations.

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: Lightweight real-time deepfake detection network that combines spatial and frequency features with efficient multi-scale fusion for practical deployment.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detectors achieve high accuracy but are computationally expensive, hindering real-time deployment in applications like video conferencing and social media.

Method: Proposed SFMFNet with spatial-frequency hybrid aware module using gated mechanism, token-selective cross attention for multi-level feature interaction, and residual-enhanced blur pooling for downsampling.

Result: Achieves favorable balance between accuracy and efficiency on benchmark datasets with strong generalization capabilities.

Conclusion: SFMFNet provides a practical solution for real-time deepfake detection with good performance and computational efficiency.

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: A lightweight medical image classification method using dual-model weight selection and self-knowledge distillation to achieve comparable performance to large models with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Address computational constraints in real-world medical settings where deploying large-scale models is impractical, requiring lightweight alternatives that maintain performance.

Method: Uses dual-model weight selection from large pretrained models, applies self-knowledge distillation for knowledge transfer, and fine-tunes for target classification tasks without excessive computational overhead.

Result: Superior performance and robustness demonstrated on chest X-ray, lung CT scans, and brain MRI datasets compared to existing methods.

Conclusion: The combined approach effectively overcomes limitations of conventional methods by retaining critical information in compact models while maintaining computational efficiency.

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: A novel LiDAR point cloud compression method that uses geometry re-densification and cross-scale feature propagation to achieve state-of-the-art compression ratios with real-time performance (26 FPS).


<details>
  <summary>Details</summary>
Motivation: High-precision LiDAR scans incur substantial storage and transmission overhead, and existing methods struggle with efficient context modeling due to extreme sparsity of geometric details, limiting compression performance and speed.

Method: Two lightweight modules: 1) Geometry Re-Densification Module that re-densifies sparse geometry, extracts features at denser scale, then re-sparsifies for predictive coding; 2) Cross-scale Feature Propagation Module that leverages multi-resolution occupancy cues to guide hierarchical feature propagation across scales.

Result: Achieves state-of-the-art compression ratios on KITTI dataset with real-time performance (26 FPS for both encoding and decoding at 12-bit quantization).

Conclusion: The proposed framework generates compact feature representations that enable efficient context modeling and accelerated coding process, addressing the challenges of LiDAR point cloud compression while maintaining high performance.

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: Droplet3D leverages video data to overcome 3D data scarcity by using video commonsense priors for spatial consistency and semantic fidelity in 3D generation, introducing a large-scale annotated video dataset and generative model.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in 3D domain by utilizing abundant video data that contains spatial consistency priors and rich semantic information, overcoming limitations of limited native 3D data availability.

Method: Introduce Droplet3D-4M dataset with multi-view annotations and train Droplet3D generative model supporting both image and dense text input, leveraging video modality for 3D asset generation.

Result: Produces spatially consistent and semantically plausible 3D content, demonstrates potential for scene-level applications, and shows video commonsense priors significantly facilitate 3D creation.

Conclusion: Video modality provides effective supervisory signal for 3D generation, overcoming data scarcity limitations and enabling high-quality, semantically faithful 3D asset creation with potential for broader scene-level applications.

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor is a framework for photorealistic object editing in driving videos using 3D Gaussian representation and hierarchical features for precise pose control and visual quality.


<details>
  <summary>Details</summary>
Motivation: Collecting real-world corner cases for autonomous driving validation is costly and hazardous. Existing editing methods suffer from limited visual fidelity or imprecise pose control.

Method: Leverages 3D Gaussian representation as dense prior in denoising process, uses scene-level 3D bounding box layout for occlusion handling, and incorporates hierarchical fine-grained features for appearance guidance.

Result: Outperforms existing methods in pose controllability and visual quality on Waymo Open Dataset, effectively supporting object repositioning, insertion, and deletion.

Conclusion: G^2Editor provides a unified framework for realistic object editing in driving scenarios that benefits downstream data-driven autonomous driving tasks.

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: A pathology-informed domain randomization strategy that simulates CCD brain alterations from healthy data alone, enabling robust fetal brain segmentation without requiring pathological annotations.


<details>
  <summary>Details</summary>
Motivation: Accurate fetal brain segmentation is crucial for assessing neurodevelopment in conditions like corpus callosum dysgenesis (CCD), but the rarity of CCD severely limits annotated data, hindering deep learning model generalization.

Method: Propose a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline, simulating diverse brain alterations from healthy data alone.

Result: Achieved substantial improvements on CCD cases while maintaining performance on healthy fetuses and other pathologies. Reduced LCC estimation error from 1.89mm to 0.80mm in healthy cases and from 10.9mm to 0.7mm in CCD cases. Produced segmentations with improved topological consistency.

Conclusion: Incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations like CCD.

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: First unified framework that integrates sign language, lip movements, and audio for spoken-language text generation, achieving state-of-the-art performance across multiple communication modalities.


<details>
  <summary>Details</summary>
Motivation: Traditional ASR systems are inaccessible to deaf/hard-of-hearing individuals, and existing visual alternatives (sign language, lip reading) have been studied in isolation without unified integration.

Method: Designed a unified modality-agnostic architecture capable of processing heterogeneous inputs (sign language, lip movements, audio), with explicit modeling of lip movements as separate modality.

Result: Achieved performance on par with or superior to state-of-the-art specialized models across SLT, VSR, ASR, and AVSR tasks. Explicit lip movement modeling significantly improved SLT performance.

Conclusion: The unified framework successfully integrates multiple communication modalities, demonstrating strong synergy between modalities and proving that lip movements serve as important non-manual cues in sign language comprehension.

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: Video-MTR is a reinforced multi-turn reasoning framework that iteratively selects key video segments and comprehends questions, outperforming existing methods in long-form video understanding.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding faces challenges with long-range temporal dependencies and multiple events. Existing methods rely on static reasoning or external VLMs, leading to complexity and sub-optimal performance due to lack of end-to-end training.

Method: Proposes a reinforced multi-turn reasoning framework that performs iterative key video segment selection and question comprehension. Uses a novel gated bi-level reward system combining trajectory-level rewards (answer correctness) and turn-level rewards (frame-query relevance) for end-to-end training.

Result: Extensive experiments on VideoMME, MLVU, and EgoSchema benchmarks show Video-MTR outperforms existing methods in both accuracy and efficiency.

Conclusion: Video-MTR advances state-of-the-art in long video understanding by enabling iterative reasoning and eliminating the need for external VLMs through end-to-end training with a novel reward system.

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: DUO is a test-time adaptation framework that addresses both semantic and geometric uncertainties in monocular 3D object detection through dual uncertainty optimization and semantic-aware geometric constraints.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection reliability deteriorates under real-world domain shifts, and existing TTA methods fail to address the dual uncertainty (semantic and geometric) inherent in M3OD tasks.

Method: Proposes Dual Uncertainty Optimization (DUO) framework with convex optimization of focal loss, unsupervised uncertainty weighting, and semantic-aware normal field constraint to preserve geometric coherence in semantically clear regions.

Result: Extensive experiments demonstrate superiority over existing methods across various datasets and domain shift types.

Conclusion: DUO effectively bridges the gap in addressing dual uncertainties for robust monocular 3D object detection under domain shifts through complementary semantic-geometric optimization.

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: CaddieSet is a new golf dataset that links swing posture to ball trajectory using computer vision and expert-defined metrics, enabling interpretable swing analysis and trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning studies haven't quantitatively established the relationship between golf swing posture and ball trajectory, limiting their ability to provide meaningful swing improvement insights.

Method: Created CaddieSet dataset with joint information from swing videos segmented into 8 phases using computer vision, plus ball information. Defined 15 key swing metrics based on expert domain knowledge.

Result: Demonstrated feasibility for predicting ball trajectories using various benchmarks. Showed that interpretable models provide swing feedback quantitatively consistent with established golf domain knowledge.

Conclusion: CaddieSet offers new insights for golf swing analysis in both academia and sports industry by enabling interpretable relationship between swing posture and ball trajectory outcomes.

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: IAENet is a novel ensemble framework that combines 2D and 3D experts for surface anomaly detection, using an Importance-Aware Fusion module to dynamically weight predictions and achieve state-of-the-art performance with lower false positives.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud-based anomaly detection is underexplored despite richer geometric cues, lacking powerful pretrained backbones comparable to 2D methods. The absence of effective fusion strategies for disparate modalities limits performance.

Method: Proposed Importance-Aware Ensemble Network (IAENet) with Importance-Aware Fusion module that dynamically assesses contribution of 2D and 3D experts and reweights anomaly scores. Includes critical loss functions to optimize fusion while preserving unique strengths of each modality.

Result: Extensive experiments on MVTec 3D-AD show IAENet achieves new state-of-the-art performance with markedly lower false positive rate, demonstrating practical value for industrial deployment.

Conclusion: The proposed ensemble framework successfully bridges the gap between 2D and 3D anomaly detection by effectively combining their strengths through dynamic importance-aware fusion, setting a new benchmark for industrial surface anomaly detection.

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit reframes instruction-based image editing as reference-image-based text-to-image generation using a Cross-Attentive UNet to inject reference image features, overcoming dataset limitations and improving editing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address limitations in semantic image editing where inversion-based methods introduce reconstruction errors and instruction-based models suffer from poor dataset quality and scale.

Method: Proposes a framework that uses reference images and prompts as input, adding attention bridges in a Cross-Attentive UNet to inject reference image features into the text-to-image generation process without architectural changes.

Result: Experiments on Emu Edit benchmark show improved editing accuracy and consistency, with seamless integration with ControlNet, IP-Adapter and other extensions.

Conclusion: DescriptiveEdit preserves generative power of text-to-image models while overcoming dataset limitations, making semantic image editing more scalable and effective.

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: DCFS is a novel continual test-time adaptation framework that uses dual-path feature consistency and confidence-aware learning to address error accumulation and pseudo-label noise issues in target domain adaptation without source data.


<details>
  <summary>Details</summary>
Motivation: Current CTTA methods rely on pseudo-labels from model predictions, but these labels suffer from quality issues and error accumulation problems that need to be addressed.

Method: Proposes dual classifiers to disentangle semantic-related and domain-related features, maintains consistency between sub-features and whole features, and uses adaptive thresholds with confidence scores for weighted self-supervised learning.

Result: Extensive experiments on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets demonstrate consistent performance improvements in continual test-time adaptation scenarios.

Conclusion: DCFS effectively reduces pseudo-label noise and alleviates error accumulation by comprehensively capturing data features from multiple perspectives through dual-path feature consistency and confidence-aware learning.

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: Using 3DGS model to fine-tune camera calibration through backpropagation of novel view color loss, achieving 0.4 dB PSNR improvement on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Camera calibration quality significantly impacts novel view synthesis reconstruction, with even 1-pixel errors having substantial effects on output quality. Current methods lack ground truth for real scenes, relying on synthesis quality for assessment.

Method: Proposes using 3D Gaussian Splatting (3DGS) model to fine-tune camera calibration by backpropagating novel view color loss with respect to camera parameters.

Result: Achieves average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS, demonstrating significant calibration enhancement.

Conclusion: While the fine-tuning process may be time-consuming, the method is particularly valuable for reference scene calibration where novel view quality is paramount, such as in Mip-NeRF 360 applications.

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: Proposes an active sequential domain adaptation framework for multi-modal medical image segmentation that dynamically selects the most informative and representative samples to reduce annotation costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of previous active domain adaptation methods that suffer from negative transfer due to one-off sample selection and lack of strategies for multi-modal medical data, while reducing the high annotation costs in medical image segmentation.

Method: Develops an active and sequential domain adaptation framework with a query strategy that prioritizes labeling based on both informativeness and representativeness of samples for multi-modal medical data.

Result: Empirical validation on diverse gross tumor volume segmentation tasks shows the method achieves favorable segmentation performance and significantly outperforms state-of-the-art ADA methods.

Conclusion: The proposed framework effectively reduces annotation costs while maintaining high segmentation performance through dynamic multi-modal sample selection based on informativeness and representativeness.

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: A novel data-level fusion framework for unsupervised 3D object detection that integrates RGB images and LiDAR data early in the process using vision foundation models, bi-directional fusion, and filtering methods to improve pseudo-box quality without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based 3D object detectors require time-consuming manual annotations. Current unsupervised methods simply fuse pseudo-boxes from LiDAR and RGB at label level, overlooking the complementary nature of these data modalities and providing limited improvements.

Method: Proposes data-level fusion using vision foundation models for instance segmentation and depth estimation. Uses bi-directional fusion where real points get category labels from 2D space and 2D pixels enhance 3D point density. Implements local and global filtering to reduce noise from depth and segmentation errors. Includes dynamic self-evolution strategy to iteratively refine pseudo-boxes.

Result: Achieves 28.4% mAP on nuScenes validation benchmark, significantly outperforming previous state-of-the-art unsupervised 3D object detection methods.

Conclusion: The proposed data-level fusion framework effectively leverages complementary information from LiDAR and RGB modalities, demonstrating superior performance in unsupervised 3D object detection without requiring manual annotations.

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: A deep learning method for BMI estimation from smartphone images using a large proprietary dataset (WayBED) with automatic filtering, achieving state-of-the-art results (7.9% MAPE) and robust generalization to unseen data.


<details>
  <summary>Details</summary>
Motivation: To enable rapid weight assessment via camera images when traditional BMI measurement methods are unavailable or impractical, particularly in telehealth and emergency scenarios where existing computer vision approaches have been limited by small datasets.

Method: Developed a deep learning-based BMI estimation method trained on the WayBED dataset (84,963 smartphone images from 25,353 individuals). Introduced automatic filtering using posture clustering and person detection to remove low-quality images, retaining 71,322 high-quality images for training. Deployed the pipeline on Android devices using CLAID framework.

Result: Achieved 7.9% MAPE on hold-out test set (lowest in published literature), 13% MAPE on completely unseen VisualBodyToBMI dataset (comparable to state-of-the-art), and 8.56% MAPE after fine-tuning on VisualBodyToBMI (lowest reported).

Conclusion: The method demonstrates robust generalization and state-of-the-art performance for BMI estimation from images, with complete code released as open-source for model training, filtering, and mobile deployment.

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: Comprehensive study of 7 domain adaptation techniques across 5 natural and 8 medical image datasets, showing DSAN algorithm's superior performance particularly for medical data and dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: To better understand domain adaptation benefits for both natural and medical images, addressing performance bias in mainstream datasets and challenges with medical data.

Method: Conducted 557 simulation studies using 7 widely-used DA techniques for image classification across various scenarios including out-of-distribution, dynamic data streams, and limited training samples.

Result: DSAN achieved 91.2% accuracy on COVID-19 dataset using Resnet50, showed +6.7% improvement in dynamic data streams, and demonstrated remarkable explainability on medical datasets.

Conclusion: DSAN algorithm shows outstanding performance and explainability for medical domain adaptation, providing valuable insights for effective model adaptation to medical data.

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: CLAB method uses contrastive learning with dynamic loss weighting to improve video object detection without increasing computational cost during inference, achieving state-of-the-art performance on ImageNet VID.


<details>
  <summary>Details</summary>
Motivation: Video object detection is challenging due to image deterioration issues like motion blur and occlusion. Existing methods improve performance but at the cost of increased computational demands during inference.

Method: Introduces Contrastive Learning through Auxiliary Branch (CLAB) with contrastive loss to enhance feature representation, plus dynamic loss weighting that prioritizes auxiliary learning early and detection later in training.

Result: Achieves 84.0% mAP with ResNet-101 and 85.2% mAP with ResNeXt-101 on ImageNet VID dataset, setting state-of-the-art for CNN-based models without additional post-processing.

Conclusion: CLAB effectively improves video object detection robustness to image degradation without adding computational overhead during inference, demonstrating consistent performance gains through simple yet effective contrastive learning approach.

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: Analysis of CLIP vision encoders under typographic attacks reveals specialized attention heads that transmit text information. A training-free defense method selectively ablates these heads, improving robustness by up to 19.6% on typographic ImageNet-100 with minimal standard accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Typographic attacks exploit multi-modal systems by injecting text into images, causing targeted misclassifications, malicious content generation, and VLM jailbreaks. Understanding and defending against these attacks is crucial for safety-critical applications.

Method: Analyze CLIP vision encoder behavior under typographic attacks to locate specialized attention heads. Introduce a training-free defense by selectively ablating the typographic circuit (specific attention heads) without requiring finetuning.

Result: Method improves performance by up to 19.6% on typographic ImageNet-100 variant while reducing standard ImageNet-100 accuracy by less than 1%. Remains competitive with state-of-the-art finetuning-based defenses. Releases family of dyslexic CLIP models as drop-in replacements.

Conclusion: The training-free ablation approach effectively defends against typographic attacks while maintaining standard performance. Dyslexic CLIP models provide robust drop-in solutions for safety-critical applications where text manipulation risks outweigh text recognition utility.

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: GLaRE: Graph-based Landmark Region Embedding network for facial expression recognition using hierarchical coarsening of facial landmarks to achieve state-of-the-art performance on AffectNet and FERG datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional FER systems face challenges with occlusion, expression variability, and lack of interpretability. GNNs offer structured and interpretable learning by modeling relational dependencies between facial landmarks.

Method: Extract facial landmarks using 3D facial alignment, construct quotient graph via hierarchical coarsening to preserve spatial structure while reducing complexity, and use region-level embeddings for emotion recognition.

Result: Achieves 64.89% accuracy on AffectNet and 94.24% on FERG, outperforming existing baselines. Ablation studies confirm region-level embeddings from quotient graphs improve prediction performance.

Conclusion: GLaRE demonstrates that graph-based approaches with hierarchical coarsening and region-level embeddings effectively address FER challenges, providing both performance improvements and interpretability.

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit is a high-speed multi-reference virtual try-on framework using cacheable diffusion architecture that achieves 3.5x speedup while maintaining quality, with a new dataset DressCode-MR for complex outfit compositions.


<details>
  <summary>Details</summary>
Motivation: Current virtual try-on methods cannot support multi-reference outfit compositions (garments and accessories) and suffer from significant inefficiency due to redundant re-computation of reference features in each denoising step.

Method: Proposes FastFit with Semi-Attention mechanism and class embeddings instead of timestep embeddings for reference items, fully decoupling reference feature encoding from denoising process. Also introduces DressCode-MR dataset with 28,179 sets of high-quality paired images across five categories.

Result: Achieves average 3.5x speedup over comparable methods with negligible parameter overhead. Surpasses state-of-the-art methods on key fidelity metrics across VITON-HD, DressCode, and DressCode-MR datasets.

Conclusion: FastFit breaks the efficiency bottleneck in virtual try-on by enabling reference features to be computed once and reused across all steps, while supporting complex multi-reference compositions with superior performance.

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: UTA-Sign: Unsupervised thermal-event fusion for traffic signage in low-light conditions, combining thermal cameras' low-light perception with event cameras' motion sensitivity to overcome signage blind spots.


<details>
  <summary>Details</summary>
Motivation: Thermal cameras struggle with signage detection on similar-material objects, while event cameras excel in low-light high-speed environments but have non-uniform sampling. Their complementary characteristics can address autonomous driving safety concerns in nighttime conditions.

Method: Dual-boosting mechanism that fuses thermal frames and event signals. Thermal frames provide accurate motion cues as temporal references for aligning uneven event signals, while event signals contribute subtle signage content to enhance thermal frames.

Result: Validated on real-world datasets, demonstrating superior quality in traffic signage sketching and improved detection accuracy at the perceptual level.

Conclusion: The proposed UTA-Sign method effectively addresses signage blind spots in thermal imaging and non-uniform sampling in event cameras, providing consistent signage representation for safer autonomous driving in low-illumination environments.

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: Active defense method using low-frequency perceptual perturbations to disrupt face-swapping deepfakes by targeting the generative process while preserving visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods are passive and focus on post-event analysis rather than preventing attacks, creating a need for active defense mechanisms.

Method: Combines frequency and spatial domain features using discrete wavelet transform (DWT) to extract low-frequency components. Features encoder, perturbation generator, and decoder architecture to introduce artifacts that disrupt facial manipulation while preserving high-frequency details.

Result: Experiments on CelebA-HQ and LFW datasets show significant reductions in face-swapping effectiveness, improved defense success rates, and maintained visual quality.

Conclusion: The proposed active defense method effectively disrupts deepfake generation processes while ensuring output remains visually plausible, offering a proactive approach to deepfake prevention.

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: Diverse-T2M introduces uncertainty modeling and stochastic sampling to generate diverse 3D human motions from text while maintaining text-motion consistency.


<details>
  <summary>Details</summary>
Motivation: Achieving diversity in generated 3D human motions from text remains challenging despite recent advancements in precise motion generation. Current methods struggle to produce varied motions while preserving semantic consistency with the input text.

Method: Introduces uncertainty into transformer-based generation by using noise signals as diversity carriers. Projects text into continuous latent representations instead of rigid one-to-one mappings, and integrates a latent space sampler for stochastic sampling during generation.

Result: Demonstrates significant diversity enhancement while maintaining state-of-the-art text consistency performance on benchmark datasets (HumanML3D and KIT-ML).

Conclusion: The proposed method successfully addresses the diversity challenge in text-to-motion generation by explicitly modeling uncertainty and introducing stochastic sampling, achieving both high diversity and text consistency.

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: Proposes an optimization-based calibration method using 3D-printed phantom for accurate 3D IVUS volume reconstruction to bridge preoperative CT and intraoperative ultrasound in liver surgery.


<details>
  <summary>Details</summary>
Motivation: Intraoperative ultrasound images are challenging to interpret due to limited field of view and complex anatomy. Bridging preoperative CT and intraoperative data is crucial for effective surgical guidance in liver procedures.

Method: Optimization-based calibration method using a 3D-printed phantom for accurate 3D Intravascular Ultrasound volume reconstruction. Validated with in vivo swine liver images.

Result: Achieved calibration error from 0.88 to 1.80 mm and registration error from 3.40 to 5.71 mm between 3D IVUS data and corresponding CT scans.

Conclusion: Provides reliable and accurate calibration and volume reconstruction method that can register intraoperative ultrasound with preoperative CT images to enhance intraoperative guidance in liver surgery.

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: Proposes PI-GenMFI, a physics-informed diffusion model to generate synthetic Magnetic Field Images (MFI) for semiconductor defect detection, addressing data scarcity issues by incorporating physical constraints to create training data for ML-based defect localization.


<details>
  <summary>Details</summary>
Motivation: Limited availability of MFI datasets due to proprietary concerns creates a bottleneck for training machine learning models in semiconductor defect detection, while X-ray scanning is memory-intensive and time-consuming for large-scale applications.

Method: Developed Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) using diffusion models with two physical constraints to generate synthetic MFI samples, specifically for power short defects. Compared against state-of-the-art VAE and diffusion models.

Result: Promising results from qualitative and quantitative evaluations using various image generation and signal processing metrics, along with domain expert assessment of generated samples.

Conclusion: The proposed physics-informed generative approach shows potential for optimizing defect localization processes by generating synthetic MFI training data, overcoming data scarcity challenges in semiconductor manufacturing quality control.

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: A novel GAN-based data reconstruction attack framework with progressive feature optimization that significantly outperforms existing attacks on deep neural networks, especially in high-resolution and out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing data reconstruction attacks (DRAs) in split inference are limited to shallow models and fail to leverage semantic priors effectively, leaving privacy vulnerabilities in deeper DNNs unaddressed.

Method: Proposes a GAN-based framework with Progressive Feature Optimization (PFO) that decomposes the generator into hierarchical blocks and incrementally refines intermediate representations, using L1-ball constraints to stabilize optimization and improve realism.

Result: Extensive experiments show the method outperforms prior attacks by a large margin, particularly in high-resolution scenarios, out-of-distribution settings, and against deeper, more complex DNNs.

Conclusion: The proposed framework demonstrates significantly improved reconstruction quality and generalizability across datasets and model architectures, revealing greater privacy risks in split inference than previously recognized.

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST is a diffusion-based framework for text-driven emotional talking head synthesis that addresses limitations in control flexibility, motion naturalness, and expression quality through novel appearance modeling and audio-emotion integration modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods for emotional talking head synthesis have limitations in control flexibility, motion naturalness, and expression quality, with most datasets collected in lab settings hindering practical real-world applications.

Method: Proposes EmoCAST with two key modules: 1) text-guided decoupled emotive module for appearance modeling, and 2) emotive audio attention module to capture interplay between emotion and audio. Also constructs emotional dataset with text descriptions and employs emotion-aware sampling and progressive functional training strategies.

Result: Achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos.

Conclusion: EmoCAST successfully addresses the challenges of emotional talking head synthesis through its diffusion-based framework with specialized modules for emotion comprehension and audio-emotion integration, supported by a comprehensive dataset and training strategies.

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: SwinUNETR-based deep learning framework for breast cancer detection from MRI, achieving second place in multi-center challenge with public code release.


<details>
  <summary>Details</summary>
Motivation: Early detection of breast cancer is crucial for improving outcomes, especially in high-risk women or those with dense breast tissue where mammography is less effective. MRI offers high sensitivity but requires AI solutions for improved diagnosis and classification.

Method: Developed a SwinUNETR-based deep learning framework incorporating breast region masking, extensive data augmentation, and ensemble learning. Used multi-center dataset of 511 studies from six European centers with 1.5T and 3T scanners from multiple vendors.

Result: Achieved second place on the ODELIA consortium challenge leaderboard, demonstrating robust performance and generalizability across different scanner vendors and field strengths.

Conclusion: The framework shows strong potential to support clinical breast MRI interpretation and has been made publicly available to foster further research in AI-based breast cancer diagnosis.

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack is a plug-and-play framework that improves 3D Gaussian avatar reconstruction by addressing poor back-head modeling through synthetic back-view generation and adaptive spatial alignment.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian Splatting methods for head avatars rely heavily on frontal-view images, resulting in poorly constructed back-head regions with geometric inconsistencies, structural blurring, and reduced realism, limiting overall avatar fidelity.

Method: Proposes AvatarBack with two core innovations: 1) Subject-specific Generator (SSG) that synthesizes identity-consistent back-view pseudo-images from sparse frontal inputs, and 2) Adaptive Spatial Alignment Strategy (ASA) using learnable transformation matrices to resolve pose and coordinate discrepancies between synthetic views and 3D Gaussian representation.

Result: Extensive experiments on NeRSemble and K-hairstyle datasets show significant improvements in back-head reconstruction quality while preserving frontal fidelity, with consistent visual realism under diverse motions and full animatability.

Conclusion: AvatarBack successfully addresses the back-head reconstruction challenge in 3D Gaussian avatars through its novel plug-and-play framework, enabling complete and consistent avatar modeling with enhanced realism.

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: Foundation models fine-tuned and integrated with traditional facial recognition networks significantly improve sitter identification in historical paintings, overcoming domain shift and artistic variations.


<details>
  <summary>Details</summary>
Motivation: Automated facial recognition struggles with historical paintings due to domain shift, high intra-class variation, and artistic factors like style and intent, making sitter identification challenging for art historians.

Method: Fine-tune foundation models and integrate their embeddings with conventional facial recognition networks to handle artistic variations and domain differences in paintings.

Result: Demonstrates notable improvements over current state-of-the-art methods, showing foundation models can effectively bridge the gap where traditional methods fail.

Conclusion: Foundation models offer a promising solution for improving facial recognition in artworks, successfully addressing the challenges posed by artistic variations and domain shift in historical paintings.

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti is an end-to-end text-guided graffiti generation framework that preserves facial identity during extreme stylistic transformations using a style-first approach with face-consistent self-attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Preserving facial identity in graffiti art is challenging because extreme stylistic transformations can erase recognizability, undermining personal and cultural authenticity in generative art.

Method: Uses LoRA-fine-tuned pretrained diffusion transformer for style transfer, then enforces identity fidelity through face-consistent self-attention with explicit identity embeddings. CLIP-guided prompt extension enables pose customization without keypoints.

Result: Achieves competitive facial feature consistency, state-of-the-art aesthetic and human preference scores. Reduces attribute drift compared to reverse order approaches. Successfully deployed at Cruilla Festival.

Conclusion: CraftGraffiti advances identity-respectful AI-assisted artistry by blending stylistic freedom with recognizability through a principled style-first, identity-after paradigm.

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: A novel self-evaluation approach for Large Visual-Language Models that generates debiased self-judgment scores internally to improve visual-linguistic alignment without external resources, reducing hallucinations and enhancing safety.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for LVLMs rely on external datasets, human annotations, or complex post-processing, which limit scalability and increase costs while still struggling with hallucinations and safety concerns.

Method: Proposes generating debiased self-judgment scores internally within the model as a self-evaluation metric, enabling autonomous alignment improvement without external resources. This enhances both decoding strategies and preference tuning processes.

Result: Empirical results show significant outperformance over traditional methods, with reduced hallucinations, enhanced safety, and improved overall capability in visual-linguistic alignment.

Conclusion: The approach offers a more effective and scalable solution for aligning LVLMs by enabling autonomous self-improvement through internal self-evaluation metrics, addressing key limitations of existing external-resource-dependent methods.

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: S-HArM dataset for intent-aware classification of AI-generated images, with 9,576 real-world image-text pairs labeled as Humor/Satire, Art, or Misinformation, plus synthetic data from Stable Diffusion prompting strategies.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal AI efforts overlook the intent behind AI-generated images, creating a gap in understanding whether synthetic content is created for humor, art, or misinformation purposes.

Method: Created multimodal dataset from Twitter/X and Reddit, used three prompting strategies (image-guided, description-guided, multimodally-guided) with Stable Diffusion to generate synthetic training data, and tested various models including modality fusion, contrastive learning, reconstruction networks, attention mechanisms, and large vision-language models.

Result: Models trained on image- and multimodally-guided data generalized better to real-world content due to preserved visual context, but overall performance remained limited, indicating the complexity of intent inference.

Conclusion: Inferring intent behind AI-generated images is challenging and requires specialized architectures beyond current multimodal approaches, with synthetic data generation strategies showing promise but needing further refinement.

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: MobileCLIP2 improves upon MobileCLIP with better teacher ensembles and captioner fine-tuning, achieving state-of-the-art zero-shot accuracy at low latencies with 2.2% ImageNet-1k improvement.


<details>
  <summary>Details</summary>
Motivation: To enhance the multi-modal reinforced training of MobileCLIP for better zero-shot capabilities while maintaining low latency and small model sizes suitable for mobile devices.

Method: Improved multi-modal reinforced training through: 1) better CLIP teacher ensembles trained on DFN dataset, 2) improved captioner teachers trained on DFN and fine-tuned on diverse high-quality image-caption datasets, with insights on temperature tuning and synthetic caption combination.

Result: MobileCLIP2 achieves SOTA ImageNet-1k zero-shot accuracies at low latencies, with 2.2% improvement over MobileCLIP-B. MobileCLIP2-S4 matches SigLIP-SO400M/14 accuracy while being 2x smaller and outperforms DFN ViT-L/14 at 2.5x lower latency.

Conclusion: MobileCLIP2 demonstrates significant improvements in zero-shot accuracy through enhanced training methodology while maintaining efficiency, making it suitable for mobile applications. Models and data generation code are publicly released.

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: A dynamic neural video compression framework with variable coding routes and rate control agent that achieves precise bitrate control while maintaining high compression performance.


<details>
  <summary>Details</summary>
Motivation: Neural video compression has shown remarkable performance but struggles with precise rate control due to inherent limitations of learning-based codecs, making variable bitrate scenarios challenging.

Method: Proposes Dynamic-Route Autoencoder with variable coding routes (each with partial computational complexity and distinct RD trade-off), Rate Control Agent for bitrate estimation and route adjustment, and Joint-Routes Optimization for collaborative training.

Result: Achieves average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods with average bitrate error of 1.66% on HEVC and UVG datasets.

Conclusion: The framework successfully achieves Rate-Distortion-Complexity Optimization for various bitrate and bitrate-constrained applications, providing effective variable bitrate neural video compression.

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet is a recurrent Bayesian deep learning framework for 3D cardiac motion estimation that uses shape-guided registration instead of intensity-based methods, achieving superior performance on UK Biobank data with lower uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing cardiac motion estimation methods struggle with accuracy because they rely on intensity-based image registration that may overlook cardiac anatomical regions, leading to suboptimal motion capture.

Method: A recurrent variational autoencoder framework that models spatio-temporal dependencies, uses two posterior models for bi-ventricular segmentation and motion estimation, and employs Bayesian formulation to guide registration of segmentation maps without intensity-based similarity loss.

Result: Superior performance in cardiac motion estimation on UK Biobank dataset, outperforming state-of-the-art methods with better warped mask shape alignment to ground truth, and lower uncertainty values in cardiac regions compared to other probabilistic methods.

Conclusion: CardioMorphNet demonstrates that shape-guided registration using Bayesian deep learning with recurrent networks effectively addresses limitations of intensity-based methods, providing more accurate cardiac motion estimation with higher confidence predictions.

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: A training-time recipe for domain-robust atypical mitotic figure classification that uses style perturbations, attention-based feature alignment, and EMA teacher distillation to achieve strong performance across different scanners and acquisition conditions.


<details>
  <summary>Details</summary>
Motivation: Atypical mitotic figures are important histopathological markers but are challenging to identify consistently due to domain shifts from different scanners, stains, and acquisition methods.

Method: Three key components: (1) style perturbations at early/mid backbone stages for feature diversity, (2) attention-refined feature alignment across domains using weak domain labels, (3) EMA teacher distillation with temperature-scaled KL divergence for prediction stability.

Result: Achieved balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499 on MIDOG 2025 Task 2 preliminary leaderboard.

Conclusion: The method provides strong, balanced performance with negligible inference overhead and only requires coarse domain metadata, making it a competitive solution for domain-robust AMF classification.

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [60] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: Pref-GRPO addresses reward hacking in text-to-image generation by using pairwise preference rewards instead of pointwise scoring, and introduces UniGenBench for comprehensive model evaluation.


<details>
  <summary>Details</summary>
Motivation: Current GRPO-based reinforcement learning methods for text-to-image generation suffer from reward hacking where minimal score differences are amplified, causing unstable training and over-optimization for trivial gains.

Method: Pref-GRPO uses pairwise preference reward models where images are compared within groups and win rates serve as reward signals, shifting optimization from score maximization to preference fitting. Also introduces UniGenBench with 600 prompts across 5 themes and detailed evaluation criteria using MLLM.

Result: Pref-GRPO effectively differentiates subtle image quality differences, provides stable advantages, and mitigates reward hacking. UniGenBench reveals strengths/weaknesses of T2I models and validates Pref-GRPO's effectiveness.

Conclusion: The proposed Pref-GRPO method with pairwise preference optimization and the comprehensive UniGenBench benchmark significantly improve text-to-image generation stability and evaluation capabilities.

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [61] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS is a novel framework for generalizable Gaussian splatting that improves novel view synthesis from sparse input views by incorporating context-aware, cross-dimension, and cross-scale constraints to enhance feature learning and geometry construction.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for generalizable Gaussian splatting struggle with encoding discriminative, multi-view consistent features for Gaussian predictions, particularly when constructing accurate geometry from sparse input views.

Method: Proposes C3-GS framework with three lightweight modules integrated into a unified rendering pipeline: context-aware, cross-dimension, and cross-scale constraints to improve feature fusion without requiring additional supervision.

Result: Extensive experiments on benchmark datasets show that C3-GS achieves state-of-the-art rendering quality and generalization ability for novel view synthesis.

Conclusion: The proposed C3-GS framework effectively addresses feature learning challenges in generalizable Gaussian splatting, enabling photorealistic synthesis from sparse views without per-scene optimization.

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [62] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: SeqVLM is a zero-shot 3D visual grounding framework that uses multi-view scene images with spatial information to localize objects from natural language descriptions without scene-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot 3DVG methods suffer from spatial-limited reasoning due to single-view localization and contextual omissions. Supervised methods require scene-specific training, limiting real-world applicability.

Method: Generates 3D instance proposals via semantic segmentation, refines through semantic filtering, uses proposal-guided multi-view projection to preserve spatial relationships, and implements dynamic scheduling for VLM processing of sequence-query prompts.

Result: Achieves state-of-the-art performance on ScanRefer (55.6% Acc@0.25) and Nr3D (53.2% Acc@0.25) benchmarks, surpassing previous zero-shot methods by 4.0% and 5.2% respectively.

Conclusion: SeqVLM advances 3D visual grounding toward greater generalization and real-world applicability by eliminating scene-specific training requirements while improving spatial reasoning and contextual preservation.

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [63] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: CLIP's robustness to occlusion in military environments is tested, revealing Transformer models outperform CNNs, fine-grained occlusions are more damaging, and backbone finetuning significantly improves occlusion resilience.


<details>
  <summary>Details</summary>
Motivation: Evaluate CLIP's robustness for military applications where partial occlusion and degraded SNR are common challenges, addressing a gap in current research.

Method: Tested CLIP variants on custom military vehicle dataset using NAUC metric across occlusion percentages, comparing Transformer vs CNN architectures and analyzing fine-tuning approaches.

Result: Transformer-based CLIP outperforms CNNs; fine-grained occlusions degrade performance more than large ones; linear-probed models drop at 35% occlusion; backbone finetuning pushes drop point to 60%+ occlusion.

Conclusion: Occlusion-specific augmentations during training and patch-level sensitivity analysis are crucial for real-world CLIP deployment in challenging military environments.

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [64] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: Proposes SKGE-Swin architecture for autonomous vehicles using Swin Transformer with skip-stage mechanism to enhance global feature representation and maintain critical information throughout feature extraction.


<details>
  <summary>Details</summary>
Motivation: To develop an end-to-end autonomous vehicle model with improved pixel-to-pixel context awareness for better understanding of complex patterns in vehicle surroundings.

Method: Utilizes Swin Transformer with skip-stage mechanism and Shifted Window-based Multi-head Self-Attention (SW-MSA) to extract information from distant pixels and retain critical information across network levels.

Result: Achieves superior Driving Score on CARLA platform compared to previous methods when tested with adversarial scenarios simulating real-world conditions.

Conclusion: The SKGE-Swin architecture effectively enhances autonomous vehicle perception capabilities through improved global feature representation and information retention, with planned ablation studies to validate component contributions.

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [65] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: Survey paper examining abstract concept recognition in videos using foundation models, advocating for leveraging decades of community experience to address this grand challenge.


<details>
  <summary>Details</summary>
Motivation: Humans can recognize abstract concepts like justice and freedom in videos, while current AI systems focus on concrete entities. Abstract concept understanding is crucial for aligning models with human reasoning and values.

Method: Survey study of different tasks and datasets for abstract concept understanding in video content, analyzing historical approaches and advocating for building on decades of community experience.

Result: The paper provides a comprehensive overview of the field, observing that researchers have periodically attempted to solve abstract concept recognition tasks using available tools throughout different eras.

Conclusion: Foundation models create an ideal setting to address abstract concept understanding in videos. Drawing on community experience will help avoid re-inventing the wheel and advance this important open challenge.

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [66] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: Proposed Global Class Activation Probabilistic Map Evaluation method for trustworthy skin lesion diagnosis by analyzing all classes' activation probability maps probabilistically at pixel level, combined with SafeML for false diagnosis detection.


<details>
  <summary>Details</summary>
Motivation: Address distrust in AI models for medical diagnosis by providing trustworthy, explainable diagnoses beyond just high accuracy, overcoming limitations of existing explainability methods like LIME inconsistency and CAM's failure to consider all classes.

Method: Global Class Activation Probabilistic Map Evaluation that analyzes all classes' activation probability maps probabilistically at pixel level, combined with SafeML for detecting false diagnoses and issuing warnings.

Result: Method enables visualization of diagnostic process in unified manner, reduces misdiagnosis risk, enhances diagnostic reliability through false diagnosis detection, and improves patient safety.

Conclusion: The proposed approach provides more trustworthy and explainable AI diagnoses for skin lesion classification, addressing key limitations of existing methods and enhancing reliability in medical practice.

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [67] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: Diffusion Classifier shows improved compositional generalization compared to CLIP, performing well on concept binding tasks but struggling with relational reasoning like all VLMs.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP often fail at compositional semantics, incorrectly combining attributes and objects. This research explores whether generative diffusion-based classifiers can overcome these limitations.

Method: Evaluated three models (Diffusion Classifier, CLIP, ViLT) on binding objects with attributes and relations in zero-shot and generalized zero-shot learning settings using compositional tasks.

Result: Diffusion Classifier and ViLT performed well on concept binding tasks, but all models struggled significantly with relational reasoning. CLIP embeddings showed overly similar representations for relational concepts like left/right.

Conclusion: While diffusion-based classifiers show promise for compositional generalization, relational reasoning remains a fundamental challenge for vision-language models that requires further research.

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [68] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: A novel surfel-based pose learning regression method for point cloud registration that uses SE(3) equivariant features and handles both position and rotation to improve robustness against noise and aggressive rotations.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud registration methods ignore point orientations and uncertainties, making them susceptible to noise and aggressive rotations like orthogonal transformations, requiring extensive training data with transformations.

Method: Initializes surfels from Lidar point clouds using virtual perspective camera parameters, learns explicit SE(3) equivariant features through equivariant convolutional kernels, and uses cross-attention mechanism, fully-connected decoder with Huber loss.

Result: Experimental results on indoor and outdoor datasets demonstrate superior and robust performance on real point-cloud scans compared to state-of-the-art methods.

Conclusion: The proposed surfel-based approach with SE(3) equivariant features effectively addresses limitations of traditional point cloud registration methods, providing robust performance against noise and aggressive rotations.

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [69] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: DVCTNet is a dual-view co-training network that combines global panoramic X-ray screening with detailed tooth-level inspection for superior dental caries detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current dental caries detection methods have suboptimal accuracy due to subtle contrast variations and diverse lesion morphology in panoramic X-rays. The clinical workflow where dentists combine whole-image screening with detailed tooth-level inspection inspired this approach.

Method: Uses automated tooth detection to create global (panoramic) and local (cropped tooth) views. Pretrains two vision foundation models separately, then integrates them using a Gated Cross-View Attention module that dynamically fuses dual-view features for final caries detection.

Result: DVCTNet demonstrates superior performance against state-of-the-art methods on both public dataset and newly curated high-precision dataset annotated with double verification (intra-oral images + panoramic X-rays).

Conclusion: The method shows clinical applicability for accurate dental caries detection by effectively mimicking the dentist's dual-view inspection workflow through automated global and local feature integration.

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [70] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: FusionCounting integrates crowd counting with visible and infrared image fusion in a unified multi-task framework, using dynamic loss weighting and adversarial training to improve both fusion quality and counting accuracy in dense scenes.


<details>
  <summary>Details</summary>
Motivation: Traditional VIF methods focus only on image quality, while recent semantic-guided approaches face annotation challenges (segmentation requires extensive labels, detection struggles with crowded scenes). Crowd counting provides quantitative density measurement with minimal annotation, making it ideal for dense environments.

Method: Multi-task learning framework that jointly optimizes VIF and crowd counting. Uses dynamic loss function weighting for task balance and convergence. Incorporates adversarial training to enhance robustness against attacks for both tasks.

Result: Experimental results show FusionCounting improves both image fusion quality and crowd counting performance compared to existing methods on public datasets.

Conclusion: Integrating crowd counting with VIF in a unified framework is effective, especially for dense scenes. The multi-task approach with dynamic weighting and adversarial training provides mutual benefits for both fusion quality and counting accuracy.

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [71] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: Novel pipeline using Vision Language Models (VLMs) with LoRA fine-tuning for surgical tool 2D keypoint estimation, outperforming traditional CNN/Transformer methods with minimal training.


<details>
  <summary>Details</summary>
Motivation: Traditional CNN and Transformer-based approaches often suffer from overfitting in small-scale medical datasets, requiring a more generalized approach that can work effectively with limited data.

Method: Leverages pre-trained VLMs fine-tuned using LoRA technique, with carefully designed prompts to create instruction-tuning dataset that aligns visual features with semantic keypoint descriptions.

Result: With only two epochs of fine-tuning, the adapted VLM outperforms baseline models, demonstrating LoRA's effectiveness in low-resource scenarios.

Conclusion: This approach not only improves keypoint detection performance but also paves the way for future work in 3D surgical hands and tools pose estimation.

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [72] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: PointDGRWKV is the first RWKV-based framework for Domain Generalization in Point Cloud Classification, addressing spatial distortion and attention drift issues through adaptive geometric token shift and cross-domain key distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Existing DG PCC methods using convolutional networks, Transformers, or Mamba architectures suffer from limited receptive fields, high computational cost, or insufficient long-range dependency modeling. RWKV offers linear complexity and global receptive fields but introduces spatial distortions and attention drift when directly applied to point clouds.

Method: Proposes PointDGRWKV with two key modules: 1) Adaptive Geometric Token Shift to model local neighborhood structures and improve geometric context awareness, 2) Cross-Domain key feature Distribution Alignment to mitigate attention drift by aligning key feature distributions across domains.

Result: Extensive experiments on multiple benchmarks demonstrate state-of-the-art performance in Domain Generalization for Point Cloud Classification.

Conclusion: PointDGRWKV successfully adapts RWKV architecture for DG PCC, overcoming spatial distortion and attention drift challenges while maintaining linear efficiency, achieving superior generalization performance across domains.

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [73] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: PathMR is a multimodal visual reasoning framework that generates cell-level diagnostic explanations and segmentation masks for pathological images, improving interpretability in AI-assisted pathology diagnosis.


<details>
  <summary>Details</summary>
Motivation: Current deep learning diagnostic tools lack transparency and traceable rationale, limiting clinical adoption despite improved efficiency. There's a need for models that provide both localization of lesions and expert-style explanations.

Method: Proposed PathMR framework processes pathological images with textual queries to generate diagnostic narratives while predicting cell distribution patterns. Uses multimodal visual reasoning architecture with pixel-level segmentation and semantically aligned text generation.

Result: Outperforms state-of-the-art methods on PathGen and GADVR datasets in text generation quality, segmentation accuracy, and cross-modal alignment. Consistently achieves better performance across multiple evaluation metrics.

Conclusion: PathMR demonstrates strong potential for improving interpretability in AI-driven pathological diagnosis by providing transparent, expert-level explanations alongside accurate cell distribution predictions, addressing key barriers to clinical adoption.

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [74] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: Deep learning framework using autofluorescence and SHG imaging achieves over 90% accuracy for early pancreatic cancer detection, outperforming manual analysis methods.


<details>
  <summary>Details</summary>
Motivation: PDAC has extremely low survival rates due to late detection, creating urgent need for early diagnostic methods to improve patient outcomes.

Method: Analyzed 40 patient samples with dual-modality imaging, evaluated 6 deep learning architectures (CNNs vs ViTs), used modified ResNet with frozen pre-trained layers and class-weighted training to handle limited data and class imbalance.

Result: Achieved over 90% accuracy in distinguishing normal, fibrotic, and cancerous tissue, significantly improving upon current manual analysis methods.

Conclusion: Establishes robust automated PDAC detection pipeline for clinical deployment, provides foundation for expanding to other cancers, and offers insights for deep learning on limited medical imaging datasets.

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [75] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: This thesis develops counterfactual frameworks for explaining, auditing, and mitigating bias in vision classifiers and generative models through systematic attribute variation and causal analysis.


<details>
  <summary>Details</summary>
Motivation: To address interpretability and fairness in AI by using counterfactual reasoning to uncover spurious correlations, probe causal dependencies, and build more robust systems that avoid biased behaviors.

Method: Developed multiple frameworks: CAVLI (integrates LIME and TCAV for concept-level analysis), ASAC (adversarial counterfactuals with curriculum learning), TIBET (scalable pipeline for TTI model bias evaluation), BiasConnect (causal graphs for intersectional biases), and InterMit (training-free bias mitigation via causal sensitivity scores).

Result: The methods successfully quantify concept dependencies in vision classifiers, improve fairness and accuracy while avoiding stereotypes, enable causal auditing of identity-related biases in generative models, and provide scalable solutions for intersectional bias diagnosis and mitigation.

Conclusion: Counterfactuals serve as a unifying framework for interpretability, fairness, and causality across both discriminative and generative models, establishing principled and scalable methods for socially responsible bias evaluation and mitigation.

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [76] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: Survey paper on unified perception for autonomous vehicles that integrates detection, tracking, and prediction into shared architectures to overcome limitations of modular pipelines.


<details>
  <summary>Details</summary>
Motivation: Traditional modular perception pipelines suffer from error accumulation and limited inter-task synergy. Unified perception aims to improve robustness, contextual reasoning, and efficiency while maintaining interpretability.

Method: Comprehensive survey introducing a taxonomy categorizing methods by task integration, tracking formulation, and representation flow. Defines three paradigms: Early, Late, and Full Unified Perception, and systematically reviews architectures, training strategies, datasets, and open-source availability.

Result: Establishes the first comprehensive framework for understanding unified perception, consolidates fragmented research efforts, and provides systematic categorization of existing approaches.

Conclusion: This work guides future research toward more robust, generalizable, and interpretable perception systems for autonomous vehicles by providing a structured foundation and highlighting future research directions.

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [77] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: Dino U-Net leverages DINOv3 foundation model features for medical image segmentation, achieving state-of-the-art results across 7 datasets with scalable performance as model size increases.


<details>
  <summary>Details</summary>
Motivation: Effectively transferring representations from large-scale natural image foundation models to precise medical image segmentation applications remains challenging.

Method: Encoder-decoder architecture with frozen DINOv3 backbone, specialized adapter for feature fusion, and fidelity-aware projection module (FAPM) to preserve feature quality during dimensionality reduction.

Result: Achieves state-of-the-art performance across 7 diverse medical image datasets, with segmentation accuracy consistently improving as backbone model size increases up to 7B parameters.

Conclusion: Leveraging dense-pretrained features from general-purpose foundation models provides highly effective and parameter-efficient approach for advancing medical image segmentation accuracy.

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [78] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: Ensemble of ConvNeXtBase models with rule-based refinement for classifying atypical mitotic figures, achieving 84.02% balanced accuracy on MIDOG25 test set


<details>
  <summary>Details</summary>
Motivation: Differentiating atypical mitotic figures from normal ones is challenging due to time-consuming and subjective manual annotation, requiring automated solutions

Method: Trained ensemble of ConvNeXtBase models using AUCMEDI framework with additional rule-based refinement module

Result: Achieved 84.02% balanced accuracy on MIDOG25 test set; RBR increased specificity but reduced sensitivity and overall performance

Conclusion: Deep ensembles perform well for atypical mitotic figure classification, but rule-based refinement requires further research despite improving specific metrics

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [79] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: COMETH is a lightweight multi-view human pose fusion algorithm that uses convex optimization and biomechanical constraints to improve accuracy and temporal consistency for real-time industrial applications.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of multi-camera centralized setups (high computational cost, bandwidth requirements) and edge device constraints (accuracy degradation, temporal/spatial inconsistencies) in human activity monitoring for Industry 5.0.

Method: Proposes COMETH algorithm with three key concepts: 1) integrates kinematic and biomechanical constraints for joint positioning accuracy, 2) uses convex optimization-based inverse kinematics for spatial fusion, 3) implements state observer for temporal consistency.

Result: Outperforms state-of-the-art methods in localization, detection, and tracking accuracy on both public and industrial datasets.

Conclusion: COMETH enables accurate and scalable human motion tracking suitable for industrial and safety-critical applications, with publicly available code.

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [80] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: Novel olive tree segmentation method using SAM with field alignment and shape constraints achieves 98% accuracy, significantly improving over baseline SAM performance.


<details>
  <summary>Details</summary>
Motivation: Address climate change impacts by maintaining olive biodiversity through early anomaly detection using remote sensing technology for effective agricultural management.

Method: Leverages Segment Anything Model (SAM) with additional corrections based on tree alignment patterns in fields and learnable constraints about tree shape and size.

Result: Achieved 98% accuracy rate in olive tree segmentation, significantly surpassing the initial SAM performance of 82%.

Conclusion: The integrated approach combining SAM with field-specific constraints provides highly accurate olive tree segmentation, enabling better biodiversity monitoring and management.

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [81] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: E-ConvNeXt integrates CSPNet with ConvNeXt to create a lightweight network that reduces complexity by up to 80% while maintaining high accuracy, achieving 78.3-81.9% Top-1 accuracy on ImageNet with 0.9-3.1GFLOPs.


<details>
  <summary>Details</summary>
Motivation: Many high-performance networks were not designed for lightweight application scenarios, restricting their scope of application. The paper aims to create a more efficient version of ConvNeXt suitable for resource-constrained environments.

Method: Three core innovations: (1) Integrate Cross Stage Partial Network (CSPNet) with ConvNeXt, reducing complexity by up to 80%; (2) Optimize Stem and Block structures for better feature expression and efficiency; (3) Replace Layer Scale with channel attention.

Result: E-ConvNeXt-mini: 78.3% Top-1 accuracy at 0.9GFLOPs; E-ConvNeXt-small: 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer learning tests on object detection confirm strong generalization capability.

Conclusion: E-ConvNeXt successfully achieves superior accuracy-efficiency balance, making ConvNeXt suitable for lightweight applications while maintaining high performance across different complexity configurations.

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [82] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++ is an efficient framework for realistic reconstruction and controllable editing of dynamic autonomous driving scenes using 3D Gaussians and LiDAR priors, supporting training-free editing operations like texture modification, weather simulation, and object manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of realistic reconstruction and controllable editing of dynamic autonomous driving scenes with accurate positions, occlusions, and photorealistic surround-view synthesis.

Method: Models static background with incremental 3D Gaussians and reconstructs moving objects using a composite dynamic Gaussian graph. Integrates LiDAR prior for detailed reconstruction and leverages multi-view images with depth priors. Uses large language models for automatic motion trajectory generation.

Result: Outperforms existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. Achieves consistent and realistic editing results while significantly enhancing scene diversity.

Conclusion: DrivingGaussian++ provides an effective framework for reconstructing and editing dynamic driving scenes with realistic results and enhanced controllability through LLM integration and training-free editing capabilities.

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [83] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: Novel methods to address image manipulation localization data scarcity using web data, including automated pixel-level annotation (CAAAv2), quality filtering (QES), and a large-scale dataset MIMLv2 with 246K images, plus Web-IML model achieving 31% performance gain.


<details>
  <summary>Details</summary>
Motivation: High cost of data acquisition and severe lack of high-quality annotated datasets for image manipulation localization, which poses significant risks to social security when manipulated images mislead viewers.

Method: Leverage web data through CAAAv2 for automated pixel-level annotation, QES metric for filtering unreliable annotations, Object Jitter for generating manipulation artifacts, and develop Web-IML model for web-scale supervision.

Result: Constructed MIMLv2 dataset with 246,212 images (120x larger than IMD20), Web-IML achieves 31% performance gain and surpasses previous SOTA TruFor by 24.1 average IoU points on multiple real-world forgery benchmarks.

Conclusion: The approach substantially alleviates data scarcity problem and significantly improves manipulation localization performance, with publicly available dataset and code to advance the field.

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [84] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: ExpertSim is a deep learning approach using Mixture-of-Generative-Experts to simulate detector responses in CERN's ALICE experiment, replacing computationally expensive Monte Carlo methods with more efficient and accurate ML-based simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo simulations for particle collision detection at CERN are computationally expensive and strain computational resources. The data distribution varies significantly across simulations, making standard methods inadequate.

Method: Mixture-of-Generative-Experts architecture where each expert specializes in simulating different subsets of data from the Zero Degree Calorimeter in the ALICE experiment, allowing focused and precise generation.

Result: ExpertSim improves simulation accuracy and provides significant speedup compared to traditional Monte Carlo methods, enabling more efficient detector simulations.

Conclusion: The approach offers a promising solution for high-efficiency detector simulations in particle physics experiments at CERN, with code made publicly available.

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [85] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: A modular framework that decouples causal reasoning from answer generation using interpretable natural language causal chains, outperforming state-of-the-art VideoQA models while improving explainability and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with higher-order reasoning, rely on opaque monolithic pipelines, and lack interpretability, depending on shallow heuristics rather than transparent causal inference.

Method: Two-stage architecture: Causal Chain Extractor (CCE) generates causal chains from video-question pairs, and Causal Chain-Driven Answerer (CCDA) produces answers grounded in these chains. Uses LLMs to generate causal chains from existing datasets and introduces CauCo evaluation metric.

Result: Outperforms state-of-the-art models on three large-scale benchmarks, with substantial gains in explainability, user trust, and generalization. CCE serves as reusable causal reasoning engine across domains.

Conclusion: The modular framework with explicit causal chains enables transparent and logically coherent inference, bridging low-level video content with high-level causal reasoning while addressing interpretability limitations of existing approaches.

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [86] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE is a one-step distillation framework that accelerates large-scale video diffusion models by 100x, enabling high-quality video generation in a single step through phased stability priming and adversarial equilibrium training.


<details>
  <summary>Details</summary>
Motivation: Existing video acceleration methods suffer from limitations in modeling temporal coherence and lack single-step distillation capabilities for large-scale video models, creating bottlenecks in sampling efficiency.

Method: Two-phase distillation process: (1) Stability priming - warm-up mechanism to stabilize adversarial distillation across SNR regimes, (2) Unified adversarial equilibrium - self-adversarial training towards Nash equilibrium in Gaussian noise space, plus conditional adversarial consistency for semantic and frame consistency.

Result: Outperforms other acceleration methods by average 7.15% on VBench-I2V metrics, reduces latency from 1000 seconds to 10 seconds (100x improvement) while maintaining competitive performance.

Conclusion: POSE successfully bridges the gap in video acceleration by enabling efficient single-step generation of high-quality videos through novel distillation techniques that preserve temporal coherence and video quality.

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [87] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: Training-free method that clusters semantically similar prompts and shares computation in early diffusion steps to reduce redundancy and computational costs in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are computationally expensive, and prior work focuses on per-inference efficiency rather than reducing redundancy across correlated prompts.

Method: Leverages the coarse-to-fine nature of diffusion models by clustering prompts based on semantic similarity and sharing computation in early denoising steps that capture shared structures among similar prompts.

Result: Significantly reduces compute cost while improving image quality, especially for models trained conditioned on image embeddings. Integrates with existing pipelines and scales with prompt sets.

Conclusion: The approach effectively reduces the environmental and financial burden of large-scale text-to-image generation by exploiting prompt correlations and optimizing diffusion step allocation.

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [88] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: A Mamba-based VM-UNet approach with stain augmentation for mitosis detection under domain shift, submitted to MIDOG challenge track 1.


<details>
  <summary>Details</summary>
Motivation: Mitosis detection is crucial for tumor assessment but ML algorithms suffer performance drops when tested on domains different from training data. Domain shift is a significant challenge in medical imaging.

Method: Proposes a Mamba-based VM-UNet architecture with stain augmentation operations to improve model robustness against domain shift in mitosis detection.

Result: Preliminary experiments on MIDOG++ dataset show large room for improvement for the proposed method.

Conclusion: The Mamba-based approach shows potential but requires further development to effectively handle domain shift in mitosis detection tasks.

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [89] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: Multi-task learning approach for domain generalization in atypical mitosis detection, submitted to MIDOG challenge


<details>
  <summary>Details</summary>
Motivation: Machine learning models for recognizing atypical mitotic figures suffer performance drops under domain shift, requiring better generalization across different histopathology datasets

Method: Multi-task learning with auxiliary tasks correlated to main classification, helping model focus on classification objects while ignoring domain-varying background

Result: Shows promising performance on three distinct datasets: MIDOG 2025 Atypical Training Set, Ami-Br dataset, and MIDOG25 preliminary test set

Conclusion: Proposed multi-task learning approach effectively addresses domain shift problem in atypical mitosis detection, demonstrating good generalization across different histopathology domains

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [90] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN is a one-shot handwriting synthesis framework that generates realistic, style-consistent text from a single example using frequency-aware components and a novel frequency distribution loss.


<details>
  <summary>Details</summary>
Motivation: Handwriting data scarcity limits recognition systems, and current synthesis methods struggle with long-range dependencies and ignore frequency information crucial for capturing fine-grained stylistic details.

Method: Proposes FW-GAN with phase-aware Wave-MLP generator, frequency-guided discriminator, and novel Frequency Distribution Loss to align frequency characteristics between synthetic and real handwriting.

Result: Experiments on Vietnamese and English datasets show FW-GAN generates high-quality, style-consistent handwriting that effectively augments data for low-resource handwriting recognition pipelines.

Conclusion: FW-GAN successfully addresses limitations of conventional methods by incorporating frequency information, making it a valuable tool for handwriting data augmentation in recognition systems.

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [91] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: MMG-Vid is a training-free visual token pruning framework that reduces computational overhead in Video LLMs by maximizing marginal gains at segment and token levels, achieving 75% token reduction while maintaining 99.5% performance.


<details>
  <summary>Details</summary>
Motivation: Video LLMs face computational challenges due to excessive visual tokens, and existing methods ignore dynamic characteristics and temporal dependencies of video frames.

Method: Divides video into segments based on frame similarity, dynamically allocates token budget per segment, and uses temporal-guided DPC algorithm to model inter-frame uniqueness and intra-frame diversity.

Result: Reduces 75% visual tokens, accelerates prefilling stage by 3.9x on LLaVA-OneVision-7B while maintaining over 99.5% of original performance.

Conclusion: MMG-Vid effectively maximizes limited token budget utilization, significantly improving efficiency while preserving strong video understanding performance.

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [92] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: CogVLA is an efficient Vision-Language-Action framework that uses instruction-driven routing and sparsification to achieve state-of-the-art performance while significantly reducing computational costs and latency compared to existing VLA models.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action models require extensive post-training with high computational overhead, limiting scalability and deployment. The authors aim to create a more efficient framework inspired by human multimodal coordination.

Method: 3-stage progressive architecture: 1) EFA-Routing injects instruction info into vision encoder for selective visual token aggregation, 2) LFP-Routing prunes instruction-irrelevant tokens for sparsity, 3) V-L-A Coupled Attention combines causal vision-language attention with bidirectional action parallel decoding.

Result: Achieves state-of-the-art performance with 97.4% success rate on LIBERO benchmark and 70.0% on real-world robotic tasks, while reducing training costs by 2.5x and inference latency by 2.8x compared to OpenVLA.

Conclusion: CogVLA demonstrates that cognition-aligned design with instruction-driven routing and sparsification can significantly improve both efficiency and performance in VLA models, making them more scalable and deployable for real-world applications.

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: HydraFake dataset addresses real-world deepfake detection challenges with hierarchical generalization testing, and Veritas MLLM detector uses pattern-aware reasoning to achieve superior OOD performance.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection benchmarks suffer from discrepancies with industrial practice, featuring homogeneous training sources and low-quality testing images that hinder practical deployment.

Method: Introduces HydraFake dataset with diversified deepfake techniques and in-the-wild forgeries, plus Veritas MLLM detector with pattern-aware reasoning (planning and self-reflection) and two-stage training pipeline.

Result: Previous detectors show good cross-model generalization but fail on unseen forgeries and data domains. Veritas achieves significant gains across different OOD scenarios with transparent outputs.

Conclusion: HydraFake dataset and Veritas MLLM detector effectively address real-world deepfake detection challenges through hierarchical generalization testing and human-like forensic reasoning patterns.

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: FakeParts introduces a new class of partial deepfakes with localized manipulations that blend with authentic content, making them highly deceptive. The paper presents FakePartsBench, a large-scale benchmark dataset for evaluating detection methods.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detection methods struggle with partial manipulations that blend seamlessly with real elements, creating a critical vulnerability in detection capabilities.

Method: Created FakePartsBench dataset with over 25K videos featuring pixel-level and frame-level manipulation annotations for various partial deepfake types (altered expressions, object substitutions, background modifications).

Result: FakeParts reduce human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation in state-of-the-art detection models.

Conclusion: This work identifies an urgent vulnerability in current deepfake detection and provides resources to develop more robust methods for detecting partial video manipulations.

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: First data-driven multi-view 3D point tracker that uses 4+ cameras to track arbitrary points in dynamic scenes, overcoming depth ambiguities and occlusion issues of monocular methods.


<details>
  <summary>Details</summary>
Motivation: Existing monocular trackers struggle with depth ambiguities and occlusion, while prior multi-camera methods require 20+ cameras and per-sequence optimization. Need for practical multi-view solution.

Method: Feed-forward model using known camera poses and multi-view depth. Fuses multi-view features into unified point cloud, applies k-nearest-neighbors correlation with transformer-based update for long-range 3D correspondences.

Result: Achieves median trajectory errors of 3.1 cm on Panoptic Studio and 2.0 cm on DexYCB benchmarks. Generalizes to 1-8 camera views and 24-150 frame videos.

Conclusion: Sets new standard for multi-view 3D tracking with practical camera requirements, robust performance under occlusion, and releases datasets to advance research.

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward is a unified RL framework that uses a single vision-language model as a reward model to enhance multi-task generation capabilities across different evaluation criteria, eliminating the need for task-specific supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mask-guided image generation tasks rely on task-specific supervised fine-tuning, which limits generalization and training efficiency across diverse tasks with different data distributions and evaluation metrics.

Method: Uses a single vision-language model as a generative reward model that can distinguish winners/losers for any task and evaluation criterion. Applies multi-task reinforcement learning directly on pre-trained base models without task-specific SFT.

Result: The unified edit model (Seedream 3.0 Fill) consistently outperforms both commercial (Ideogram, Adobe Photoshop) and open-source (FLUX Fill [Pro]) competitors across multiple evaluation dimensions.

Conclusion: OneReward provides an effective unified framework for multi-task generation that eliminates task-specific training requirements while achieving superior performance across diverse tasks and evaluation criteria.

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance is a video diffusion framework that generates high-quality 5-second virtual try-on videos using a single user image and various garment inputs, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality virtual try-on system that can generate realistic videos of users wearing desired garments while moving according to reference videos, addressing limitations in current solutions.

Method: Uses CondNet, a novel conditioning network with attention mechanisms to unify multi-modal inputs (text, images, videos). Trained on heterogeneous data combining limited video data with larger image datasets in a multistage progressive manner.

Result: Generates 5-second-long 24 FPS videos at 1152x720 resolution. Supports tops, bottoms, one-piece garments, and simultaneous top+bottom try-on in single pass. Outperforms existing open source and commercial solutions.

Conclusion: Dress&Dance enables high-quality, flexible virtual try-on experience with enhanced garment registration and motion fidelity through its innovative CondNet architecture and multi-stage training approach.

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: Winning solution for NeurIPS 2024 watermark removal challenge that achieves 95.7% removal rate with minimal quality impact using different approaches for black-box and beige-box attack scenarios.


<details>
  <summary>Details</summary>
Motivation: To stress-test watermark robustness against adversarial attacks and determine if existing watermarks can withstand varying degrees of adversary knowledge, addressing the need for more robust authentication and copyright protection methods.

Method: For beige-box track: adaptive VAE-based evasion attack with test-time optimization and color-contrast restoration in CIELAB space. For black-box track: cluster images by artifacts, then apply diffusion models with controlled noise injection and ChatGPT-generated semantic priors.

Result: Achieved near-perfect watermark removal (95.7%) with negligible impact on residual image quality, demonstrating successful evasion of current watermarking methods.

Conclusion: The attacks successfully expose vulnerabilities in existing watermarking techniques and should inspire the development of more robust image watermarking methods that can withstand sophisticated adversarial approaches.

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>
