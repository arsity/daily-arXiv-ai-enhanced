<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

TL;DR: The paper presents a large-scale study on physics failures in Physics Engines (PEs), addressing manifestations, detection methods, and developer insights.


<details>
  <summary>Details</summary>
Motivation: PEs are critical but prone to physics failures, which can harm reliability and user experience, yet current testing methods are inadequate.

Method: The study investigates physics failure manifestations, evaluates detection techniques (deep learning, prompt-based, multimodal models), and gathers developer insights.

Result: Key contributions include a taxonomy of physics failures, evaluation of detection methods, and actionable insights from developers.

Conclusion: The study provides tools and insights to improve physics failure detection, releasing PhysiXFails and materials for future research.

Abstract: Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.

</details>


### [2] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

TL;DR: TRIG-Bench introduces a dataset and metric (TRIGScore) to analyze trade-offs in image generation across 10 dimensions, evaluating 14 models and proposing a Dimension Trade-off Map (DTM) for visualization and improvement.


<details>
  <summary>Details</summary>
Motivation: Existing models lack exploration of complex trade-offs in image generation due to missing datasets and single-metric evaluations.

Method: Introduces TRIG-Bench (40,200 samples, 10 dimensions) and TRIGScore (VLM-as-judge metric), evaluates 14 models, and proposes DTM for trade-off visualization.

Result: DTM provides comprehensive trade-off insights, and fine-tuning based on DTM mitigates model weaknesses.

Conclusion: TRIG-Bench and TRIGScore enable detailed analysis and improvement of image generation models by addressing multi-dimensional trade-offs.

Abstract: Model performance in text-to-image (T2I) and image-to-image (I2I) generation
often depends on multiple aspects, including quality, alignment, diversity, and
robustness. However, models' complex trade-offs among these dimensions have
rarely been explored due to (1) the lack of datasets that allow fine-grained
quantification of these trade-offs, and (2) the use of a single metric for
multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in
Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,
Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains
40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we
develop TRIGScore, a VLM-as-judge metric that automatically adapts to various
dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I
and I2I tasks. In addition, we propose the Relation Recognition System to
generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among
model-specific capabilities. Our experiments demonstrate that DTM consistently
provides a comprehensive understanding of the trade-offs between dimensions for
each type of generative model. Notably, we show that the model's
dimension-specific weaknesses can be mitigated through fine-tuning on DTM to
enhance overall performance. Code is available at:
https://github.com/fesvhtr/TRIG

</details>


### [3] [AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock](https://arxiv.org/abs/2507.22101)
*Umair Nawaz,Muhammad Zaigham Zaheer,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: A survey reviewing AI applications in agriculture, covering machine learning, deep learning, and vision-language models for tasks like crop disease detection and livestock health. Discusses challenges and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in global food production (climate, resources, sustainability) using AI technologies.

Method: Systematic review of 200+ research works, analyzing ML, DL, and vision-language models in agriculture.

Result: Identifies key tasks (e.g., disease detection) and implementation challenges (data variability, deployment).

Conclusion: Highlights future research needs: multimodal data, edge-device efficiency, and adaptable AI models for diverse farming.

Abstract: Crops, fisheries and livestock form the backbone of global food production,
essential to feed the ever-growing global population. However, these sectors
face considerable challenges, including climate variability, resource
limitations, and the need for sustainable management. Addressing these issues
requires efficient, accurate, and scalable technological solutions,
highlighting the importance of artificial intelligence (AI). This survey
presents a systematic and thorough review of more than 200 research works
covering conventional machine learning approaches, advanced deep learning
techniques (e.g., vision transformers), and recent vision-language foundation
models (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such
as crop disease detection, livestock health management, and aquatic species
monitoring. We further cover major implementation challenges such as data
variability and experimental aspects: datasets, performance evaluation metrics,
and geographical focus. We finish the survey by discussing potential open
research directions emphasizing the need for multimodal data integration,
efficient edge-device deployment, and domain-adaptable AI models for diverse
farming environments. Rapid growth of evolving developments in this field can
be actively tracked on our project page:
https://github.com/umair1221/AI-in-Agriculture

</details>


### [4] [Color as the Impetus: Transforming Few-Shot Learner](https://arxiv.org/abs/2507.22136)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: The paper introduces the ColorSense Learner, a bio-inspired meta-learning framework leveraging human color perception for few-shot learning, and the ColorSense Distiller for knowledge distillation, achieving strong generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: Humans' innate meta-learning abilities, aided by color perception, inspired the development of a framework to improve few-shot learning by focusing on color-channel interactions, a neglected aspect in conventional methods.

Method: Proposes the ColorSense Learner for inter-channel feature extraction and interactive learning, and the ColorSense Distiller for knowledge distillation, enhancing meta-learning via color perception.

Result: Extensive experiments on eleven benchmarks show strong generalization, robustness, and transferability in few-shot classification tasks.

Conclusion: The framework effectively bridges the gap in meta-learning by leveraging color perception, outperforming conventional methods and demonstrating practical applicability.

Abstract: Humans possess innate meta-learning capabilities, partly attributable to
their exceptional color perception. In this paper, we pioneer an innovative
viewpoint on few-shot learning by simulating human color perception mechanisms.
We propose the ColorSense Learner, a bio-inspired meta-learning framework that
capitalizes on inter-channel feature extraction and interactive learning. By
strategically emphasizing distinct color information across different channels,
our approach effectively filters irrelevant features while capturing
discriminative characteristics. Color information represents the most intuitive
visual feature, yet conventional meta-learning methods have predominantly
neglected this aspect, focusing instead on abstract feature differentiation
across categories. Our framework bridges the gap via synergistic color-channel
interactions, enabling better intra-class commonality extraction and larger
inter-class differences. Furthermore, we introduce a meta-distiller based on
knowledge distillation, ColorSense Distiller, which incorporates prior teacher
knowledge to augment the student network's meta-learning capacity. We've
conducted comprehensive coarse/fine-grained and cross-domain experiments on
eleven few-shot benchmarks for validation. Numerous experiments reveal that our
methods have extremely strong generalization ability, robustness, and
transferability, and effortless handle few-shot classification from the
perspective of color perception.

</details>


### [5] [Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset](https://arxiv.org/abs/2507.22152)
*A. Piffer,J. A. Buchner,A. G. Gennari,P. Grehten,S. Sirin,E. Ross,I. Ezhov,M. Rosier,J. C. Peeken,M. Piraud,B. Menze,A. Guerreiro St√ºcklin,A. Jakab,F. Kofler*

Main category: cs.CV

TL;DR: DL-based segmentation for paediatric brain tumours shows robust performance for whole tumour and T2-hyperintensity, comparable to human variability, but struggles with enhancing tumour and cystic components.


<details>
  <summary>Details</summary>
Motivation: To evaluate the feasibility and performance of deep learning in segmenting diverse paediatric brain tumour subtypes using MRI.

Method: A 3D nnU-Net model was trained on a retrospective cohort of 174 patients with various tumour types, using T1, T1-C, T2, and FLAIR MRI sequences. Performance was assessed via Dice similarity coefficient (DSC).

Result: The model performed well for whole tumour and T2-hyperintensity (mean DSC: 0.85), matching human variability (mean DSC: 0.86), but was less accurate for enhancing tumour (mean DSC: 0.75) and poor for cystic components.

Conclusion: DL is viable for paediatric brain tumour segmentation, especially for T2H and WT, but requires refinement for ET and CC. Simplifying MRI protocols could enhance workflow efficiency.

Abstract: Background Brain tumours are the most common solid malignancies in children,
encompassing diverse histological, molecular subtypes and imaging features and
outcomes. Paediatric brain tumours (PBTs), including high- and low-grade
gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose
diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation
offers promising tools for tumour delineation, yet its performance across
heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A
retrospective single-centre cohort of 174 paediatric patients with HGG, LGG,
medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI
sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual
annotations were provided for four tumour subregions: whole tumour (WT),
T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D
nnU-Net model was trained and tested (121/53 split), with segmentation
performance assessed using the Dice similarity coefficient (DSC) and compared
against intra- and inter-rater variability. Results The model achieved robust
performance for WT and T2H (mean DSC: 0.85), comparable to human annotator
variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean
DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by
tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2
alone produced results nearly equivalent to the full protocol. Conclusions DL
is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and
CC segmentation, highlighting the need for further refinement. These findings
support the potential for protocol simplification and automation to enhance
volumetric assessment and streamline paediatric neuro-oncology workflows.

</details>


### [6] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

TL;DR: Frontier-Seg is a method for temporally consistent unsupervised terrain segmentation in mobile robot video streams, addressing the limitations of costly labeled data and ambiguous environments.


<details>
  <summary>Details</summary>
Motivation: Current terrain-aware navigation relies on expensive labeled data and struggles in unstructured environments with ambiguous semantics. Unsupervised methods lack temporal consistency, crucial for robust perception.

Method: Frontier-Seg clusters superpixel-level features from DINOv2 foundation models and enforces temporal consistency to identify terrain boundaries without human supervision.

Result: Evaluated on RUGD and RELLIS-3D datasets, Frontier-Seg successfully performs unsupervised segmentation in diverse off-road environments.

Conclusion: Frontier-Seg offers a scalable, unsupervised solution for terrain segmentation, overcoming data and consistency challenges in unstructured environments.

Abstract: Rapid progress in terrain-aware autonomous ground navigation has been driven
by advances in supervised semantic segmentation. However, these methods rely on
costly data collection and labor-intensive ground truth labeling to train deep
models. Furthermore, autonomous systems are increasingly deployed in
unrehearsed, unstructured environments where no labeled data exists and
semantic categories may be ambiguous or domain-specific. Recent zero-shot
approaches to unsupervised segmentation have shown promise in such settings but
typically operate on individual frames, lacking temporal consistency-a critical
property for robust perception in unstructured environments. To address this
gap we introduce Frontier-Seg, a method for temporally consistent unsupervised
segmentation of terrain from mobile robot video streams. Frontier-Seg clusters
superpixel-level features extracted from foundation model
backbones-specifically DINOv2-and enforces temporal consistency across frames
to identify persistent terrain boundaries or frontiers without human
supervision. We evaluate Frontier-Seg on a diverse set of benchmark
datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform
unsupervised segmentation across unstructured off-road environments.

</details>


### [7] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

TL;DR: The paper addresses CLIP's limitations in handling misaligned and entangled image-text data by proposing a novel framework, SmartCLIP, for flexible and disentangled cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with misaligned and entangled representations in image-text datasets, limiting its generalization for tasks involving short prompts.

Method: The authors establish theoretical conditions for flexible alignment and introduce SmartCLIP, a modular approach to align relevant visual and textual representations.

Result: SmartCLIP demonstrates superior performance in handling misalignment and disentangling representations across various tasks.

Conclusion: The proposed framework effectively addresses CLIP's limitations, enabling better generalization and fine-grained concept learning.

Abstract: Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}
has emerged as a pivotal model in computer vision and multimodal learning,
achieving state-of-the-art performance at aligning visual and textual
representations through contrastive learning. However, CLIP struggles with
potential information misalignment in many image-text datasets and suffers from
entangled representation. On the one hand, short captions for a single image in
datasets like MSCOCO may describe disjoint regions in the image, leaving the
model uncertain about which visual features to retain or disregard. On the
other hand, directly aligning long captions with images can lead to the
retention of entangled details, preventing the model from learning
disentangled, atomic concepts -- ultimately limiting its generalization on
certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible
alignment between textual and visual representations across varying levels of
granularity. Specifically, our framework ensures that a model can not only
\emph{preserve} cross-modal semantic information in its entirety but also
\emph{disentangle} visual representations to capture fine-grained textual
concepts. Building on this foundation, we introduce \ours, a novel approach
that identifies and aligns the most relevant visual and textual representations
in a modular manner. Superior performance across various tasks demonstrates its
capability to handle information misalignment and supports our identification
theory. The code is available at https://github.com/Mid-Push/SmartCLIP.

</details>


### [8] [HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification](https://arxiv.org/abs/2507.22274)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: The paper proposes HOG-CNN, a hybrid model combining HOG and CNN features for automated retinal disease diagnosis, achieving high performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Manual diagnosis of retinal diseases is time- and resource-intensive, prompting the need for an automated, interpretable solution.

Method: The HOG-CNN model integrates handcrafted HOG features with deep CNN representations to capture local and high-level features from fundus images.

Result: HOG-CNN achieves high accuracy (e.g., 98.5% for binary DR, 92.8% for AMD) and outperforms state-of-the-art models across multiple datasets.

Conclusion: HOG-CNN is a robust, scalable, and interpretable tool suitable for automated retinal disease screening in resource-constrained settings.

Abstract: The analysis of fundus images is critical for the early detection and
diagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and
Age-related Macular Degeneration (AMD). Traditional diagnostic workflows,
however, often depend on manual interpretation and are both time- and
resource-intensive. To address these limitations, we propose an automated and
interpretable clinical decision support framework based on a hybrid feature
extraction model called HOG-CNN. Our key contribution lies in the integration
of handcrafted Histogram of Oriented Gradients (HOG) features with deep
convolutional neural network (CNN) representations. This fusion enables our
model to capture both local texture patterns and high-level semantic features
from retinal fundus images. We evaluated our model on three public benchmark
datasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for
Glaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates
consistently high performance. It achieves 98.5\% accuracy and 99.2 AUC for
binary DR classification, and 94.2 AUC for five-class DR classification. On the
IC-AMD dataset, it attains 92.8\% accuracy, 94.8\% precision, and 94.5 AUC,
outperforming several state-of-the-art models. For Glaucoma detection on ORIGA,
our model achieves 83.9\% accuracy and 87.2 AUC, showing competitive
performance despite dataset limitations. We show, through comprehensive
appendix studies, the complementary strength of combining HOG and CNN features.
The model's lightweight and interpretable design makes it particularly suitable
for deployment in resource-constrained clinical environments. These results
position HOG-CNN as a robust and scalable tool for automated retinal disease
screening.

</details>


### [9] [AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data](https://arxiv.org/abs/2507.22291)
*Christopher F. Brown,Michal R. Kazmierski,Valerie J. Pasquarella,William J. Rucklidge,Masha Samsikova,Chenhui Zhang,Evan Shelhamer,Estefania Lahera,Olivia Wiles,Simon Ilyushchenko,Noel Gorelick,Lihui Lydia Zhang,Sophia Alj,Emily Schechter,Sean Askay,Oliver Guinan,Rebecca Moore,Alexis Boukouvalas,Pushmeet Kohli*

Main category: cs.CV

TL;DR: AlphaEarth Foundations introduces a geospatial embedding model that outperforms existing methods, enabling efficient map production from sparse labels.


<details>
  <summary>Details</summary>
Motivation: High-quality labels for Earth observation data are scarce, requiring significant effort. The paper addresses this by developing a generalizable geospatial representation.

Method: AlphaEarth Foundations uses an embedding field model to assimilate spatial, temporal, and measurement contexts across multiple data sources.

Result: The model consistently outperforms previous featurization methods without re-training and will release a global dataset (2017-2024).

Conclusion: AlphaEarth Foundations provides a scalable solution for accurate mapping and monitoring, addressing label scarcity in Earth observation.

Abstract: Unprecedented volumes of Earth observation data are continually collected
around the world, but high-quality labels remain scarce given the effort
required to make physical measurements and observations. This has led to
considerable investment in bespoke modeling efforts translating sparse labels
into maps. Here we introduce AlphaEarth Foundations, an embedding field model
yielding a highly general, geospatial representation that assimilates spatial,
temporal, and measurement contexts across multiple sources, enabling accurate
and efficient production of maps and monitoring systems from local to global
scales. The embeddings generated by AlphaEarth Foundations are the only to
consistently outperform all previous featurization approaches tested on a
diverse set of mapping evaluations without re-training. We will release a
dataset of global, annual, analysis-ready embedding field layers from 2017
through 2024.

</details>


### [10] [LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction](https://arxiv.org/abs/2507.22316)
*Chi Ding,Qingchao Zhang,Ge Wang,Xiaojing Ye,Yunmei Chen*

Main category: cs.CV

TL;DR: The paper introduces LAMA, a learned alternating minimization algorithm for image reconstruction, and proves its convergence to Clarke stationary points. It also presents LAMA-Net and iLAMA-Net, demonstrating their stability, robustness, and improved performance in experiments.


<details>
  <summary>Details</summary>
Motivation: To address nonconvex and nonsmooth optimization problems in image reconstruction by leveraging complementary information from image and measurement domains.

Method: Proposes LAMA, a learned alternating minimization algorithm with a residual learning architecture, and extends it to LAMA-Net and iLAMA-Net for improved performance.

Result: LAMA converges to Clarke stationary points; LAMA-Net/iLAMA-Net show outstanding stability, robustness, and superior performance in Sparse-View Computed Tomography tasks.

Conclusion: LAMA and its neural network variants (LAMA-Net/iLAMA-Net) are effective, interpretable, and robust for image reconstruction, outperforming state-of-the-art methods.

Abstract: We propose a learnable variational model that learns the features and
leverages complementary information from both image and measurement domains for
image reconstruction. In particular, we introduce a learned alternating
minimization algorithm (LAMA) from our prior work, which tackles two-block
nonconvex and nonsmooth optimization problems by incorporating a residual
learning architecture in a proximal alternating framework. In this work, our
goal is to provide a complete and rigorous convergence proof of LAMA and show
that all accumulation points of a specified subsequence of LAMA must be Clarke
stationary points of the problem. LAMA directly yields a highly interpretable
neural network architecture called LAMA-Net. Notably, in addition to the
results shown in our prior work, we demonstrate that the convergence property
of LAMA yields outstanding stability and robustness of LAMA-Net in this work.
We also show that the performance of LAMA-Net can be further improved by
integrating a properly designed network that generates suitable initials, which
we call iLAMA-Net. To evaluate LAMA-Net/iLAMA-Net, we conduct several
experiments and compare them with several state-of-the-art methods on popular
benchmark datasets for Sparse-View Computed Tomography.

</details>


### [11] [Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment](https://arxiv.org/abs/2507.22321)
*Yuzhen Gao,Qianqian Wang,Yongheng Sun,Cui Wang,Yongquan Liang,Mingxia Liu*

Main category: cs.CV

TL;DR: A Collaborative Domain Adaptation (CDA) framework using Vision Transformer (ViT) and CNN improves late-life depression (LLD) detection from T1-weighted MRIs by addressing domain heterogeneity and limited sample sizes.


<details>
  <summary>Details</summary>
Motivation: Accurate LLD identification is crucial for disease monitoring, but existing methods struggle with small datasets and domain differences.

Method: CDA combines ViT for global context and CNN for local features, involving supervised source training, self-supervised target adaptation, and collaborative training with pseudo-labels.

Result: CDA outperforms state-of-the-art unsupervised domain adaptation methods in multi-site MRI experiments.

Conclusion: The CDA framework effectively enhances LLD detection by improving domain robustness and generalization.

Abstract: Accurate identification of late-life depression (LLD) using structural brain
MRI is essential for monitoring disease progression and facilitating timely
intervention. However, existing learning-based approaches for LLD detection are
often constrained by limited sample sizes (e.g., tens), which poses significant
challenges for reliable model training and generalization. Although
incorporating auxiliary datasets can expand the training set, substantial
domain heterogeneity, such as differences in imaging protocols, scanner
hardware, and population demographics, often undermines cross-domain
transferability. To address this issue, we propose a Collaborative Domain
Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA
leverages a Vision Transformer (ViT) to capture global anatomical context and a
Convolutional Neural Network (CNN) to extract local structural features, with
each branch comprising an encoder and a classifier. The CDA framework consists
of three stages: (a) supervised training on labeled source data, (b)
self-supervised target feature adaptation and (c) collaborative training on
unlabeled target data. We first train ViT and CNN on source data, followed by
self-supervised target feature adaptation by minimizing the discrepancy between
classifier outputs from two branches to make the categorical boundary clearer.
The collaborative training stage employs pseudo-labeled and augmented
target-domain MRIs, enforcing prediction consistency under strong and weak
augmentation to enhance domain robustness and generalization. Extensive
experiments conducted on multi-site T1-weighted MRI data demonstrate that the
CDA consistently outperforms state-of-the-art unsupervised domain adaptation
methods.

</details>


### [12] [UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views](https://arxiv.org/abs/2507.22342)
*Yuki Fujimura,Takahiro Kushida,Kazuya Kitano,Takuya Funatomi,Yasuhiro Mukaigawa*

Main category: cs.CV

TL;DR: A pose-free, feed-forward 3D Gaussian Splatting framework is introduced to handle unfavorable input views, leveraging pretrained models and novel adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: Existing feed-forward 3DGS models are limited to favorable views, restricting real-world applicability. This work aims to overcome this by adapting models for unfavorable views.

Method: The framework uses recentered images, LoRA layers, a Gaussian adapter module, and alignment methods. Training leverages an off-the-shelf dataset of favorable images.

Result: Experiments on synthetic and real datasets show the method effectively handles unfavorable views.

Conclusion: The proposed framework successfully extends 3DGS models to unfavorable views, enhancing their practical utility.

Abstract: This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)
framework designed to handle unfavorable input views. A common rendering setup
for training feed-forward approaches places a 3D object at the world origin and
renders it from cameras pointed toward the origin -- i.e., from favorable
views, limiting the applicability of these models to real-world scenarios
involving varying and unknown camera poses. To overcome this limitation, we
introduce a novel adaptation framework that enables pretrained pose-free
feed-forward 3DGS models to handle unfavorable views. We leverage priors
learned from favorable images by feeding recentered images into a pretrained
model augmented with low-rank adaptation (LoRA) layers. We further propose a
Gaussian adapter module to enhance the geometric consistency of the Gaussians
derived from the recentered inputs, along with a Gaussian alignment method to
render accurate target views for training. Additionally, we introduce a new
training strategy that utilizes an off-the-shelf dataset composed solely of
favorable images. Experimental results on both synthetic images from the Google
Scanned Objects dataset and real images from the OmniObject3D dataset validate
the effectiveness of our method in handling unfavorable input views.

</details>


### [13] [DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception](https://arxiv.org/abs/2507.22346)
*Pei Deng,Wenqian Zhou,Hanlin Wu*

Main category: cs.CV

TL;DR: The paper introduces RSICA, a new paradigm combining change detection and visual question answering for interactive analysis of bi-temporal remote sensing images, supported by the DeltaVLM model and ChangeChat-105k dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods for land-cover change analysis are limited to static outputs, lacking interactive, query-driven capabilities.

Method: Proposes DeltaVLM, an end-to-end architecture with a bi-temporal vision encoder, visual difference perception module, and instruction-guided Q-former, trained on the ChangeChat-105k dataset.

Result: DeltaVLM achieves state-of-the-art performance in single-turn captioning and multi-turn interactive change analysis.

Conclusion: RSICA and DeltaVLM enable advanced, interactive change analysis in remote sensing, outperforming existing models.

Abstract: Accurate interpretation of land-cover changes in multi-temporal satellite
imagery is critical for real-world scenarios. However, existing methods
typically provide only one-shot change masks or static captions, limiting their
ability to support interactive, query-driven analysis. In this work, we
introduce remote sensing image change analysis (RSICA) as a new paradigm that
combines the strengths of change detection and visual question answering to
enable multi-turn, instruction-guided exploration of changes in bi-temporal
remote sensing images. To support this task, we construct ChangeChat-105k, a
large-scale instruction-following dataset, generated through a hybrid
rule-based and GPT-assisted process, covering six interaction types: change
captioning, classification, quantification, localization, open-ended question
answering, and multi-turn dialogues. Building on this dataset, we propose
DeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM
features three innovations: (1) a fine-tuned bi-temporal vision encoder to
capture temporal differences; (2) a visual difference perception module with a
cross-semantic relation measuring (CSRM) mechanism to interpret changes; and
(3) an instruction-guided Q-former to effectively extract query-relevant
difference information from visual changes, aligning them with textual
instructions. We train DeltaVLM on ChangeChat-105k using a frozen large
language model, adapting only the vision and alignment modules to optimize
efficiency. Extensive experiments and ablation studies demonstrate that
DeltaVLM achieves state-of-the-art performance on both single-turn captioning
and multi-turn interactive change analysis, outperforming existing multimodal
large language models and remote sensing vision-language models. Code, dataset
and pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.

</details>


### [14] [FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation](https://arxiv.org/abs/2507.22353)
*Yunseok Oh,Dong-Wan Choi*

Main category: cs.CV

TL;DR: The paper introduces Generalized Face Discovery (GFD), a task combining face identification and generalized category discovery (GCD), and proposes FaceGCD, a dynamic method using lightweight prefixes for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recognizing both labeled and unlabeled known identities while discovering new ones in face recognition, advancing toward artificial general intelligence (AGI).

Method: FaceGCD dynamically constructs instance-specific feature extractors using lightweight, layer-wise prefixes generated by a HyperNetwork, adapting to each input image.

Result: FaceGCD outperforms existing GCD methods and ArcFace, achieving state-of-the-art results on GFD.

Conclusion: FaceGCD advances open-world face recognition by effectively handling high cardinality and fine-grained face IDs, setting a new benchmark for the GFD task.

Abstract: Recognizing and differentiating among both familiar and unfamiliar faces is a
critical capability for face recognition systems and a key step toward
artificial general intelligence (AGI). Motivated by this ability, this paper
introduces generalized face discovery (GFD), a novel open-world face
recognition task that unifies traditional face identification with generalized
category discovery (GCD). GFD requires recognizing both labeled and unlabeled
known identities (IDs) while simultaneously discovering new, previously unseen
IDs. Unlike typical GCD settings, GFD poses unique challenges due to the high
cardinality and fine-grained nature of face IDs, rendering existing GCD
approaches ineffective. To tackle this problem, we propose FaceGCD, a method
that dynamically constructs instance-specific feature extractors using
lightweight, layer-wise prefixes. These prefixes are generated on the fly by a
HyperNetwork, which adaptively outputs a set of prefix generators conditioned
on each input image. This dynamic design enables FaceGCD to capture subtle
identity-specific cues without relying on high-capacity static models.
Extensive experiments demonstrate that FaceGCD significantly outperforms
existing GCD methods and a strong face recognition baseline, ArcFace, achieving
state-of-the-art results on the GFD task and advancing toward open-world face
recognition.

</details>


### [15] [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](https://arxiv.org/abs/2507.22360)
*Kunyang Li,Jeffrey A Chan Santiago,Sarinda Dhanesh Samarasinghe,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

TL;DR: GVD (Guiding Video Diffusion) is a diffusion-based video distillation method that efficiently captures spatial and temporal features, outperforming existing methods on MiniUCF and HMDB51 datasets with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational and storage demands of large video datasets while maintaining comparable training performance.

Method: Proposes GVD, a diffusion-based approach that jointly distills spatial and temporal features for high-fidelity video generation.

Result: Achieves 78.29% of original performance with 1.98% of frames in MiniUCF and 73.83% with 3.30% in HMDB51, outperforming state-of-the-art methods.

Conclusion: GVD sets a new benchmark for video dataset distillation, enabling efficient training with minimal data while preserving performance.

Abstract: To address the larger computation and storage requirements associated with
large video datasets, video dataset distillation aims to capture spatial and
temporal information in a significantly smaller dataset, such that training on
the distilled data has comparable performance to training on all of the data.
We propose GVD: Guiding Video Diffusion, the first diffusion-based video
distillation method. GVD jointly distills spatial and temporal features,
ensuring high-fidelity video generation across diverse actions while capturing
essential motion information. Our method's diverse yet representative
distillations significantly outperform previous state-of-the-art approaches on
the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC).
Specifically, our method achieves 78.29 percent of the original dataset's
performance using only 1.98 percent of the total number of frames in MiniUCF.
Additionally, it reaches 73.83 percent of the performance with just 3.30
percent of the frames in HMDB51. Experimental results across benchmark video
datasets demonstrate that GVD not only achieves state-of-the-art performance
but can also generate higher resolution videos and higher IPC without
significantly increasing computational cost.

</details>


### [16] [Object Recognition Datasets and Challenges: A Review](https://arxiv.org/abs/2507.22361)
*Aria Salari,Abtin Djavadifar,Xiangrui Liu,Homayoun Najjaran*

Main category: cs.CV

TL;DR: A survey analyzing over 160 object recognition datasets, their characteristics, benchmarks, and evaluation metrics, highlighting their importance in advancing computer vision research.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on large, high-quality datasets for deep learning and the need for standardized benchmarks in object recognition research.

Method: Detailed statistical analysis and descriptions of datasets, along with an overview of benchmarks, competitions, and evaluation metrics.

Result: Comprehensive scrutiny of datasets and benchmarks, providing a resource for researchers to understand and utilize them effectively.

Conclusion: The survey emphasizes the critical role of datasets in object recognition research and offers a centralized resource for the community.

Abstract: Object recognition is among the fundamental tasks in the computer vision
applications, paving the path for all other image understanding operations. In
every stage of progress in object recognition research, efforts have been made
to collect and annotate new datasets to match the capacity of the
state-of-the-art algorithms. In recent years, the importance of the size and
quality of datasets has been intensified as the utility of the emerging deep
network techniques heavily relies on training data. Furthermore, datasets lay a
fair benchmarking means for competitions and have proved instrumental to the
advancements of object recognition research by providing quantifiable
benchmarks for the developed models. Taking a closer look at the
characteristics of commonly-used public datasets seems to be an important first
step for data-driven and machine learning researchers. In this survey, we
provide a detailed analysis of datasets in the highly investigated object
recognition areas. More than 160 datasets have been scrutinized through
statistics and descriptions. Additionally, we present an overview of the
prominent object recognition benchmarks and competitions, along with a
description of the metrics widely adopted for evaluation purposes in the
computer vision community. All introduced datasets and challenges can be found
online at github.com/AbtinDjavadifar/ORDC.

</details>


### [17] [Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring](https://arxiv.org/abs/2507.22369)
*Sinh Trong Vu,Hieu Trung Pham,Dung Manh Nguyen,Hieu Minh Hoang,Nhu Hoang Le,Thu Ha Pham,Tai Tan Mai*

Main category: cs.CV

TL;DR: The paper explores using VQA models (LLaMA2, LLaMA3, QWEN3, NVILA) for classroom behavior analysis, introduces the BAV-Classroom-VQA dataset, and shows promising results.


<details>
  <summary>Details</summary>
Motivation: Classroom behavior monitoring is vital for student engagement and learning outcomes, and VQA models offer automated analysis tools.

Method: Evaluates state-of-the-art VQA models on the BAV-Classroom-VQA dataset, detailing data collection and annotation.

Result: All four models perform well in answering behavior-related visual questions.

Conclusion: VQA models show potential for classroom analytics and intervention systems.

Abstract: Classroom behavior monitoring is a critical aspect of educational research,
with significant implications for student engagement and learning outcomes.
Recent advancements in Visual Question Answering (VQA) models offer promising
tools for automatically analyzing complex classroom interactions from video
recordings. In this paper, we investigate the applicability of several
state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and
NVILA, in the context of classroom behavior analysis. To facilitate rigorous
evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world
classroom video recordings at the Banking Academy of Vietnam. We present the
methodology for data collection, annotation, and benchmark the performance of
the selected VQA models on this dataset. Our initial experimental results
demonstrate that all four models achieve promising performance levels in
answering behavior-related visual questions, showcasing their potential in
future classroom analytics and intervention systems.

</details>


### [18] [Gems: Group Emotion Profiling Through Multimodal Situational Understanding](https://arxiv.org/abs/2507.22393)
*Anubhav Kataria,Surbhi Madan,Shreya Ghosh,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

TL;DR: GEMS introduces a multimodal framework for predicting fine-grained individual to coarse-grained group and event-level emotions, leveraging swin-transformer and S3Attention. It extends VGAF-GEMS for holistic analysis and outperforms state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Understanding multi-person social situations requires analyzing emotions at individual, group, and event levels with contextual information. Existing benchmarks lack fine-grained and holistic analysis.

Method: GEMS uses a multimodal swin-transformer and S3Attention architecture to process scenes, group members, and context for joint emotion predictions. It extends VGAF-GEMS for finer annotations.

Result: GEMS effectively predicts discrete/continuous emotions (valence, arousal) at individual, group, and event levels, outperforming adapted state-of-the-art models.

Conclusion: GEMS advances multi-person emotion analysis holistically and paves the way for future research. Code and data are publicly available.

Abstract: Understanding individual, group and event level emotions along with
contextual information is crucial for analyzing a multi-person social
situation. To achieve this, we frame emotion comprehension as the task of
predicting fine-grained individual emotion to coarse grained group and event
level emotion. We introduce GEMS that leverages a multimodal swin-transformer
and S3Attention based architecture, which processes an input scene, group
members, and context information to generate joint predictions. Existing
multi-person emotion related benchmarks mainly focus on atomic interactions
primarily based on emotion perception over time and group level. To this end,
we extend and propose VGAF-GEMS to provide more fine grained and holistic
analysis on top of existing group level annotation of VGAF dataset. GEMS aims
to predict basic discrete and continuous emotions (including valence and
arousal) as well as individual, group and event level perceived emotions. Our
benchmarking effort links individual, group and situational emotional responses
holistically. The quantitative and qualitative comparisons with adapted
state-of-the-art models demonstrate the effectiveness of GEMS framework on
VGAF-GEMS benchmarking. We believe that it will pave the way of further
research. The code and data is available at:
https://github.com/katariaak579/GEMS

</details>


### [19] [On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations](https://arxiv.org/abs/2507.22398)
*Jordan Vice,Naveed Akhtar,Yansong Gao,Richard Hartley,Ajmal Mian*

Main category: cs.CV

TL;DR: VLMs are vulnerable to subtle frequency-domain perturbations, affecting tasks like DeepFake detection and captioning, revealing their unreliability.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in VLMs when exposed to frequency-domain perturbations, undermining their reliability in critical tasks.

Method: Designed targeted frequency-domain image transformations to perturb VLM outputs, tested across five state-of-the-art VLMs and ten datasets.

Result: VLMs are sensitive to frequency cues, often misaligning with semantic content, and fail under imperceptible perturbations.

Conclusion: VLMs' fragility in real-world tasks highlights the need for more robust multimodal perception systems.

Abstract: Vision-Language Models (VLMs) are increasingly used as perceptual modules for
visual content reasoning, including through captioning and DeepFake detection.
In this work, we expose a critical vulnerability of VLMs when exposed to
subtle, structured perturbations in the frequency domain. Specifically, we
highlight how these feature transformations undermine authenticity/DeepFake
detection and automated image captioning tasks. We design targeted image
transformations, operating in the frequency domain to systematically adjust VLM
outputs when exposed to frequency-perturbed real and synthetic images. We
demonstrate that the perturbation injection method generalizes across five
state-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP
models. Experimenting across ten real and generated image datasets reveals that
VLM judgments are sensitive to frequency-based cues and may not wholly align
with semantic content. Crucially, we show that visually-imperceptible spatial
frequency transformations expose the fragility of VLMs deployed for automated
image captioning and authenticity detection tasks. Our findings under
realistic, black-box constraints challenge the reliability of VLMs,
underscoring the need for robust multimodal perception systems.

</details>


### [20] [MINR: Implicit Neural Representations with Masked Image Modelling](https://arxiv.org/abs/2507.22404)
*Sua Lee,Joonhun Lee,Myungjoo Kang*

Main category: cs.CV

TL;DR: MINR combines implicit neural representations with masked image modeling for robust, generalizable image reconstructions, outperforming MAE in both in-domain and out-of-distribution settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of MAE, such as dependency on masking strategies and degraded performance on out-of-distribution data.

Method: Introduce MINR, a framework integrating implicit neural representations with masked image modeling to learn continuous image functions.

Result: MINR outperforms MAE in in-domain and out-of-distribution scenarios, reduces model complexity, and is versatile for self-supervised learning.

Conclusion: MINR is a robust, efficient alternative to existing frameworks like MAE.

Abstract: Self-supervised learning methods like masked autoencoders (MAE) have shown
significant promise in learning robust feature representations, particularly in
image reconstruction-based pretraining task. However, their performance is
often strongly dependent on the masking strategies used during training and can
degrade when applied to out-of-distribution data. To address these limitations,
we introduce the masked implicit neural representations (MINR) framework that
synergizes implicit neural representations with masked image modeling. MINR
learns a continuous function to represent images, enabling more robust and
generalizable reconstructions irrespective of masking strategies. Our
experiments demonstrate that MINR not only outperforms MAE in in-domain
scenarios but also in out-of-distribution settings, while reducing model
complexity. The versatility of MINR extends to various self-supervised learning
applications, confirming its utility as a robust and efficient alternative to
existing frameworks.

</details>


### [21] [Moir√© Zero: An Efficient and High-Performance Neural Architecture for Moir√© Removal](https://arxiv.org/abs/2507.22407)
*Seungryong Lee,Woojeong Baek,Younghyun Kim,Eunwoo Kim,Haru Moon,Donggon Yoo,Eunbyung Park*

Main category: cs.CV

TL;DR: MZNet, a U-shaped network, effectively removes moir√© patterns using multi-scale and multi-shape components, achieving state-of-the-art performance with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Moir√© patterns hinder applications like photography and defect inspection; existing CNN-based methods struggle due to limited receptive fields.

Method: MZNet integrates Multi-Scale Dual Attention Block (MSDAB), Multi-Shape Large Kernel Convolution Block (MSLKB), and Feature Fusion-Based Skip Connection.

Result: MZNet outperforms on high-resolution datasets and is competitive on lower-resolution ones, with low computational cost.

Conclusion: MZNet is an efficient, practical solution for moir√© pattern removal in real-world applications.

Abstract: Moir\'e patterns, caused by frequency aliasing between fine repetitive
structures and a camera sensor's sampling process, have been a significant
obstacle in various real-world applications, such as consumer photography and
industrial defect inspection. With the advancements in deep learning
algorithms, numerous studies-predominantly based on convolutional neural
networks-have suggested various solutions to address this issue. Despite these
efforts, existing approaches still struggle to effectively eliminate artifacts
due to the diverse scales, orientations, and color shifts of moir\'e patterns,
primarily because the constrained receptive field of CNN-based architectures
limits their ability to capture the complex characteristics of moir\'e
patterns. In this paper, we propose MZNet, a U-shaped network designed to bring
images closer to a 'Moire-Zero' state by effectively removing moir\'e patterns.
It integrates three specialized components: Multi-Scale Dual Attention Block
(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large
Kernel Convolution Block (MSLKB) for capturing diverse moir\'e structures, and
Feature Fusion-Based Skip Connection for enhancing information flow. Together,
these components enhance local texture restoration and large-scale artifact
suppression. Experiments on benchmark datasets demonstrate that MZNet achieves
state-of-the-art performance on high-resolution datasets and delivers
competitive results on lower-resolution dataset, while maintaining a low
computational cost, suggesting that it is an efficient and practical solution
for real-world applications. Project page:
https://sngryonglee.github.io/MoireZero

</details>


### [22] [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/abs/2507.22412)
*Sijie Wang,Siqi Li,Yawei Zhang,Shangshu Yu,Shenghai Yuan,Rui She,Quanjiang Guo,JinXuan Zheng,Ong Kang Howe,Leonrich Chandra,Shrivarshann Srijeyan,Aditya Sivadas,Toshan Aggarwal,Heyuan Liu,Hongming Zhang,Chujie Chen,Junyu Jiang,Lihua Xie,Wee Peng Tay*

Main category: cs.CV

TL;DR: UAVScenes is a large-scale multi-modal UAV dataset addressing the lack of frame-wise annotations for high-level scene understanding tasks, enhancing the MARS-LVIG dataset with semantic labels and 6-DoF poses.


<details>
  <summary>Details</summary>
Motivation: Existing UAV datasets are biased toward localization and 3D reconstruction or lack detailed annotations, limiting their use for advanced scene understanding.

Method: UAVScenes enhances the MARS-LVIG dataset by adding manual semantic annotations for images and LiDAR point clouds, along with 6-DoF poses.

Result: The dataset supports tasks like segmentation, depth estimation, 6-DoF localization, place recognition, and novel view synthesis.

Conclusion: UAVScenes fills a critical gap in multi-modal UAV perception, enabling diverse high-level tasks and advancing research in the field.

Abstract: Multi-modal perception is essential for unmanned aerial vehicle (UAV)
operations, as it enables a comprehensive understanding of the UAVs'
surrounding environment. However, most existing multi-modal UAV datasets are
primarily biased toward localization and 3D reconstruction tasks, or only
support map-level semantic segmentation due to the lack of frame-wise
annotations for both camera images and LiDAR point clouds. This limitation
prevents them from being used for high-level scene understanding tasks. To
address this gap and advance multi-modal UAV perception, we introduce
UAVScenes, a large-scale dataset designed to benchmark various tasks across
both 2D and 3D modalities. Our benchmark dataset is built upon the
well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only
for simultaneous localization and mapping (SLAM). We enhance this dataset by
providing manually labeled semantic annotations for both frame-wise images and
LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.
These additions enable a wide range of UAV perception tasks, including
segmentation, depth estimation, 6-DoF localization, place recognition, and
novel view synthesis (NVS). Our dataset is available at
https://github.com/sijieaaa/UAVScenes

</details>


### [23] [Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching](https://arxiv.org/abs/2507.22418)
*Phi Van Nguyen,Ngoc Huynh Trinh,Duy Minh Lam Nguyen,Phu Loc Nguyen,Quoc Long Tran*

Main category: cs.CV

TL;DR: The paper proposes a method using conditional flow matching to quantify aleatoric uncertainty in medical image segmentation, outperforming current diffusion-based approaches.


<details>
  <summary>Details</summary>
Motivation: Aleatoric uncertainty in medical image segmentation reflects natural variability among expert annotators, but current methods like diffusion-based models have limitations in accurately capturing this uncertainty.

Method: The method leverages conditional flow matching, a simulation-free flow-based generative model, to learn exact densities and generate segmentation samples whose variance reflects the underlying data distribution.

Result: The approach achieves competitive segmentation accuracy and produces uncertainty maps that mirror inter-annotator differences, especially in ambiguous boundary regions.

Conclusion: The proposed method effectively quantifies uncertainty and provides deeper insights into segmentation reliability, with code available for public use.

Abstract: Quantifying aleatoric uncertainty in medical image segmentation is critical
since it is a reflection of the natural variability observed among expert
annotators. A conventional approach is to model the segmentation distribution
using the generative model, but current methods limit the expression ability of
generative models. While current diffusion-based approaches have demonstrated
impressive performance in approximating the data distribution, their inherent
stochastic sampling process and inability to model exact densities limit their
effectiveness in accurately capturing uncertainty. In contrast, our proposed
method leverages conditional flow matching, a simulation-free flow-based
generative model that learns an exact density, to produce highly accurate
segmentation results. By guiding the flow model on the input image and sampling
multiple data points, our approach synthesizes segmentation samples whose
pixel-wise variance reliably reflects the underlying data distribution. This
sampling strategy captures uncertainties in regions with ambiguous boundaries,
offering robust quantification that mirrors inter-annotator differences.
Experimental results demonstrate that our method not only achieves competitive
segmentation accuracy but also generates uncertainty maps that provide deeper
insights into the reliability of the segmentation outcomes. The code for this
paper is freely available at https://github.com/huynhspm/Data-Uncertainty

</details>


### [24] [Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking](https://arxiv.org/abs/2507.22421)
*Shahla John*

Main category: cs.CV

TL;DR: A unified framework for real-time video analysis improves action recognition and object tracking with faster inference.


<details>
  <summary>Details</summary>
Motivation: Balancing accuracy and speed in real-time video analysis is challenging, especially in resource-constrained environments.

Method: Leverages spatial-temporal modeling with a hierarchical attention mechanism for adaptive focus on relevant regions.

Result: Achieves 3.2% higher action recognition accuracy, 2.8% better tracking precision, and 40% faster inference.

Conclusion: The method outperforms existing approaches in accuracy and speed, making it suitable for real-time applications.

Abstract: Real-time video analysis remains a challenging problem in computer vision,
requiring efficient processing of both spatial and temporal information while
maintaining computational efficiency. Existing approaches often struggle to
balance accuracy and speed, particularly in resource-constrained environments.
In this work, we present a unified framework that leverages advanced
spatial-temporal modeling techniques for simultaneous action recognition and
object tracking. Our approach builds upon recent advances in parallel sequence
modeling and introduces a novel hierarchical attention mechanism that
adaptively focuses on relevant spatial regions across temporal sequences. We
demonstrate that our method achieves state-of-the-art performance on standard
benchmarks while maintaining real-time inference speeds. Extensive experiments
on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action
recognition accuracy and 2.8% in tracking precision compared to existing
methods, with 40% faster inference time.

</details>


### [25] [HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://arxiv.org/abs/2507.22431)
*Zhixiang Wei,Guangting Wang,Xiaoxiao Ma,Ke Mei,Huaian Chen,Yi Jin,Fengyun Rao*

Main category: cs.CV

TL;DR: The paper introduces a data refinement pipeline using LVLMs to enhance image-text pair quality, leading to improved CLIP models like HQ-CLIP, which outperforms standard CLIP on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore whether LVLMs can reciprocally improve image-text pair data quality, enabling a self-reinforcing cycle for continuous enhancement.

Method: An LVLM-driven pipeline generates multi-grained textual annotations (positive/negative descriptions and tags) for refining datasets. A training paradigm extends contrastive learning with these annotations.

Result: HQ-CLIP achieves state-of-the-art performance in zero-shot classification, cross-modal retrieval, and fine-grained tasks, surpassing CLIP models trained on larger datasets.

Conclusion: The proposed pipeline and training paradigm effectively enhance CLIP models, demonstrating the potential for iterative improvement in vision-language tasks.

Abstract: Large-scale but noisy image-text pair data have paved the way for the success
of Contrastive Language-Image Pretraining (CLIP). As the foundation vision
encoder, CLIP in turn serves as the cornerstone for most large vision-language
models (LVLMs). This interdependence naturally raises an interesting question:
Can we reciprocally leverage LVLMs to enhance the quality of image-text pair
data, thereby opening the possibility of a self-reinforcing cycle for
continuous improvement? In this work, we take a significant step toward this
vision by introducing an LVLM-driven data refinement pipeline. Our framework
leverages LVLMs to process images and their raw alt-text, generating four
complementary textual formulas: long positive descriptions, long negative
descriptions, short positive tags, and short negative tags. Applying this
pipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset
enriched with multi-grained annotations. Based on this dataset, we further
propose a training paradigm that extends conventional contrastive learning by
incorporating negative descriptions and short tags as additional supervised
signals. The resulting model, namely HQ-CLIP, demonstrates remarkable
improvements across diverse benchmarks. Within a comparable training data
scale, our approach achieves state-of-the-art performance in zero-shot
classification, cross-modal retrieval, and fine-grained visual understanding
tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models
trained on the DFN-2B dataset, which contains 10$\times$ more training data
than ours. All code, data, and models are available at
https://zxwei.site/hqclip.

</details>


### [26] [From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](https://arxiv.org/abs/2507.22438)
*Youngho Kim,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: A novel domain adaptation approach using event cameras to bridge the gap between sharp and blurred images for robust human pose estimation under motion blur.


<details>
  <summary>Details</summary>
Motivation: Motion blur in rapid motion or low-light conditions degrades pose estimation, and existing datasets lack blurred images, creating a domain gap.

Method: Leverages event cameras for high temporal resolution data, uses event-based augmentation to generate blurred images, and employs a student-teacher framework with mutual uncertainty masking for pseudo-label refinement.

Result: Outperforms conventional domain-adaptive methods, achieving robust pose estimation in blurred environments without target domain annotations.

Conclusion: Event cameras offer a scalable solution for domain adaptation in motion blur, with potential for real-world applications.

Abstract: Human pose estimation is critical for applications such as rehabilitation,
sports analytics, and AR/VR systems. However, rapid motion and low-light
conditions often introduce motion blur, significantly degrading pose estimation
due to the domain gap between sharp and blurred images. Most datasets assume
stable conditions, making models trained on sharp images struggle in blurred
environments. To address this, we introduce a novel domain adaptation approach
that leverages event cameras, which capture high temporal resolution motion
data and are inherently robust to motion blur. Using event-based augmentation,
we generate motion-aware blurred images, effectively bridging the domain gap
between sharp and blurred domains without requiring paired annotations.
Additionally, we develop a student-teacher framework that iteratively refines
pseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect
labels and enable more effective learning. Experimental results demonstrate
that our approach outperforms conventional domain-adaptive human pose
estimation methods, achieving robust pose estimation under motion blur without
requiring annotations in the target domain. Our findings highlight the
potential of event cameras as a scalable and effective solution for domain
adaptation in real-world motion blur environments. Our project codes are
available at https://github.com/kmax2001/EvSharp2Blur.

</details>


### [27] [TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2507.22454)
*Jiuming Liu,Zheng Huang,Mengmeng Liu,Tianchen Deng,Francesco Nex,Hao Cheng,Hesheng Wang*

Main category: cs.CV

TL;DR: TopoLiDM integrates GNNs with diffusion models for high-fidelity LiDAR scene generation, addressing geometric realism and topological consistency.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR generation methods lack geometric realism and global topological consistency, limiting their effectiveness.

Method: TopoLiDM combines a topological-preserving VAE with latent diffusion models, using GNNs and 0-dimensional persistent homology constraints.

Result: TopoLiDM outperforms state-of-the-art methods with 22.6% lower FRID and 9.2% lower MMD, achieving fast generation speed (1.68 samples/s).

Conclusion: TopoLiDM offers a scalable, efficient solution for realistic LiDAR scene generation, with potential for real-world applications.

Abstract: LiDAR scene generation is critical for mitigating real-world LiDAR data
collection costs and enhancing the robustness of downstream perception tasks in
autonomous driving. However, existing methods commonly struggle to capture
geometric realism and global topological consistency. Recent LiDAR Diffusion
Models (LiDMs) predominantly embed LiDAR points into the latent space for
improved generation efficiency, which limits their interpretable ability to
model detailed geometric structures and preserve global topological
consistency. To address these challenges, we propose TopoLiDM, a novel
framework that integrates graph neural networks (GNNs) with diffusion models
under topological regularization for high-fidelity LiDAR generation. Our
approach first trains a topological-preserving VAE to extract latent graph
representations by graph construction and multiple graph convolutional layers.
Then we freeze the VAE and generate novel latent topological graphs through the
latent diffusion models. We also introduce 0-dimensional persistent homology
(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world
global topological structures. Extensive experiments on the KITTI-360 dataset
demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving
improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower
Minimum Matching Distance (MMD). Notably, our model also enables fast
generation speed with an average inference time of 1.68 samples/s, showcasing
its scalability for real-world applications. We will release the related codes
at https://github.com/IRMVLab/TopoLiDM.

</details>


### [28] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: EDTR leverages diffusion prior to restore task-relevant details in degraded images, improving both task performance and visual quality.


<details>
  <summary>Details</summary>
Motivation: Addressing performance drops in high-level vision tasks due to low-quality inputs and the limitations of existing TDIR methods in handling complex degradations.

Method: Proposes EDTR, which uses diffusion prior by generating from pre-restored LQ images with mild noise and employs few denoising steps to avoid redundant details.

Result: Significantly enhances task performance and visual quality across diverse tasks with complex degradations.

Conclusion: EDTR effectively harnesses diffusion prior for TDIR, outperforming previous methods in practical scenarios.

Abstract: Task-driven image restoration (TDIR) has recently emerged to address
performance drops in high-level vision tasks caused by low-quality (LQ) inputs.
Previous TDIR methods struggle to handle practical scenarios in which images
are degraded by multiple complex factors, leaving minimal clues for
restoration. This motivates us to leverage the diffusion prior, one of the most
powerful natural image priors. However, while the diffusion prior can help
generate visually plausible results, using it to restore task-relevant details
remains challenging, even when combined with recent TDIR methods. To address
this, we propose EDTR, which effectively harnesses the power of diffusion prior
to restore task-relevant details. Specifically, we propose directly leveraging
useful clues from LQ images in the diffusion process by generating from
pixel-error-based pre-restored LQ images with mild noise added. Moreover, we
employ a small number of denoising steps to prevent the generation of redundant
details that dilute crucial task-related information. We demonstrate that our
method effectively utilizes diffusion prior for TDIR, significantly enhancing
task performance and visual quality across diverse tasks with multiple complex
degradations.

</details>


### [29] [Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.22465)
*Zheng Xiangyu,He Songcheng,Li Wanyun,Li Xiaoqiang,Zhang Wei*

Main category: cs.CV

TL;DR: The paper introduces HMHI-Net, a hierarchical memory architecture for UVOS, addressing the flaw of over-reliance on high-level semantic features by incorporating both shallow- and high-level features and proposing a heterogeneous interaction mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing UVOS methods rely too heavily on high-level semantic features, lacking fine-grained details due to no prior pixel-level annotations. This limits performance despite sophisticated memory designs.

Method: Proposes HMHI-Net with a hierarchical memory architecture (shallow- and high-level features) and a heterogeneous interaction mechanism (PLAM and SGIM modules) to balance pixel and semantic information.

Result: HMHI-Net achieves state-of-the-art performance on UVOS and video saliency benchmarks, demonstrating robustness across different backbones.

Conclusion: The hierarchical memory and heterogeneous interaction design effectively addresses the limitations of existing UVOS methods, leading to superior and robust performance.

Abstract: Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level
masks for the most salient objects in videos without any prior annotations.
While memory mechanisms have been proven critical in various video segmentation
paradigms, their application in UVOS yield only marginal performance gains
despite sophisticated design. Our analysis reveals a simple but fundamental
flaw in existing methods: over-reliance on memorizing high-level semantic
features. UVOS inherently suffers from the deficiency of lacking fine-grained
information due to the absence of pixel-level prior knowledge. Consequently,
memory design relying solely on high-level features, which predominantly
capture abstract semantic cues, is insufficient to generate precise
predictions. To resolve this fundamental issue, we propose a novel hierarchical
memory architecture to incorporate both shallow- and high-level features for
memory, which leverages the complementary benefits of pixel and semantic
information. Furthermore, to balance the simultaneous utilization of the pixel
and semantic memory features, we propose a heterogeneous interaction mechanism
to perform pixel-semantic mutual interactions, which explicitly considers their
inherent feature discrepancies. Through the design of Pixel-guided Local
Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM),
we achieve delicate integration of the fine-grained details in shallow-level
memory and the semantic representations in high-level memory. Our Hierarchical
Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves
state-of-the-art performance across all UVOS and video saliency detection
benchmarks. Moreover, HMHI-Net consistently exhibits high performance across
different backbones, further demonstrating its superiority and robustness.
Project page: https://github.com/ZhengxyFlow/HMHI-Net .

</details>


### [30] [Visual Language Models as Zero-Shot Deepfake Detectors](https://arxiv.org/abs/2507.22469)
*Viacheslav Pirogov*

Main category: cs.CV

TL;DR: The paper proposes a VLM-based zero-shot approach for deepfake detection, outperforming traditional methods on a high-quality dataset and the DFDC-P dataset.


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose a growing threat, and existing detection methods lack robustness by focusing solely on image classification without auxiliary tasks.

Method: Utilizes Vision Language Models (VLMs) for zero-shot deepfake detection, tested on a 60,000-image dataset and DFDC-P, comparing zero-shot and fine-tuned performance.

Result: VLMs, especially InstructBLIP, outperform traditional classifiers in both zero-shot and fine-tuned scenarios.

Conclusion: VLMs offer a superior alternative to traditional deepfake detection methods, leveraging zero-shot capabilities for robustness.

Abstract: The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models
for face swapping, presents a substantial and evolving threat in digital media,
identity verification, and a multitude of other systems. The majority of
existing methods for detecting deepfakes rely on training specialized
classifiers to distinguish between genuine and manipulated images, focusing
only on the image domain without incorporating any auxiliary tasks that could
enhance robustness. In this paper, inspired by the zero-shot capabilities of
Vision Language Models, we propose a novel VLM-based approach to image
classification and then evaluate it for deepfake detection. Specifically, we
utilize a new high-quality deepfake dataset comprising 60,000 images, on which
our zero-shot models demonstrate superior performance to almost all existing
methods. Subsequently, we compare the performance of the best-performing
architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against
traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our
results demonstrate the superiority of VLMs over traditional classifiers.

</details>


### [31] [LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks](https://arxiv.org/abs/2507.22477)
*Hui Liu,Chen Jia,Fan Shi,Xu Cheng,Mengfei Shi,Xia Xie,Shengyong Chen*

Main category: cs.CV

TL;DR: A lightweight network (LIDAR) for crack segmentation efficiently fuses multimodal data, outperforming SOTA methods with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with adaptive perception and efficient fusion of cross-modal features in crack segmentation.

Method: LIDAR combines LacaVSS (adaptive cue modeling) and LD3CF (dynamic fusion of spatial/frequency cues) with LDMK for efficient computation.

Result: Achieves 0.8204 F1 and 0.8465 mIoU on a dataset with only 5.35M parameters.

Conclusion: LIDAR is effective for pixel-level crack segmentation with low computational overhead.

Abstract: Achieving pixel-level segmentation with low computational cost using
multimodal data remains a key challenge in crack segmentation tasks. Existing
methods lack the capability for adaptive perception and efficient interactive
fusion of cross-modal features. To address these challenges, we propose a
Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently
perceives and integrates morphological and textural cues from different
modalities under multimodal crack scenarios, generating clear pixel-level crack
segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive
Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain
Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack
cues through the proposed mask-guided Efficient Dynamic Guided Scanning
Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain
Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture
spatial and frequency-domain cues across modalities. Moreover, we design a
Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive
complex morphological structures with minimal computational overhead, replacing
most convolutional operations in LIDAR. Experiments on three datasets
demonstrate that our method outperforms other state-of-the-art (SOTA) methods.
On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465
in mIoU with only 5.35M parameters. Code and datasets are available at
https://github.com/Karl1109/LIDAR-Mamba.

</details>


### [32] [Estimating 2D Camera Motion with Hybrid Motion Basis](https://arxiv.org/abs/2507.22480)
*Haipeng Li,Tianhao Zhou,Zhanglei Yang,Yi Wu,Yan Chen,Zijing Mao,Shen Cheng,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: CamFlow introduces a hybrid framework combining physical and stochastic motion bases for robust 2D camera motion estimation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for 2D camera motion estimation are limited by planar scene assumptions or struggle with complex transformations.

Method: CamFlow uses hybrid motion bases (physical and stochastic) and a Laplace-based probabilistic loss for robust training.

Result: CamFlow outperforms existing methods, showing superior robustness and generalization in zero-shot settings.

Conclusion: The hybrid approach of CamFlow effectively addresses limitations of current methods, validated by a new benchmark and experiments.

Abstract: Estimating 2D camera motion is a fundamental computer vision task that models
the projection of 3D camera movements onto the 2D image plane. Current methods
rely on either homography-based approaches, limited to planar scenes, or
meshflow techniques that use grid-based local homographies but struggle with
complex non-linear transformations. A key insight of our work is that combining
flow fields from different homographies creates motion patterns that cannot be
represented by any single homography. We introduce CamFlow, a novel framework
that represents camera motion using hybrid motion bases: physical bases derived
from camera geometry and stochastic bases for complex scenarios. Our approach
includes a hybrid probabilistic loss function based on the Laplace distribution
that enhances training robustness. For evaluation, we create a new benchmark by
masking dynamic objects in existing optical flow datasets to isolate pure
camera motion. Experiments show CamFlow outperforms state-of-the-art methods
across diverse scenarios, demonstrating superior robustness and generalization
in zero-shot settings. Code and datasets are available at our project page:
https://lhaippp.github.io/CamFlow/.

</details>


### [33] [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/abs/2507.22498)
*Yuhwan Jeong,Yunseo Yang,Youngjo Yoon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: SSGformer uses spectral decomposition and group-wise attention for multi-weather image restoration, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Adverse weather causes complex degradation patterns, and current AiO models struggle with localized distortions.

Method: Decomposes images into high/low-frequency features, uses multi-head linear attention, and introduces a grouping-mask with group-wise attention.

Result: Superior performance in handling diverse adverse weather degradations.

Conclusion: SSGformer effectively addresses varied weather distortions through spectral and spatial grouping techniques.

Abstract: Adverse weather conditions cause diverse and complex degradation patterns,
driving the development of All-in-One (AiO) models. However, recent AiO
solutions still struggle to capture diverse degradations, since global
filtering methods like direct operations on the frequency domain fail to handle
highly variable and localized distortions. To address these issue, we propose
Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that
leverages spectral decomposition and group-wise attention for multi-weather
image restoration. SSGformer decomposes images into high-frequency edge
features using conventional edge detection and low-frequency information via
Singular Value Decomposition. We utilize multi-head linear attention to
effectively model the relationship between these features. The fused features
are integrated with the input to generate a grouping-mask that clusters regions
based on the spatial similarity and image texture. To fully leverage this mask,
we introduce a group-wise attention mechanism, enabling robust adverse weather
removal and ensuring consistent performance across diverse weather conditions.
We also propose a Spatial Grouping Transformer Block that uses both channel
attention and spatial attention, effectively balancing feature-wise
relationships and spatial dependencies. Extensive experiments show the
superiority of our approach, validating its effectiveness in handling the
varied and intricate adverse weather degradations.

</details>


### [34] [DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement](https://arxiv.org/abs/2507.22501)
*Chang Huang,Jiahang Cao,Jun Ma,Kieren Yu,Cong Li,Huayong Yang,Kaishun Wu*

Main category: cs.CV

TL;DR: A degradation-aware conditional diffusion model is proposed to enhance underwater images by predicting degradation levels and using adaptive noise scheduling and feature refinement.


<details>
  <summary>Details</summary>
Motivation: Underwater images suffer from colour distortions, low visibility, and structural clarity issues due to scattering and absorption, limiting their usability in visual tasks. Existing methods fail to adapt to diverse degradation conditions or leverage underwater-specific priors effectively.

Method: The method involves predicting degradation levels with a dual-stream convolutional network, followed by a conditional diffusion-based restoration network with a Swin UNet backbone. It includes adaptive feature fusion and a hybrid loss function.

Result: The method restores underwater images with superior colour fidelity, perceptual quality, and structural details, outperforming state-of-the-art approaches in quantitative and qualitative assessments.

Conclusion: The proposed framework effectively addresses underwater image degradation, offering adaptive and robust enhancement with significant improvements over existing methods.

Abstract: Underwater images typically suffer from severe colour distortions, low
visibility, and reduced structural clarity due to complex optical effects such
as scattering and absorption, which greatly degrade their visual quality and
limit the performance of downstream visual perception tasks. Existing
enhancement methods often struggle to adaptively handle diverse degradation
conditions and fail to leverage underwater-specific physical priors
effectively. In this paper, we propose a degradation-aware conditional
diffusion model to enhance underwater images adaptively and robustly. Given a
degraded underwater image as input, we first predict its degradation level
using a lightweight dual-stream convolutional network, generating a continuous
degradation score as semantic guidance. Based on this score, we introduce a
novel conditional diffusion-based restoration network with a Swin UNet
backbone, enabling adaptive noise scheduling and hierarchical feature
refinement. To incorporate underwater-specific physical priors, we further
propose a degradation-guided adaptive feature fusion module and a hybrid loss
function that combines perceptual consistency, histogram matching, and
feature-level contrast. Comprehensive experiments on benchmark datasets
demonstrate that our method effectively restores underwater images with
superior colour fidelity, perceptual quality, and structural details. Compared
with SOTA approaches, our framework achieves significant improvements in both
quantitative metrics and qualitative visual assessments.

</details>


### [35] [AlphaDent: A dataset for automated tooth pathology detection](https://arxiv.org/abs/2507.22512)
*Evgeniy I. Sosnin,Yuriy L. Vasilev,Roman A. Solovyev,Aleksandr L. Stempkovskiy,Dmitry V. Telpukhov,Artem A. Vasilev,Aleksandr A. Amerikanov,Aleksandr Y. Romanov*

Main category: cs.CV

TL;DR: A new dental dataset, AlphaDent, with 1200+ DSLR images from 295 patients, labeled for instance segmentation into 9 classes, is introduced. The paper details the dataset, labeling, and successful neural network training results, with open access to data and code.


<details>
  <summary>Details</summary>
Motivation: To provide a high-quality, openly accessible dental dataset for advancing research in instance segmentation, addressing the lack of such resources in dental research.

Method: The dataset (AlphaDent) was created using DSLR camera photographs of teeth from 295 patients, labeled for instance segmentation into 9 classes. Neural networks were trained on this dataset to solve the instance segmentation problem.

Result: High-quality predictions were achieved in the instance segmentation experiments, demonstrating the dataset's effectiveness.

Conclusion: AlphaDent is a valuable, openly available resource for dental research, with proven utility in instance segmentation tasks.

Abstract: In this article, we present a new unique dataset for dental research -
AlphaDent. This dataset is based on the DSLR camera photographs of the teeth of
295 patients and contains over 1200 images. The dataset is labeled for solving
the instance segmentation problem and is divided into 9 classes. The article
provides a detailed description of the dataset and the labeling format. The
article also provides the details of the experiment on neural network training
for the Instance Segmentation problem using this dataset. The results obtained
show high quality of predictions. The dataset is published under an open
license; and the training/inference code and model weights are also available
under open licenses.

</details>


### [36] [Recognizing Actions from Robotic View for Natural Human-Robot Interaction](https://arxiv.org/abs/2507.22522)
*Ziyi Wang,Peiming Li,Hong Liu,Zhichao Deng,Can Wang,Jun Liu,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: The paper introduces ACTIVE, a dataset for natural human-robot interaction (N-HRI), addressing gaps in existing benchmarks by including diverse actions, participants, and environments. It also proposes ACTIVE-PC, a method for accurate long-distance action recognition.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for human action recognition lack the complexity needed for N-HRI, such as diverse data, modalities, and dynamic environments.

Method: The authors create the ACTIVE dataset with 30 action categories, 80 participants, and 46,868 videos (RGB and point cloud). They also propose ACTIVE-PC, using advanced techniques like Multilevel Neighborhood Sampling and Elastic Ellipse Query.

Result: ACTIVE-PC effectively recognizes human actions at long distances, as demonstrated by experimental results.

Conclusion: ACTIVE and ACTIVE-PC advance N-HRI research by providing a comprehensive benchmark and an effective method for action recognition in dynamic, real-world scenarios.

Abstract: Natural Human-Robot Interaction (N-HRI) requires robots to recognize human
actions at varying distances and states, regardless of whether the robot itself
is in motion or stationary. This setup is more flexible and practical than
conventional human action recognition tasks. However, existing benchmarks
designed for traditional action recognition fail to address the unique
complexities in N-HRI due to limited data, modalities, task categories, and
diversity of subjects and environments. To address these challenges, we
introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored
specifically for perception-centric robotic views prevalent in mobile service
robots. ACTIVE comprises 30 composite action categories, 80 participants, and
46,868 annotated video instances, covering both RGB and point cloud modalities.
Participants performed various human actions in diverse environments at
distances ranging from 3m to 50m, while the camera platform was also mobile,
simulating real-world scenarios of robot perception with varying camera heights
due to uneven ground. This comprehensive and challenging benchmark aims to
advance action and attribute recognition research in N-HRI. Furthermore, we
propose ACTIVE-PC, a method that accurately perceives human actions at long
distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic
Ellipse Query, and precise decoupling of kinematic interference from human
actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our
code is available at:
https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.

</details>


### [37] [HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors](https://arxiv.org/abs/2507.22530)
*Xincheng Yao,Yijun Yang,Kangwei Guo,Ruiqiang Xiao,Haipeng Zhou,Haisu Tao,Jian Yang,Lei Zhu*

Main category: cs.CV

TL;DR: The paper introduces a high-quality annotated dataset for hepatic vasculature segmentation in surgical videos and proposes a novel HRVVS network with a pretrained VAR model and dynamic memory decoder, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The lack of appropriate datasets and the complexity of hepatic vasculature segmentation in surgical videos motivated the creation of a new dataset and method.

Method: The authors introduce a dataset of 35 annotated hepatectomy videos and propose HRVVS, embedding a pretrained VAR model in the encoder and using a dynamic memory decoder for multi-view segmentation.

Result: HRVVS outperforms state-of-the-art methods in extensive experiments on surgical video datasets.

Conclusion: The HRVVS network and dataset address the challenges of hepatic vasculature segmentation, with the source code and dataset made publicly available.

Abstract: The segmentation of the hepatic vasculature in surgical videos holds
substantial clinical significance in the context of hepatectomy procedures.
However, owing to the dearth of an appropriate dataset and the inherently
complex task characteristics, few researches have been reported in this domain.
To address this issue, we first introduce a high quality frame-by-frame
annotated hepatic vasculature dataset containing 35 long hepatectomy videos and
11442 high-resolution frames. On this basis, we propose a novel high-resolution
video vasculature segmentation network, dubbed as HRVVS. We innovatively embed
a pretrained visual autoregressive modeling (VAR) model into different layers
of the hierarchical encoder as prior information to reduce the information
degradation generated during the downsampling process. In addition, we designed
a dynamic memory decoder on a multi-view segmentation network to minimize the
transmission of redundant information while preserving more details between
frames. Extensive experiments on surgical video datasets demonstrate that our
proposed HRVVS significantly outperforms the state-of-the-art methods. The
source code and dataset will be publicly available at
\href{https://github.com/scott-yjyang/xx}{https://github.com/scott-yjyang/HRVVS}.

</details>


### [38] [RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning](https://arxiv.org/abs/2507.22553)
*Kiseong Hong,Gyeong-hyeon Kim,Eunwoo Kim*

Main category: cs.CV

TL;DR: A novel prompt-evolving mechanism is proposed to enhance prompt diversity in continual learning, outperforming existing methods by 9.07% and 7.40% in image and video tasks.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based continual learning methods lack representational diversity due to fixed or entangled prompts, limiting task-specific knowledge integration.

Method: The approach adaptively aggregates task-specific prompts into a unified prompt using a learnable probabilistic gate to activate layers during evolution.

Result: Achieves average gains of 9.07% (image classification) and 7.40% (video action recognition) over existing methods.

Conclusion: The prompt-evolving mechanism effectively enhances knowledge integration and diversity, improving continual learning performance.

Abstract: Prompt-based continual learning provides a rehearsal-free solution by tuning
small sets of parameters while keeping pre-trained models frozen. To meet the
complex demands of sequential tasks, it is crucial to integrate task-specific
knowledge within prompts effectively. However, existing works rely on either
fixed learned prompts (i.e., prompts whose representations remain unchanged
during new task learning) or on prompts generated from an entangled task-shared
space, limiting the representational diversity of the integrated prompt. To
address this issue, we propose a novel prompt-evolving mechanism to adaptively
aggregate base prompts (i.e., task-specific prompts) into a unified prompt
while ensuring diversity. By transforming and aligning base prompts, both
previously learned and newly introduced, our approach continuously evolves
accumulated knowledge to facilitate learning new tasks. We further introduce a
learnable probabilistic gate that adaptively determines which layers to
activate during the evolution process. We validate our method on image
classification and video action recognition tasks in class-incremental
learning, achieving average gains of 9.07% and 7.40% over existing methods
across all scenarios.

</details>


### [39] [Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound](https://arxiv.org/abs/2507.22568)
*Shijing Chen,Xinrui Zhou,Yuhao Wang,Yuhao Huang,Ao Chang,Dong Ni,Ruobing Huang*

Main category: cs.CV

TL;DR: A dual-phase framework for long-tailed breast lesion classification uses generative augmentation and reinforcement learning to balance data distribution and improve recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of breast lesion subtypes is crucial for personalized treatment, but skewed long-tailed data distribution complicates automated recognition.

Method: The framework employs generative augmentation with a reinforcement learning-driven adaptive sampler and a class-controllable synthetic network using anatomical priors.

Result: The method outperforms state-of-the-art approaches on long-tailed and imbalanced breast US datasets.

Conclusion: The proposed framework effectively mitigates data imbalance and enhances classification performance for breast lesion subtypes.

Abstract: Accurate identification of breast lesion subtypes can facilitate personalized
treatment and interventions. Ultrasound (US), as a safe and accessible imaging
modality, is extensively employed in breast abnormality screening and
diagnosis. However, the incidence of different subtypes exhibits a skewed
long-tailed distribution, posing significant challenges for automated
recognition. Generative augmentation provides a promising solution to rectify
data distribution. Inspired by this, we propose a dual-phase framework for
long-tailed classification that mitigates distributional bias through
high-fidelity data synthesis while avoiding overuse that corrupts holistic
performance. The framework incorporates a reinforcement learning-driven
adaptive sampler, dynamically calibrating synthetic-real data ratios by
training a strategic multi-agent to compensate for scarcities of real data
while ensuring stable discriminative capability. Furthermore, our
class-controllable synthetic network integrates a sketch-grounded perception
branch that harnesses anatomical priors to maintain distinctive class features
while enabling annotation-free inference. Extensive experiments on an in-house
long-tailed and a public imbalanced breast US datasets demonstrate that our
method achieves promising performance compared to state-of-the-art approaches.
More synthetic images can be found at
https://github.com/Stinalalala/Breast-LT-GenAug.

</details>


### [40] [COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP](https://arxiv.org/abs/2507.22576)
*Galadrielle Humblot-Renaux,Gianni Franchi,Sergio Escalera,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: COOkeD introduces a heterogeneous ensemble method combining a closed-world classifier, a zero-shot CLIP classifier, and a linear probe classifier for superior OOD detection performance.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods are limited by single-classifier constraints, hindering performance. COOkeD aims to leverage diverse classifiers for better robustness.

Method: COOkeD ensembles a closed-world classifier, a zero-shot CLIP classifier, and a linear probe classifier on CLIP features, using pre-trained models for efficiency.

Result: COOkeD achieves state-of-the-art OOD detection performance and robustness across various challenging benchmarks like CIFAR100 and ImageNet.

Conclusion: COOkeD demonstrates that a modular, post-hoc ensemble of diverse classifiers significantly improves OOD detection, outperforming single-classifier approaches.

Abstract: Out-of-distribution (OOD) detection is an important building block in
trustworthy image recognition systems as unknown classes may arise at
test-time. OOD detection methods typically revolve around a single classifier,
leading to a split in the research field between the classical supervised
setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot
setting (class names fed as prompts to CLIP). In both cases, an overarching
challenge is that the OOD detection performance is implicitly constrained by
the classifier's capabilities on in-distribution (ID) data. In this work, we
show that given a little open-mindedness from both ends, remarkable OOD
detection can be achieved by instead creating a heterogeneous ensemble - COOkeD
combines the predictions of a closed-world classifier trained end-to-end on a
specific dataset, a zero-shot CLIP classifier, and a linear probe classifier
trained on CLIP image features. While bulky at first sight, this approach is
modular, post-hoc and leverages the availability of pre-trained VLMs, thus
introduces little overhead compared to training a single standard classifier.
We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also
consider more challenging, realistic settings ranging from training-time label
noise, to test-time covariate shift, to zero-shot shift which has been
previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art
performance and greater robustness compared to both classical and CLIP-based
OOD detection methods. Code is available at https://github.com/glhr/COOkeD

</details>


### [41] [Robust Deepfake Detection for Electronic Know Your Customer Systems Using Registered Images](https://arxiv.org/abs/2507.22601)
*Takuma Amada,Kazuya Kakizaki,Taiki Miyagawa,Akinori F. Ebihara,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: A deepfake detection algorithm for eKYC systems detects face swapping and reenactment by analyzing temporal inconsistencies and identity discrepancies, using a robust feature extractor.


<details>
  <summary>Details</summary>
Motivation: To ensure eKYC system reliability against deepfake attacks by detecting manipulated videos.

Method: Uses temporal inconsistencies in identity vectors, compares input videos with registered images, and employs a robust face feature extractor.

Result: Accurate detection of face swapping and reenactment, robust against image degradation.

Conclusion: The proposed method effectively enhances eKYC security against deepfakes.

Abstract: In this paper, we present a deepfake detection algorithm specifically
designed for electronic Know Your Customer (eKYC) systems. To ensure the
reliability of eKYC systems against deepfake attacks, it is essential to
develop a robust deepfake detector capable of identifying both face swapping
and face reenactment, while also being robust to image degradation. We address
these challenges through three key contributions: (1)~Our approach evaluates
the video's authenticity by detecting temporal inconsistencies in identity
vectors extracted by face recognition models, leading to comprehensive
detection of both face swapping and face reenactment. (2)~In addition to
processing video input, the algorithm utilizes a registered image (assumed to
be genuine) to calculate identity discrepancies between the input video and the
registered image, significantly improving detection accuracy. (3)~We find that
employing a face feature extractor trained on a larger dataset enhances both
detection performance and robustness against image degradation. Our
experimental results show that our proposed method accurately detects both face
swapping and face reenactment comprehensively and is robust against various
forms of unseen image degradation. Our source code is publicly available
https://github.com/TaikiMiyagawa/DeepfakeDetection4eKYC.

</details>


### [42] [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/abs/2507.22604)
*Xiefan Guo,Miaomiao Cui,Liefeng Bo,Di Huang*

Main category: cs.CV

TL;DR: ShortFT introduces a shortcut-based fine-tuning strategy for diffusion models, improving efficiency and alignment with reward functions by using a shorter denoising chain.


<details>
  <summary>Details</summary>
Motivation: Existing backpropagation-based approaches for aligning diffusion models with reward functions face computational costs and gradient explosion risks due to lengthy denoising chains, leading to suboptimal results.

Method: ShortFT employs a trajectory-preserving few-step diffusion model to create a shorter denoising chain, optimizing fine-tuning efficiency and effectiveness.

Result: The method significantly improves alignment performance with various reward functions, outperforming state-of-the-art alternatives.

Conclusion: ShortFT offers a practical and efficient solution for fine-tuning diffusion models, enhancing their alignment with reward functions.

Abstract: Backpropagation-based approaches aim to align diffusion models with reward
functions through end-to-end backpropagation of the reward gradient within the
denoising chain, offering a promising perspective. However, due to the
computational costs and the risk of gradient explosion associated with the
lengthy denoising chain, existing approaches struggle to achieve complete
gradient backpropagation, leading to suboptimal results. In this paper, we
introduce Shortcut-based Fine-Tuning (ShortFT), an efficient fine-tuning
strategy that utilizes the shorter denoising chain. More specifically, we
employ the recently researched trajectory-preserving few-step diffusion model,
which enables a shortcut over the original denoising chain, and construct a
shortcut-based denoising chain of shorter length. The optimization on this
chain notably enhances the efficiency and effectiveness of fine-tuning the
foundational model. Our method has been rigorously tested and can be
effectively applied to various reward functions, significantly improving
alignment performance and surpassing state-of-the-art alternatives.

</details>


### [43] [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/abs/2507.22607)
*Ruifeng Yuan,Chenghao Xiao,Sicong Leng,Jianyu Wang,Long Li,Weiwen Xu,Hou Pong Chan,Deli Zhao,Tingyang Xu,Zhongyu Wei,Hao Zhang,Yu Rong*

Main category: cs.CV

TL;DR: VL-Cogito, a multimodal reasoning model, uses Progressive Curriculum Reinforcement Learning (PCuRL) to improve performance across diverse tasks by dynamically adjusting training difficulty and reasoning path length.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with unstable performance in multimodal tasks due to their complexity and diversity.

Method: VL-Cogito employs PCuRL with two innovations: an online difficulty soft weighting mechanism and a dynamic length reward mechanism.

Result: VL-Cogito outperforms existing models in multimodal benchmarks across mathematics, science, logic, and general understanding.

Conclusion: The PCuRL framework effectively enhances multimodal reasoning, validating its innovative approach.

Abstract: Reinforcement learning has proven its effectiveness in enhancing the
reasoning capabilities of large language models. Recent research efforts have
progressively extended this paradigm to multimodal reasoning tasks. Due to the
inherent complexity and diversity of multimodal tasks, especially in semantic
content and problem formulations, existing models often exhibit unstable
performance across various domains and difficulty levels. To address these
limitations, we propose VL-Cogito, an advanced multimodal reasoning model
trained via a novel multi-stage Progressive Curriculum Reinforcement Learning
(PCuRL) framework. PCuRL systematically guides the model through tasks of
gradually increasing difficulty, substantially improving its reasoning
abilities across diverse multimodal contexts. The framework introduces two key
innovations: (1) an online difficulty soft weighting mechanism, dynamically
adjusting training difficulty across successive RL training stages; and (2) a
dynamic length reward mechanism, which encourages the model to adaptively
regulate its reasoning path length according to task complexity, thus balancing
reasoning efficiency with correctness. Experimental evaluations demonstrate
that VL-Cogito consistently matches or surpasses existing reasoning-oriented
models across mainstream multimodal benchmarks spanning mathematics, science,
logic, and general understanding, validating the effectiveness of our approach.

</details>


### [44] [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/abs/2507.22615)
*Daehee Park,Monu Surana,Pranav Desai,Ashish Mehta,Reuben MV John,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: GALTraj improves trajectory prediction by using generative active learning to augment rare tail samples without changing model architectures, enhancing performance on both tail and head samples.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of rarely observed long-tail scenarios in trajectory prediction without modifying model architectures.

Method: Introduces GALTraj, a generative active learning method that identifies rare tail samples, augments them with a controllable diffusion model, and ensures diversity, realism, and tail-case preservation.

Result: Significantly boosts performance on tail samples and enhances accuracy on head samples across multiple datasets (WOMD, Argoverse2) and backbones (QCNet, MTR).

Conclusion: GALTraj effectively improves trajectory prediction by focusing on rare scenarios through generative active learning, outperforming prior methods.

Abstract: While data-driven trajectory prediction has enhanced the reliability of
autonomous driving systems, it still struggles with rarely observed long-tail
scenarios. Prior works addressed this by modifying model architectures, such as
using hypernetworks. In contrast, we propose refining the training process to
unlock each model's potential without altering its structure. We introduce
Generative Active Learning for Trajectory prediction (GALTraj), the first
method to successfully deploy generative active learning into trajectory
prediction. It actively identifies rare tail samples where the model fails and
augments these samples with a controllable diffusion model during training. In
our framework, generating scenarios that are diverse, realistic, and preserve
tail-case characteristics is paramount. Accordingly, we design a tail-aware
generation method that applies tailored diffusion guidance to generate
trajectories that both capture rare behaviors and respect traffic rules. Unlike
prior simulation methods focused solely on scenario diversity, GALTraj is the
first to show how simulator-driven augmentation benefits long-tail learning in
trajectory prediction. Experiments on multiple trajectory datasets (WOMD,
Argoverse2) with popular backbones (QCNet, MTR) confirm that our method
significantly boosts performance on tail samples and also enhances accuracy on
head samples.

</details>


### [45] [Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation](https://arxiv.org/abs/2507.22626)
*Shenghao Zhu,Yifei Chen,Weihong Chen,Yuanhan Wang,Chang Liu,Shuo Jiang,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: MST-KDNet improves brain tumor segmentation by addressing missing modalities and feature transfer issues using multi-scale transformers and knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurate brain tumor segmentation, especially with missing modalities, and unresolved issues like tumor boundary insensitivity and feature transfer.

Method: Multi-Scale Transformer Knowledge Distillation, Dual-Mode Logit Distillation, and Global Style Matching Module.

Result: Outperforms leading methods on BraTS and FeTS 2024 datasets in Dice and HD95 scores, especially with modality loss.

Conclusion: MST-KDNet is robust, generalizes well, and is suitable for real-world clinical use.

Abstract: Accurate and reliable brain tumor segmentation, particularly when dealing
with missing modalities, remains a critical challenge in medical image
analysis. Previous studies have not fully resolved the challenges of tumor
boundary segmentation insensitivity and feature transfer in the absence of key
imaging modalities. In this study, we introduce MST-KDNet, aimed at addressing
these critical issues. Our model features Multi-Scale Transformer Knowledge
Distillation to effectively capture attention weights at various resolutions,
Dual-Mode Logit Distillation to improve the transfer of knowledge, and a Global
Style Matching Module that integrates feature matching with adversarial
learning. Comprehensive experiments conducted on the BraTS and FeTS 2024
datasets demonstrate that MST-KDNet surpasses current leading methods in both
Dice and HD95 scores, particularly in conditions with substantial modality
loss. Our approach shows exceptional robustness and generalization potential,
making it a promising candidate for real-world clinical applications. Our
source code is available at https://github.com/Quanato607/MST-KDNet.

</details>


### [46] [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/abs/2507.22627)
*Federico Girella,Davide Talon,Ziyue Liu,Zanxi Ruan,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS introduces a method for generating fashion images using localized sketch-text pairs and a novel diffusion adaptation strategy, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Fashion design combines visual and textual elements, but existing methods lack precise control over localized details. LOTS addresses this gap.

Method: LOTS uses a Modularized Pair-Centric representation for sketches and text, followed by Diffusion Pair Guidance for integrating local and global features in a diffusion model.

Result: LOTS outperforms existing methods in image generation, validated by quantitative metrics and human evaluation.

Conclusion: LOTS enables unprecedented customization in fashion design, demonstrated by its performance on the Sketchy dataset.

Abstract: Fashion design is a complex creative process that blends visual and textual
expressions. Designers convey ideas through sketches, which define spatial
structure and design elements, and textual descriptions, capturing material,
texture, and stylistic details. In this paper, we present LOcalized Text and
Sketch for fashion image generation (LOTS), an approach for compositional
sketch-text based generation of complete fashion outlooks. LOTS leverages a
global description with paired localized sketch + text information for
conditioning and introduces a novel step-based merging strategy for diffusion
adaptation. First, a Modularized Pair-Centric representation encodes sketches
and text into a shared latent space while preserving independent localized
features; then, a Diffusion Pair Guidance phase integrates both local and
global conditioning via attention-based guidance within the diffusion model's
multi-step denoising process. To validate our method, we build on Fashionpedia
to release Sketchy, the first fashion dataset where multiple text-sketch pairs
are provided per image. Quantitative results show LOTS achieves
state-of-the-art image generation performance on both global and localized
metrics, while qualitative examples and a human evaluation study highlight its
unprecedented level of design customization.

</details>


### [47] [SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection, Tracking and Payload Identification](https://arxiv.org/abs/2507.22650)
*Shahriar Kabir,Istiak Ahmmed Rifti,H. M. Shadman Tabib,Mushfiqur Rahman,Sadatul Islam Sadi,Hasnaen Adil,Ahmed Mahir Sultan Rumi,Ch Md Rakin Haider*

Main category: cs.CV

TL;DR: A dual-stream drone monitoring framework using YOLOv11n on RGB and IR data streams achieves high accuracy and real-time performance for drone detection, tracking, and payload identification.


<details>
  <summary>Details</summary>
Motivation: Addressing security concerns from drone proliferation by creating a robust surveillance system for the 2025 VIP Cup challenge.

Method: Independent YOLOv11n detectors on parallel RGB and IR streams, with domain-specific preprocessing, augmentation, and hyperparameter tuning.

Result: Lightweight models achieve high accuracy in distinguishing drones from birds and classifying payloads, maintaining real-time performance.

Conclusion: The dual-modality design, specialized training, and optimizations enable efficient and accurate drone surveillance across RGB and IR.

Abstract: The proliferation of drones in civilian airspace has raised urgent security
concerns, necessitating robust real-time surveillance systems. In response to
the 2025 VIP Cup challenge tasks - drone detection, tracking, and payload
identification - we propose a dual-stream drone monitoring framework. Our
approach deploys independent You Only Look Once v11-nano (YOLOv11n) object
detectors on parallel infrared (thermal) and visible (RGB) data streams,
deliberately avoiding early fusion. This separation allows each model to be
specifically optimized for the distinct characteristics of its input modality,
addressing the unique challenges posed by small aerial objects in diverse
environmental conditions. We customize data preprocessing and augmentation
strategies per domain - such as limiting color jitter for IR imagery - and
fine-tune training hyperparameters to enhance detection performance under
conditions of heavy noise, low light, and motion blur. The resulting
lightweight YOLOv11n models demonstrate high accuracy in distinguishing drones
from birds and in classifying payload types, all while maintaining real-time
performance. This report details the rationale for a dual-modality design, the
specialized training pipelines, and the architectural optimizations that
collectively enable efficient and accurate drone surveillance across RGB and IR
channels.

</details>


### [48] [Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation](https://arxiv.org/abs/2507.22668)
*Hongbin Lin,Yifan Jiang,Juangui Xu,Jesse Jiaxi Xu,Yi Lu,Zhengyu Hu,Ying-Cong Chen,Hao Wang*

Main category: cs.CV

TL;DR: A graph-guided data augmentation framework for 3D point cloud segmentation improves scene synthesis by considering global structural dependencies, enhancing segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Existing augmentation methods lack global structural dependencies, limiting realistic scene synthesis for 3D point cloud segmentation.

Method: Proposes a framework with dual-level constraints: local (geometric plausibility, semantic consistency) and global (topological structure alignment via guiding graphs).

Result: Generates diverse, high-quality augmented scenes, improving segmentation performance on indoor and outdoor datasets.

Conclusion: The framework effectively addresses the limitation of global structural dependencies, enhancing 3D point cloud segmentation.

Abstract: 3D point cloud segmentation aims to assign semantic labels to individual
points in a scene for fine-grained spatial understanding. Existing methods
typically adopt data augmentation to alleviate the burden of large-scale
annotation. However, most augmentation strategies only focus on local
transformations or semantic recomposition, lacking the consideration of global
structural dependencies within scenes. To address this limitation, we propose a
graph-guided data augmentation framework with dual-level constraints for
realistic 3D scene synthesis. Our method learns object relationship statistics
from real-world data to construct guiding graphs for scene generation.
Local-level constraints enforce geometric plausibility and semantic consistency
between objects, while global-level constraints maintain the topological
structure of the scene by aligning the generated layout with the guiding graph.
Extensive experiments on indoor and outdoor datasets demonstrate that our
framework generates diverse and high-quality augmented scenes, leading to
consistent improvements in point cloud segmentation performance across various
models.

</details>


### [49] [MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model](https://arxiv.org/abs/2507.22675)
*Meiqi Hu,Lingzhi Lu,Chengxi Han,Xiaoping Liu*

Main category: cs.CV

TL;DR: MergeSAM introduces an unsupervised change detection method for remote sensing imagery using SAM, with MaskMatching and MaskSplitting strategies to handle complex changes.


<details>
  <summary>Details</summary>
Motivation: To enhance unsupervised change detection by leveraging SAM's segmentation capabilities for complex real-world scenarios.

Method: Uses SAM for multitemporal mask construction, with MaskMatching and MaskSplitting to address object splitting, merging, and intricate changes.

Result: Improved change detection by embedding spatial structure into the process.

Conclusion: MergeSAM shows promise for practical applications in remote sensing change detection.

Abstract: Recently, large foundation models trained on vast datasets have demonstrated
exceptional capabilities in feature extraction and general feature
representation. The ongoing advancements in deep learning-driven large models
have shown great promise in accelerating unsupervised change detection methods,
thereby enhancing the practical applicability of change detection technologies.
Building on this progress, this paper introduces MergeSAM, an innovative
unsupervised change detection method for high-resolution remote sensing
imagery, based on the Segment Anything Model (SAM). Two novel strategies,
MaskMatching and MaskSplitting, are designed to address real-world complexities
such as object splitting, merging, and other intricate changes. The proposed
method fully leverages SAM's object segmentation capabilities to construct
multitemporal masks that capture complex changes, embedding the spatial
structure of land cover into the change detection process.

</details>


### [50] [Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing](https://arxiv.org/abs/2507.22685)
*Yimeng Liu,Maolin Gan,Yidong Ren,Gen Li,Jingkai Lin,Younsuk Dong,Zhichao Cao*

Main category: cs.CV

TL;DR: A new multi-modal dataset for leaf wetness detection is introduced, combining mmWave, SAR, and RGB data, with benchmarks using the Hydra model.


<details>
  <summary>Details</summary>
Motivation: Existing sensing systems lack robustness and accuracy for real-world leaf wetness detection.

Method: A multi-modal dataset (mmWave, SAR, RGB) is created and evaluated using the Hydra model with various fusion strategies.

Result: The dataset provides benchmarks for detection accuracy under diverse conditions and aids SAR imaging optimization.

Conclusion: The dataset advances machine learning for leaf wetness detection and serves as a benchmark for future research.

Abstract: Leaf wetness detection is a crucial task in agricultural monitoring, as it
directly impacts the prediction and protection of plant diseases. However,
existing sensing systems suffer from limitations in robustness, accuracy, and
environmental resilience when applied to natural leaves under dynamic
real-world conditions. To address these challenges, we introduce a new
multi-modal dataset specifically designed for evaluating and advancing machine
learning algorithms in leaf wetness detection. Our dataset comprises
synchronized mmWave raw data, Synthetic Aperture Radar (SAR) images, and RGB
images collected over six months from five diverse plant species in both
controlled and outdoor field environments. We provide detailed benchmarks using
the Hydra model, including comparisons against single modality baselines and
multiple fusion strategies, as well as performance under varying scan
distances. Additionally, our dataset can serve as a benchmark for future SAR
imaging algorithm optimization, enabling a systematic evaluation of detection
accuracy under diverse conditions.

</details>


### [51] [Zero-Shot Image Anomaly Detection Using Generative Foundation Models](https://arxiv.org/abs/2507.22692)
*Lemar Abdi,Amaan Valiuddin,Francisco Caetano,Christiaan Viviers,Fons van der Sommen*

Main category: cs.CV

TL;DR: The paper proposes using diffusion models as perceptual templates for out-of-distribution (OOD) detection, leveraging denoising trajectories and Stein score errors for anomaly identification without dataset-specific retraining.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety of vision systems in open-world environments by improving OOD detection using generative models.

Method: Utilizes Denoising Diffusion Models (DDMs) to analyze Stein score errors and Structural Similarity Index Metric (SSIM) for detecting anomalies, trained on CelebA as a base distribution.

Result: Achieves near-perfect performance on some benchmarks, outperforming state-of-the-art methods and demonstrating the effectiveness of generative foundation models.

Conclusion: Generative foundation models, particularly DDMs, show strong potential for OOD detection, with room for further improvement in certain settings.

Abstract: Detecting out-of-distribution (OOD) inputs is pivotal for deploying safe
vision systems in open-world environments. We revisit diffusion models, not as
generators, but as universal perceptual templates for OOD detection. This
research explores the use of score-based generative models as foundational
tools for semantic anomaly detection across unseen datasets. Specifically, we
leverage the denoising trajectories of Denoising Diffusion Models (DDMs) as a
rich source of texture and semantic information. By analyzing Stein score
errors, amplified through the Structural Similarity Index Metric (SSIM), we
introduce a novel method for identifying anomalous samples without requiring
re-training on each target dataset. Our approach improves over state-of-the-art
and relies on training a single model on one dataset -- CelebA -- which we find
to be an effective base distribution, even outperforming more commonly used
datasets like ImageNet in several settings. Experimental results show
near-perfect performance on some benchmarks, with notable headroom on others,
highlighting both the strength and future potential of generative foundation
models in anomaly detection.

</details>


### [52] [Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints](https://arxiv.org/abs/2507.22699)
*Thuy Tran,Ruochen Chen,Shaifali Parashar*

Main category: cs.CV

TL;DR: Proposes an unsupervised Shape-from-Template (SfT) method using image observations and mesh constraints, achieving faster and more accurate 3D reconstruction than existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional SfT relies on point correspondences and struggles with occlusions, while modern methods require large supervised datasets. This work aims to overcome these limitations with an unsupervised approach.

Method: Uses color features, gradients, silhouettes, and mesh inextensibility constraints for 3D reconstruction without correspondences or supervision.

Result: Achieves 400x faster reconstruction than best-performing unsupervised SfT and outperforms in handling finer details and severe occlusions.

Conclusion: The proposed unsupervised SfT method is efficient, robust to occlusions, and superior in detail reconstruction, with code publicly available.

Abstract: Shape-from-Template (SfT) refers to the class of methods that reconstruct the
3D shape of a deforming object from images/videos using a 3D template.
Traditional SfT methods require point correspondences between images and the
texture of the 3D template in order to reconstruct 3D shapes from images/videos
in real time. Their performance severely degrades when encountered with severe
occlusions in the images because of the unavailability of correspondences. In
contrast, modern SfT methods use a correspondence-free approach by
incorporating deep neural networks to reconstruct 3D objects, thus requiring
huge amounts of data for supervision. Recent advances use a fully unsupervised
or self-supervised approach by combining differentiable physics and graphics to
deform 3D template to match input images. In this paper, we propose an
unsupervised SfT which uses only image observations: color features, gradients
and silhouettes along with a mesh inextensibility constraint to reconstruct at
a $400\times$ faster pace than (best-performing) unsupervised SfT. Moreover,
when it comes to generating finer details and severe occlusions, our method
outperforms the existing methodologies by a large margin. Code is available at
https://github.com/dvttran/nsft.

</details>


### [53] [A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks](https://arxiv.org/abs/2507.22733)
*Hang Su,Yunlong Feng,Daniel Gehrig,Panfeng Jiang,Ling Gao,Xavier Lagorce,Laurent Kneip*

Main category: cs.CV

TL;DR: A unified approach for structure and linear motion estimation from 2D point correspondences with arbitrary timestamps, applicable to various camera types.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms like 5-point or 8-point are limited to synchronized views, failing for asynchronous data from rolling shutter or event cameras.

Method: Formulates the problem using first-order dynamics and a constant velocity model, deriving a linear point incidence relation for efficient recovery of velocity and 3D points.

Result: Validated on simulated and real-world data, showing consistent improvement over recent approaches across all modalities.

Conclusion: The method enables efficient structure and motion estimation from asynchronous data, with potential for diverse sensor applications.

Abstract: Structure and continuous motion estimation from point correspondences is a
fundamental problem in computer vision that has been powered by well-known
algorithms such as the familiar 5-point or 8-point algorithm. However, despite
their acclaim, these algorithms are limited to processing point correspondences
originating from a pair of views each one representing an instantaneous capture
of the scene. Yet, in the case of rolling shutter cameras, or more recently,
event cameras, this synchronization breaks down. In this work, we present a
unified approach for structure and linear motion estimation from 2D point
correspondences with arbitrary timestamps, from an arbitrary set of views. By
formulating the problem in terms of first-order dynamics and leveraging a
constant velocity motion model, we derive a novel, linear point incidence
relation allowing for the efficient recovery of both linear velocity and 3D
points with predictable degeneracies and solution multiplicities. Owing to its
general formulation, it can handle correspondences from a wide range of sensing
modalities such as global shutter, rolling shutter, and event cameras, and can
even combine correspondences from different collocated sensors. We validate the
effectiveness of our solver on both simulated and real-world data, where we
show consistent improvement across all modalities when compared to recent
approaches. We believe our work opens the door to efficient structure and
motion estimation from asynchronous data. Code can be found at
https://github.com/suhang99/AsyncTrack-Motion-Solver.

</details>


### [54] [Social-Pose: Enhancing Trajectory Prediction with Human Body Pose](https://arxiv.org/abs/2507.22742)
*Yang Gao,Saeed Saadatnejad,Alexandre Alahi*

Main category: cs.CV

TL;DR: The paper proposes 'Social-pose', an attention-based pose encoder for human trajectory prediction, improving accuracy by leveraging body poses and social relations.


<details>
  <summary>Details</summary>
Motivation: Existing models often miss visual cues humans use for navigation. The study explores using body poses instead of just Cartesian locations for better trajectory prediction.

Method: Introduces 'Social-pose', an attention-based pose encoder capturing poses and social relations, integrable into various prediction architectures (LSTM, GAN, MLP, Transformer).

Result: Shows improvements on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians and Cyclists in Road Traffic, JRDB) datasets. Also examines 2D vs. 3D poses and noisy pose effects.

Conclusion: Using body poses enhances trajectory prediction, with 'Social-pose' proving effective across architectures and datasets, applicable to robot navigation.

Abstract: Accurate human trajectory prediction is one of the most crucial tasks for
autonomous driving, ensuring its safety. Yet, existing models often fail to
fully leverage the visual cues that humans subconsciously communicate when
navigating the space. In this work, we study the benefits of predicting human
trajectories using human body poses instead of solely their Cartesian space
locations in time. We propose `Social-pose', an attention-based pose encoder
that effectively captures the poses of all humans in a scene and their social
relations. Our method can be integrated into various trajectory prediction
architectures. We have conducted extensive experiments on state-of-the-art
models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over
all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians
and Cyclists in Road Traffic, and JRDB) datasets. We also explored the
advantages of using 2D versus 3D poses, as well as the effect of noisy poses
and the application of our pose-based predictor in robot navigation scenarios.

</details>


### [55] [HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training](https://arxiv.org/abs/2507.22781)
*Xuecheng Wu,Danlei Huang,Heli Sun,Xinyi Yin,Yifan Wang,Hao Wang,Jia Zhang,Fei Wang,Peihao Guo,Suyu Xing,Junxiao Xue,Liang He*

Main category: cs.CV

TL;DR: HOLA is a two-stage framework for video-level deepfake detection, leveraging large-scale pre-training and innovative modules like cross-modal learning and hierarchical modeling, achieving top performance in the 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detection techniques struggle with advanced generative AI, necessitating a more robust solution like HOLA.

Method: HOLA uses audio-visual self-supervised pre-training, iterative-aware cross-modal learning, hierarchical contextual modeling, and a pyramid-like refiner, enhanced by pseudo-supervised signal injection.

Result: HOLA outperforms others by 0.0476 AUC on the TestA set, ranking 1st in the challenge.

Conclusion: HOLA's innovative design and large-scale pre-training make it highly effective for video-level deepfake detection.

Abstract: Advances in Generative AI have made video-level deepfake detection
increasingly challenging, exposing the limitations of current detection
techniques. In this paper, we present HOLA, our solution to the Video-Level
Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by
the success of large-scale pre-training in the general domain, we first scale
audio-visual self-supervised pre-training in the multimodal video-level
deepfake detection, which leverages our self-built dataset of 1.81M samples,
thereby leading to a unified two-stage framework. To be specific, HOLA features
an iterative-aware cross-modal learning module for selective audio-visual
interactions, hierarchical contextual modeling with gated aggregations under
the local-global perspective, and a pyramid-like refiner for scale-aware
cross-grained semantic enhancements. Moreover, we propose the pseudo supervised
singal injection strategy to further boost model performance. Extensive
experiments across expert models and MLLMs impressivly demonstrate the
effectiveness of our proposed HOLA. We also conduct a series of ablation
studies to explore the crucial design factors of our introduced components.
Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the
TestA set.

</details>


### [56] [Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques](https://arxiv.org/abs/2507.22791)
*Weide Liu,Wei Zhou,Jun Liu,Ping Hu,Jun Cheng,Jungong Han,Weisi Lin*

Main category: cs.CV

TL;DR: A survey on feature matching in computer vision, comparing traditional handcrafted methods with modern deep learning approaches across various modalities like RGB, depth, LiDAR, and medical images.


<details>
  <summary>Details</summary>
Motivation: Feature matching is critical for tasks like image retrieval, 3D reconstruction, and SLAM, but traditional methods struggle with modality gaps. This survey explores advancements to address these challenges.

Method: Reviews traditional methods (e.g., Harris, SIFT, ORB) and deep learning approaches (e.g., SuperPoint, LoFTR), focusing on modality-specific solutions like geometric descriptors for depth images and attention networks for LiDAR.

Result: Deep learning methods outperform traditional ones in robustness and adaptability, especially in cross-modal applications like medical imaging and vision-language tasks.

Conclusion: Feature matching has evolved significantly with deep learning, enabling better handling of diverse modalities and complex applications.

Abstract: Feature matching is a cornerstone task in computer vision, essential for
applications such as image retrieval, stereo matching, 3D reconstruction, and
SLAM. This survey comprehensively reviews modality-based feature matching,
exploring traditional handcrafted methods and emphasizing contemporary deep
learning approaches across various modalities, including RGB images, depth
images, 3D point clouds, LiDAR scans, medical images, and vision-language
interactions. Traditional methods, leveraging detectors like Harris corners and
descriptors such as SIFT and ORB, demonstrate robustness under moderate
intra-modality variations but struggle with significant modality gaps.
Contemporary deep learning-based methods, exemplified by detector-free
strategies like CNN-based SuperPoint and transformer-based LoFTR, substantially
improve robustness and adaptability across modalities. We highlight
modality-aware advancements, such as geometric and depth-specific descriptors
for depth images, sparse and dense learning methods for 3D point clouds,
attention-enhanced neural networks for LiDAR scans, and specialized solutions
like the MIND descriptor for complex medical image matching. Cross-modal
applications, particularly in medical image registration and vision-language
tasks, underscore the evolution of feature matching to handle increasingly
diverse data interactions.

</details>


### [57] [Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future](https://arxiv.org/abs/2507.22792)
*Guoping Xu,Jayaram K. Udupa,Yajun Yu,Hua-Chieh Shao,Songlin Zhao,Wei Liu,You Zhang*

Main category: cs.CV

TL;DR: A survey on SAM/SAM2-based methods for Video Object Segmentation and Tracking (VOST), focusing on past, present, and future temporal dimensions, highlighting advancements and challenges.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional VOST methods in domain generalization, temporal consistency, and efficiency by leveraging foundation models like SAM/SAM2.

Method: Review SAM/SAM2-based approaches, analyzing strategies for historical retention (past), current feature extraction (present), and future motion prediction (future).

Result: Identifies innovations like motion-aware memory and trajectory-guided prompting, while noting challenges like memory redundancy and prompt inefficiency.

Conclusion: Provides a structured overview to guide future research in advancing VOST using foundation models.

Abstract: Video Object Segmentation and Tracking (VOST) presents a complex yet critical
challenge in computer vision, requiring robust integration of segmentation and
tracking across temporally dynamic frames. Traditional methods have struggled
with domain generalization, temporal consistency, and computational efficiency.
The emergence of foundation models like the Segment Anything Model (SAM) and
its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven
segmentation with strong generalization capabilities. Building upon these
advances, this survey provides a comprehensive review of SAM/SAM2-based methods
for VOST, structured along three temporal dimensions: past, present, and
future. We examine strategies for retaining and updating historical information
(past), approaches for extracting and optimizing discriminative features from
the current frame (present), and motion prediction and trajectory estimation
mechanisms for anticipating object dynamics in subsequent frames (future). In
doing so, we highlight the evolution from early memory-based architectures to
the streaming memory and real-time segmentation capabilities of SAM2. We also
discuss recent innovations such as motion-aware memory selection and
trajectory-guided prompting, which aim to enhance both accuracy and efficiency.
Finally, we identify remaining challenges including memory redundancy, error
accumulation, and prompt inefficiency, and suggest promising directions for
future research. This survey offers a timely and structured overview of the
field, aiming to guide researchers and practitioners in advancing the state of
VOST through the lens of foundation models.

</details>


### [58] [Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings](https://arxiv.org/abs/2507.22802)
*Dongli He,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: FetalCLIP, a vision-language model, is adapted for automated fetal ultrasound image quality assessment, achieving high performance and improving prenatal care in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: The scarcity of trained sonographers in low-income countries makes high-quality fetal ultrasound measurements challenging, necessitating automated solutions.

Method: FetalCLIP is adapted using Low-Rank Adaptation (LoRA) for image quality assessment (IQA) and evaluated against CNN and Transformer baselines. An adapted segmentation model is also repurposed for classification.

Result: FetalCLIP$_{CLS}$ achieves an F1 score of 0.757, and the repurposed segmentation model further improves performance to 0.771.

Conclusion: Parameter-efficient fine-tuning of fetal ultrasound foundation models enables task-specific adaptations, advancing prenatal care in resource-limited settings.

Abstract: Accurate fetal biometric measurements, such as abdominal circumference, play
a vital role in prenatal care. However, obtaining high-quality ultrasound
images for these measurements heavily depends on the expertise of sonographers,
posing a significant challenge in low-income countries due to the scarcity of
trained personnel. To address this issue, we leverage FetalCLIP, a
vision-language model pretrained on a curated dataset of over 210,000 fetal
ultrasound image-caption pairs, to perform automated fetal ultrasound image
quality assessment (IQA) on blind-sweep ultrasound data. We introduce
FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank
Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN
and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of
0.757. Moreover, we show that an adapted segmentation model, when repurposed
for classification, further improves performance, achieving an F1 score of
0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal
ultrasound foundation models can enable task-specific adaptations, advancing
prenatal care in resource-limited settings. The experimental code is available
at: https://github.com/donglihe-hub/FetalCLIP-IQA.

</details>


### [59] [MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention](https://arxiv.org/abs/2507.22805)
*Yuqi Pang,Bowen Yang,Yun Cao,Fan Rong,Xiaoyu Li,Chen He*

Main category: cs.CV

TL;DR: MoCHA is a novel visual framework integrating multiple vision backbones and a Mixture of Experts Connectors (MoECs) module to enhance visual feature extraction and reduce redundancy, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Address high training/inference costs and challenges in extracting visual details and bridging modalities in Vision Large Language Models (VLLMs).

Method: Integrates CLIP, SigLIP, DINOv2, and ConvNeXt backbones; uses MoECs for dynamic expert selection and Hierarchical Group Attention (HGA) for adaptive feature gating.

Result: Outperforms open-weight models, e.g., 3.25% improvement in POPE and 153-point rise on MME.

Conclusion: MoCHA's MoECs and HGA modules are effective and robust, enhancing performance in visual tasks.

Abstract: Vision large language models (VLLMs) are focusing primarily on handling
complex and fine-grained visual information by incorporating advanced vision
encoders and scaling up visual models. However, these approaches face high
training and inference costs, as well as challenges in extracting visual
details, effectively bridging across modalities. In this work, we propose a
novel visual framework, MoCHA, to address these issues. Our framework
integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to
extract complementary visual features and is equipped with a sparse Mixture of
Experts Connectors (MoECs) module to dynamically select experts tailored to
different visual dimensions. To mitigate redundant or insufficient use of the
visual information encoded by the MoECs module, we further design a
Hierarchical Group Attention (HGA) with intra- and inter-group operations and
an adaptive gating strategy for encoded visual features. We train MoCHA on two
mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance
across various benchmarks. Notably, MoCHA outperforms state-of-the-art
open-weight models on various tasks. For example, compared to CuMo
(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate
hallucination by showing improvements of 3.25% in POPE and to follow visual
instructions by raising 153 points on MME. Finally, ablation studies further
confirm the effectiveness and robustness of the proposed MoECs and HGA in
improving the overall performance of MoCHA.

</details>


### [60] [DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion](https://arxiv.org/abs/2507.22813)
*Hossein Mirzaei,Zeinab Taghavi,Sepehr Rezaee,Masoud Hadi,Moein Madadi,Mackenzie W. Mathis*

Main category: cs.CV

TL;DR: The paper proposes DISTIL, a data-free, zero-shot trigger-inversion strategy for detecting Trojan attacks in deep neural networks, outperforming existing methods by up to 9.4%.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to Trojan attacks, posing risks in critical applications. Current trigger-inversion methods lack guarantees and rely on strong assumptions.

Method: DISTIL uses a diffusion-based generator guided by the target classifier to iteratively produce candidate triggers aligned with the model's malicious behavior.

Result: DISTIL achieves up to 7.1% higher accuracy on BackdoorBench and 9.4% improvement in trojaned model scanning, outperforming alternatives.

Conclusion: DISTIL offers a reliable, assumption-free defense against backdoor attacks, validated by empirical results.

Abstract: Deep neural networks have demonstrated remarkable success across numerous
tasks, yet they remain vulnerable to Trojan (backdoor) attacks, raising serious
concerns about their safety in real-world mission-critical applications. A
common countermeasure is trigger inversion -- reconstructing malicious
"shortcut" patterns (triggers) inserted by an adversary during training.
Current trigger-inversion methods typically search the full pixel space under
specific assumptions but offer no assurances that the estimated trigger is more
than an adversarial perturbation that flips the model output. Here, we propose
a data-free, zero-shot trigger-inversion strategy that restricts the search
space while avoiding strong assumptions on trigger appearance. Specifically, we
incorporate a diffusion-based generator guided by the target classifier;
through iterative generation, we produce candidate triggers that align with the
internal representations the model relies on for malicious behavior. Empirical
evaluations, both quantitative and qualitative, show that our approach
reconstructs triggers that effectively distinguish clean versus Trojaned
models. DISTIL surpasses alternative methods by high margins, achieving up to
7.1% higher accuracy on the BackdoorBench dataset and a 9.4% improvement on
trojaned object detection model scanning, offering a promising new direction
for reliable backdoor defense without reliance on extensive data or strong
prior assumptions about triggers. The code is available at
https://github.com/AdaptiveMotorControlLab/DISTIL.

</details>


### [61] [Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards Generalisable Neural Surrogate Models](https://arxiv.org/abs/2507.22817)
*Patryk Rygiel,Julian Suk,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: A geometric deep learning model is proposed to estimate hemodynamics in AAA patients, showing strong generalization across real-world variations like geometry remodeling and boundary condition changes.


<details>
  <summary>Details</summary>
Motivation: Traditional CFD simulations for AAA hemodynamics are computationally expensive, prompting the need for faster, accurate alternatives like geometric deep learning.

Method: An E(3)-equivariant deep learning model with robust geometrical descriptors and projective geometric algebra is trained on CT scans of 100 AAA patients, using lumen geometries and reference CFD simulations.

Result: The model generalizes well within and outside the training distribution, handles geometry remodeling, boundary condition changes, and different artery tree topologies, and is mesh resolution-agnostic.

Conclusion: The proposed model offers accurate, generalizable hemodynamic estimation, with potential clinical applications.

Abstract: Abdominal aortic aneurysms (AAAs) are pathologic dilatations of the abdominal
aorta posing a high fatality risk upon rupture. Studying AAA progression and
rupture risk often involves in-silico blood flow modelling with computational
fluid dynamics (CFD) and extraction of hemodynamic factors like time-averaged
wall shear stress (TAWSS) or oscillatory shear index (OSI). However, CFD
simulations are known to be computationally demanding. Hence, in recent years,
geometric deep learning methods, operating directly on 3D shapes, have been
proposed as compelling surrogates, estimating hemodynamic parameters in just a
few seconds. In this work, we propose a geometric deep learning approach to
estimating hemodynamics in AAA patients, and study its generalisability to
common factors of real-world variation. We propose an E(3)-equivariant deep
learning model utilising novel robust geometrical descriptors and projective
geometric algebra. Our model is trained to estimate transient WSS using a
dataset of CT scans of 100 AAA patients, from which lumen geometries are
extracted and reference CFD simulations with varying boundary conditions are
obtained. Results show that the model generalizes well within the distribution,
as well as to the external test set. Moreover, the model can accurately
estimate hemodynamics across geometry remodelling and changes in boundary
conditions. Furthermore, we find that a trained model can be applied to
different artery tree topologies, where new and unseen branches are added
during inference. Finally, we find that the model is to a large extent agnostic
to mesh resolution. These results show the accuracy and generalisation of the
proposed model, and highlight its potential to contribute to hemodynamic
parameter estimation in clinical practice.

</details>


### [62] [Bi-Level Optimization for Self-Supervised AI-Generated Face Detection](https://arxiv.org/abs/2507.22824)
*Mian Zou,Nan Zhong,Baosheng Yu,Yibing Zhan,Kede Ma*

Main category: cs.CV

TL;DR: A self-supervised method using bi-level optimization improves AI-generated face detection by pretraining on photographic images and optimizing pretext tasks, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated face detectors rely on specific generators, limiting generalization to new techniques. This work aims to enhance detection across diverse generative methods.

Method: Bi-level optimization: inner loop pretrains a vision encoder on photographic images using weighted pretext tasks (EXIF tag classification, ranking, manipulation detection). Outer loop optimizes task weights for better detection.

Result: Detectors outperform existing methods in one-class and binary classification, showing strong generalization to unseen generators.

Conclusion: The self-supervised approach aligns pretraining with detection goals, achieving robust performance and generalization for AI-generated face detection.

Abstract: AI-generated face detectors trained via supervised learning typically rely on
synthesized images from specific generators, limiting their generalization to
emerging generative techniques. To overcome this limitation, we introduce a
self-supervised method based on bi-level optimization. In the inner loop, we
pretrain a vision encoder only on photographic face images using a set of
linearly weighted pretext tasks: classification of categorical exchangeable
image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of
artificial face manipulations. The outer loop then optimizes the relative
weights of these pretext tasks to enhance the coarse-grained detection of
manipulated faces, serving as a proxy task for identifying AI-generated faces.
In doing so, it aligns self-supervised learning more closely with the ultimate
goal of AI-generated face detection. Once pretrained, the encoder remains
fixed, and AI-generated faces are detected either as anomalies under a Gaussian
mixture model fitted to photographic face features or by a lightweight
two-layer perceptron serving as a binary classifier. Extensive experiments
demonstrate that our detectors significantly outperform existing approaches in
both one-class and binary classification settings, exhibiting strong
generalization to unseen generators.

</details>


### [63] [DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion](https://arxiv.org/abs/2507.22825)
*Qingcheng Zhao,Xiang Zhang,Haiyang Xu,Zeyuan Chen,Jianwen Xie,Yuan Gao,Zhuowen Tu*

Main category: cs.CV

TL;DR: DepR is a depth-guided single-view scene reconstruction framework that uses depth throughout training and inference, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Previous methods underutilize depth information, focusing only on object layout estimation during inference. DepR aims to fully exploit depth for better reconstruction.

Method: DepR integrates depth-guided conditioning into diffusion models for shape priors and uses depth to guide DDIM sampling and layout optimization during inference.

Result: DepR achieves state-of-the-art performance and strong generalization on synthetic and real-world datasets.

Conclusion: Depth-guided conditioning and inference significantly improve single-view scene reconstruction, even with limited training data.

Abstract: We propose DepR, a depth-guided single-view scene reconstruction framework
that integrates instance-level diffusion within a compositional paradigm.
Instead of reconstructing the entire scene holistically, DepR generates
individual objects and subsequently composes them into a coherent 3D layout.
Unlike previous methods that use depth solely for object layout estimation
during inference and therefore fail to fully exploit its rich geometric
information, DepR leverages depth throughout both training and inference.
Specifically, we introduce depth-guided conditioning to effectively encode
shape priors into diffusion models. During inference, depth further guides DDIM
sampling and layout optimization, enhancing alignment between the
reconstruction and the input image. Despite being trained on limited synthetic
data, DepR achieves state-of-the-art performance and demonstrates strong
generalization in single-view scene reconstruction, as shown through
evaluations on both synthetic and real-world datasets.

</details>


### [64] [ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827)
*Yilei Jiang,Yaozhi Zheng,Yuxuan Wan,Jiaming Han,Qunzhong Wang,Michael R. Lyu,Xiangyu Yue*

Main category: cs.CV

TL;DR: A modular multi-agent framework for UI-to-code generation improves robustness and interpretability by grounding, planning, and generating code in stages, outperforming black-box methods.


<details>
  <summary>Details</summary>
Motivation: Automating UI-to-code transformation can speed up development and democratize design, but existing text-to-code LLMs lack spatial and visual intent capture.

Method: A three-stage framework: grounding (detects UI components), planning (constructs hierarchical layout), and generation (produces HTML/CSS code). Extended into a scalable data engine for synthetic image-code pairs.

Result: Achieves state-of-the-art performance in layout accuracy, structural coherence, and code correctness. Fine-tuned VLM improves UI understanding and code quality.

Conclusion: The framework enhances UI-to-code generation with interpretability and scalability, offering practical benefits over end-to-end methods.

Abstract: Automating the transformation of user interface (UI) designs into front-end
code holds significant promise for accelerating software development and
democratizing design workflows. While recent large language models (LLMs) have
demonstrated progress in text-to-code generation, many existing approaches rely
solely on natural language prompts, limiting their effectiveness in capturing
spatial layout and visual design intent. In contrast, UI development in
practice is inherently multimodal, often starting from visual sketches or
mockups. To address this gap, we introduce a modular multi-agent framework that
performs UI-to-code generation in three interpretable stages: grounding,
planning, and generation. The grounding agent uses a vision-language model to
detect and label UI components, the planning agent constructs a hierarchical
layout using front-end engineering priors, and the generation agent produces
HTML/CSS code via adaptive prompt-based synthesis. This design improves
robustness, interpretability, and fidelity over end-to-end black-box methods.
Furthermore, we extend the framework into a scalable data engine that
automatically produces large-scale image-code pairs. Using these synthetic
examples, we fine-tune and reinforce an open-source VLM, yielding notable gains
in UI understanding and code quality. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in layout accuracy,
structural coherence, and code correctness. Our code is made publicly available
at https://github.com/leigest519/ScreenCoder.

</details>


### [65] [CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models](https://arxiv.org/abs/2507.22828)
*Kedong Xiu,Saiqian Zhang*

Main category: cs.CV

TL;DR: CapRecover is a framework to recover high-level semantic content (e.g., labels, captions) from intermediate features in split-DNN VLMs, addressing privacy risks without image reconstruction.


<details>
  <summary>Details</summary>
Motivation: Privacy risks from semantic information leakage in split-DNN VLMs, where existing methods produce blurry images, necessitating a direct semantic recovery approach.

Method: Proposes CapRecover, a cross-modality inversion framework that recovers semantic content directly from intermediate features, avoiding image reconstruction.

Result: Achieves 92.71% Top-1 label accuracy on CIFAR-10 and fluent captions on COCO2017 (ROUGE-L 0.52). Deeper layers encode more semantics. Introduces noise-based protection.

Conclusion: CapRecover effectively recovers semantics and mitigates leakage with a noise-based method, offering privacy without extra training costs.

Abstract: As Vision-Language Models (VLMs) are increasingly deployed in split-DNN
configurations--with visual encoders (e.g., ResNet, ViT) operating on user
devices and sending intermediate features to the cloud--there is a growing
privacy risk from semantic information leakage. Existing approaches to
reconstructing images from these intermediate features often result in blurry,
semantically ambiguous images. To directly address semantic leakage, we propose
CapRecover, a cross-modality inversion framework that recovers high-level
semantic content, such as labels or captions, directly from intermediate
features without image reconstruction.
  We evaluate CapRecover on multiple datasets and victim models, demonstrating
strong performance in semantic recovery. Specifically, CapRecover achieves up
to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from
ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis
further reveals that deeper convolutional layers encode significantly more
semantic information compared to shallow layers. To mitigate semantic leakage,
we introduce a simple yet effective protection method: adding random noise to
intermediate features at each layer and removing the noise in the next layer.
Experimental results show that this approach prevents semantic leakage without
additional training costs.

</details>


### [66] [TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning](https://arxiv.org/abs/2507.22872)
*Siqi Luo,Haoran Yang,Yi Xin,Mingyang Yi,Guangyang Wu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: TR-PTS is a task-driven framework for efficient fine-tuning of large models by selecting task-relevant parameters and tokens, improving both accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Large pre-trained models are costly to fine-tune, and existing PEFT methods are task-agnostic, leading to inefficiency and suboptimal performance.

Method: TR-PTS uses Fisher Information Matrix for layer-wise parameter selection and dynamic token selection to focus on task-discriminative information.

Result: TR-PTS outperforms full fine-tuning by 3.40% on FGVC and 10.35% on VTAB-1k, achieving state-of-the-art performance.

Conclusion: TR-PTS effectively balances computational efficiency and accuracy by task-specific adaptation, setting a new benchmark in PEFT methods.

Abstract: Large pre-trained models achieve remarkable performance in vision tasks but
are impractical for fine-tuning due to high computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating
only a subset of parameters; however, most existing approaches are
task-agnostic, failing to fully exploit task-specific adaptations, which leads
to suboptimal efficiency and performance. To address this limitation, we
propose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven
framework that enhances both computational efficiency and accuracy.
Specifically, we introduce Task-Relevant Parameter Selection, which utilizes
the Fisher Information Matrix (FIM) to identify and fine-tune only the most
informative parameters in a layer-wise manner, while keeping the remaining
parameters frozen. Simultaneously, Task-Relevant Token Selection dynamically
preserves the most informative tokens and merges redundant ones, reducing
computational overhead. By jointly optimizing parameters and tokens, TR-PTS
enables the model to concentrate on task-discriminative information. We
evaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves
state-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%,
respectively. The code are available at https://github.com/synbol/TR-PTS.

</details>


### [67] [LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content](https://arxiv.org/abs/2507.22873)
*Simon Pochinda,Momen K. Tageldeen,Mark Thompson,Tony Rinaldi,Troy Giorshev,Keith Lee,Jie Zhou,Frederick Walls*

Main category: cs.CV

TL;DR: An AI-based low-complexity scaler (LCS) is proposed to offload GPU workload to NPUs, achieving better perceptual quality than existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the growing GPU workload in modern games by leveraging AI for efficient upscaling.

Method: Train LCS on GameIR image pairs using adversarial training, reparameterization, and quantization. Compare with AMD EASF and FSR1.

Result: LCS outperforms AMD methods in perceptual quality, showing promise for resource-constrained devices.

Conclusion: ESR models like LCS are viable for efficient upscaling, reducing GPU dependency.

Abstract: The increasing complexity of content rendering in modern games has led to a
problematic growth in the workload of the GPU. In this paper, we propose an
AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient
super-resolution (ESR) models which could offload the workload on the GPU to a
low-power device such as a neural processing unit (NPU). The LCS is trained on
GameIR image pairs natively rendered at low and high resolution. We utilize
adversarial training to encourage reconstruction of perceptually important
details, and apply reparameterization and quantization techniques to reduce
model complexity and size. In our comparative analysis we evaluate the LCS
alongside the publicly available AMD hardware-based Edge Adaptive Scaling
Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different
metrics, and find that the LCS achieves better perceptual quality,
demonstrating the potential of ESR models for upscaling on resource-constrained
devices.

</details>


### [68] [Viser: Imperative, Web-based 3D Visualization in Python](https://arxiv.org/abs/2507.22885)
*Brent Yi,Chung Min Kim,Justin Kerr,Gina Wu,Rebecca Feng,Anthony Zhang,Jonas Kulhanek,Hongsuk Choi,Yi Ma,Matthew Tancik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: Viser is a 3D visualization library for Python, designed for computer vision and robotics, offering easy-to-use and extensible 3D and 2D GUI primitives.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance 3D visualization in Python for computer vision and robotics applications.

Method: Provides an imperative-style API and a web-based viewer, along with a comprehensive set of 3D scene and 2D GUI primitives.

Result: A flexible and compatible library that supports modern programming workflows.

Conclusion: Viser successfully bridges the gap in 3D visualization tools for Python, offering extensibility and ease of use.

Abstract: We present Viser, a 3D visualization library for computer vision and
robotics. Viser aims to bring easy and extensible 3D visualization to Python:
we provide a comprehensive set of 3D scene and 2D GUI primitives, which can be
used independently with minimal setup or composed to build specialized
interfaces. This technical report describes Viser's features, interface, and
implementation. Key design choices include an imperative-style API and a
web-based viewer, which improve compatibility with modern programming patterns
and workflows.

</details>


### [69] [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation](https://arxiv.org/abs/2507.22886)
*Kaining Ying,Henghui Ding,Guanquan Jie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper introduces OmniAVS, a new dataset for referring audio-visual segmentation, and OISA, a method leveraging MLLM for multimodal reasoning and segmentation, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: To address challenges in integrating multimodal information and reasoning about audiovisual content in referring audio-visual segmentation (RAVS).

Method: Proposes OmniAVS dataset with diverse multimodal expressions and introduces OISA, which uses MLLM for reasoning-based segmentation.

Result: OISA outperforms existing methods on OmniAVS and achieves competitive results on related tasks.

Conclusion: OmniAVS and OISA advance RAVS by enabling deeper multimodal understanding and reasoning, setting a foundation for future research.

Abstract: Referring audio-visual segmentation (RAVS) has recently seen significant
advancements, yet challenges remain in integrating multimodal information and
deeply understanding and reasoning about audiovisual content. To extend the
boundaries of RAVS and facilitate future research in this field, we propose
Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset
containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS
stands out with three key innovations: (1) 8 types of multimodal expressions
that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on
understanding audio content beyond just detecting their presence; and (3) the
inclusion of complex reasoning and world knowledge in expressions. Furthermore,
we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the
challenges of multimodal reasoning and fine-grained understanding of
audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and
perform reasoning-based segmentation. Extensive experiments show that OISA
outperforms existing methods on OmniAVS and achieves competitive results on
other related tasks.

</details>
