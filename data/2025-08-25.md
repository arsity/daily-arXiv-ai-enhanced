<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 55]
- [q-bio.NC](#q-bio.NC) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text-Driven 3D Hand Motion Generation from Sign Language Data](https://arxiv.org/abs/2508.15902)
*Léore Bensabath,Mathis Petrovich,Gül Varol*

Main category: cs.CV

TL;DR: Training a text-conditioned 3D hand motion generative model using large-scale sign language data and LLM-generated descriptions, achieving robust performance across domains.


<details>
  <summary>Details</summary>
Motivation: To create a generative model that can produce 3D hand motions based on natural language descriptions specifying motion characteristics like handshapes, locations, and movements.

Method: Automatically build pairs of 3D hand motions and textual labels using large-scale sign language videos with pseudo-annotated categories, translated via LLM using sign attribute dictionary and motion-script cues. Train a text-conditioned hand motion diffusion model (HandMDM).

Result: The model demonstrates robustness across domains including unseen sign categories from the same language, signs from different sign languages, and non-sign hand movements.

Conclusion: Successfully developed a text-to-hand-motion generation system with extensive experimental validation, with plans to release trained models and data to support future research in this emerging field.

Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on
natural language descriptions specifying motion characteristics such as
handshapes, locations, finger/hand/arm movements. To this end, we automatically
build pairs of 3D hand motions and their associated textual labels with
unprecedented scale. Specifically, we leverage a large-scale sign language
video dataset, along with noisy pseudo-annotated sign categories, which we
translate into hand motion descriptions via an LLM that utilizes a dictionary
of sign attributes, as well as our complementary motion-script cues. This data
enables training a text-conditioned hand motion diffusion model HandMDM, that
is robust across domains such as unseen sign categories from the same sign
language, but also signs from another sign language and non-sign hand
movements. We contribute extensive experimental investigation of these
scenarios and will make our trained models and data publicly available to
support future research in this relatively new field.

</details>


### [2] [VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos](https://arxiv.org/abs/2508.15903)
*Kaining Li,Shuwei He,Zihan Xu*

Main category: cs.CV

TL;DR: VT-LVLM-AR framework bridges LVLMs with video action recognition using efficient video-to-event mapping and prompt tuning, achieving SOTA results on NTU datasets with improved interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning struggles with long-term video action recognition due to computational overhead and limited temporal understanding. While LLMs/LVLMs excel at multi-modal reasoning, they haven't been effectively applied to continuous video streams for fine-grained action recognition.

Method: VT-LVLM-AR uses Video-to-Event Mapper (VTEM) to convert raw video into compact visual event sequences via spatio-temporal feature extraction, adaptive pooling, and conceptual quantization. These sequences are processed by frozen LLaVA-1.5 model adapted with P-Tuning v2 for action classification.

Result: Achieves state-of-the-art performance: 94.1% accuracy on NTU RGB+D X-Sub. Outperforms existing methods on both NTU RGB+D and NTU RGB+D 120 datasets. Ablation studies confirm VTEM components' importance and prompt tuning efficacy.

Conclusion: Demonstrates successful LVLM application to video action recognition through effective video-to-language translation and efficient adaptation. Highlights potential for robust and interpretable video understanding.

Abstract: Human action recognition in long-term videos, characterized by complex
backgrounds and subtle action differences, poses significant challenges for
traditional deep learning models due to computational overhead, difficulty in
capturing long-range temporal dependencies, and limited semantic understanding.
While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
have shown remarkable capabilities in multi-modal understanding and reasoning,
their direct application to continuous video streams for fine-grained action
recognition remains an open problem. This paper introduces VT-LVLM-AR
(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a
novel framework designed to bridge this gap. VT-LVLM-AR comprises a
Video-to-Event Mapper (VTEM) that efficiently transforms raw video into
compact, semantically rich, and temporally coherent "visual event sequences"
through lightweight spatio-temporal feature extraction, adaptive temporal
pooling, and conceptual quantization with an event coherence bias. These visual
event sequences are then fed into an LVLM-based Action Reasoning module,
specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt
Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on
the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR
consistently achieves state-of-the-art performance, surpassing existing methods
(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the
critical contributions of VTEM's components and the efficacy of Prompt Tuning,
while human evaluations underscore the interpretability of our visual event
representations. This work highlights the immense potential of leveraging LVLMs
for robust and interpretable video action understanding through effective
video-to-language translation and efficient model adaptation.

</details>


### [3] [Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping](https://arxiv.org/abs/2508.15904)
*Dexuan He,Xiao Zhou,Wenbin Guan,Liyuan Zhang,Xiaoman Zhang,Sinuo Xu,Ge Wang,Lifeng Wang,Xiaojun Yuan,Xin Sun,Yanfeng Wang,Kun Sun,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: PathPT is a novel vision-language framework that improves rare cancer diagnosis by leveraging pathology foundation models through spatially-aware visual aggregation and task-specific prompt tuning, outperforming existing methods on both rare and common cancer datasets.


<details>
  <summary>Details</summary>
Motivation: Rare cancers face diagnostic challenges due to limited expert availability, especially in pediatric oncology where they represent over 70% of cases. Existing multi-instance learning methods rely only on visual features and overlook cross-modal knowledge, compromising interpretability for rare cancer diagnosis.

Method: PathPT converts WSI-level supervision into fine-grained tile-level guidance using VL models' zero-shot capabilities. It employs spatially-aware visual aggregation and task-specific prompt tuning to preserve cancerous region localization and enable cross-modal reasoning through histopathology-aligned prompts.

Result: PathPT consistently delivered superior performance across eight rare cancer datasets (56 subtypes, 2,910 WSIs) and three common cancer datasets, achieving substantial gains in subtyping accuracy and cancerous region grounding ability compared to four state-of-the-art VL models and four MIL frameworks.

Conclusion: PathPT advances AI-assisted diagnosis for rare cancers by providing a scalable solution that improves subtyping accuracy in settings with limited access to specialized expertise, effectively leveraging vision-language foundation models for clinical pathology applications.

Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic
challenges due to limited expert availability-especially in pediatric oncology,
where they represent over 70% of cases. While pathology vision-language (VL)
foundation models show promising zero-shot capabilities for common cancer
subtyping, their clinical performance for rare cancers remains limited.
Existing multi-instance learning (MIL) methods rely only on visual features,
overlooking cross-modal knowledge and compromising interpretability critical
for rare cancer diagnosis. To address this limitation, we propose PathPT, a
novel framework that fully exploits the potential of vision-language pathology
foundation models through spatially-aware visual aggregation and task-specific
prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision
into fine-grained tile-level guidance by leveraging the zero-shot capabilities
of VL models, thereby preserving localization on cancerous regions and enabling
cross-modal reasoning through prompts aligned with histopathological semantics.
We benchmark PathPT on eight rare cancer datasets(four adult and four
pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer
datasets, evaluating four state-of-the-art VL models and four MIL frameworks
under three few-shot settings. Results show that PathPT consistently delivers
superior performance, achieving substantial gains in subtyping accuracy and
cancerous region grounding ability. This work advances AI-assisted diagnosis
for rare cancers, offering a scalable solution for improving subtyping accuracy
in settings with limited access to specialized expertise.

</details>


### [4] [Semantic-Aware Ship Detection with Vision-Language Integration](https://arxiv.org/abs/2508.15930)
*Jiahao Li,Jiancheng Pan,Yuze Sun,Xiaomeng Huang*

Main category: cs.CV

TL;DR: A novel ship detection framework combining Vision-Language Models with multi-scale adaptive sliding window strategy, using specialized ShipSem-VL dataset for fine-grained semantic-aware detection.


<details>
  <summary>Details</summary>
Motivation: Existing ship detection methods struggle to capture fine-grained semantic information in complex scenarios, limiting effectiveness for maritime monitoring, shipping logistics, and environmental studies.

Method: Proposes a detection framework that integrates Vision-Language Models with multi-scale adaptive sliding window strategy, using a specialized ShipSem-VL dataset designed for fine-grained ship attributes.

Result: The framework is evaluated through three well-defined tasks, demonstrating comprehensive performance analysis and effectiveness in advancing Semantic-Aware Ship Detection from multiple perspectives.

Conclusion: The proposed framework effectively addresses limitations of existing methods by enabling fine-grained semantic-aware ship detection through VLMs and specialized dataset, showing promise for complex maritime scenarios.

Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging
applications, such as maritime activity monitoring, shipping logistics, and
environmental studies. However, existing methods often struggle to capture
fine-grained semantic information, limiting their effectiveness in complex
scenarios. To address these challenges, we propose a novel detection framework
that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding
window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we
introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture
fine-grained ship attributes. We evaluate our framework through three
well-defined tasks, providing a comprehensive analysis of its performance and
demonstrating its effectiveness in advancing SASD from multiple perspectives.

</details>


### [5] [Automatic Retrieval of Specific Cows from Unlabeled Videos](https://arxiv.org/abs/2508.15945)
*Jiawen Lyu,Manu Ramesh,Madison Simonds,Jacquelyn P. Boerman,Amy R. Reibman*

Main category: cs.CV

TL;DR: A hands-free automated video system for cataloging and identifying dairy cows using computer vision without deep learning, demonstrated on unconstrained cow movement videos.


<details>
  <summary>Details</summary>
Motivation: Few automated video systems exist for hands-free cataloging and identification of dairy cows in herds, creating a need for efficient cow management solutions.

Method: System includes AutoCattloger (builds cow catalog from single video clip per cow), eidetic cow recognizer (no deep learning), and CowFinder (identifies cows in continuous video streams).

Result: Successfully demonstrated value in finding individual cows in unlabeled, unsegmented videos of cows walking unconstrained through milking parlor holding areas.

Conclusion: The system provides an effective automated solution for dairy cow identification and cataloging using video analysis without requiring deep learning approaches.

Abstract: Few automated video systems are described in the open literature that enable
hands-free cataloging and identification (ID) of cows in a dairy herd. In this
work, we describe our system, composed of an AutoCattloger, which builds a
Cattlog of dairy cows in a herd with a single input video clip per cow, an
eidetic cow recognizer which uses no deep learning to ID cows, and a CowFinder,
which IDs cows in a continuous stream of video. We demonstrate its value in
finding individuals in unlabeled, unsegmented videos of cows walking
unconstrained through the holding area of a milking parlor.

</details>


### [6] [Investigating Different Geo Priors for Image Classification](https://arxiv.org/abs/2508.15946)
*Angela Zhu,Christian Lange,Max Hamilton*

Main category: cs.CV

TL;DR: SINR models serve as effective geographical priors for visual species classification when location data is available, though their optimization differs from creating accurate range maps.


<details>
  <summary>Details</summary>
Motivation: To leverage spatial patterns from species distribution models as priors for vision-based species classification, particularly when location information is accessible.

Method: Evaluated various Spatial Implicit Neural Representations (SINR) models as geographical priors, tested different model configurations, and developed strategies for handling predictions of species not included in Geo Prior training.

Result: Identified key factors that contribute to the effectiveness of SINR models as Geo Priors, which differ from the factors needed for creating accurate species range maps.

Conclusion: SINR models can effectively serve as geographical priors for visual species classification, but require different optimization approaches than those used for traditional range mapping.

Abstract: Species distribution models encode spatial patterns of species occurrence
making them effective priors for vision-based species classification when
location information is available. In this study, we evaluate various SINR
(Spatial Implicit Neural Representations) models as a geographical prior for
visual classification of species from iNaturalist observations. We explore the
impact of different model configurations and adjust how we handle predictions
for species not included in Geo Prior training. Our analysis reveals factors
that contribute to the effectiveness of these models as Geo Priors, factors
that may differ from making accurate range maps.

</details>


### [7] [Representation Learning with Adaptive Superpixel Coding](https://arxiv.org/abs/2508.15959)
*Mahmoud Khalil,Ahmad Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: ASC is a self-supervised Transformer model that uses adaptive superpixel layers instead of fixed patch partitioning, outperforming existing methods on image tasks.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional Vision Transformers that rely on fixed-size, non-adaptive patch partitioning and domain-specific grid structures.

Method: Proposes Adaptive Superpixel Coding (ASC) with adaptive superpixel layers that dynamically adjust to image content, using self-supervised learning with Transformers.

Result: Outperforms widely-used alternatives on standard image downstream task benchmarks.

Conclusion: Adaptive superpixel-based partitioning is more effective than fixed patch approaches for vision Transformers, enabling better performance on image tasks.

Abstract: Deep learning vision models are typically tailored for specific modalities
and often rely on domain-specific assumptions, such as the grid structures used
by nearly all existing vision models. In this work, we propose a
self-supervised model based on Transformers, which we call Adaptive Superpixel
Coding (ASC). The key insight of our model is to overcome the limitations of
traditional Vision Transformers, which depend on fixed-size and non-adaptive
patch partitioning. Instead, ASC employs adaptive superpixel layers that
dynamically adjust to the underlying image content. We analyze key properties
of the approach that make it effective, and find that our method outperforms
widely-used alternatives on standard image downstream task benchmarks.

</details>


### [8] [Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification](https://arxiv.org/abs/2508.15960)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: Glo-VLMs framework adapts vision-language models for fine-grained glomerular classification with limited labeled data, achieving strong performance with only 8 shots per class.


<details>
  <summary>Details</summary>
Motivation: Vision-language models show potential in digital pathology but struggle with fine-grained disease-specific classification tasks like distinguishing glomerular subtypes due to subtle morphological variations and difficulty aligning visual patterns with clinical terminology.

Method: Systematic framework leveraging curated pathology images and clinical text prompts for joint image-text representation learning. Evaluates various VLM architectures and adaptation strategies under few-shot learning paradigm with standardized multi-class metrics.

Result: Fine-tuned VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with only 8 shots per class, demonstrating effective adaptation for fine-grained medical image classification with limited supervision.

Conclusion: Foundation models can be effectively adapted for specialized clinical research applications even with highly limited supervision, showing practical potential for fine-grained medical image classification tasks.

Abstract: Vision-language models (VLMs) have shown considerable potential in digital
pathology, yet their effectiveness remains limited for fine-grained,
disease-specific classification tasks such as distinguishing between glomerular
subtypes. The subtle morphological variations among these subtypes, combined
with the difficulty of aligning visual patterns with precise clinical
terminology, make automated diagnosis in renal pathology particularly
challenging. In this work, we explore how large pretrained VLMs can be
effectively adapted to perform fine-grained glomerular classification, even in
scenarios where only a small number of labeled examples are available. In this
work, we introduce Glo-VLMs, a systematic framework designed to explore the
adaptation of VLMs to fine-grained glomerular classification in
data-constrained settings. Our approach leverages curated pathology images
alongside clinical text prompts to facilitate joint image-text representation
learning for nuanced renal pathology subtypes. By assessing various VLMs
architectures and adaptation strategies under a few-shot learning paradigm, we
explore how both the choice of method and the amount of labeled data impact
model performance in clinically relevant scenarios. To ensure a fair
comparison, we evaluate all models using standardized multi-class metrics,
aiming to clarify the practical requirements and potential of large pretrained
models for specialized clinical research applications. As a result, fine-tuning
the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with
only 8 shots per class, demonstrating that even with highly limited
supervision, foundation models can be effectively adapted for fine-grained
medical image classification.

</details>


### [9] [Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing](https://arxiv.org/abs/2508.15973)
*Minh-Tan Pham*

Main category: cs.CV

TL;DR: A comprehensive overview of label-efficient learning methods for computer vision and remote sensing, focusing on learning from limited annotations and leveraging unlabeled data across four main research axes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning effectively from limited or partially annotated data in real-world applications, particularly in remote sensing where data exhibits unique characteristics like multi-modality, spatial resolution variability, and scene heterogeneity.

Method: Four main approaches: (1) weakly supervised learning with anomaly-aware representations, (2) multi-task learning with disjoint annotations, (3) self-supervised/supervised contrastive learning with multimodal data, and (4) few-shot learning with hierarchical class modeling.

Result: Extensive experimental results across natural and remote sensing datasets show successful performance improvements in object detection, semantic segmentation, and scene classification tasks using label-efficient methods.

Conclusion: The research demonstrates effective label-efficient learning approaches for computer vision and remote sensing, with ongoing work focused on scaling and enhancing these methods for broader real-world applications.

Abstract: This manuscript presents a series of my selected contributions to the topic
of label-efficient learning in computer vision and remote sensing. The central
focus of this research is to develop and adapt methods that can learn
effectively from limited or partially annotated data, and can leverage abundant
unlabeled data in real-world applications. The contributions span both
methodological developments and domain-specific adaptations, in particular
addressing challenges unique to Earth observation data such as multi-modality,
spatial resolution variability, and scene heterogeneity. The manuscript is
organized around four main axes including (1) weakly supervised learning for
object discovery and detection based on anomaly-aware representations learned
from large amounts of background images; (2) multi-task learning that jointly
trains on multiple datasets with disjoint annotations to improve performance on
object detection and semantic segmentation; (3) self-supervised and supervised
contrastive learning with multimodal data to enhance scene classification in
remote sensing; and (4) few-shot learning for hierarchical scene classification
using both explicit and implicit modeling of class hierarchies. These
contributions are supported by extensive experimental results across natural
and remote sensing datasets, reflecting the outcomes of several collaborative
research projects. The manuscript concludes by outlining ongoing and future
research directions focused on scaling and enhancing label-efficient learning
for real-world applications.

</details>


### [10] [Panoptic Segmentation of Environmental UAV Images : Litter Beach](https://arxiv.org/abs/2508.15985)
*Ousmane Youme,Jean Marie Dembélé,Eugene C. Ezin,Christophe Cambier*

Main category: cs.CV

TL;DR: Using CNN-based segmentation methods for marine litter detection from UAV imagery to overcome challenges with heterogeneous beach environments.


<details>
  <summary>Details</summary>
Motivation: Marine litter monitoring is a global problem, and UAVs provide higher resolution imagery than satellites for local areas, but basic CNN models struggle with beach heterogeneity including sand reflections, human footprints, shadows, algae, and other artifacts.

Method: Employ instance-based segmentation and panoptic segmentation methods that demonstrate good accuracy even with limited training samples.

Result: The segmentation approaches show improved robustness and reduced inference errors compared to basic CNN models when dealing with complex beach environments.

Conclusion: CNN-based segmentation methods are more appropriate than basic CNN models for marine litter detection in heterogeneous beach conditions captured by UAV imagery, offering better accuracy with fewer samples.

Abstract: Convolutional neural networks (CNN) have been used efficiently in several
fields, including environmental challenges. In fact, CNN can help with the
monitoring of marine litter, which has become a worldwide problem. UAVs have
higher resolution and are more adaptable in local areas than satellite images,
making it easier to find and count trash. Since the sand is heterogeneous, a
basic CNN model encounters plenty of inferences caused by reflections of sand
color, human footsteps, shadows, algae present, dunes, holes, and tire tracks.
For these types of images, other CNN models, such as CNN-based segmentation
methods, may be more appropriate. In this paper, we use an instance-based
segmentation method and a panoptic segmentation method that show good accuracy
with just a few samples. The model is more robust and less

</details>


### [11] [Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset](https://arxiv.org/abs/2508.15986)
*Jerry Cao-Xue,Tien Comlekoglu,Keyi Xue,Guanliang Wang,Jiang Li,Gordon Laurie*

Main category: cs.CV

TL;DR: Synthetic fundus dataset SynFundus-1M enables training of multi-label retinal disease classifiers that generalize well to real clinical data, achieving AUC scores up to 0.9973 on synthetic validation and 0.8800-0.9126 on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Overcome scarcity of large annotated clinical datasets for retinal disease classification due to privacy concerns and high costs by leveraging synthetic data.

Method: Trained six modern architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, RETFound) with 5-fold multi-label stratified cross-validation, then created meta-ensemble with XGBoost on out-of-fold predictions.

Result: Ensemble model achieved macro-AUC of 0.9973 on internal validation. Strong generalization to real clinical datasets: 0.7972 AUC on DR dataset, 0.9126 on AIROGS glaucoma, 0.8800 macro-AUC on RFMiD multi-label dataset.

Conclusion: Synthetic data training enables accurate multi-pathology classification and effective generalization to real clinical images, providing viable pathway for comprehensive AI development in ophthalmology.

Abstract: The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.

</details>


### [12] [Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production](https://arxiv.org/abs/2508.15988)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: Proposes a novel Latent Diffusion Model approach for Sign Language Production that captures diversity while preserving visual quality and modeling non-manual attributes like emotions, achieving superior results on perceptual metrics.


<details>
  <summary>Details</summary>
Motivation: Existing SLP models struggle to capture diversity while maintaining visual quality and modeling non-manual attributes such as facial expressions and emotions in sign language representation.

Method: Leverages Latent Diffusion Model (LDM) to synthesize photorealistic digital avatars from generated reference images, with a novel sign feature aggregation module that explicitly models non-manual (face) and manual (hand) features separately.

Result: Experiments on YouTube-SL-25 dataset show superior visual quality compared to state-of-the-art methods, with significant improvements on perceptual metrics, while preserving linguistic content and enabling diversity through different ethnic backgrounds.

Conclusion: The proposed approach successfully addresses the diversity challenge in SLP by combining LDM with explicit modeling of manual and non-manual features, achieving both high visual quality and representation diversity.

Abstract: The diversity of sign representation is essential for Sign Language
Production (SLP) as it captures variations in appearance, facial expressions,
and hand movements. However, existing SLP models are often unable to capture
diversity while preserving visual quality and modelling non-manual attributes
such as emotions. To address this problem, we propose a novel approach that
leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital
avatars from a generated reference image. We propose a novel sign feature
aggregation module that explicitly models the non-manual features
(\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands).
We show that our proposed module ensures the preservation of linguistic content
while seamlessly using reference images with different ethnic backgrounds to
ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show
that our pipeline achieves superior visual quality compared to state-of-the-art
methods, with significant improvements on perceptual metrics.

</details>


### [13] [DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions](https://arxiv.org/abs/2508.16016)
*Aykut Sirma,Angelos Plastropoulos,Argyrios Zolotas,Gilbert Tang*

Main category: cs.CV

TL;DR: DRespNeT is a high-resolution aerial dataset for instance segmentation in post-earthquake environments, enabling real-time detection of structural elements, debris, and rescue assets to improve search-and-rescue operations.


<details>
  <summary>Details</summary>
Motivation: Timely identification of accessible entry points and structural obstacles is essential for effective search-and-rescue operations after earthquakes, but existing datasets rely on satellite imagery or coarse semantic labeling.

Method: Created DRespNeT dataset with detailed polygon-level instance segmentation annotations from 1080p aerial footage of disaster zones, including 28 critical classes. Evaluated using YOLO-based instance segmentation models (YOLOv8-seg).

Result: Optimized YOLOv8-DRN model achieves 92.7% mAP50 with 27 FPS inference speed on RTX-4090 GPU, meeting real-time operational requirements for multi-target detection.

Conclusion: DRespNeT dataset and models significantly enhance real-time situational awareness, support SAR teams and robotic systems, and improve human-robot collaboration for emergency response and survivor outcomes.

Abstract: Recent advancements in computer vision and deep learning have enhanced
disaster-response capabilities, particularly in the rapid assessment of
earthquake-affected urban environments. Timely identification of accessible
entry points and structural obstacles is essential for effective
search-and-rescue (SAR) operations. To address this need, we introduce
DRespNeT, a high-resolution dataset specifically developed for aerial instance
segmentation of post-earthquake structural environments. Unlike existing
datasets, which rely heavily on satellite imagery or coarse semantic labeling,
DRespNeT provides detailed polygon-level instance segmentation annotations
derived from high-definition (1080p) aerial footage captured in disaster zones,
including the 2023 Turkiye earthquake and other impacted regions. The dataset
comprises 28 operationally critical classes, including structurally compromised
buildings, access points such as doors, windows, and gaps, multiple debris
levels, rescue personnel, vehicles, and civilian visibility. A distinctive
feature of DRespNeT is its fine-grained annotation detail, enabling
differentiation between accessible and obstructed areas, thereby improving
operational planning and response efficiency. Performance evaluations using
YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate
significant gains in real-time situational awareness and decision-making. Our
optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27
FPS on an RTX-4090 GPU for multi-target detection, meeting real-time
operational requirements. The dataset and models support SAR teams and robotic
systems, providing a foundation for enhancing human-robot collaboration,
streamlining emergency response, and improving survivor outcomes.

</details>


### [14] [NeuralMeshing: Complete Object Mesh Extraction from Casual Captures](https://arxiv.org/abs/2508.16026)
*Floris Erich,Naoya Chiba,Abdullah Mustafa,Ryo Hanai,Noriaki Ando,Yusuke Yoshiyasu,Yukiyasu Domae*

Main category: cs.CV

TL;DR: Automated system for generating complete 3D object models from multiple videos using Structure-from-Motion and minimal manual input


<details>
  <summary>Details</summary>
Motivation: To enable accessible 3D scanning of everyday objects without requiring commercial 3D scanners, making geometric modeling more widely available

Method: Uses multiple videos with Structure-from-Motion techniques, requiring only one known point per video (automatically detected via fiducial markers like checkerboards or AR markers), then merges results to create complete meshes

Result: Successfully generates complete object meshes without hole filling by combining data from multiple video sources

Conclusion: The system provides an effective, accessible alternative to commercial 3D scanners for everyday object modeling using standard video input and minimal manual annotation

Abstract: How can we extract complete geometric models of objects that we encounter in
our daily life, without having access to commercial 3D scanners? In this paper
we present an automated system for generating geometric models of objects from
two or more videos. Our system requires the specification of one known point in
at least one frame of each video, which can be automatically determined using a
fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The
remaining frames are automatically positioned in world space by using
Structure-from-Motion techniques. By using multiple videos and merging results,
a complete object mesh can be generated, without having to rely on hole
filling. Code for our system is available from
https://github.com/FlorisE/NeuralMeshing.

</details>


### [15] [CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars](https://arxiv.org/abs/2508.16030)
*Jinyue Song,Hansol Ku,Jayneel Vora,Nelson Lee,Ahmad Kamari,Prasant Mohapatra,Parth Pathak*

Main category: cs.CV

TL;DR: CoVeRaP dataset enables multi-vehicle radar cooperation, showing 9x mAP improvement at IoU 0.9 through middle fusion with intensity encoding compared to single-vehicle baselines.


<details>
  <summary>Details</summary>
Motivation: FMCW radars are reliable in adverse weather but produce sparse, noisy point clouds that limit 3D object detection performance. Cooperative perception from multiple vehicles can overcome these limitations.

Method: Created CoVeRaP dataset with 21k time-aligned radar, camera, and GPS streams from multiple vehicles. Developed unified cooperative-perception framework with middle/late fusion options using multi-branch PointNet encoder with self-attention to fuse spatial, Doppler, and intensity features.

Result: Middle fusion with intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and consistently outperforms single-vehicle baselines.

Conclusion: Affordable radar sharing significantly improves detection robustness, establishing the first reproducible benchmark for multi-vehicle FMCW-radar perception. Dataset and code are publicly available.

Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse,
noisy point clouds constrain 3-D object detection. We therefore release
CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and
GPS streams from multiple vehicles across diverse manoeuvres. Built on this
data, we propose a unified cooperative-perception framework with middle- and
late-fusion options. Its baseline network employs a multi-branch PointNet-style
encoder enhanced with self-attention to fuse spatial, Doppler, and intensity
cues into a common latent space, which a decoder converts into 3-D bounding
boxes and per-point depth confidence. Experiments show that middle fusion with
intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and
consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the
first reproducible benchmark for multi-vehicle FMCW-radar perception and
demonstrates that affordable radar sharing markedly improves detection
robustness. Dataset and code are publicly available to encourage further
research.

</details>


### [16] [Wavelet-Enhanced PaDiM for Industrial Anomaly Detection](https://arxiv.org/abs/2508.16034)
*Cory Gardner,Byungseok Min,Tae-Hyuk Ahn*

Main category: cs.CV

TL;DR: WE-PaDiM enhances PaDiM by replacing random channel selection with structured wavelet-based feature selection, achieving strong anomaly detection performance on MVTec AD dataset.


<details>
  <summary>Details</summary>
Motivation: PaDiM's random channel selection for dimensionality reduction may discard structured information important for anomaly detection. The authors propose a more principled approach using wavelet transforms to select frequency-relevant features.

Method: WE-PaDiM applies 2D Discrete Wavelet Transform (DWT) to CNN feature maps from multiple backbone layers, selects specific frequency subbands (LL, LH, HL), spatially aligns them, and concatenates them channel-wise before modeling with PaDiM's multivariate Gaussian framework.

Result: Achieves 99.32% Image-AUC and 92.10% Pixel-AUC average results across 15 MVTec AD categories. Wavelet choices affect performance trade-offs: simpler wavelets with detail subbands enhance localization, while approximation bands improve image-level detection.

Conclusion: WE-PaDiM provides a competitive and interpretable alternative to random feature selection in PaDiM, achieving robust results suitable for industrial inspection with comparable efficiency.

Abstract: Anomaly detection and localization in industrial images are essential for
automated quality inspection. PaDiM, a prominent method, models the
distribution of normal image features extracted by pre-trained Convolutional
Neural Networks (CNNs) but reduces dimensionality through random channel
selection, potentially discarding structured information. We propose
Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform
(DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM
applies 2D DWT to feature maps from multiple backbone layers, selects specific
frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates
them channel-wise before modeling with PaDiM's multivariate Gaussian framework.
This DWT-before-concatenation strategy provides a principled method for feature
selection based on frequency content relevant to anomalies, leveraging
multi-scale wavelet information as an alternative to random selection. We
evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones
(ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in
anomaly detection and localization, yielding average results of 99.32%
Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized
configurations. Our analysis shows that wavelet choices affect performance
trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH)
often enhance localization, while approximation bands (LL) improve image-level
detection. WE-PaDiM thus offers a competitive and interpretable alternative to
random feature selection in PaDiM, achieving robust results suitable for
industrial inspection with comparable efficiency.

</details>


### [17] [Expandable Residual Approximation for Knowledge Distillation](https://arxiv.org/abs/2508.16050)
*Zhaoyi Yan,Binghui Chen,Yunfan Liu,Qixiang Ye*

Main category: cs.CV

TL;DR: ERA is a novel knowledge distillation method that uses residual decomposition and teacher weight integration to bridge the capacity gap between teacher and student models, achieving significant performance improvements on ImageNet and COCO benchmarks.


<details>
  <summary>Details</summary>
Motivation: The inherent learning capacity gap between teacher and student models hinders sufficient knowledge transfer in knowledge distillation, motivating the need for methods that can better bridge this disparity.

Method: Proposes Expandable Residual Approximation (ERA) that decomposes residual knowledge approximation into multiple steps using a Multi-Branched Residual Network (MBRNet), and introduces Teacher Weight Integration (TWI) to reuse teacher's head weights.

Result: Improves Top-1 accuracy on ImageNet by 1.41% and AP on MS COCO by 1.40, achieving leading performance across computer vision tasks.

Conclusion: ERA effectively addresses the capacity gap issue in knowledge distillation through residual decomposition and teacher weight reuse, demonstrating superior performance on major benchmarks.

Abstract: Knowledge distillation (KD) aims to transfer knowledge from a large-scale
teacher model to a lightweight one, significantly reducing computational and
storage requirements. However, the inherent learning capacity gap between the
teacher and student often hinders the sufficient transfer of knowledge,
motivating numerous studies to address this challenge. Inspired by the
progressive approximation principle in the Stone-Weierstrass theorem, we
propose Expandable Residual Approximation (ERA), a novel KD method that
decomposes the approximation of residual knowledge into multiple steps,
reducing the difficulty of mimicking the teacher's representation through a
divide-and-conquer approach. Specifically, ERA employs a Multi-Branched
Residual Network (MBRNet) to implement this residual knowledge decomposition.
Additionally, a Teacher Weight Integration (TWI) strategy is introduced to
mitigate the capacity disparity by reusing the teacher's head weights.
Extensive experiments show that ERA improves the Top-1 accuracy on the ImageNet
classification benchmark by 1.41% and the AP on the MS COCO object detection
benchmark by 1.40, as well as achieving leading performance across computer
vision tasks. Codes and models are available at
https://github.com/Zhaoyi-Yan/ERA.

</details>


### [18] [Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals](https://arxiv.org/abs/2508.16062)
*Ziqi Li,Abderraouf Amrani,Shri Rai,Hamid Laga*

Main category: cs.CV

TL;DR: Survey paper on deep learning-based 3D reconstruction of animal geometry, pose, and motion from RGB images/videos, categorizing state-of-the-art methods and discussing challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D scanning methods for animals are intrusive, expensive, and difficult to deploy in natural environments. There's a need for non-intrusive reconstruction methods using only RGB observations.

Method: Categorizes and analyzes deep learning techniques based on input modalities, 3D representation methods, reconstruction techniques, and training mechanisms. Compares performance of key methods.

Result: Provides comprehensive survey of emerging field, identifying strengths and limitations of current approaches and performance benchmarks.

Conclusion: Identifies current challenges and future research directions for non-intrusive 3D animal reconstruction from visual data, highlighting the field's growth and potential applications.

Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a
long-standing problem, which has a wide range of applications, from biology,
livestock management, and animal conservation and welfare to content creation
in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally,
3D models of real animals are obtained using 3D scanners. These, however, are
intrusive, often prohibitively expensive, and difficult to deploy in the
natural environment of the animals. In recent years, we have seen a significant
surge in deep learning-based techniques that enable the 3D reconstruction, in a
non-intrusive manner, of the shape and motion of dynamic objects just from
their RGB image and/or video observations. Several papers have explored their
application and extension to various types of animals. This paper surveys the
latest developments in this emerging and growing field of research. It
categorizes and discusses the state-of-the-art methods based on their input
modalities, the way the 3D geometry and motion of animals are represented, the
type of reconstruction techniques they use, and the training mechanisms they
adopt. It also analyzes the performance of some key methods, discusses their
strengths and limitations, and identifies current challenges and directions for
future research.

</details>


### [19] [A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection](https://arxiv.org/abs/2508.16069)
*Qifeng Liu,Dawei Zhao,Yabo Dong,Linzhi Shang,Liang Xiao,Juan Wang,Kunkong Zhao,Dongming Lu,Qi Zhu*

Main category: cs.CV

TL;DR: Proposes Voxel Diffusion Module (VDM) to enhance spatial diffusion in point cloud object detection, combining sparse 3D convolutions with Transformer/SSM models for improved accuracy across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Voxel-based representations in Transformer and SSM models lack spatial diffusion capability due to strict input-output dimension consistency requirements, limiting detection accuracy compared to CNN-based approaches.

Method: VDM module composed of sparse 3D convolutions, submanifold sparse convolutions, and residual connections. Outputs downsampled to 1/4 resolution for efficiency. Enhances voxel features through spatial context diffusion and fine-grained information aggregation.

Result: VDM-SSMs achieve 74.7 mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP on ONCE, setting new state-of-the-art performance across all datasets with consistent improvements over baseline models.

Conclusion: VDM effectively enhances voxel-level representation and spatial diffusion, demonstrating strong generalizability when integrated with Transformer- or SSM-based detection models for improved point cloud object detection accuracy.

Abstract: Recent advances in point cloud object detection have increasingly adopted
Transformer-based and State Space Models (SSMs), demonstrating strong
performance. However, voxelbased representations in these models require strict
consistency in input and output dimensions due to their serialized processing,
which limits the spatial diffusion capability typically offered by
convolutional operations. This limitation significantly affects detection
accuracy. Inspired by CNN-based object detection architectures, we propose a
novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and
diffusion in point cloud data. VDM is composed of sparse 3D convolutions,
submanifold sparse convolutions, and residual connections. To ensure
computational efficiency, the output feature maps are downsampled to one-fourth
of the original input resolution. VDM serves two primary functions: (1)
diffusing foreground voxel features through sparse 3D convolutions to enrich
spatial context, and (2) aggregating fine-grained spatial information to
strengthen voxelwise feature representation. The enhanced voxel features
produced by VDM can be seamlessly integrated into mainstream Transformer- or
SSM-based detection models for accurate object classification and localization,
highlighting the generalizability of our method. We evaluate VDM on several
benchmark datasets by embedding it into both Transformerbased and SSM-based
models. Experimental results show that our approach consistently improves
detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7
mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP
on ONCE, setting new stateof-the-art performance across all datasets. Our code
will be made publicly available.

</details>


### [20] [Ensemble learning of foundation models for precision oncology](https://arxiv.org/abs/2508.16085)
*Xiangde Luo,Xiyue Wang,Feyisope Eweje,Xiaoming Zhang,Sen Yang,Ryan Quinton,Jinxi Xiang,Yuchen Li,Yuanfeng Ji,Zhe Li,Yijiang Chen,Colin Bergstrom,Ted Kim,Francesca Maria Olguin,Kelley Yuan,Matthew Abikenari,Andrew Heider,Sierra Willens,Sanjeeth Rajaram,Robert West,Joel Neal,Maximilian Diehn,Ruijiang Li*

Main category: cs.CV

TL;DR: ELF is an ensemble learning framework that integrates five pathology foundation models to create unified slide-level representations, outperforming individual models and existing approaches across various clinical applications in precision oncology.


<details>
  <summary>Details</summary>
Motivation: Existing pathology foundation models are trained on disparate datasets with varying strategies, leading to inconsistent performance and limited generalizability in clinical applications.

Method: ELF integrates five state-of-the-art pathology foundation models using ensemble learning on 53,699 whole-slide images across 20 anatomical sites, generating unified slide-level representations that capture complementary information while maintaining high data efficiency.

Result: ELF consistently outperformed all constituent foundation models and existing slide-level models across disease classification, biomarker detection, and response prediction to major anticancer therapies (chemotherapy, targeted therapy, immunotherapy) in multiple cancer types.

Conclusion: ELF demonstrates the power of ensemble learning for pathology foundation models and provides a scalable, generalizable solution for advancing AI-assisted precision oncology, particularly advantageous in clinical contexts with limited data.

Abstract: Histopathology is essential for disease diagnosis and treatment
decision-making. Recent advances in artificial intelligence (AI) have enabled
the development of pathology foundation models that learn rich visual
representations from large-scale whole-slide images (WSIs). However, existing
models are often trained on disparate datasets using varying strategies,
leading to inconsistent performance and limited generalizability. Here, we
introduce ELF (Ensemble Learning of Foundation models), a novel framework that
integrates five state-of-the-art pathology foundation models to generate
unified slide-level representations. Trained on 53,699 WSIs spanning 20
anatomical sites, ELF leverages ensemble learning to capture complementary
information from diverse models while maintaining high data efficiency. Unlike
traditional tile-level models, ELF's slide-level architecture is particularly
advantageous in clinical contexts where data are limited, such as therapeutic
response prediction. We evaluated ELF across a wide range of clinical
applications, including disease classification, biomarker detection, and
response prediction to major anticancer therapies, cytotoxic chemotherapy,
targeted therapy, and immunotherapy, across multiple cancer types. ELF
consistently outperformed all constituent foundation models and existing
slide-level models, demonstrating superior accuracy and robustness. Our results
highlight the power of ensemble learning for pathology foundation models and
suggest ELF as a scalable and generalizable solution for advancing AI-assisted
precision oncology.

</details>


### [21] [Two-flow Feedback Multi-scale Progressive Generative Adversarial Network](https://arxiv.org/abs/2508.16089)
*Sun Weikai,Song Shijie,Chi Wenjie*

Main category: cs.CV

TL;DR: Proposes MSPG-SEN, a novel two-flow feedback multi-scale progressive GAN that improves image quality, simplifies training, and reduces costs while achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: GANs still have development potential despite diffusion model progress, due to their unique advantages. The paper aims to improve GAN training efficiency, stability, and image quality while reducing computational costs.

Method: Two-flow feedback multi-scale progressive GAN (MSPG-SEN) with four key components: adaptive perception-behavioral feedback loop (APFL) for robustness, globally connected two-flow dynamic residual network for efficiency, and dynamic embedded attention mechanism (DEMA) for feature capture.

Result: Achieved state-of-the-art results on five datasets: INKK (89.7%), AWUN (78.3%), IONJ (85.5%), POKL (88.7%), OPIN (96.4%). The model shows improved training efficiency, reduced computational resources (88.7% less), and strong cross-task capability.

Conclusion: MSPG-SEN successfully addresses GAN training challenges by improving image quality, training stability, and efficiency while reducing costs. The proposed components demonstrate strong performance across multiple datasets and tasks.

Abstract: Although diffusion model has made good progress in the field of image
generation, GAN\cite{huang2023adaptive} still has a large development space due
to its unique advantages, such as WGAN\cite{liu2021comparing},
SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so
on. In this paper, we propose a novel two-flow feedback multi-scale progressive
generative adversarial network (MSPG-SEN) for GAN models. This paper has four
contributions: 1) : We propose a two-flow feedback multi-scale progressive
Generative Adversarial network (MSPG-SEN), which not only improves image
quality and human visual perception on the basis of retaining the advantages of
the existing GAN model, but also simplifies the training process and reduces
the training cost of GAN networks. Our experimental results show that, MSPG-SEN
has achieved state-of-the-art generation results on the following five
datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset
is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We
propose an adaptive perception-behavioral feedback loop (APFL), which
effectively improves the robustness and training stability of the model and
reduces the training cost. 3) : We propose a globally connected two-flow
dynamic residual network(). After ablation experiments, it can effectively
improve the training efficiency and greatly improve the generalization ability,
with stronger flexibility. 4) : We propose a new dynamic embedded attention
mechanism (DEMA). After experiments, the attention can be extended to a variety
of image processing tasks, which can effectively capture global-local
information, improve feature separation capability and feature expression
capabilities, and requires minimal computing resources only 88.7\% with INJK
With strong cross-task capability.

</details>


### [22] [Domain Adaptation via Feature Refinement](https://arxiv.org/abs/2508.16124)
*Savvas Karatsiolis,Andreas Kamilaris*

Main category: cs.CV

TL;DR: DAFR2 is a simple unsupervised domain adaptation framework that combines batch normalization adaptation, feature distillation, and hypothesis transfer to create robust domain-invariant features without target labels or complex architectures.


<details>
  <summary>Details</summary>
Motivation: To address distribution shift in unsupervised domain adaptation by creating robust feature spaces that generalize across domains without requiring target labels or complex training objectives.

Method: Combines three components: adaptation of Batch Normalization statistics using unlabeled target data, feature distillation from source-trained model, and hypothesis transfer to align feature distributions at statistical and representational levels.

Result: Outperforms prior methods on benchmark datasets (CIFAR10-C, CIFAR100-C, MNIST-C, PatchCamelyon-C) in robustness to corruption. Achieves improved feature alignment, increased mutual information between domains, and reduced sensitivity to input perturbations.

Conclusion: DAFR2 provides an effective and simple framework for unsupervised domain adaptation that produces domain-invariant feature spaces and demonstrates superior robustness to corruption compared to existing methods.

Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet
effective framework for unsupervised domain adaptation under distribution
shift. The proposed method synergistically combines three key components:
adaptation of Batch Normalization statistics using unlabeled target data,
feature distillation from a source-trained model and hypothesis transfer. By
aligning feature distributions at the statistical and representational levels,
DAFR2 produces robust and domain-invariant feature spaces that generalize
across similar domains without requiring target labels, complex architectures
or sophisticated training objectives. Extensive experiments on benchmark
datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C,
demonstrate that the proposed algorithm outperforms prior methods in robustness
to corruption. Theoretical and empirical analyses further reveal that our
method achieves improved feature alignment, increased mutual information
between the domains and reduced sensitivity to input perturbations.

</details>


### [23] [4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration](https://arxiv.org/abs/2508.16138)
*Hao Tang,Rongxi Yi,Lei Li,Kaiyi Cao,Jiapeng Zhao,Yihan Xiao,Minghai Shi,Peng Yuan,Yan Xi,Hui Tang,Wei Li,Zhan Wu,Yixin Zhou*

Main category: cs.CV

TL;DR: A 4D joint analysis platform combining dual robotic CBCT with dynamic 2D X-rays using deep learning fusion for accurate, low-dose dynamic joint imaging.


<details>
  <summary>Details</summary>
Motivation: Conventional CT cannot capture dynamic weight-bearing joint motion, and current 4D imaging methods suffer from excessive radiation or incomplete spatial information from 2D techniques.

Method: Integrated platform with: 1) dual robotic arm CBCT system with programmable trajectory, 2) hybrid imaging pipeline fusing static 3D CBCT with dynamic 2D X-rays using deep learning preprocessing and iterative optimization, 3) clinically validated kinematic assessment framework.

Result: Achieved sub-voxel accuracy (0.235 mm) with 99.18% success rate in simulations, outperforming conventional methods. Clinical evaluation showed accurate quantification of tibial plateau motion and medial-lateral variance in TKA patients.

Conclusion: The 4D CBCT platform enables fast, accurate, low-dose dynamic joint imaging for biomechanical research, precision diagnostics, and personalized orthopedic care.

Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic,
weight-bearing joint motion. Functional evaluation, particularly after surgical
intervention, requires four-dimensional (4D) imaging, but current methods are
limited by excessive radiation exposure or incomplete spatial information from
2D techniques. We propose an integrated 4D joint analysis platform that
combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a
programmable, gantry-free trajectory optimized for upright scanning; (2) a
hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using
deep learning-based preprocessing, 3D-2D projection, and iterative
optimization; and (3) a clinically validated framework for quantitative
kinematic assessment. In simulation studies, the method achieved sub-voxel
accuracy (0.235 mm) with a 99.18 percent success rate, outperforming
conventional and state-of-the-art registration approaches. Clinical evaluation
further demonstrated accurate quantification of tibial plateau motion and
medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D
CBCT platform enables fast, accurate, and low-dose dynamic joint imaging,
offering new opportunities for biomechanical research, precision diagnostics,
and personalized orthopedic care.

</details>


### [24] [High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection](https://arxiv.org/abs/2508.16140)
*Jincheng Li,Danyang Dong,Menglin Zheng,Jingbo Zhang,Yueqin Hang,Lichi Zhang,Lili Zhao*

Main category: cs.CV

TL;DR: Proposes a hypergraph-based network for cervical abnormal cell detection that fuses spatial correlation features with deep discriminative features using multi-level fusion and hypergraph computation.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms fail to effectively model spatial correlation features in cervical cell images and lack integration of inter-correlation features with intra-discriminative features for end-to-end detection.

Method: Uses Multi-level Fusion Sub-network (MLF-SNet) for enhanced feature extraction and Cross-level Feature Fusion Strategy with Hypergraph Computation (CLFFS-HC) module to integrate mixed spatial correlation and deep discriminative features.

Result: Experiments on three publicly available datasets demonstrate significant performance improvement in cervical abnormal cell detection compared to existing methods.

Conclusion: The proposed hypergraph-based network effectively fuses different feature types and significantly enhances cervical abnormal cell detection performance, addressing limitations of existing algorithms.

Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test
(TCT) images is a critical component in the development of intelligent
computer-aided diagnostic systems. However, existing algorithms typically fail
to effectively model the correlations of visual features, while these spatial
correlation features actually contain critical diagnostic information.
Furthermore, no detection algorithm has the ability to integrate
inter-correlation features of cells with intra-discriminative features of
cells, lacking a fusion strategy for the end-to-end detection model. In this
work, we propose a hypergraph-based cell detection network that effectively
fuses different types of features, combining spatial correlation features and
deep discriminative features. Specifically, we use a Multi-level Fusion
Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we
introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation
module (CLFFS-HC), to integrate mixed features. Finally, we conducted
experiments on three publicly available datasets, and the results demonstrate
that our method significantly improves the performance of cervical abnormal
cell detection.

</details>


### [25] [Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection](https://arxiv.org/abs/2508.16157)
*Pi-Wei Chen,Jerry Chun-Wei Lin,Wei-Han Chen,Jia Ji,Zih-Ching Chen,Feng-Hao Yeh,Chao-Chun Chen*

Main category: cs.CV

TL;DR: APT is a novel few-shot anomaly detection framework that uses adaptive prompt tuning with self-generated anomaly samples and semantic alignment, eliminating the need for human-designed prompts or prior knowledge.


<details>
  <summary>Details</summary>
Motivation: Previous vision-language model approaches for anomaly detection are limited by reliance on human-designed prompts and lack of accessible anomaly samples, creating gaps in context-specific anomaly understanding.

Method: APT uses self-generated anomaly samples with noise perturbations to train learnable prompts. It employs a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) to iteratively align prompts with general anomaly semantics while preventing overfitting to synthetic noise.

Result: The system achieves state-of-the-art performance on multiple benchmark datasets and advances pixel-wise anomaly detection capabilities.

Conclusion: APT establishes a robust and versatile prior knowledge-free solution for real-world anomaly detection, overcoming limitations of traditional prompt-based approaches.

Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in
detecting anomalies. However, previous approaches are fundamentally limited by
their reliance on human-designed prompts and the lack of accessible anomaly
samples, leading to significant gaps in context-specific anomaly understanding.
In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning
with semantic alignment for anomaly detection (APT), a groundbreaking prior
knowledge-free, few-shot framework and overcomes the limitations of traditional
prompt-based approaches. APT uses self-generated anomaly samples with noise
perturbations to train learnable prompts that capture context-dependent
anomalies in different scenarios. To prevent overfitting to synthetic noise, we
propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively
aligns the prompts with general anomaly semantics while incorporating diverse
synthetic anomaly. Our system not only advances pixel-wise anomaly detection,
but also achieves state-of-the-art performance on multiple benchmark datasets
without requiring prior knowledge for prompt crafting, establishing a robust
and versatile solution for real-world anomaly detection.

</details>


### [26] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: RAGSR method uses regional attention and fine-grained captions to improve super-resolution of multiple objects in images, overcoming limitations of existing vision-language models.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models struggle with generating clear regional details in multi-object scenarios due to lack of fine-grained descriptions and insufficient complex prompt handling.

Method: Proposes Regional Attention Guided Super-Resolution (RAGSR) that localizes object regions, assigns fine-grained captions as region-text pairs, and uses novel regional attention mechanism to prevent unwanted interactions between unrelated regions.

Result: Superior performance on benchmark datasets, generating perceptually authentic visual details while maintaining contextual consistency compared to existing approaches.

Conclusion: RAGSR effectively overcomes traditional SISR limitations by providing finer control over text-image integration through regional attention and localized fine-grained information encoding.

Abstract: The rich textual information of large vision-language models (VLMs) combined
with the powerful generative prior of pre-trained text-to-image (T2I) diffusion
models has achieved impressive performance in single-image super-resolution
(SISR). However, existing methods still face significant challenges in
generating clear and accurate regional details, particularly in scenarios
involving multiple objects. This challenge primarily stems from a lack of
fine-grained regional descriptions and the models' insufficient ability to
capture complex prompts. To address these limitations, we propose a Regional
Attention Guided Super-Resolution (RAGSR) method that explicitly extracts
localized fine-grained information and effectively encodes it through a novel
regional attention mechanism, enabling both enhanced detail and overall
visually coherent SR results. Specifically, RAGSR localizes object regions in
an image and assigns fine-grained caption to each region, which are formatted
as region-text pairs as textual priors for T2I models. A regional guided
attention is then leveraged to ensure that each region-text pair is properly
considered in the attention process while preventing unwanted interactions
between unrelated region-text pairs. By leveraging this attention mechanism,
our approach offers finer control over the integration of text and image
information, thereby effectively overcoming limitations faced by traditional
SISR techniques. Experimental results on benchmark datasets demonstrate that
our approach exhibits superior performance in generating perceptually authentic
visual details while maintaining contextual consistency compared to existing
approaches.

</details>


### [27] [Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation](https://arxiv.org/abs/2508.16159)
*Jiaqi Ma,Guo-Sen Xie,Fang Zhao,Zechao Li*

Main category: cs.CV

TL;DR: TLG proposes a homologous but heterogeneous network for few-shot semantic segmentation, using heterogeneous visual aggregation and transfer modules to address over-semantic homogenization, achieving state-of-the-art results with significantly fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional meta-learning approaches use identical network architectures for support-query pairs, leading to over-semantic homogenization. The authors aim to address this limitation by introducing heterogeneity while preserving semantic commonality.

Method: Proposes a homologous but heterogeneous network with: 1) Heterogeneous Visual Aggregation (HA) modules to enhance complementarity while preserving semantic commonality, 2) Heterogeneous Transfer (HT) module to reduce semantic noise and amplify uniqueness, and 3) Heterogeneous CLIP (HC) textual information for multimodal generalization.

Result: Achieves 13.2% improvement on Pascal-5i and 9.7% improvement on COCO-20i with only 1/24 of parameters compared to existing state-of-the-art models. First weakly supervised model to outperform fully supervised models under same backbone architectures.

Conclusion: The proposed heterogeneous network design effectively addresses over-semantic homogenization in meta-learning, demonstrating superior performance in weakly-supervised few-shot semantic segmentation with significantly reduced computational requirements.

Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs,
characterized by the same categories and similar attributes, and extract useful
inductive biases through identical network architectures. However, this
identical network design results in over-semantic homogenization. To address
this, we propose a novel homologous but heterogeneous network. By treating
support-query pairs as dual perspectives, we introduce heterogeneous visual
aggregation (HA) modules to enhance complementarity while preserving semantic
commonality. To further reduce semantic noise and amplify the uniqueness of
heterogeneous semantics, we design a heterogeneous transfer (HT) module.
Finally, we propose heterogeneous CLIP (HC) textual information to enhance the
generalization capability of multimodal models. In the weakly-supervised
few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of
existing state-of-the-art models, TLG achieves a 13.2\% improvement on
Pascal-5\textsuperscript{i} and a 9.7\% improvement on
COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first
weakly supervised (image-level) model that outperforms fully supervised
(pixel-level) models under the same backbone architectures. The code is
available at https://github.com/jarch-ma/TLG.

</details>


### [28] [FTIO: Frequent Temporally Integrated Objects](https://arxiv.org/abs/2508.16183)
*Mohammad Mohammadzadeh Kalati,Farhad Maleki,Ian McQuillan*

Main category: cs.CV

TL;DR: FTIO is a post-processing framework for unsupervised video object segmentation that improves object selection and corrects temporal inconsistencies through frequent object extraction and three-stage mask integration.


<details>
  <summary>Details</summary>
Motivation: Address challenges in unsupervised VOS including initial segmentation uncertainty, object proposal reliability issues with small/complex objects, and temporal inconsistencies from deformation/fast motion.

Method: Two key components: 1) Combined criterion for object selection that extracts frequently appearing salient objects, 2) Three-stage method to correct temporal inconsistencies by integrating missing object mask regions.

Result: Achieves state-of-the-art performance in multi-object unsupervised video object segmentation.

Conclusion: FTIO effectively addresses key challenges in UVOS through improved object selection and temporal consistency correction, demonstrating superior performance in multi-object scenarios.

Abstract: Predicting and tracking objects in real-world scenarios is a critical
challenge in Video Object Segmentation (VOS) tasks. Unsupervised VOS (UVOS) has
the additional challenge of finding an initial segmentation of salient objects,
which affects the entire process and keeps a permanent uncertainty about the
object proposals. Moreover, deformation and fast motion can lead to temporal
inconsistencies. To address these problems, we propose Frequent Temporally
Integrated Objects (FTIO), a post-processing framework with two key components.
First, we introduce a combined criterion to improve object selection,
mitigating failures common in UVOS--particularly when objects are small or
structurally complex--by extracting frequently appearing salient objects.
Second, we present a three-stage method to correct temporal inconsistencies by
integrating missing object mask regions. Experimental results demonstrate that
FTIO achieves state-of-the-art performance in multi-object UVOS. Code is
available at: https://github.com/MohammadMohammadzadehKalati/FTIO

</details>


### [29] [SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)
*Yicheng Ji,Jun Zhang,Heming Xia,Jinpeng Chen,Lidan Shou,Gang Chen,Huan Li*

Main category: cs.CV

TL;DR: SpecVLM is a training-free speculative decoding framework that accelerates video LLMs by pruning up to 90% of video tokens through a two-stage process, achieving 2.68× speedup without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Video LLMs suffer from substantial memory and computational overhead due to dense video token representations, and existing token reduction methods cause information loss while not accelerating the decoding stage effectively.

Method: Two-stage video token pruning: Stage I selects informative tokens using attention signals from the verifier model, Stage II prunes remaining redundant tokens in a spatially uniform manner. Uses speculative decoding framework tailored for video LLMs.

Result: Achieves up to 2.68× decoding speedup for LLaVA-OneVision-72B and 2.11× speedup for Qwen2.5-VL-32B on four video understanding benchmarks, demonstrating effectiveness and robustness without sacrificing accuracy.

Conclusion: SpecVLM successfully accelerates Vid-LLMs losslessly by leveraging the insight that draft model speculation is insensitive to video token pruning, enabling efficient decoding while maintaining performance.

Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in
understanding video content. However, their reliance on dense video token
representations introduces substantial memory and computational overhead in
both prefilling and decoding. To mitigate the information loss of recent video
token reduction methods and accelerate the decoding stage of Vid-LLMs
losslessly, we introduce SpecVLM, a training-free speculative decoding (SD)
framework tailored for Vid-LLMs that incorporates staged video token pruning.
Building on our novel finding that the draft model's speculation exhibits low
sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,
enabling efficient speculation without sacrificing accuracy. To achieve this,
it performs a two-stage pruning process: Stage I selects highly informative
tokens guided by attention signals from the verifier (target model), while
Stage II prunes remaining redundant ones in a spatially uniform manner.
Extensive experiments on four video understanding benchmarks demonstrate the
effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$
decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for
Qwen2.5-VL-32B.

</details>


### [30] [\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring](https://arxiv.org/abs/2508.16207)
*Thinesh Thiyakesan Ponbagavathi,Kunyu Peng,Alina Roitberg*

Main category: cs.CV

TL;DR: T-Mask is a new image-to-video probing method that improves cross-view driver monitoring accuracy by leveraging temporal token masking and emphasizing dynamic video regions, outperforming existing probing and PEFT methods without adding parameters.


<details>
  <summary>Details</summary>
Motivation: Camera perspective changes are a common obstacle in driver monitoring, and while foundation models show potential for generalization, their robustness to unseen viewpoints remains underexplored.

Method: Adapt image foundation models (DINOv2 and CLIP) using single training view, benchmark linear probes, advanced probing strategies, and introduce T-Mask with temporal token masking to emphasize dynamic video regions.

Result: T-Mask improves cross-view top-1 accuracy by +1.23% over probing baselines and +8.0% over PEFT methods, with +5.42% improvement for underrepresented secondary activities under trained view and +1.36% under cross-view settings.

Conclusion: Lightweight probing methods like T-Mask have strong potential in fine-grained driver observation, especially in cross-view and low-data settings, highlighting the importance of temporal token selection for robust driver monitoring systems.

Abstract: Changes of camera perspective are a common obstacle in driver monitoring.
While deep learning and pretrained foundation models show strong potential for
improved generalization via lightweight adaptation of the final layers
('probing'), their robustness to unseen viewpoints remains underexplored. We
study this challenge by adapting image foundation models to driver monitoring
using a single training view, and evaluating them directly on unseen
perspectives without further adaptation. We benchmark simple linear probes,
advanced probing strategies, and compare two foundation models (DINOv2 and
CLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.
Building on these insights, we introduce \textsc{T-Mask} -- a new
image-to-video probing method that leverages temporal token masking and
emphasizes more dynamic video regions. Benchmarked on the public Drive\&Act
dataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ over
strong probing baselines and $+8.0\%$ over PEFT methods, without adding any
parameters. It proves particularly effective for underrepresented secondary
activities, boosting recognition by $+5.42\%$ under the trained view and
$+1.36\%$ under cross-view settings. This work provides encouraging evidence
that adapting foundation models with lightweight probing methods like
\textsc{T-Mask} has strong potential in fine-grained driver observation,
especially in cross-view and low-data settings. These results highlight the
importance of temporal token selection when leveraging foundation models to
build robust driver monitoring systems. Code and models will be made available
at https://github.com/th-nesh/T-MASK to support ongoing research.

</details>


### [31] [Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers](https://arxiv.org/abs/2508.16211)
*Shikang Zheng,Liang Feng,Xinyu Wang,Qinming Zhou,Peiliang Cai,Chang Zou,Jiacheng Liu,Yuqi Lin,Junjie Chen,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: FoCa is a new feature caching method that treats diffusion transformer inference as an ODE solving problem, achieving near-lossless 3-6x speedups on various DiT models without additional training.


<details>
  <summary>Details</summary>
Motivation: Current feature caching techniques for Diffusion Transformers struggle to maintain generation quality at high acceleration ratios due to prediction errors from long-step forecasting instability.

Method: Proposes FoCa (Forecast-then-Calibrate) which models layer representations as a feature-ODE and treats feature caching as an ODE solving problem to robustly integrate historical features under large skipping intervals.

Result: Achieves near-lossless speedups: 5.50x on FLUX, 6.45x on HunyuanVideo, 3.17x on Inf-DiT, and maintains high quality with 4.53x speedup on DiT.

Conclusion: FoCa effectively addresses the degradation issues in existing caching strategies by adopting an ODE perspective, enabling high-quality acceleration of diffusion transformers without additional training.

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in
high-fidelity image and video generation. To reduce their substantial
computational costs, feature caching techniques have been proposed to
accelerate inference by reusing hidden representations from previous timesteps.
However, current methods often struggle to maintain generation quality at high
acceleration ratios, where prediction errors increase sharply due to the
inherent instability of long-step forecasting. In this work, we adopt an
ordinary differential equation (ODE) perspective on the hidden-feature
sequence, modeling layer representations along the trajectory as a feature-ODE.
We attribute the degradation of existing caching strategies to their inability
to robustly integrate historical features under large skipping intervals. To
address this, we propose FoCa (Forecast-then-Calibrate), which treats feature
caching as a feature-ODE solving problem. Extensive experiments on image
synthesis, video generation, and super-resolution tasks demonstrate the
effectiveness of FoCa, especially under aggressive acceleration. Without
additional training, FoCa achieves near-lossless speedups of 5.50 times on
FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high
quality with a 4.53 times speedup on DiT.

</details>


### [32] [OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models](https://arxiv.org/abs/2508.16212)
*Huanpeng Chu,Wei Wu,Guanyu Fen,Yutao Zhang*

Main category: cs.CV

TL;DR: OmniCache is a training-free acceleration method for diffusion Transformers that exploits global redundancy in denoising process through strategic cache distribution and dynamic noise filtering.


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers face high computational costs from many sampling steps and complex computations, making real-time deployment challenging despite their strong generative performance.

Method: Systematically analyzes sampling trajectories, strategically distributes cache reuse across entire sampling process (not just later steps), and dynamically estimates/filters noise during cache reuse.

Result: Extensive experiments show accelerated sampling while maintaining competitive generative quality.

Conclusion: OmniCache provides a practical training-free solution for efficient deployment of diffusion-based generative models by better utilizing computational redundancy throughout the diffusion trajectory.

Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks
such as image synthesis and video generation, with Transformer architectures
further enhancing performance. However, the high computational cost of
diffusion Transformers-stemming from a large number of sampling steps and
complex per-step computations-presents significant challenges for real-time
deployment. In this paper, we introduce OmniCache, a training-free acceleration
method that exploits the global redundancy inherent in the denoising process.
Unlike existing methods that determine caching strategies based on inter-step
similarities and tend to prioritize reusing later sampling steps, our approach
originates from the sampling perspective of DIT models. We systematically
analyze the model's sampling trajectories and strategically distribute cache
reuse across the entire sampling process. This global perspective enables more
effective utilization of cached computations throughout the diffusion
trajectory, rather than concentrating reuse within limited segments of the
sampling procedure.In addition, during cache reuse, we dynamically estimate the
corresponding noise and filter it out to reduce its impact on the sampling
direction.Extensive experiments demonstrate that our approach accelerates the
sampling process while maintaining competitive generative quality, offering a
promising and practical solution for efficient deployment of diffusion-based
generative models.

</details>


### [33] [MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine](https://arxiv.org/abs/2508.16213)
*Kaiyuan Ji,Yijin Guo,Zicheng Zhang,Xiangyang Zhu,Yuan Tian,Ning Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: MedOmni-45 Degrees is a benchmark that evaluates medical LLMs' reasoning vulnerabilities through manipulative hints, measuring accuracy, faithfulness, and anti-sycophancy to reveal safety-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks collapse reasoning vulnerabilities into single accuracy scores, failing to assess Chain-of-Thought faithfulness and sycophancy risks in medical LLMs under manipulative conditions.

Method: Created benchmark with 1,804 medical questions across 6 specialties, each paired with 7 manipulative hint types + baseline. Evaluated 7 LLMs with 3 metrics (Accuracy, CoT-Faithfulness, Anti-Sycophancy) combined into composite scores visualized via 45 Degrees plots.

Result: Consistent safety-performance trade-off observed across all models, with none surpassing the diagonal. Open-source QwQ-32B performed closest to ideal (43.81 Degrees) but no model excelled in both safety and accuracy.

Conclusion: MedOmni-45 Degrees effectively exposes reasoning vulnerabilities in medical LLMs and provides a framework for guiding safer model development through comprehensive evaluation of safety-performance trade-offs.

Abstract: With the increasing use of large language models (LLMs) in medical
decision-support, it is essential to evaluate not only their final answers but
also the reliability of their reasoning. Two key risks are Chain-of-Thought
(CoT) faithfulness -- whether reasoning aligns with responses and medical facts
-- and sycophancy, where models follow misleading cues over correctness.
Existing benchmarks often collapse such vulnerabilities into single accuracy
scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and
workflow designed to quantify safety-performance trade-offs under manipulative
hint conditions. It contains 1,804 reasoning-focused medical questions across
six specialties and three task types, including 500 from MedMCQA. Each question
is paired with seven manipulative hint types and a no-hint baseline, producing
about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source,
general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling
over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and
Anti-Sycophancy -- are combined into a composite score visualized with a 45
Degrees plot. Results show a consistent safety-performance trade-off, with no
model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81
Degrees), balancing safety and accuracy but not leading in both. MedOmni-45
Degrees thus provides a focused benchmark for exposing reasoning
vulnerabilities in medical LLMs and guiding safer model development.

</details>


### [34] [PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting](https://arxiv.org/abs/2508.16217)
*Hohyun Na,Seunghoo Hong,Simon S. Woo*

Main category: cs.CV

TL;DR: PromptFlare is a novel adversarial protection method that uses cross-attention mechanisms to inject noise that suppresses diffusion-based inpainting models, preventing malicious image modifications while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To address concerns about misuse of diffusion models for unauthorized image modifications, overcoming limitations of previous adversarial attack methods that relied on image-level inconsistencies and couldn't effectively handle textual prompts.

Method: Leverages cross-attention mechanism to exploit prompt embeddings, identifies and targets shared tokens that are invariant and semantically uninformative, and injects adversarial noise to suppress the sampling process as a cross-attention decoy.

Result: Achieves state-of-the-art performance on EditBench dataset across various metrics while significantly reducing computational overhead and GPU memory usage.

Conclusion: PromptFlare provides robust and efficient protection against unauthorized image manipulations by diffusion-based inpainting models through targeted adversarial noise injection in the cross-attention space.

Abstract: The success of diffusion models has enabled effortless, high-quality image
modifications that precisely align with users' intentions, thereby raising
concerns about their potential misuse by malicious actors. Previous studies
have attempted to mitigate such misuse through adversarial attacks. However,
these approaches heavily rely on image-level inconsistencies, which pose
fundamental limitations in addressing the influence of textual prompts. In this
paper, we propose PromptFlare, a novel adversarial protection method designed
to protect images from malicious modifications facilitated by diffusion-based
inpainting models. Our approach leverages the cross-attention mechanism to
exploit the intrinsic properties of prompt embeddings. Specifically, we
identify and target shared token of prompts that is invariant and semantically
uninformative, injecting adversarial noise to suppress the sampling process.
The injected noise acts as a cross-attention decoy, diverting the model's focus
away from meaningful prompt-image alignments and thereby neutralizing the
effect of prompt. Extensive experiments on the EditBench dataset demonstrate
that our method achieves state-of-the-art performance across various metrics
while significantly reducing computational overhead and GPU memory usage. These
findings highlight PromptFlare as a robust and efficient protection against
unauthorized image manipulations. The code is available at
https://github.com/NAHOHYUN-SKKU/PromptFlare.

</details>


### [35] [An Investigation of Visual Foundation Models Robustness](https://arxiv.org/abs/2508.16225)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: This paper analyzes robustness requirements and defense mechanisms for Visual Foundation Models in computer vision systems, focusing on adaptation to dynamic environments and protection against various real-world challenges.


<details>
  <summary>Details</summary>
Motivation: As Visual Foundation Models become ubiquitous in security-sensitive domains like biometric verification, autonomous vehicles, and medical imaging, robustness is essential for building trust between technology and end-users in dynamic environments affected by lighting, weather, and sensor variations.

Method: The article investigates network robustness requirements, examines prevalent empirical defenses and robust training techniques, and provides comprehensive analysis of challenges including network properties, components for ablation studies, and benchmarking metrics.

Result: The study systematically examines how VFMs can be made robust against distributional shifts, noisy inputs, spatial distortions, and adversarial attacks in real-world computer vision applications.

Conclusion: The paper provides a framework for understanding and evaluating robustness in Visual Foundation Models, offering guidance for ablation studies and benchmarking to ensure reliable performance in critical security-sensitive applications.

Abstract: Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,
powering systems for diverse tasks such as object detection, image
classification, segmentation, pose estimation, and motion tracking. VFMs are
capitalizing on seminal innovations in deep learning models, such as LeNet-5,
AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver
superior performance across a range of critical computer vision applications.
These include security-sensitive domains like biometric verification,
autonomous vehicle perception, and medical image analysis, where robustness is
essential to fostering trust between technology and the end-users. This article
investigates network robustness requirements crucial in computer vision systems
to adapt effectively to dynamic environments influenced by factors such as
lighting, weather conditions, and sensor characteristics. We examine the
prevalent empirical defenses and robust training employed to enhance vision
network robustness against real-world challenges such as distributional shifts,
noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we
provide a comprehensive analysis of the challenges associated with these
defense mechanisms, including network properties and components to guide
ablation studies and benchmarking metrics to evaluate network robustness.

</details>


### [36] [FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing](https://arxiv.org/abs/2508.16230)
*Jiahao Chen,Zhiyong Ma,Wenbiao Du,Qingyuan Chuai*

Main category: cs.CV

TL;DR: FlexMUSE is a novel framework for multi-modal creative writing that enables optional visual inputs, promotes creativity through semantic alignment gating and cross-modality fusion, and achieves promising results in consistency and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal generative methods require specific modality inputs or costly training, and suffer from semantic inconsistencies between modalities when applied to the new challenge of multi-modal creative writing where text and visual contexts are not strictly related.

Method: Proposes FlexMUSE with T2I module for optional visual input, modality semantic alignment gating (msaGate) to restrict textual input, attention-based cross-modality fusion for semantic enhancement, and modality semantic creative direct preference optimization (mscDPO) to extend rejected samples for creativity.

Result: FlexMUSE achieves promising results demonstrating consistency, creativity and coherence. The authors also expose ArtMUSE dataset containing around 3k calibrated text-image pairs to advance MMCW research.

Conclusion: FlexMUSE provides an economical solution for multi-modal creative writing with flexible interactive patterns and better semantic alignment between modalities, addressing the challenges of this new abstract task.

Abstract: Multi-modal creative writing (MMCW) aims to produce illustrated articles.
Unlike common multi-modal generative (MMG) tasks such as storytelling or
caption generation, MMCW is an entirely new and more abstract challenge where
textual and visual contexts are not strictly related to each other. Existing
methods for related tasks can be forcibly migrated to this track, but they
require specific modality inputs or costly training, and often suffer from
semantic inconsistencies between modalities. Therefore, the main challenge lies
in economically performing MMCW with flexible interactive patterns, where the
semantics between the modalities of the output are more aligned. In this work,
we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE
promotes creativity and emphasizes the unification between modalities by
proposing the modality semantic alignment gating (msaGate) to restrict the
textual input. Besides, an attention-based cross-modality fusion is proposed to
augment the input features for semantic enhancement. The modality semantic
creative direct preference optimization (mscDPO) within FlexMUSE is designed by
extending the rejected samples to facilitate the writing creativity. Moreover,
to advance the MMCW, we expose a dataset called ArtMUSE which contains with
around 3k calibrated text-image pairs. FlexMUSE achieves promising results,
demonstrating its consistency, creativity and coherence.

</details>


### [37] [UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](https://arxiv.org/abs/2508.16239)
*Nan wang,Zhiyi Xia,Yiming Li,Shi Tang,Zuxin Fan,Xi Fang,Haoyi Tao,Xiaochen Cai,Guolin Ke,Linfeng Zhang,Yanhui Hong*

Main category: cs.CV

TL;DR: UniEM-3M is the first large-scale multimodal electron micrograph dataset with 3M instance segmentation labels and text descriptions, plus a diffusion model for data augmentation and benchmarking.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of large-scale, diverse, expert-annotated electron micrograph datasets due to high acquisition costs, privacy concerns, and annotation complexity in materials science.

Method: Created UniEM-3M dataset with 5,091 high-resolution EMs, 3M instance segmentation labels, and attribute-disentangled textual descriptions. Developed a text-to-image diffusion model for data augmentation and released UniEM-Net as a strong baseline flow-based instance segmentation model.

Result: UniEM-Net outperforms other advanced instance segmentation methods on the challenging UniEM-3M benchmark. The multifaceted release (partial dataset, generative model, comprehensive benchmark) enables accelerated progress in automated materials analysis.

Conclusion: UniEM-3M provides the first large-scale multimodal EM dataset for instance-level understanding, addressing critical data scarcity issues in materials science and establishing a rigorous benchmark for future research in automated microstructural characterization.

Abstract: Quantitative microstructural characterization is fundamental to materials
science, where electron micrograph (EM) provides indispensable high-resolution
insights. However, progress in deep learning-based EM characterization has been
hampered by the scarcity of large-scale, diverse, and expert-annotated
datasets, due to acquisition costs, privacy concerns, and annotation
complexity. To address this issue, we introduce UniEM-3M, the first large-scale
and multimodal EM dataset for instance-level understanding. It comprises 5,091
high-resolution EMs, about 3 million instance segmentation labels, and
image-level attribute-disentangled textual descriptions, a subset of which will
be made publicly available. Furthermore, we are also releasing a text-to-image
diffusion model trained on the entire collection to serve as both a powerful
data augmentation tool and a proxy for the complete data distribution. To
establish a rigorous benchmark, we evaluate various representative instance
segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong
baseline model. Quantitative experiments demonstrate that this flow-based model
outperforms other advanced methods on this challenging benchmark. Our
multifaceted release of a partial dataset, a generative model, and a
comprehensive benchmark -- available at huggingface -- will significantly
accelerate progress in automated materials analysis.

</details>


### [38] [Structuring GUI Elements through Vision Language Models: Towards Action Space Generation](https://arxiv.org/abs/2508.16271)
*Yi Xu,Yesheng Zhang,jiajia Liu,Jingdong Chen*

Main category: cs.CV

TL;DR: IAML training paradigm improves MLLM performance in GUI coordinate generation by addressing semantic gaps in numerical coordinate representation through IoU-based data augmentation.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with precise UI coordinate generation due to semantic voids around numerical coordinates in language spaces and limitations of next-token prediction training.

Method: Introduces IoU-Augmented Maximum Likelihood (IAML) training with novel IoU-based coordinate sampling pipeline for data augmentation, considering proximity to ground truth coordinates.

Result: Superior performance over traditional training paradigms demonstrated through extensive experiments.

Conclusion: IAML paradigm effectively addresses exposure bias in traditional maximum likelihood estimation and enhances MLLM capabilities in GUI element structuring.

Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in
enhancing human-computer interaction. In this paper we focus on the application
of MLLMs in the field of graphical user interface (GUI) elements structuring,
where they assist in processing user instructions based on screen contents.
Despite the promise of MLLMs, their performance in precisely generating UI
element coordinates, a critical aspect of GUI understanding, is hindered by the
nature of next-token prediction training. This challenge arises from the
semantic void surrounding numerical UI coordinates in language representation
spaces, necessitating a substantial and diverse dataset to bolster visual
module capabilities. To address these limitations, we introduce an
IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our
approach involves a novel pipeline for IoU-based coordinate sampling to augment
the training data, which considers the proximity to ground truth coordinates.
This data augmentation strategy is then employed to fine-tune MLLMs under the
IAML paradigm, which is designed to mitigate the exposure bias problem inherent
in traditional maximum likelihood estimation. Through extensive experiments, we
demonstrate the superior performance of our IAML training approach over
traditional training paradigms.

</details>


### [39] [IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization](https://arxiv.org/abs/2508.16272)
*Yu Meng,Ligao Deng,Zhihao Xi,Jiansheng Chen,Jingbo Chen,Anzhi Yue,Diyou Liu,Kai Li,Chenhao Wang,Kaiyu Li,Yupeng Deng,Xian Sun*

Main category: cs.CV

TL;DR: IRSAMap is the first global remote sensing dataset for large-scale, high-resolution land cover vector mapping, addressing limitations of existing datasets with comprehensive annotations, intelligent workflow, global coverage, and multi-task adaptability.


<details>
  <summary>Details</summary>
Motivation: Land cover mapping is shifting from pixel-level segmentation to object-based vector modeling, but existing datasets face challenges with limited class annotations, small scale, and lack of spatial structural information.

Method: Created IRSAMap dataset with: 1) comprehensive vector annotation system with 1.8M+ instances of 10 object types, 2) intelligent annotation workflow combining manual and AI methods, 3) global coverage across 79 regions in 6 continents, 4) multi-task adaptability for various computer vision tasks.

Result: A standardized benchmark dataset that enables the transition from pixel-based to object-based approaches in remote sensing, providing semantic and spatial accuracy for geographic feature automation.

Conclusion: IRSAMap advances geographic feature automation and collaborative modeling, supporting global geographic information updates and digital twin construction. The dataset is publicly available for research use.

Abstract: With the enhancement of remote sensing image resolution and the rapid
advancement of deep learning, land cover mapping is transitioning from
pixel-level segmentation to object-based vector modeling. This shift demands
more from deep learning models, requiring precise object boundaries and
topological consistency. However, existing datasets face three main challenges:
limited class annotations, small data scale, and lack of spatial structural
information. To overcome these issues, we introduce IRSAMap, the first global
remote sensing dataset for large-scale, high-resolution, multi-feature land
cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive
vector annotation system with over 1.8 million instances of 10 typical objects
(e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an
intelligent annotation workflow combining manual and AI-based methods to
improve efficiency and consistency; 3) global coverage across 79 regions in six
continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks
like pixel-level classification, building outline extraction, road centerline
extraction, and panoramic segmentation. IRSAMap provides a standardized
benchmark for the shift from pixel-based to object-based approaches, advancing
geographic feature automation and collaborative modeling. It is valuable for
global geographic information updates and digital twin construction. The
dataset is publicly available at https://github.com/ucas-dlg/IRSAMap

</details>


### [40] [Robust Small Methane Plume Segmentation in Satellite Imagery](https://arxiv.org/abs/2508.16282)
*Khai Duc Minh Tran,Hoa Van Nguyen,Aimuni Binti Muhammad Rawi,Hareeshrao Athinarayanarao,Ba-Ngu Vo*

Main category: cs.CV

TL;DR: Novel U-Net with ResNet34 encoder and dual spectral enhancement techniques achieves 78.39% F1-score for detecting small methane plumes (down to 400 m²) in Sentinel-2 imagery.


<details>
  <summary>Details</summary>
Motivation: To detect methane plumes (potent greenhouse gas) for climate change mitigation, overcoming limitations of traditional methods that can only detect larger plumes.

Method: Deep learning solution based on U-Net with ResNet34 encoder, integrating dual spectral enhancement techniques (Varon ratio and Sanchez regression) to optimize input features for heightened sensitivity.

Result: Achieves 78.39% F1-score on validation set, with ability to detect small plumes down to 400 m² (single pixel at 20m resolution), surpassing traditional methods.

Conclusion: Superior performance in sensitivity and precision over existing remote sensing techniques for automated methane monitoring, especially for small plumes.

Abstract: This paper tackles the challenging problem of detecting methane plumes, a
potent greenhouse gas, using Sentinel-2 imagery. This contributes to the
mitigation of rapid climate change. We propose a novel deep learning solution
based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement
techniques (Varon ratio and Sanchez regression) to optimise input features for
heightened sensitivity. A key achievement is the ability to detect small plumes
down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing
traditional methods limited to larger plumes. Experiments show our approach
achieves a 78.39% F1-score on the validation set, demonstrating superior
performance in sensitivity and precision over existing remote sensing
techniques for automated methane monitoring, especially for small plumes.

</details>


### [41] [EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents](https://arxiv.org/abs/2508.16284)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TL;DR: EdgeDoc is a novel document forgery detection system that combines lightweight convolutional transformers with noiseprint features to effectively detect and localize document manipulations, achieving competitive performance in the ICCV 2025 DeepID Challenge.


<details>
  <summary>Details</summary>
Motivation: The increasing ease of digital document forgery poses serious threats to KYC processes and remote onboarding systems, requiring effective detection methods to preserve service integrity and security.

Method: Combines a lightweight convolutional transformer architecture with auxiliary noiseprint features extracted from images to enhance detection of subtle document manipulations.

Result: Achieved third place in ICCV 2025 DeepID Challenge and outperformed baseline approaches on the FantasyID dataset, demonstrating effectiveness in real-world scenarios.

Conclusion: EdgeDoc presents an effective approach for document forgery detection and localization, showing competitive performance and practical applicability in security-critical applications.

Abstract: The widespread availability of tools for manipulating images and documents
has made it increasingly easy to forge digital documents, posing a serious
threat to Know Your Customer (KYC) processes and remote onboarding systems.
Detecting such forgeries is essential to preserving the integrity and security
of these services. In this work, we present EdgeDoc, a novel approach for the
detection and localization of document forgeries. Our architecture combines a
lightweight convolutional transformer with auxiliary noiseprint features
extracted from the images, enhancing its ability to detect subtle
manipulations. EdgeDoc achieved third place in the ICCV 2025 DeepID Challenge,
demonstrating its competitiveness. Experimental results on the FantasyID
dataset show that our method outperforms baseline approaches, highlighting its
effectiveness in realworld scenarios. Project page : https://www.idiap.
ch/paper/edgedoc/

</details>


### [42] [Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment](https://arxiv.org/abs/2508.16291)
*Fengshun Wang,Qiurui Wang,Peilin Zhao*

Main category: cs.CV

TL;DR: A two-stream Mamba pyramid network that separates TES (visual features) and PCS (audio-visual features) evaluation streams for figure skating scoring, addressing challenges in element separation, long-range context handling, and proper feature alignment with judging criteria.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to properly separate TES and PCS evaluation criteria, treat action elements as a whole rather than individually, and struggle with lengthy competition videos and long-range context modeling.

Method: Two-stream architecture: visual-only stream for TES evaluation using multi-scale Mamba pyramid for element localization, and audio-visual stream for PCS evaluation with multi-level fusion. Uses Mamba for efficient long-range dependency modeling.

Result: Achieves state-of-the-art performance on the FineFS benchmark, demonstrating superior handling of lengthy videos and accurate element-wise scoring.

Conclusion: The proposed method effectively addresses the three major challenges in figure skating assessment by aligning with actual judging criteria, separating evaluation streams, and leveraging Mamba's efficient long-range modeling capabilities.

Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations
in figure skating demand precise assessment of athletic actions and artistic
interpretation, respectively. Existing methods face three major challenges.
Firstly, video and audio cues are regarded as common features for both TES and
PCS predictions in previous works without considering the prior evaluation
criterion of figure skating. Secondly, action elements in competitions are
separated in time, TES should be derived from each element's score, but
existing methods try to give an overall TES prediction without evaluating each
action element. Thirdly, lengthy competition videos make it difficult and
inefficient to handle long-range contexts. To address these challenges, we
propose a two-stream Mamba pyramid network that aligns with actual judging
criteria to predict TES and PCS by separating visual-feature based TES
evaluation stream from audio-visual-feature based PCS evaluation stream. In the
PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee
that video-based features remain unaffected when assessing TES, and enhance PCS
estimation by fusing visual and auditory cues across each contextual level of
the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and
TES head we proposed effectively address the challenges of localizing and
evaluating action elements with various temporal scales and give score
predictions. With Mamba's superior ability to capture long-range dependencies
and its linear computational complexity, our method is ideal for handling
lengthy figure skating videos. Comprehensive experimentation demonstrates that
our framework attains state-of-the-art performance on the FineFS benchmark. Our
source code is available at
https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.

</details>


### [43] [Enhanced Hybrid Technique for Efficient Digitization of Handwritten Marksheets](https://arxiv.org/abs/2508.16295)
*Junaid Ahmed Sifat,Abir Chowdhury,Hasnat Md. Imtiaz,Md. Irtiza Hossain,Md. Imran Bin Azad*

Main category: cs.CV

TL;DR: Hybrid method combining OpenCV for table detection and PaddleOCR/YOLOv8 for handwritten text recognition achieves high accuracy in digitizing complex marksheets.


<details>
  <summary>Details</summary>
Motivation: Digitizing handwritten marksheets is challenging due to diverse handwriting styles and complex table structures, requiring automated solutions to reduce manual work.

Method: Integrates OpenCV for table detection (rows/columns), PaddleOCR for sequential text recognition, and YOLOv8/Modified YOLOv8 for handwritten text recognition within detected tables.

Result: Modified YOLOv8 achieves 92.72% accuracy, outperforming PaddleOCR (91.37%) and standard YOLOv8 (88.91%) on custom dataset with diverse handwriting and complex layouts.

Conclusion: Provides an efficient, practical solution for document automation that reduces manual work and enhances handwritten document understanding with reliable, scalable technology integration.

Abstract: The digitization of handwritten marksheets presents huge challenges due to
the different styles of handwriting and complex table structures in such
documents like marksheets. This work introduces a hybrid method that integrates
OpenCV for table detection and PaddleOCR for recognizing sequential handwritten
text. The image processing capabilities of OpenCV efficiently detects rows and
columns which enable computationally lightweight and accurate table detection.
Additionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text
recognition within the detected table structures alongside PaddleOCR which
further enhance the system's versatility. The proposed model achieves high
accuracy on our custom dataset which is designed to represent different and
diverse handwriting styles and complex table layouts. Experimental results
demonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent,
outperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This
efficiency reduces the necessity for manual work which makes this a practical
and fast solution for digitizing academic as well as administrative documents.
This research serves the field of document automation, particularly handwritten
document understanding, by providing operational and reliable methods to scale,
enhance, and integrate the technologies involved.

</details>


### [44] [A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension](https://arxiv.org/abs/2508.16300)
*Mohammad Zia Ur Rehman,Devraj Raghuvanshi,Umang Jain,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: MM-ORIENT is a multimodal-multitask framework that uses cross-modal relation graphs and hierarchical attention to reduce noise effects and preserve discriminative information across modalities for multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Multimodal learning faces challenges with noise in individual modalities affecting joint representations, and fusion techniques often neglect valuable discriminative information within single modalities.

Method: Proposes cross-modal relation graphs that reconstruct monomodal features without explicit modality interaction, and Hierarchical Interactive Monomodal Attention (HIMA) to focus on pertinent information within each modality while enabling multitasking.

Result: Extensive experimental evaluation on three datasets demonstrates the approach effectively comprehends multimodal content for multiple tasks.

Conclusion: The proposed MM-ORIENT framework successfully reduces noise effects at the latent stage while preserving discriminative features, enabling effective multimodal understanding across multiple tasks.

Abstract: A major challenge in multimodal learning is the presence of noise within
individual modalities. This noise inherently affects the resulting multimodal
representations, especially when these representations are obtained through
explicit interactions between different modalities. Moreover, the multimodal
fusion techniques while aiming to achieve a strong joint representation, can
neglect valuable discriminative information within the individual modalities.
To this end, we propose a Multimodal-Multitask framework with crOss-modal
Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective
for multiple tasks. The proposed approach acquires multimodal representations
cross-modally without explicit interaction between different modalities,
reducing the noise effect at the latent stage. To achieve this, we propose
cross-modal relation graphs that reconstruct monomodal features to acquire
multimodal representations. The features are reconstructed based on the node
neighborhood, where the neighborhood is decided by the features of a different
modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA)
to focus on pertinent information within a modality. While cross-modal relation
graphs help comprehend high-order relationships between two modalities, HIMA
helps in multitasking by learning discriminative features of individual
modalities before late-fusing them. Finally, extensive experimental evaluation
on three datasets demonstrates that the proposed approach effectively
comprehends multimodal content for multiple tasks.

</details>


### [45] [Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers](https://arxiv.org/abs/2508.16311)
*Lucas Maisonnave,Karim Haroun,Tom Pegeot*

Main category: cs.CV

TL;DR: EAM reduces transformer computational complexity by identifying and quantizing low-entropy attention heads that contribute less information, achieving competitive performance with up to 20% sparsity.


<details>
  <summary>Details</summary>
Motivation: Multi-Head Self-Attention mechanisms in transformers have high computational complexity and memory demands that hinder edge deployment. There is significant information redundancy in attention maps that can be exploited for acceleration.

Method: Quantify information in attention heads using Shannon entropy, identify low-entropy heads with deterministic behavior, freeze their weights, and quantize values to low precision to avoid redundant computation.

Result: On ImageNet-1k, EAM achieves similar or higher accuracy at ≤20% sparsity in attention maps and maintains competitive performance beyond this level for DeiT and Swin Transformer models.

Conclusion: Targeted compression of low-entropy attention heads through entropy analysis enables efficient transformer inference without significant performance degradation, making transformers more deployable at the edge.

Abstract: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where
each attention head contributes to the final representation. However, their
computational complexity and high memory demands due to MHSA hinders their
deployment at the edge. In this work, we analyze and exploit information
redundancy in attention maps to accelerate model inference. By quantifying the
information captured by each attention head using Shannon entropy, our analysis
reveals that attention heads with lower entropy, i.e., exhibiting more
deterministic behavior, tend to contribute less information, motivating
targeted compression strategies. Relying on these insights, we propose Entropy
Attention Maps (EAM), a model that freezes the weights of low-entropy attention
maps and quantizes these values to low precision to avoid redundant
re-computation. Empirical validation on ImageNet-1k shows that EAM achieves
similar or higher accuracy at $\leq$20\% sparsity in attention maps and
competitive performance beyond this level for the DeiT and Swin Transformer
models.

</details>


### [46] [Vision encoders should be image size agnostic and task driven](https://arxiv.org/abs/2508.16317)
*Nedyalko Prisadnikov,Danda Pani Paudel,Yuqian Fu,Luc Van Gool*

Main category: cs.CV

TL;DR: Vision encoders should be task-driven and image size agnostic, drawing inspiration from biological efficiency where computational effort depends on task requirements rather than image size.


<details>
  <summary>Details</summary>
Motivation: Modern vision encoders lack the efficiency of biological vision systems, which dynamically allocate computational resources based on task demands rather than processing all visual data uniformly regardless of importance.

Method: Proposes a proof-of-concept solution for image classification that demonstrates task-dependent computational complexity, where the encoder dynamically adjusts processing based on the specific task requirements.

Result: The proof-of-concept shows that a task-driven, image size agnostic approach is feasible and promising for vision encoders, though classification alone doesn't fully represent the intended capabilities.

Conclusion: Next-generation vision encoders should adopt biological efficiency principles by being dynamic and task-dependent in their computational complexity, moving away from fixed processing based on image size.

Abstract: This position paper argues that the next generation of vision encoders should
be image size agnostic and task driven. The source of our inspiration is
biological. Not a structural aspect of biological vision, but a behavioral
trait -- efficiency. We focus on a couple of ways in which vision in nature is
efficient, but modern vision encoders not. We -- humans and animals -- deal
with vast quantities of visual data, and need to be smart where we focus our
limited energy -- it depends on the task. It is our belief that vision encoders
should be dynamic and the computational complexity should depend on the task at
hand rather than the size of the image. We, also, provide concrete first steps
towards our vision -- a proof-of-concept solution for image classification.
Despite classification being not very representative for what we are trying to
achieve, it shows that our approach is feasible and promising.

</details>


### [47] [Attention Mechanism in Randomized Time Warping](https://arxiv.org/abs/2508.16366)
*Yutaro Hiraoka,Kazuya Okamura,Kota Suto,Kazuhiro Fukui*

Main category: cs.CV

TL;DR: RTW functions as a self-attention mechanism similar to Transformers, with RTW achieving 5% better performance on motion recognition tasks due to its global attention approach.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that Randomized Time Warping (RTW) can be interpreted as a self-attention mechanism and compare its effectiveness against Transformer's self-attention in motion recognition.

Method: Analyzed RTW as a self-attention mechanism, compared weight patterns with Transformer self-attention, and evaluated performance on Something-Something V2 dataset.

Result: RTW and self-attention weights show high correlation (0.80 average), with RTW achieving 5% performance improvement over Transformer due to its global attention approach.

Conclusion: RTW functions as an effective self-attention mechanism with global pattern processing capabilities, outperforming Transformer's local attention approach in motion recognition tasks.

Abstract: This paper reveals that we can interpret the fundamental function of
Randomized Time Warping (RTW) as a type of self-attention mechanism, a core
technology of Transformers in motion recognition. The self-attention is a
mechanism that enables models to identify and weigh the importance of different
parts of an input sequential pattern. On the other hand, RTW is a general
extension of Dynamic Time Warping (DTW), a technique commonly used for matching
and comparing sequential patterns. In essence, RTW searches for optimal
contribution weights for each element of the input sequential patterns to
produce discriminative features. Although the two approaches look different,
these contribution weights can be interpreted as self-attention weights. In
fact, the two weight patterns look similar, producing a high average
correlation of 0.80 across the ten smallest canonical angles. However, they
work in different ways: RTW attention operates on an entire input sequential
pattern, while self-attention focuses on only a local view which is a subset of
the input sequential pattern because of the computational costs of the
self-attention matrix. This targeting difference leads to an advantage of RTW
against Transformer, as demonstrated by the 5\% performance improvement on the
Something-Something V2 dataset.

</details>


### [48] [A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection](https://arxiv.org/abs/2508.16397)
*Yong Zhang,Cunjian Chen,Qiang Gao,Yi Wang,Bin Fang*

Main category: cs.CV

TL;DR: GMBINet is a lightweight real-time surface defect detection framework that uses Group Multiscale Bidirectional Interactive modules to achieve high accuracy with minimal computational overhead, achieving 1048 FPS on GPU with only 0.19M parameters.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for surface defect detection suffer from high computational complexity and slow inference speeds, limiting deployment in resource-constrained industrial environments. Current lightweight approaches using multibranch architectures have increased computational overhead and lack effective cross-scale feature interaction.

Method: Proposes GMBINet with novel Group Multiscale Bidirectional Interactive (GMBI) modules that use group-wise strategy for multiscale feature extraction with scale-agnostic complexity. Integrates Bidirectional Progressive Feature Interactor (BPFI) and parameter-free Element-Wise Multiplication-Summation (EWMS) operation for enhanced cross-scale interaction without additional computational overhead.

Result: Achieves competitive accuracy with real-time speeds of 1048 FPS on GPU and 16.53 FPS on CPU at 512 resolution, using only 0.19M parameters. Demonstrates strong generalization on NEU-CLS defect classification dataset.

Conclusion: GMBINet provides an efficient solution for real-time surface defect detection with excellent performance and generalization capability, making it suitable for broader industrial vision applications beyond surface defect detection.

Abstract: Real-time surface defect detection is critical for maintaining product
quality and production efficiency in the steel manufacturing industry. Despite
promising accuracy, existing deep learning methods often suffer from high
computational complexity and slow inference speeds, which limit their
deployment in resource-constrained industrial environments. Recent lightweight
approaches adopt multibranch architectures based on depthwise separable
convolution (DSConv) to capture multiscale contextual information. However,
these methods often suffer from increased computational overhead and lack
effective cross-scale feature interaction, limiting their ability to fully
leverage multiscale representations. To address these challenges, we propose
GMBINet, a lightweight framework that enhances multiscale feature extraction
and interaction through novel Group Multiscale Bidirectional Interactive (GMBI)
modules. The GMBI adopts a group-wise strategy for multiscale feature
extraction, ensuring scale-agnostic computational complexity. It further
integrates a Bidirectional Progressive Feature Interactor (BPFI) and a
parameter-free Element-Wise Multiplication-Summation (EWMS) operation to
enhance cross-scale interaction without introducing additional computational
overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that
GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU
and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.
Additional evaluations on the NEU-CLS defect classification dataset further
confirm the strong generalization ability of our method, demonstrating its
potential for broader industrial vision applications beyond surface defect
detection. The dataset and code are publicly available at:
https://github.com/zhangyongcode/GMBINet.

</details>


### [49] [SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2508.16408)
*Edoardo Palladin,Roland Dietze,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: A novel multimodal sensor fusion approach that improves object detection reliability in adverse weather conditions by fusing RGB, LiDAR, NIR gated camera, and radar data through attentive depth-based blending and transformer-based modality weighting.


<details>
  <summary>Details</summary>
Motivation: Current fusion methods fail in adverse weather conditions like heavy fog, snow, or soiling, creating a critical gap for autonomous vehicle safety in real-world scenarios.

Method: Fuses multimodal sensor data (RGB, LiDAR, NIR gated camera, radar) through attentive depth-based blending schemes with learned refinement on Bird's Eye View plane. Uses transformer decoder to weigh modalities based on distance and visibility.

Result: Improves average precision by 17.2 AP compared to next best method for vulnerable pedestrians in long distances and challenging foggy scenes.

Conclusion: The approach successfully bridges the gap between ideal conditions and real-world edge cases, significantly enhancing multimodal sensor fusion reliability in adverse weather for autonomous vehicles.

Abstract: Multimodal sensor fusion is an essential capability for autonomous robots,
enabling object detection and decision-making in the presence of failing or
uncertain inputs. While recent fusion methods excel in normal environmental
conditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or
obstructions due to soiling. We introduce a novel multi-sensor fusion approach
tailored to adverse weather conditions. In addition to fusing RGB and LiDAR
sensors, which are employed in recent autonomous driving literature, our sensor
fusion stack is also capable of learning from NIR gated camera and radar
modalities to tackle low light and inclement weather. We fuse multimodal sensor
data through attentive, depth-based blending schemes, with learned refinement
on the Bird's Eye View (BEV) plane to combine image and range features
effectively. Our detections are predicted by a transformer decoder that weighs
modalities based on distance and visibility. We demonstrate that our method
improves the reliability of multimodal sensor fusion in autonomous vehicles
under challenging weather conditions, bridging the gap between ideal conditions
and real-world edge cases. Our approach improves average precision by 17.2 AP
compared to the next best method for vulnerable pedestrians in long distances
and challenging foggy scenes. Our project page is available at
https://light.princeton.edu/samfusion/

</details>


### [50] [HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction](https://arxiv.org/abs/2508.16433)
*Sara Rojas,Matthieu Armando,Bernard Ghamen,Philippe Weinzaepfel,Vincent Leroy,Gregory Rogez*

Main category: cs.CV

TL;DR: HAMSt3R extends MASt3R for joint human and scene 3D reconstruction from sparse uncalibrated images, using a distilled encoder and additional heads for human segmentation, DensePose correspondences, and depth estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods like DUSt3R and MASt3R perform well on outdoor static scenes but struggle with human-centric scenarios, creating a need for specialized approaches that can handle both human and scene reconstruction.

Method: Uses DUNE (distilled encoder from MASt3R and multi-HMR) with additional network heads for human segmentation, DensePose correspondences, and depth estimation in human environments. Fully feed-forward approach without complex optimization.

Result: Evaluated on EgoHumans and EgoExo4D benchmarks, shows effective human reconstruction while maintaining strong performance in general 3D reconstruction tasks. Validates generalization to multi-view stereo and pose regression.

Conclusion: HAMSt3R bridges the gap between human and scene understanding in 3D vision, providing efficient and comprehensive reconstruction suitable for real-world human-centric applications.

Abstract: Recovering the 3D geometry of a scene from a sparse set of uncalibrated
images is a long-standing problem in computer vision. While recent
learning-based approaches such as DUSt3R and MASt3R have demonstrated
impressive results by directly predicting dense scene geometry, they are
primarily trained on outdoor scenes with static environments and struggle to
handle human-centric scenarios. In this work, we introduce HAMSt3R, an
extension of MASt3R for joint human and scene 3D reconstruction from sparse,
uncalibrated multi-view images. First, we exploit DUNE, a strong image encoder
obtained by distilling, among others, the encoders from MASt3R and from a
state-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better
understanding of scene geometry and human bodies. Our method then incorporates
additional network heads to segment people, estimate dense correspondences via
DensePose, and predict depth in human-centric environments, enabling a more
comprehensive 3D reconstruction. By leveraging the outputs of our different
heads, HAMSt3R produces a dense point map enriched with human semantic
information in 3D. Unlike existing methods that rely on complex optimization
pipelines, our approach is fully feed-forward and efficient, making it suitable
for real-world applications. We evaluate our model on EgoHumans and EgoExo4D,
two challenging benchmarks con taining diverse human-centric scenarios.
Additionally, we validate its generalization to traditional multi-view stereo
and multi-view pose regression tasks. Our results demonstrate that our method
can reconstruct humans effectively while preserving strong performance in
general 3D reconstruction tasks, bridging the gap between human and scene
understanding in 3D vision.

</details>


### [51] [HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images](https://arxiv.org/abs/2508.16465)
*Anilkumar Swamy,Vincent Leroy,Philippe Weinzaepfel,Jean-Sébastien Franco,Grégory Rogez*

Main category: cs.CV

TL;DR: HOSt3R is a keypoint detector-free method for hand-object 3D reconstruction from monocular video that eliminates reliance on SfM and hand-keypoint optimization, achieving state-of-the-art performance without pre-scanned templates or camera intrinsics.


<details>
  <summary>Details</summary>
Motivation: Existing hand-object 3D reconstruction methods struggle with diverse object geometries, weak textures, and mutual occlusions due to their reliance on keypoint detection techniques like SfM, limiting scalability and generalization for real-world applications.

Method: Proposes a robust keypoint detector-free approach for estimating hand-object 3D transformations from monocular video, integrated with a multi-view reconstruction pipeline to recover accurate 3D shapes without requiring pre-scanned object templates or camera intrinsics.

Result: Achieves state-of-the-art performance on SHOWMe benchmark for object-agnostic hand-object 3D transformation and shape estimation, and demonstrates generalization to unseen object categories on HO3D dataset sequences.

Conclusion: HOSt3R provides a scalable and generalizable solution for hand-object 3D reconstruction that overcomes limitations of keypoint-based methods, enabling non-intrusive applications in human-robot interaction and AR/VR experiences.

Abstract: Hand-object 3D reconstruction has become increasingly important for
applications in human-robot interaction and immersive AR/VR experiences. A
common approach for object-agnostic hand-object reconstruction from RGB
sequences involves a two-stage pipeline: hand-object 3D tracking followed by
multi-view 3D reconstruction. However, existing methods rely on keypoint
detection techniques, such as Structure from Motion (SfM) and hand-keypoint
optimization, which struggle with diverse object geometries, weak textures, and
mutual hand-object occlusions, limiting scalability and generalization. As a
key enabler to generic and seamless, non-intrusive applicability, we propose in
this work a robust, keypoint detector-free approach to estimating hand-object
3D transformations from monocular motion video/images. We further integrate
this with a multi-view reconstruction pipeline to accurately recover
hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely
on pre-scanned object templates or camera intrinsics, and reaches
state-of-the-art performance for the tasks of object-agnostic hand-object 3D
transformation and shape estimation on the SHOWMe benchmark. We also experiment
on sequences from the HO3D dataset, demonstrating generalization to unseen
object categories.

</details>


### [52] [Arbitrary-Scale 3D Gaussian Super-Resolution](https://arxiv.org/abs/2508.16467)
*Huimin Zeng,Yue Bai,Yun Fu*

Main category: cs.CV

TL;DR: A novel 3D Gaussian Splatting framework that enables arbitrary-scale super-resolution with a single model, combining scale-aware rendering, generative priors, and progressive optimization for high-quality HR views while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS super-resolution methods only handle fixed scale factors and are resource-intensive. Direct arbitrary-scale rendering causes aliasing artifacts, while post-processing upsamplers reduce efficiency and complicate frameworks.

Method: Integrated framework with scale-aware rendering, generative prior-guided optimization, and progressive super-resolving. Supports both integer and non-integer scale rendering with a single 3D model.

Result: Achieves 6.59 dB PSNR gain over vanilla 3DGS, maintains structural consistency with LR views and across scales, and renders at 85 FPS for 1080p resolution.

Conclusion: The proposed method enables efficient, high-quality arbitrary-scale super-resolution with a single 3D model while preserving real-time rendering performance and structural consistency.

Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically
perform high-resolution (HR) rendering of fixed scale factors, making them
impractical for resource-limited scenarios. Directly rendering arbitrary-scale
HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of
scale-aware rendering ability, while adding a post-processing upsampler for
3DGS complicates the framework and reduces rendering efficiency. To tackle
these issues, we build an integrated framework that incorporates scale-aware
rendering, generative prior-guided optimization, and progressive
super-resolving to enable 3D Gaussian super-resolution of arbitrary scale
factors with a single 3D model. Notably, our approach supports both integer and
non-integer scale rendering to provide more flexibility. Extensive experiments
demonstrate the effectiveness of our model in rendering high-quality
arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It
preserves structural consistency with LR views and across different scales,
while maintaining real-time rendering speed (85 FPS at 1080p).

</details>


### [53] [Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](https://arxiv.org/abs/2508.16512)
*Chun-Peng Chang,Chen-Yu Wang,Julian Schmidt,Holger Caesar,Alain Pagani*

Main category: cs.CV

TL;DR: Fine-tuning video generation models on driving datasets improves visual quality but degrades spatial accuracy of dynamic elements due to prioritizing surface-level realism over dynamic understanding.


<details>
  <summary>Details</summary>
Motivation: To investigate how fine-tuning affects video generation models in structured driving datasets, particularly examining the trade-off between visual fidelity and spatial accuracy of dynamic elements.

Method: Analyzed existing fine-tuning approaches on driving datasets and tested simple continual learning strategies like replay from diverse domains to balance visual quality and spatial accuracy.

Result: Found that fine-tuning improves visual quality but degrades spatial accuracy in modeling dynamic elements, while continual learning with diverse domain replay preserves both spatial accuracy and visual quality.

Conclusion: There's a trade-off between visual quality and dynamic accuracy in driving scene video generation, and continual learning strategies can provide a balanced solution that maintains both objectives.

Abstract: Recent advancements in video generation have substantially improved visual
quality and temporal coherence, making these models increasingly appealing for
applications such as autonomous driving, particularly in the context of driving
simulation and so-called "world models". In this work, we investigate the
effects of existing fine-tuning video generation approaches on structured
driving datasets and uncover a potential trade-off: although visual fidelity
improves, spatial accuracy in modeling dynamic elements may degrade. We
attribute this degradation to a shift in the alignment between visual quality
and dynamic understanding objectives. In datasets with diverse scene structures
within temporal space, where objects or perspective shift in varied ways, these
objectives tend to highly correlated. However, the very regular and repetitive
nature of driving scenes allows visual quality to improve by modeling dominant
scene motion patterns, without necessarily preserving fine-grained dynamic
behavior. As a result, fine-tuning encourages the model to prioritize
surface-level realism over dynamic accuracy. To further examine this
phenomenon, we show that simple continual learning strategies, such as replay
from diverse domains, can offer a balanced alternative by preserving spatial
accuracy while maintaining strong visual quality.

</details>


### [54] [Towards Open World Detection: A Survey](https://arxiv.org/abs/2508.16527)
*Andrei-Stefan Bulzan,Cosmin Cernazanu-Glavan*

Main category: cs.CV

TL;DR: This survey paper proposes Open World Detection (OWD) as an umbrella term to unify class-agnostic detection models, tracing the convergence of specialized computer vision tasks from early saliency detection to modern VLLMs.


<details>
  <summary>Details</summary>
Motivation: To address the fragmentation in computer vision research where specialized niches developed separately, and to chart the convergence of these tasks into a unified perception framework.

Method: The paper conducts a comprehensive survey covering the history of foundational vision subdomains, key concepts, methodologies, and datasets, analyzing topics from saliency detection to Vision Large Language Models.

Result: The analysis reveals increasing overlap and convergence between previously separate computer vision subdomains, demonstrating their potential to unify into a singular perception domain.

Conclusion: Open World Detection represents the emerging unification of computer vision tasks, with the field moving toward a comprehensive perception framework that integrates specialized approaches into a cohesive whole.

Abstract: For decades, Computer Vision has aimed at enabling machines to perceive the
external world. Initial limitations led to the development of highly
specialized niches. As success in each task accrued and research progressed,
increasingly complex perception tasks emerged. This survey charts the
convergence of these tasks and, in doing so, introduces Open World Detection
(OWD), an umbrella term we propose to unify class-agnostic and generally
applicable detection models in the vision domain. We start from the history of
foundational vision subdomains and cover key concepts, methodologies and
datasets making up today's state-of-the-art landscape. This traverses topics
starting from early saliency detection, foreground/background separation, out
of distribution detection and leading up to open world object detection,
zero-shot detection and Vision Large Language Models (VLLMs). We explore the
overlap between these subdomains, their increasing convergence, and their
potential to unify into a singular domain in the future, perception.

</details>


### [55] [MV-RAG: Retrieval Augmented Multiview Diffusion](https://arxiv.org/abs/2508.16577)
*Yosef Dayani,Omer Benishu,Sagie Benaim*

Main category: cs.CV

TL;DR: MV-RAG is a text-to-3D generation pipeline that uses retrieved 2D images to condition a multiview diffusion model, improving performance on out-of-domain and rare concepts while maintaining 3D consistency and photorealism.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-3D methods struggle with out-of-domain and rare concepts, producing inconsistent or inaccurate results due to limitations in pretrained 2D diffusion priors.

Method: Proposes a retrieval-conditioned multiview diffusion model trained with hybrid strategy: using multiview data with augmented conditioning views for view-specific reconstruction, and training on retrieved 2D images with held-out view prediction to infer 3D consistency.

Result: Significantly improves 3D consistency, photorealism, and text adherence for OOD/rare concepts while maintaining competitive performance on standard benchmarks compared to state-of-the-art baselines.

Conclusion: MV-RAG effectively bridges 2D image retrieval with multiview synthesis to address limitations in text-to-3D generation for challenging out-of-domain concepts.

Abstract: Text-to-3D generation approaches have advanced significantly by leveraging
pretrained 2D diffusion priors, producing high-quality and 3D-consistent
outputs. However, they often fail to produce out-of-domain (OOD) or rare
concepts, yielding inconsistent or inaccurate results. To this end, we propose
MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images
from a large in-the-wild 2D database and then conditions a multiview diffusion
model on these images to synthesize consistent and accurate multiview outputs.
Training such a retrieval-conditioned model is achieved via a novel hybrid
strategy bridging structured multiview data and diverse 2D image collections.
This involves training on multiview data using augmented conditioning views
that simulate retrieval variance for view-specific reconstruction, alongside
training on sets of retrieved real-world 2D images using a distinctive held-out
view prediction objective: the model predicts the held-out view from the other
views to infer 3D consistency from 2D data. To facilitate a rigorous OOD
evaluation, we introduce a new collection of challenging OOD prompts.
Experiments against state-of-the-art text-to-3D, image-to-3D, and
personalization baselines show that our approach significantly improves 3D
consistency, photorealism, and text adherence for OOD/rare concepts, while
maintaining competitive performance on standard benchmarks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [56] [NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents](https://arxiv.org/abs/2508.16414)
*Badhan Mazumder,Aline Kotoski,Vince D. Calhoun,Dong Hye Ye*

Main category: q-bio.NC

TL;DR: NeuroKoop is a graph neural network framework that integrates structural and functional brain networks using Koopman operator theory to better classify prenatal drug exposure and understand its neurodevelopmental impact.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully capture complementary features in structural and functional brain connectomes, limiting biological insight and predictive performance for understanding prenatal drug exposure effects on adolescent brain development.

Method: Developed NeuroKoop - a graph neural network framework that integrates structural (source-based morphometry) and functional (functional network connectivity) brain networks using neural Koopman operator-driven latent space fusion to unify node embeddings.

Result: NeuroKoop outperformed relevant baselines on the ABCD dataset, achieved enhanced representation learning, more robust classification of prenatal drug exposure status, and revealed salient structural-functional connections.

Conclusion: The framework advances understanding of neurodevelopmental impact of prenatal drug exposure by successfully integrating multimodal neuroimaging data and overcoming limitations of conventional analytic methods.

Abstract: Understanding how prenatal exposure to psychoactive substances such as
cannabis shapes adolescent brain organization remains a critical challenge,
complicated by the complexity of multimodal neuroimaging data and the
limitations of conventional analytic methods. Existing approaches often fail to
fully capture the complementary features embedded within structural and
functional connectomes, constraining both biological insight and predictive
performance. To address this, we introduced NeuroKoop, a novel graph neural
network-based framework that integrates structural and functional brain
networks utilizing neural Koopman operator-driven latent space fusion. By
leveraging Koopman theory, NeuroKoop unifies node embeddings derived from
source-based morphometry (SBM) and functional network connectivity (FNC) based
brain graphs, resulting in enhanced representation learning and more robust
classification of prenatal drug exposure (PDE) status. Applied to a large
adolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant
baselines and revealed salient structural-functional connections, advancing our
understanding of the neurodevelopmental impact of PDE.

</details>
