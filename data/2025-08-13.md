<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Evaluation of State-of-the-Art Deep Learning Techniques for Plant Disease and Pest Detection](https://arxiv.org/abs/2508.08317)
*Saptarshi Banerjee,Tausif Mallick,Amlan Chakroborty,Himadri Nath Saha,Nityananda T. Takur*

Main category: cs.CV

TL;DR: The paper reviews AI-based methods for detecting plant diseases and pests, highlighting their superiority over traditional techniques, with vision transformers achieving over 99.3% accuracy.


<details>
  <summary>Details</summary>
Motivation: Enhancing crop production and preventing economic losses by improving disease and pest detection using AI, ML, and DL.

Method: Categorizes detection techniques into hyperspectral imaging, non-visualization, visualization, modified deep learning architectures, and transformer models.

Result: Modern AI methods, especially vision transformers like HvT, outperform older techniques with over 99.3% accuracy.

Conclusion: Identifies system design challenges, proposes solutions, and suggests future research directions for AI-based plant disease detection.

Abstract: Addressing plant diseases and pests is critical for enhancing crop production
and preventing economic losses. Recent advances in artificial intelligence
(AI), machine learning (ML), and deep learning (DL) have significantly improved
the precision and efficiency of detection methods, surpassing the limitations
of manual identification. This study reviews modern computer-based techniques
for detecting plant diseases and pests from images, including recent AI
developments. The methodologies are organized into five categories:
hyperspectral imaging, non-visualization techniques, visualization approaches,
modified deep learning architectures, and transformer models. This structured
taxonomy provides researchers with detailed, actionable insights for selecting
advanced state-of-the-art detection methods. A comprehensive survey of recent
work and comparative studies demonstrates the consistent superiority of modern
AI-based approaches, which often outperform older image analysis methods in
speed and accuracy. In particular, vision transformers such as the Hierarchical
Vision Transformer (HvT) have shown accuracy exceeding 99.3% in plant disease
detection, outperforming architectures like MobileNetV3. The study concludes by
discussing system design challenges, proposing solutions, and outlining
promising directions for future research.

</details>


### [2] [ImageDDI: Image-enhanced Molecular Motif Sequence Representation for Drug-Drug Interaction Prediction](https://arxiv.org/abs/2508.08338)
*Yuqin He,Tengfei Ma,Chaoyi Li,Pengsen Ma,Hongxin Xiang,Jianmin Wang,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CV

TL;DR: ImageDDI, a framework for drug-drug interaction prediction, combines functional motif sequences with molecular image data for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current DDI prediction methods by leveraging both local (motif-based) and global (image-based) molecular structures.

Method: Tokenizes molecules into functional motifs, embeds them using a transformer, and enhances representation with molecular image data via Adaptive Feature Fusion.

Result: Outperforms state-of-the-art methods on standard datasets and performs well in 2D/3D image-enhanced scenarios.

Conclusion: ImageDDI effectively integrates motif and image data for superior DDI prediction, demonstrating broad applicability.

Abstract: To mitigate the potential adverse health effects of simultaneous multi-drug
use, including unexpected side effects and interactions, accurately identifying
and predicting drug-drug interactions (DDIs) is considered a crucial task in
the field of deep learning. Although existing methods have demonstrated
promising performance, they suffer from the bottleneck of limited functional
motif-based representation learning, as DDIs are fundamentally caused by motif
interactions rather than the overall drug structures. In this paper, we propose
an Image-enhanced molecular motif sequence representation framework for
\textbf{DDI} prediction, called ImageDDI, which represents a pair of drugs from
both global and local structures. Specifically, ImageDDI tokenizes molecules
into functional motifs. To effectively represent a drug pair, their motifs are
combined into a single sequence and embedded using a transformer-based encoder,
starting from the local structure representation. By leveraging the
associations between drug pairs, ImageDDI further enhances the spatial
representation of molecules using global molecular image information (e.g.
texture, shadow, color, and planar spatial relationships). To integrate
molecular visual information into functional motif sequence, ImageDDI employs
Adaptive Feature Fusion, enhancing the generalization of ImageDDI by
dynamically adapting the fusion process of feature representations.
Experimental results on widely used datasets demonstrate that ImageDDI
outperforms state-of-the-art methods. Moreover, extensive experiments show that
ImageDDI achieved competitive performance in both 2D and 3D image-enhanced
scenarios compared to other models.

</details>


### [3] [Designing Object Detection Models for TinyML: Foundations, Comparative Analysis, Challenges, and Emerging Solutions](https://arxiv.org/abs/2508.08352)
*Christophe EL Zeinaty,Wassim Hamidouche,Glenn Herrou,Daniel Menard*

Main category: cs.CV

TL;DR: This survey paper addresses optimization challenges for deploying object detection models on resource-constrained IoT devices using TinyML, covering techniques like quantization and pruning, and compares KPIs of existing implementations.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of IoT devices and their limited computational resources necessitate efficient object detection solutions, but existing surveys overlook optimization challenges in TinyML environments.

Method: The paper analyzes key optimization techniques (quantization, pruning, knowledge distillation, neural architecture search) and compares KPIs of existing implementations on microcontroller devices.

Result: The survey highlights the maturity of solutions in terms of accuracy and efficiency and provides a public repository for tracking advancements.

Conclusion: The paper bridges the gap between theory and practice in deploying object detection models on TinyML devices, offering a comprehensive resource for researchers and practitioners.

Abstract: Object detection (OD) has become vital for numerous computer vision
applications, but deploying it on resource-constrained IoT devices presents a
significant challenge. These devices, often powered by energy-efficient
microcontrollers, struggle to handle the computational load of deep
learning-based OD models. This issue is compounded by the rapid proliferation
of IoT devices, predicted to surpass 150 billion by 2030. TinyML offers a
compelling solution by enabling OD on ultra-low-power devices, paving the way
for efficient and real-time processing at the edge. Although numerous survey
papers have been published on this topic, they often overlook the optimization
challenges associated with deploying OD models in TinyML environments. To
address this gap, this survey paper provides a detailed analysis of key
optimization techniques for deploying OD models on resource-constrained
devices. These techniques include quantization, pruning, knowledge
distillation, and neural architecture search. Furthermore, we explore both
theoretical approaches and practical implementations, bridging the gap between
academic research and real-world edge artificial intelligence deployment.
Finally, we compare the key performance indicators (KPIs) of existing OD
implementations on microcontroller devices, highlighting the achieved maturity
level of these solutions in terms of both prediction accuracy and efficiency.
We also provide a public repository to continually track developments in this
fast-evolving field:
https://github.com/christophezei/Optimizing-Object-Detection-Models-for-TinyML-A-Comprehensive-Survey.

</details>


### [4] [Neural Tangent Knowledge Distillation for Optical Convolutional Networks](https://arxiv.org/abs/2508.08421)
*Jinlin Xiang,Minho Choi,Yubo Zhang,Zhihao Zhou,Arka Majumdar,Eli Shlizerman*

Main category: cs.CV

TL;DR: A task-agnostic and hardware-agnostic pipeline improves Hybrid Optical Neural Networks (ONNs) by addressing accuracy gaps and hardware discrepancies using Neural Tangent Knowledge Distillation (NTKD).


<details>
  <summary>Details</summary>
Motivation: Hybrid ONNs are energy-efficient but face accuracy gaps and hardware discrepancies, limiting their adoption. Existing solutions lack generalization across tasks and hardware.

Method: Proposes a pipeline for image classification and segmentation, estimating model accuracy pre-training and using NTKD to align optical models with electronic teachers. Post-fabrication, NTKD fine-tunes the digital backend.

Result: The pipeline improves ONN performance across datasets (MNIST, CIFAR, Carvana Masking) and hardware configurations in simulations and physical implementations.

Conclusion: The proposed method enhances ONN practicality by bridging accuracy gaps and adapting to hardware discrepancies, enabling broader deployment.

Abstract: Hybrid Optical Neural Networks (ONNs, typically consisting of an optical
frontend and a digital backend) offer an energy-efficient alternative to fully
digital deep networks for real-time, power-constrained systems. However, their
adoption is limited by two main challenges: the accuracy gap compared to
large-scale networks during training, and discrepancies between simulated and
fabricated systems that further degrade accuracy. While previous work has
proposed end-to-end optimizations for specific datasets (e.g., MNIST) and
optical systems, these approaches typically lack generalization across tasks
and hardware designs. To address these limitations, we propose a task-agnostic
and hardware-agnostic pipeline that supports image classification and
segmentation across diverse optical systems. To assist optical system design
before training, we estimate achievable model accuracy based on user-specified
constraints such as physical size and the dataset. For training, we introduce
Neural Tangent Knowledge Distillation (NTKD), which aligns optical models with
electronic teacher networks, thereby narrowing the accuracy gap. After
fabrication, NTKD also guides fine-tuning of the digital backend to compensate
for implementation errors. Experiments on multiple datasets (e.g., MNIST,
CIFAR, Carvana Masking) and hardware configurations show that our pipeline
consistently improves ONN performance and enables practical deployment in both
pre-fabrication simulations and physical implementations.

</details>


### [5] [MAViS: A Multi-Agent Framework for Long-Sequence Video Storytelling](https://arxiv.org/abs/2508.08487)
*Qian Wang,Ziqi Huang,Ruoxi Jia,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: MAViS is a multi-agent framework for long-sequence video storytelling, addressing limitations like poor assistive capability and suboptimal visual quality by orchestrating specialized agents across stages like script writing and video animation.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current long-sequence video generation frameworks, such as poor assistive capability and limited expressiveness.

Method: MAViS uses a multi-agent collaborative approach, following the 3E Principle (Explore, Examine, Enhance) across stages like script writing, keyframe generation, and audio generation. Script Writing Guidelines are introduced to optimize compatibility with generative tools.

Result: MAViS achieves state-of-the-art performance in assistive capability, visual quality, and expressiveness, producing high-quality, expressive videos with narratives and background music.

Conclusion: MAViS is a scalable, modular framework that enhances creativity and inspiration for users, uniquely offering multimodal design outputs.

Abstract: Despite recent advances, long-sequence video generation frameworks still
suffer from significant limitations: poor assistive capability, suboptimal
visual quality, and limited expressiveness. To mitigate these limitations, we
propose MAViS, an end-to-end multi-agent collaborative framework for
long-sequence video storytelling. MAViS orchestrates specialized agents across
multiple stages, including script writing, shot designing, character modeling,
keyframe generation, video animation, and audio generation. In each stage,
agents operate under the 3E Principle -- Explore, Examine, and Enhance -- to
ensure the completeness of intermediate outputs. Considering the capability
limitations of current generative models, we propose the Script Writing
Guidelines to optimize compatibility between scripts and generative tools.
Experimental results demonstrate that MAViS achieves state-of-the-art
performance in assistive capability, visual quality, and video expressiveness.
Its modular framework further enables scalability with diverse generative
models and tools. With just a brief user prompt, MAViS is capable of producing
high-quality, expressive long-sequence video storytelling, enriching
inspirations and creativity for users. To the best of our knowledge, MAViS is
the only framework that provides multimodal design output -- videos with
narratives and background music.

</details>


### [6] [MuGa-VTON: Multi-Garment Virtual Try-On via Diffusion Transformers with Prompt Customization](https://arxiv.org/abs/2508.08488)
*Ankan Deria,Dwarikanath Mahapatra,Behzad Bozorgtabar,Mohna Chakraborty,Snehashis Chakraborty,Sudipta Roy*

Main category: cs.CV

TL;DR: MuGa-VTON is a unified multi-garment diffusion framework for virtual try-on, preserving identity and garment fidelity better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods handle garments separately, require heavy preprocessing, and fail to preserve person-specific cues like tattoos and body shape.

Method: MuGa-VTON uses three modules: Garment Representation Module (GRM), Person Representation Module (PRM), and A-DiT fusion module, integrating garment, person, and text-prompt features via a diffusion transformer.

Result: Outperforms existing methods on VITON-HD and DressCode benchmarks, producing high-fidelity, identity-preserving results.

Conclusion: MuGa-VTON is suitable for real-world virtual try-on applications, offering prompt-based customization and improved realism.

Abstract: Virtual try-on seeks to generate photorealistic images of individuals in
desired garments, a task that must simultaneously preserve personal identity
and garment fidelity for practical use in fashion retail and personalization.
However, existing methods typically handle upper and lower garments separately,
rely on heavy preprocessing, and often fail to preserve person-specific cues
such as tattoos, accessories, and body shape-resulting in limited realism and
flexibility. To this end, we introduce MuGa-VTON, a unified multi-garment
diffusion framework that jointly models upper and lower garments together with
person identity in a shared latent space. Specifically, we proposed three key
modules: the Garment Representation Module (GRM) for capturing both garment
semantics, the Person Representation Module (PRM) for encoding identity and
pose cues, and the A-DiT fusion module, which integrates garment, person, and
text-prompt features through a diffusion transformer. This architecture
supports prompt-based customization, allowing fine-grained garment
modifications with minimal user input. Extensive experiments on the VITON-HD
and DressCode benchmarks demonstrate that MuGa-VTON outperforms existing
methods in both qualitative and quantitative evaluations, producing
high-fidelity, identity-preserving results suitable for real-world virtual
try-on applications.

</details>


### [7] [CObL: Toward Zero-Shot Ordinal Layering without User Prompting](https://arxiv.org/abs/2508.08498)
*Aneel Damaraju,Dean Hazineh,Todd Zickler*

Main category: cs.CV

TL;DR: CObL is a diffusion-based architecture that generates occlusion-ordered object layers from images, generalizing to real-world scenes without prior knowledge of object count.


<details>
  <summary>Details</summary>
Motivation: To improve vision by grouping pixels into objects and understanding their spatial relationships, including occlusion and depth.

Method: Uses a diffusion-based architecture (CObL) with Stable Diffusion as a prior and inference-time guidance to composite layers back to the input image. Trained on synthetic multi-object tabletop scenes.

Result: Zero-shot generalization to real-world tabletop photos with novel objects, reconstructing multiple occluded objects without user prompts.

Conclusion: CObL outperforms previous models in unsupervised object-centric representation learning by generalizing beyond its training domain.

Abstract: Vision benefits from grouping pixels into objects and understanding their
spatial relationships, both laterally and in depth. We capture this with a
scene representation comprising an occlusion-ordered stack of "object layers,"
each containing an isolated and amodally-completed object. To infer this
representation from an image, we introduce a diffusion-based architecture named
Concurrent Object Layers (CObL). CObL generates a stack of object layers in
parallel, using Stable Diffusion as a prior for natural objects and
inference-time guidance to ensure the inferred layers composite back to the
input image. We train CObL using a few thousand synthetically-generated images
of multi-object tabletop scenes, and we find that it zero-shot generalizes to
photographs of real-world tabletops with varying numbers of novel objects. In
contrast to recent models for amodal object completion, CObL reconstructs
multiple occluded objects without user prompting and without knowing the number
of objects beforehand. Unlike previous models for unsupervised object-centric
representation learning, CObL is not limited to the world it was trained in.

</details>


### [8] [Re:Verse -- Can Your VLM Read a Manga?](https://arxiv.org/abs/2508.08508)
*Aaditya Baranwal,Madhav Kataria,Naitik Agrawal,Yogesh S Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: Current VLMs excel at single-panel recognition but fail at deep narrative reasoning like temporal causality and cross-panel cohesion. A new evaluation framework reveals these gaps, showing models struggle with non-linear narratives and causal inference.


<details>
  <summary>Details</summary>
Motivation: To address the gap in VLMs' ability to understand sequential visual storytelling, particularly in manga narratives, where deep reasoning is lacking.

Method: Introduces a framework with multimodal annotation, cross-modal embedding analysis, and retrieval-augmented assessment. Evaluates VLMs on generative storytelling, dialogue grounding, and temporal reasoning using annotated manga panels.

Result: VLMs perform poorly in long-form narrative tasks, especially in non-linear stories, character consistency, and causal inference.

Conclusion: The study highlights VLMs' limitations in narrative intelligence and provides a foundation for future improvements in sequential understanding.

Abstract: Current Vision Language Models (VLMs) demonstrate a critical gap between
surface-level recognition and deep narrative reasoning when processing
sequential visual storytelling. Through a comprehensive investigation of manga
narrative understanding, we reveal that while recent large multimodal models
excel at individual panel interpretation, they systematically fail at temporal
causality and cross-panel cohesion, core requirements for coherent story
comprehension. We introduce a novel evaluation framework that combines
fine-grained multimodal annotation, cross-modal embedding analysis, and
retrieval-augmented assessment to systematically characterize these
limitations.
  Our methodology includes (i) a rigorous annotation protocol linking visual
elements to narrative structure through aligned light novel text, (ii)
comprehensive evaluation across multiple reasoning paradigms, including direct
inference and retrieval-augmented generation, and (iii) cross-modal similarity
analysis revealing fundamental misalignments in current VLMs' joint
representations. Applying this framework to Re:Zero manga across 11 chapters
with 308 annotated panels, we conduct the first systematic study of long-form
narrative understanding in VLMs through three core evaluation axes: generative
storytelling, contextual dialogue grounding, and temporal reasoning. Our
findings demonstrate that current models lack genuine story-level intelligence,
struggling particularly with non-linear narratives, character consistency, and
causal inference across extended sequences. This work establishes both the
foundation and practical methodology for evaluating narrative intelligence,
while providing actionable insights into the capability of deep sequential
understanding of Discrete Visual Narratives beyond basic recognition in
Multimodal Models.

</details>


### [9] [VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models](https://arxiv.org/abs/2508.08521)
*Mansi Phute,Ravikumar Balakrishnan*

Main category: cs.CV

TL;DR: VISOR introduces a visual input-based method for controlling Vision Language Models (VLMs) without invasive model access, achieving robust behavioral shifts while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling VLMs are either detectable or require invasive access, limiting practicality and security.

Method: VISOR uses optimized visual inputs (steering images) to induce target activation patterns in VLMs, enabling imperceptible and effective control.

Result: VISOR matches or exceeds steering vector performance (1-25% shifts) and outperforms system prompting, while maintaining 99.9% performance on unrelated tasks.

Conclusion: VISOR redefines multimodal model control, demonstrating a security vulnerability in visual steering and calling for new defenses.

Abstract: Vision Language Models (VLMs) are increasingly being used in a broad range of
applications, bringing their security and behavioral control to the forefront.
While existing approaches for behavioral control or output redirection, like
system prompting in VLMs, are easily detectable and often ineffective,
activation-based steering vectors require invasive runtime access to model
internals--incompatible with API-based services and closed-source deployments.
We introduce VISOR (Visual Input-based Steering for Output Redirection), a
novel method that achieves sophisticated behavioral control through optimized
visual inputs alone. By crafting universal steering images that induce target
activation patterns, VISOR enables practical deployment across all VLM serving
modalities while remaining imperceptible compared to explicit textual
instructions. We validate VISOR on LLaVA-1.5-7B across three critical alignment
tasks: refusal, sycophancy and survival instinct. A single 150KB steering image
matches steering vector performance within 1-2% for positive behavioral shifts
while dramatically exceeding it for negative steering--achieving up to 25%
shifts from baseline compared to steering vectors' modest changes. Unlike
system prompting (3-4% shifts), VISOR provides robust bidirectional control
while maintaining 99.9% performance on 14,000 unrelated MMLU tasks. Beyond
eliminating runtime overhead and model access requirements, VISOR exposes a
critical security vulnerability: adversaries can achieve sophisticated
behavioral manipulation through visual channels alone, bypassing text-based
defenses. Our work fundamentally re-imagines multimodal model control and
highlights the urgent need for defenses against visual steering attacks.

</details>


### [10] [Training Kindai OCR with parallel textline images and self-attention feature distance-based loss](https://arxiv.org/abs/2508.08537)
*Anh Le,Asanobu Kitamoto*

Main category: cs.CV

TL;DR: The paper proposes a method to improve OCR for historical Kindai documents by using parallel textline images and a distance-based objective function, reducing error rates significantly.


<details>
  <summary>Details</summary>
Motivation: Transcribing Kindai documents is labor-intensive, and limited annotated data hampers OCR training. The research aims to address data scarcity.

Method: Leverages parallel textline images (original and contemporary fonts) and introduces a distance-based objective function (Euclidean distance and MMD) to minimize feature gaps.

Result: Reduces character error rate (CER) by 2.23% (Euclidean) and 3.94% (MMD) over a Transformer-based OCR baseline. Improves self-attention representation quality.

Conclusion: The method effectively enhances OCR performance for historical documents by addressing data scarcity and improving feature discriminability.

Abstract: Kindai documents, written in modern Japanese from the late 19th to early 20th
century, hold significant historical value for researchers studying societal
structures, daily life, and environmental conditions of that period. However,
transcribing these documents remains a labor-intensive and time-consuming task,
resulting in limited annotated data for training optical character recognition
(OCR) systems. This research addresses this challenge of data scarcity by
leveraging parallel textline images - pairs of original Kindai text and their
counterparts in contemporary Japanese fonts - to augment training datasets. We
introduce a distance-based objective function that minimizes the gap between
self-attention features of the parallel image pairs. Specifically, we explore
Euclidean distance and Maximum Mean Discrepancy (MMD) as domain adaptation
metrics. Experimental results demonstrate that our method reduces the character
error rate (CER) by 2.23% and 3.94% over a Transformer-based OCR baseline when
using Euclidean distance and MMD, respectively. Furthermore, our approach
improves the discriminative quality of self-attention representations, leading
to more effective OCR performance for historical documents.

</details>


### [11] [Calibration Attention: Instance-wise Temperature Scaling for Vision Transformers](https://arxiv.org/abs/2508.08547)
*Wenhao Liang,Wei Emma Zhang,Lin Yue,Miao Xu,Olaf Maennel,Weitong Chen*

Main category: cs.CV

TL;DR: Calibration Attention (CalAttn) improves probability calibration in Vision Transformers by learning adaptive, per-instance temperatures from the CLS token, reducing errors by up to 4x with minimal parameter overhead.


<details>
  <summary>Details</summary>
Motivation: Probability calibration is crucial for risk-sensitive applications of Vision Transformers, but standard methods like temperature scaling are limited by global scalar use and require validation sets.

Method: CalAttn is introduced as a drop-in module that learns per-instance temperatures directly from the ViT's CLS token, avoiding the need for a validation set.

Result: CalAttn reduces calibration error by up to 4x across multiple datasets (CIFAR-10/100, MNIST, Tiny-ImageNet, ImageNet-1K) with under 0.1% additional parameters.

Conclusion: CalAttn is a simple, efficient, and architecture-agnostic solution for better probability calibration in Vision Transformers without compromising accuracy.

Abstract: Probability calibration is critical when Vision Transformers are deployed in
risk-sensitive applications. The standard fix, post-hoc temperature scaling,
uses a single global scalar and requires a held-out validation set. We
introduce Calibration Attention (CalAttn), a drop-in module that learns an
adaptive, per-instance temperature directly from the ViT's CLS token. Across
CIFAR-10/100, MNIST, Tiny-ImageNet, and ImageNet-1K, CalAttn reduces
calibration error by up to 4x on ViT-224, DeiT, and Swin, while adding under
0.1 percent additional parameters. The learned temperatures cluster tightly
around 1.0, in contrast to the large global values used by standard temperature
scaling. CalAttn is simple, efficient, and architecture-agnostic, and yields
more trustworthy probabilities without sacrificing accuracy. Code:
[https://github.com/EagleAdelaide/CalibrationAttention-CalAttn-](https://github.com/EagleAdelaide/CalibrationAttention-CalAttn-)

</details>


### [12] [Boosting Generic Semi-Supervised Medical Image Segmentation via Diverse Teaching and Label Propagation](https://arxiv.org/abs/2508.08549)
*Wei Li,Pengcheng Zhou,Linye Ma,Wenyi Zhao,Huihua Yang*

Main category: cs.CV

TL;DR: A generic framework (DTLP-Net) is proposed to address semi-supervised medical image segmentation, domain generalization, and domain adaptation by generating reliable pseudo-labels and enhancing model diversity.


<details>
  <summary>Details</summary>
Motivation: Limited annotation and domain shift hinder medical image segmentation. Existing methods are task-specific and suboptimal. A unified solution is needed.

Method: DTLP-Net uses two diverse teacher models and a student model to generate reliable pseudo-labels. It employs data augmentation and label propagation for robustness.

Result: Notable improvements over state-of-the-art methods on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks.

Conclusion: DTLP-Net effectively tackles semi-supervised challenges in medical image segmentation, demonstrating potential for broader SSL applications.

Abstract: Both limited annotation and domain shift are significant challenges
frequently encountered in medical image segmentation, leading to derivative
scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain
generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA).
Conventional methods are generally tailored to specific tasks in isolation, the
error accumulation hinders the effective utilization of unlabeled data and
limits further improvements, resulting in suboptimal performance when these
issues occur. In this paper, we aim to develop a generic framework that masters
all three tasks. We found that the key to solving the problem lies in how to
generate reliable pseudo labels for the unlabeled data in the presence of
domain shift with labeled data and increasing the diversity of the model. To
tackle this issue, we employ a Diverse Teaching and Label Propagation Network
(DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation.
Our DTLP-Net involves a single student model and two diverse teacher models,
which can generate reliable pseudo-labels for the student model. The first
teacher model decouple the training process with labeled and unlabeled data,
The second teacher is momentum-updated periodically, thus generating reliable
yet divers pseudo-labels. To fully utilize the information within the data, we
adopt inter-sample and intra-sample data augmentation to learn the global and
local knowledge. In addition, to further capture the voxel-level correlations,
we propose label propagation to enhance the model robust. We evaluate our
proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG
tasks. The results showcase notable improvements compared to state-of-the-art
methods across all five settings, indicating the potential of our framework to
tackle more challenging SSL scenarios.

</details>


### [13] [Unlocking the Potential of Diffusion Priors in Blind Face Restoration](https://arxiv.org/abs/2508.08556)
*Yunqi Miao,Zhiyu Qu,Mingqi Gao,Changrui Chen,Jifei Song,Jungong Han,Jiankang Deng*

Main category: cs.CV

TL;DR: FLIPNET addresses gaps in blind face restoration (BFR) by switching between Restoration and Degradation modes, improving authenticity and fidelity.


<details>
  <summary>Details</summary>
Motivation: The gap between vanilla diffusion models and BFR settings, due to discrepancies in image quality and synthesis vs. real-world images, hinders effective adaptation.

Method: FLIPNET uses a unified network with two modes: Restoration for integrating BFR features and Degradation for synthesizing realistic degraded images.

Result: FLIPNET outperforms prior BFR methods in authenticity and fidelity and improves degradation modeling.

Conclusion: FLIPNET effectively bridges the gap in BFR by leveraging dual modes for restoration and degradation synthesis.

Abstract: Although diffusion prior is rising as a powerful solution for blind face
restoration (BFR), the inherent gap between the vanilla diffusion model and BFR
settings hinders its seamless adaptation. The gap mainly stems from the
discrepancy between 1) high-quality (HQ) and low-quality (LQ) images and 2)
synthesized and real-world images. The vanilla diffusion model is trained on
images with no or less degradations, whereas BFR handles moderately to severely
degraded images. Additionally, LQ images used for training are synthesized by a
naive degradation model with limited degradation patterns, which fails to
simulate complex and unknown degradations in real-world scenarios. In this
work, we use a unified network FLIPNET that switches between two modes to
resolve specific gaps. In Restoration mode, the model gradually integrates
BFR-oriented features and face embeddings from LQ images to achieve authentic
and faithful face restoration. In Degradation mode, the model synthesizes
real-world like degraded images based on the knowledge learned from real-world
degradation datasets. Extensive evaluations on benchmark datasets show that our
model 1) outperforms previous diffusion prior based BFR methods in terms of
authenticity and fidelity, and 2) outperforms the naive degradation model in
modeling the real-world degradations.

</details>


### [14] [Think as Cardiac Sonographers: Marrying SAM with Left Ventricular Indicators Measurements According to Clinical Guidelines](https://arxiv.org/abs/2508.08566)
*Tuo Liu,Qinghan Yang,Yu Zhang,Rongjun Ge,Yang Chen,Guangquan Zhou*

Main category: cs.CV

TL;DR: AutoSAME combines SAM's visual understanding with segmentation and landmark localization for LV measurements, introducing FCBA and SGPA to enhance accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms struggle with small datasets and lack anatomical point identification, necessitating a robust framework for LV measurements.

Method: Proposes AutoSAME, integrating SAM with segmentation and landmark tasks, using FCBA for feature enhancement and SGPA for spatial-guided prompts.

Result: Demonstrates superior performance in LV segmentation, landmark localization, and indicator measurements on an echocardiography dataset.

Conclusion: AutoSAME effectively mimics sonographers, aligning with clinical guidelines, and offers a promising solution for automated LV quantification.

Abstract: Left ventricular (LV) indicator measurements following clinical
echocardiog-raphy guidelines are important for diagnosing cardiovascular
disease. Alt-hough existing algorithms have explored automated LV
quantification, they can struggle to capture generic visual representations due
to the normally small training datasets. Therefore, it is necessary to
introduce vision founda-tional models (VFM) with abundant knowledge. However,
VFMs represented by the segment anything model (SAM) are usually suitable for
segmentation but incapable of identifying key anatomical points, which are
critical in LV indicator measurements. In this paper, we propose a novel
framework named AutoSAME, combining the powerful visual understanding of SAM
with seg-mentation and landmark localization tasks simultaneously.
Consequently, the framework mimics the operation of cardiac sonographers,
achieving LV indi-cator measurements consistent with clinical guidelines. We
further present fil-tered cross-branch attention (FCBA) in AutoSAME, which
leverages relatively comprehensive features in the segmentation to enhance the
heatmap regression (HR) of key points from the frequency domain perspective,
optimizing the vis-ual representation learned by the latter. Moreover, we
propose spatial-guided prompt alignment (SGPA) to automatically generate prompt
embeddings guid-ed by spatial properties of LV, thereby improving the accuracy
of dense pre-dictions by prior spatial knowledge. The extensive experiments on
an echocar-diography dataset demonstrate the efficiency of each design and the
superiori-ty of our AutoSAME in LV segmentation, landmark localization, and
indicator measurements. The code will be available at
https://github.com/QC-LIU-1997/AutoSAME.

</details>


### [15] [Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation](https://arxiv.org/abs/2508.08570)
*Chenruo Liu,Hongjun Liu,Zeyu Lai,Yiqiu Shen,Chen Zhao,Qi Lei*

Main category: cs.CV

TL;DR: A method using superclass information and gradient-based attention to reduce reliance on spurious features, improving robustness without auxiliary annotations.


<details>
  <summary>Details</summary>
Motivation: Prior methods require impractical auxiliary annotations and assume identical group sets across domains, limiting real-world applicability.

Method: Leverages superclass labels and gradient-based attention guided by a vision-language model to disentangle features, promoting superclass-relevant features for prediction.

Result: Outperforms baselines in domain generalization tasks, with improvements in metrics and visualizations.

Conclusion: The approach effectively enhances robustness to spurious correlations without needing source sample annotations.

Abstract: To enhance group robustness to spurious correlations, prior work often relies
on auxiliary annotations for groups or spurious features and assumes identical
sets of groups across source and target domains. These two requirements are
both unnatural and impractical in real-world settings. To overcome these
limitations, we propose a method that leverages the semantic structure inherent
in class labels--specifically, superclass information--to naturally reduce
reliance on spurious features. Our model employs gradient-based attention
guided by a pre-trained vision-language model to disentangle
superclass-relevant and irrelevant features. Then, by promoting the use of all
superclass-relevant features for prediction, our approach achieves robustness
to more complex spurious correlations without the need to annotate any source
samples. Experiments across diverse datasets demonstrate that our method
significantly outperforms baselines in domain generalization tasks, with clear
improvements in both quantitative metrics and qualitative visualizations.

</details>


### [16] [RealisMotion: Decomposed Human Motion Control and Video Generation in the World Space](https://arxiv.org/abs/2508.08588)
*Jingyun Liang,Jingkai Zhou,Shikai Li,Chenjie Cao,Lei Sun,Yichen Qian,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: A framework for generating human videos with separate control over foreground, background, trajectory, and action, using 3D motion editing and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack separate control over key video elements (foreground, background, trajectory, action), limiting flexibility in video generation.

Method: Decomposes motion control into 3D space, uses trajectory unprojection and motion banks, and integrates with text-to-video diffusion models for composition.

Result: Achieves state-of-the-art performance in controllability and video quality on benchmarks and real-world cases.

Conclusion: The proposed framework enables flexible and realistic video generation by decoupling and controlling key elements independently.

Abstract: Generating human videos with realistic and controllable motions is a
challenging task. While existing methods can generate visually compelling
videos, they lack separate control over four key video elements: foreground
subject, background video, human trajectory and action patterns. In this paper,
we propose a decomposed human motion control and video generation framework
that explicitly decouples motion from appearance, subject from background, and
action from trajectory, enabling flexible mix-and-match composition of these
elements. Concretely, we first build a ground-aware 3D world coordinate system
and perform motion editing directly in the 3D space. Trajectory control is
implemented by unprojecting edited 2D trajectories into 3D with focal-length
calibration and coordinate transformation, followed by speed alignment and
orientation adjustment; actions are supplied by a motion bank or generated via
text-to-motion methods. Then, based on modern text-to-video diffusion
transformer models, we inject the subject as tokens for full attention,
concatenate the background along the channel dimension, and add motion
(trajectory and action) control signals by addition. Such a design opens up the
possibility for us to generate realistic videos of anyone doing anything
anywhere. Extensive experiments on benchmark datasets and real-world cases
demonstrate that our method achieves state-of-the-art performance on both
element-wise controllability and overall video quality.

</details>


### [17] [DocThinker: Explainable Multimodal Large Language Models with Rule-based Reinforcement Learning for Document Understanding](https://arxiv.org/abs/2508.08589)
*Wenwen Yu,Zhibo Yang,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: DocThinker is a rule-based RL framework for dynamic inference-time reasoning in MLLMs, improving explainability and adaptability over fixed CoT methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs lack transparency and reliability in reasoning, especially in high-stakes domains, due to black-box processes and limitations of fixed CoT methods.

Method: Proposes DocThinker, using rule-based RL to dynamically refine reasoning strategies, generating explainable intermediate results and integrating multi-objective rewards.

Result: DocThinker outperforms benchmarks, enhancing generalization, explainability, and adaptability in document understanding.

Conclusion: RL is a promising approach for improving transparency and adaptability in MLLM-based document analysis.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in document understanding. However, their reasoning processes
remain largely black-box, making it difficult to ensure reliability and
trustworthiness, especially in high-stakes domains such as legal, financial,
and medical document analysis. Existing methods use fixed Chain-of-Thought
(CoT) reasoning with supervised fine-tuning (SFT) but suffer from catastrophic
forgetting, poor adaptability, and limited generalization across domain tasks.
In this paper, we propose DocThinker, a rule-based Reinforcement Learning (RL)
framework for dynamic inference-time reasoning. Instead of relying on static
CoT templates, DocThinker autonomously refines reasoning strategies via policy
learning, generating explainable intermediate results, including structured
reasoning processes, rephrased questions, regions of interest (RoI) supporting
the answer, and the final answer. By integrating multi-objective rule-based
rewards and KL-constrained optimization, our method mitigates catastrophic
forgetting and enhances both adaptability and transparency. Extensive
experiments on multiple benchmarks demonstrate that DocThinker significantly
improves generalization while producing more explainable and
human-understandable reasoning steps. Our findings highlight RL as a powerful
alternative for enhancing explainability and adaptability in MLLM-based
document understanding. Code will be available at
https://github.com/wenwenyu/DocThinker.

</details>


### [18] [QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection](https://arxiv.org/abs/2508.08590)
*Yuxiao Wang,Wolin Liang,Yu Lei,Weiying Xue,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: QueryCraft improves HOI detection by using semantic priors and guided feature learning via transformer-based query initialization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Randomly initialized queries in DETR-based HOI detection lack explicit semantics, leading to suboptimal performance.

Method: Introduces ACTOR for cross-modal feature extraction and PDQD for object-level query distillation.

Result: Achieves state-of-the-art performance on HICO-Det and V-COCO benchmarks.

Conclusion: QueryCraft enhances query semantics and detection performance, demonstrating strong generalization.

Abstract: Human-Object Interaction (HOI) detection aims to localize human-object pairs
and recognize their interactions in images. Although DETR-based methods have
recently emerged as the mainstream framework for HOI detection, they still
suffer from a key limitation: Randomly initialized queries lack explicit
semantics, leading to suboptimal detection performance. To address this
challenge, we propose QueryCraft, a novel plug-and-play HOI detection framework
that incorporates semantic priors and guided feature learning through
transformer-based query initialization. Central to our approach is
\textbf{ACTOR} (\textbf{A}ction-aware \textbf{C}ross-modal
\textbf{T}ransf\textbf{OR}mer), a cross-modal Transformer encoder that jointly
attends to visual regions and textual prompts to extract action-relevant
features. Rather than merely aligning modalities, ACTOR leverages
language-guided attention to infer interaction semantics and produce
semantically meaningful query representations. To further enhance object-level
query quality, we introduce a \textbf{P}erceptual \textbf{D}istilled
\textbf{Q}uery \textbf{D}ecoder (\textbf{PDQD}), which distills object category
awareness from a pre-trained detector to serve as object query initiation. This
dual-branch query initialization enables the model to generate more
interpretable and effective queries for HOI detection. Extensive experiments on
HICO-Det and V-COCO benchmarks demonstrate that our method achieves
state-of-the-art performance and strong generalization. Code will be released
upon publication.

</details>


### [19] [Yan: Foundational Interactive Video Generation](https://arxiv.org/abs/2508.08601)
*Yan Team*

Main category: cs.CV

TL;DR: Yan is a framework for interactive video generation, integrating simulation, multi-modal generation, and editing for real-time, action-controllable video creation.


<details>
  <summary>Details</summary>
Motivation: To advance interactive video generation by combining simulation, generation, and editing into a unified AI-driven paradigm for creative tools and media.

Method: Uses a 3D-VAE for simulation, hierarchical autoregressive captioning for multi-modal generation, and a hybrid model for disentangled editing.

Result: Achieves real-time 1080P/60FPS simulation, flexible domain blending, and multi-granularity editing during interaction.

Conclusion: Yan integrates these capabilities into a comprehensive framework, paving the way for next-gen interactive video creation.

Abstract: We present Yan, a foundational framework for interactive video generation,
covering the entire pipeline from simulation and generation to editing.
Specifically, Yan comprises three core modules. AAA-level Simulation: We design
a highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based
shift-window denoising inference process, achieving real-time 1080P/60FPS
interactive simulation. Multi-Modal Generation: We introduce a hierarchical
autoregressive caption method that injects game-specific knowledge into
open-domain multi-modal video diffusion models (VDMs), then transforming the
VDM into a frame-wise, action-controllable, real-time infinite interactive
video generator. Notably, when the textual and visual prompts are sourced from
different domains, the model demonstrates strong generalization, allowing it to
blend and compose the style and mechanics across domains flexibly according to
user prompts. Multi-Granularity Editing: We propose a hybrid model that
explicitly disentangles interactive mechanics simulation from visual rendering,
enabling multi-granularity video content editing during interaction through
text. Collectively, Yan offers an integration of these modules, pushing
interactive video generation beyond isolated capabilities toward a
comprehensive AI-driven interactive creation paradigm, paving the way for the
next generation of creative tools, media, and entertainment. The project page
is: https://greatx3.github.io/Yan/.

</details>


### [20] [Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization](https://arxiv.org/abs/2508.08604)
*Jihwan Park,Taehoon song,Sanghyeok Lee,Miso Choi,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: TransMiter is a lightweight, model-agnostic adapter that transfers adaptation knowledge between VLMs without backpropagation, improving performance efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and limited transferability of fine-tuning large VLMs, enabling efficient knowledge reuse across models.

Method: TransMiter captures the knowledge gap between pre-trained and fine-tuned VLMs in an unsupervised manner, requiring minimal layers and no backpropagation.

Result: TransMiter effectively transfers knowledge across VLMs of varying sizes and architectures, often outperforming fine-tuned models with low computational cost.

Conclusion: TransMiter offers a scalable, efficient solution for enhancing VLMs without fine-tuning, preserving generalization while reducing resource demands.

Abstract: Vision-Language Models (VLMs) have been widely used in various visual
recognition tasks due to their remarkable generalization capabilities. As these
models grow in size and complexity, fine-tuning becomes costly, emphasizing the
need to reuse adaptation knowledge from 'weaker' models to efficiently enhance
'stronger' ones. However, existing adaptation transfer methods exhibit limited
transferability across models due to their model-specific design and high
computational demands. To tackle this, we propose Transferable Model-agnostic
adapter (TransMiter), a light-weight adapter that improves vision-language
models 'without backpropagation'. TransMiter captures the knowledge gap between
pre-trained and fine-tuned VLMs, in an 'unsupervised' manner. Once trained,
this knowledge can be seamlessly transferred across different models without
the need for backpropagation. Moreover, TransMiter consists of only a few
layers, inducing a negligible additional inference cost. Notably, supplementing
the process with a few labeled data further yields additional performance gain,
often surpassing a fine-tuned stronger model, with a marginal training cost.
Experimental results and analyses demonstrate that TransMiter effectively and
efficiently transfers adaptation knowledge while preserving generalization
abilities across VLMs of different sizes and architectures in visual
recognition tasks.

</details>


### [21] [SelfHVD: Self-Supervised Handheld Video Deblurring for Mobile Phones](https://arxiv.org/abs/2508.08605)
*Honglei Xu,Zhilu Zhang,Junjie Fan,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: A self-supervised method for handheld video deblurring using sharp clues, SEVD for data enhancement, and SCSCM for spatial consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Handheld mobile videos often suffer from blur due to shaking, and existing methods struggle with real-world blur domain gaps.

Method: Extracts sharp clues as labels, uses SEVD for better data, and SCSCM for spatial consistency.

Result: Outperforms existing self-supervised methods on synthetic and real-world datasets.

Conclusion: Proposed method effectively addresses handheld video blur with self-supervision and novel techniques.

Abstract: Shooting video with a handheld mobile phone, the most common photographic
device, often results in blurry frames due to shaking hands and other
instability factors. Although previous video deblurring methods have achieved
impressive progress, they still struggle to perform satisfactorily on
real-world handheld video due to the blur domain gap between training and
testing data. To address the issue, we propose a self-supervised method for
handheld video deblurring, which is driven by sharp clues in the video. First,
to train the deblurring model, we extract the sharp clues from the video and
take them as misalignment labels of neighboring blurry frames. Second, to
improve the model's ability, we propose a novel Self-Enhanced Video Deblurring
(SEVD) method to create higher-quality paired video data. Third, we propose a
Self-Constrained Spatial Consistency Maintenance (SCSCM) method to regularize
the model, preventing position shifts between the output and input frames.
Moreover, we construct a synthetic and a real-world handheld video dataset for
handheld video deblurring. Extensive experiments on these two and other common
real-world datasets demonstrate that our method significantly outperforms
existing self-supervised ones. The code and datasets are publicly available at
https://github.com/cshonglei/SelfHVD.

</details>


### [22] [Neural Artistic Style and Color Transfer Using Deep Learning](https://arxiv.org/abs/2508.08608)
*Justin London*

Main category: cs.CV

TL;DR: A method combining neural artistic style transfer with color transfer, evaluated using KL divergence for color and luminance histogram matching.


<details>
  <summary>Details</summary>
Motivation: To enhance artistic expression and image correction by merging neural artistic style transfer with color transfer techniques.

Method: Uses KL divergence to evaluate color and luminance histogram matching algorithms (Reinhard, IDT, Cholesky, PCA) between original and style-transferred images.

Result: Various experiments assess KL divergence and color histograms for style-to-content transfer.

Conclusion: The combined approach improves artistic visuals and image correction, validated by quantitative evaluation.

Abstract: Neural artistic style transfers and blends the content and style
representation of one image with the style of another. This enables artists to
create unique innovative visuals and enhances artistic expression in various
fields including art, design, and film. Color transfer algorithms are an
important in digital image processing by adjusting the color information in a
target image based on the colors in the source image. Color transfer enhances
images and videos in film and photography, and can aid in image correction. We
introduce a methodology that combines neural artistic style with color
transfer. The method uses the Kullback-Leibler (KL) divergence to
quantitatively evaluate color and luminance histogram matching algorithms
including Reinhard global color transfer, iteration distribution transfer
(IDT), IDT with regrain, Cholesky, and PCA between the original and neural
artistic style transferred image using deep learning. We estimate the color
channel kernel densities. Various experiments are performed to evaluate the KL
of these algorithms and their color histograms for style to content transfer.

</details>


### [23] [Hierarchical Visual Prompt Learning for Continual Video Instance Segmentation](https://arxiv.org/abs/2508.08612)
*Jiahua Dong,Hui Yin,Wenqi Liang,Hanbin Zhao,Henghui Ding,Nicu Sebe,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: A novel Hierarchical Visual Prompt Learning (HVPL) model addresses catastrophic forgetting in Video Instance Segmentation (VIS) by using frame-level and video-level prompts with orthogonal gradient correction and context decoding.


<details>
  <summary>Details</summary>
Motivation: Existing VIS approaches assume fixed object categories and suffer from catastrophic forgetting when learning new classes, limiting their practicality.

Method: HVPL introduces task-specific frame and video prompts, an orthogonal gradient correction (OGC) module for frame-level mitigation, and a video context decoder for video-level context propagation.

Result: HVPL outperforms baseline approaches by effectively preventing forgetting of old classes while learning new ones.

Conclusion: HVPL provides a robust solution for continuous learning in VIS, enhancing performance and adaptability.

Abstract: Video instance segmentation (VIS) has gained significant attention for its
capability in tracking and segmenting object instances across video frames.
However, most of the existing VIS approaches unrealistically assume that the
categories of object instances remain fixed over time. Moreover, they
experience catastrophic forgetting of old classes when required to continuously
learn object instances belonging to new categories. To resolve these
challenges, we develop a novel Hierarchical Visual Prompt Learning (HVPL) model
that overcomes catastrophic forgetting of previous categories from both
frame-level and video-level perspectives. Specifically, to mitigate forgetting
at the frame level, we devise a task-specific frame prompt and an orthogonal
gradient correction (OGC) module. The OGC module helps the frame prompt encode
task-specific global instance information for new classes in each individual
frame by projecting its gradients onto the orthogonal feature space of old
classes. Furthermore, to address forgetting at the video level, we design a
task-specific video prompt and a video context decoder. This decoder first
embeds structural inter-class relationships across frames into the frame prompt
features, and then propagates task-specific global video contexts from the
frame prompt features to the video prompt. Through rigorous comparisons, our
HVPL model proves to be more effective than baseline approaches. The code is
available at https://github.com/JiahuaDong/HVPL.

</details>


### [24] [AME: Aligned Manifold Entropy for Robust Vision-Language Distillation](https://arxiv.org/abs/2508.08644)
*Guiming Cao,Yuming Ou*

Main category: cs.CV

TL;DR: AME (Aligned Manifold Entropy) improves vision-language knowledge distillation by minimizing entropy on a shared manifold, enabling robust generalization with limited data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust knowledge distillation in vision-language models under low-data regimes due to high predictive uncertainty and impractical large-scale data collection.

Method: AME reconfigures a shared manifold for multi-modal data (image and text) using projection functions, applying entropy minimization for structural compression of cross-modal features.

Result: AME consistently enhances distillation robustness and generalization across diverse architectures and tasks, with theoretical support for tighter error bounds.

Conclusion: AME is a plug-and-play solution for robust vision-language distillation, effective in low-data scenarios without modifying backbone architectures.

Abstract: Knowledge distillation is a long-established technique for knowledge
transfer, and has regained attention in the context of the recent emergence of
large vision-language models (VLMs). However, vision-language knowledge
distillation often requires sufficient training data to achieve robust
generalization on amples with ambiguous or boundary-adjacent representations,
which are associated with high predictive uncertainty. Critically, collecting
such large-scale, task-specific data for training is often impractical in
real-world scenarios. To address this major challenge arising from the
entanglement of uncertainty and cross-modal feature representation, we propose
Aligned Manifold Entropy for Robust Vision-Language Distillation (AME), aiming
to achieve robust generalization under real-world conditions. AME applies
entropy minimization over a reconfigured shared manifold, where multi-modal
data (i.e., image and text) are bridged through a pair of projection functions,
conducive to structural compression for cross-modal feature representations.
This enables robust knowledge distillation under low-data regimes, while
requiring no architectural modifications to the backbone. As a result, it can
serve as a plug-and-play module compatible with a wide range of vision-language
distillation frameworks. Notably, our theoretical analysis reveals that
integrating knowledge distillation with entropy minimization over the shared
manifold leads to a tighter generalization error bound. Extensive experiments
across diverse distillation architectures and training settings demonstrate
that AME consistently facilitates robust knowledge distillation, resulting in
superior generalization performance across a wide spectrum of downstream tasks.

</details>


### [25] [Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation](https://arxiv.org/abs/2508.08660)
*Xin Wang,Yin Guo,Jiamin Xia,Kaiyu Zhang,Niranjan Balu,Mahmud Mossa-Basha,Linda Shapiro,Chun Yuan*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for unsupervised domain adaptation in medical image segmentation, supporting both source-accessible and source-free settings by leveraging a domain-agnostic probabilistic manifold for anatomical knowledge.


<details>
  <summary>Details</summary>
Motivation: Address the lack of a structured, generalizable anatomical knowledge construction in prior domain adaptation methods, which are narrowly tailored to specific settings.

Method: Proposes a model that learns a domain-agnostic probabilistic manifold representing anatomical regularities, enabling disentangled and interpretable predictions.

Result: Achieves state-of-the-art performance in both source-accessible and source-free settings, with strong interpretability demonstrated via manifold traversal.

Conclusion: The framework provides a semantically grounded, adaptable solution for domain adaptation in medical image segmentation, bridging the gap between settings and improving interpretability.

Abstract: Most prior unsupervised domain adaptation approaches for medical image
segmentation are narrowly tailored to either the source-accessible setting,
where adaptation is guided by source-target alignment, or the source-free
setting, which typically resorts to implicit supervision mechanisms such as
pseudo-labeling and model distillation. This substantial divergence in
methodological designs between the two settings reveals an inherent flaw: the
lack of an explicit, structured construction of anatomical knowledge that
naturally generalizes across domains and settings. To bridge this longstanding
divide, we introduce a unified, semantically grounded framework that supports
both source-accessible and source-free adaptation. Fundamentally distinct from
all prior works, our framework's adaptability emerges naturally as a direct
consequence of the model architecture, without the need for any handcrafted
adaptation strategies. Specifically, our model learns a domain-agnostic
probabilistic manifold as a global space of anatomical regularities, mirroring
how humans establish visual understanding. Thus, the structural content in each
image can be interpreted as a canonical anatomy retrieved from the manifold and
a spatial transformation capturing individual-specific geometry. This
disentangled, interpretable formulation enables semantically meaningful
prediction with intrinsic adaptability. Extensive experiments on challenging
cardiac and abdominal datasets show that our framework achieves
state-of-the-art results in both settings, with source-free performance closely
approaching its source-accessible counterpart, a level of consistency rarely
observed in prior works. Beyond quantitative improvement, we demonstrate strong
interpretability of the proposed framework via manifold traversal for smooth
shape manipulation.

</details>


### [26] [Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization](https://arxiv.org/abs/2508.08667)
*Ke Liu,Xuanhan Wang,Qilong Zhang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: HiWL is a two-stage deep watermarking method that improves invisibility, robustness, and broad applicability, outperforming existing methods with higher accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: Existing watermarking methods struggle to balance invisibility, robustness, and broad applicability simultaneously.

Method: HiWL uses hierarchical learning: stage 1 aligns distributions for visual consistency and information invariance; stage 2 disentangles watermarks from image content in RGB space.

Result: HiWL achieves 7.6% higher watermark extraction accuracy and processes 100K images in 8s.

Conclusion: HiWL effectively addresses the limitations of existing methods, offering a generalizable and efficient solution for image watermarking.

Abstract: Deep image watermarking, which refers to enable imperceptible watermark
embedding and reliable extraction in cover images, has shown to be effective
for copyright protection of image assets. However, existing methods face
limitations in simultaneously satisfying three essential criteria for
generalizable watermarking: 1) invisibility (imperceptible hide of watermarks),
2) robustness (reliable watermark recovery under diverse conditions), and 3)
broad applicability (low latency in watermarking process). To address these
limitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stage
optimization that enable a watermarking model to simultaneously achieve three
criteria. In the first stage, distribution alignment learning is designed to
establish a common latent space with two constraints: 1) visual consistency
between watermarked and non-watermarked images, and 2) information invariance
across watermark latent representations. In this way, multi-modal inputs
including watermark message (binary codes) and cover images (RGB pixels) can be
well represented, ensuring the invisibility of watermarks and robustness in
watermarking process thereby. The second stage employs generalized watermark
representation learning to establish a disentanglement policy for separating
watermarks from image content in RGB space. In particular, it strongly
penalizes substantial fluctuations in separated RGB watermarks corresponding to
identical messages. Consequently, HiWL effectively learns generalizable
latent-space watermark representations while maintaining broad applicability.
Extensive experiments demonstrate the effectiveness of proposed method. In
particular, it achieves 7.6\% higher accuracy in watermark extraction than
existing methods, while maintaining extremely low latency (100K images
processed in 8s).

</details>


### [27] [MMIF-AMIN: Adaptive Loss-Driven Multi-Scale Invertible Dense Network for Multimodal Medical Image Fusion](https://arxiv.org/abs/2508.08679)
*Tao Luo,Weihua Xu*

Main category: cs.CV

TL;DR: A novel method, MMIF-AMIN, is proposed for multimodal medical image fusion, using an Invertible Dense Network and Multi-scale Complementary Feature Extraction Module to enhance feature extraction and fusion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To integrate images from different modalities for improved medical diagnosis by capturing unique and complementary information, addressing limitations in current fusion techniques.

Method: Uses an Invertible Dense Network (IDN) for lossless feature extraction and a Multi-scale Complementary Feature Extraction Module (MCFEM) with hybrid attention, convolutional layers, and Transformers. An adaptive loss function guides learning.

Result: MMIF-AMIN outperforms nine state-of-the-art methods in quantitative and qualitative analyses, with ablation studies confirming component effectiveness.

Conclusion: The proposed method effectively addresses MMIF challenges, showing superior performance and potential for extension to other fusion tasks.

Abstract: Multimodal medical image fusion (MMIF) aims to integrate images from
different modalities to produce a comprehensive image that enhances medical
diagnosis by accurately depicting organ structures, tissue textures, and
metabolic information. Capturing both the unique and complementary information
across multiple modalities simultaneously is a key research challenge in MMIF.
To address this challenge, this paper proposes a novel image fusion method,
MMIF-AMIN, which features a new architecture that can effectively extract these
unique and complementary features. Specifically, an Invertible Dense Network
(IDN) is employed for lossless feature extraction from individual modalities.
To extract complementary information between modalities, a Multi-scale
Complementary Feature Extraction Module (MCFEM) is designed, which incorporates
a hybrid attention mechanism, convolutional layers of varying sizes, and
Transformers. An adaptive loss function is introduced to guide model learning,
addressing the limitations of traditional manually-designed loss functions and
enhancing the depth of data mining. Extensive experiments demonstrate that
MMIF-AMIN outperforms nine state-of-the-art MMIF methods, delivering superior
results in both quantitative and qualitative analyses. Ablation experiments
confirm the effectiveness of each component of the proposed method.
Additionally, extending MMIF-AMIN to other image fusion tasks also achieves
promising performance.

</details>


### [28] [PADReg: Physics-Aware Deformable Registration Guided by Contact Force for Ultrasound Sequences](https://arxiv.org/abs/2508.08685)
*Yimeng Geng,Mingyang Zhao,Fan Xu,Guanglin Cao,Gaofeng Meng,Hongbin Liu*

Main category: cs.CV

TL;DR: PADReg, a physics-aware deformable registration framework, improves ultrasound image alignment by integrating contact force data, outperforming existing methods by 21.34%.


<details>
  <summary>Details</summary>
Motivation: Ultrasound deformable registration is challenging due to low contrast, noise, and ambiguous boundaries, leading to poor alignment and lack of physical interpretability.

Method: PADReg uses synchronized contact force data to construct a stiffness map, combining it with force data to estimate deformation fields via a physics-aware module inspired by Hooke's law.

Result: PADReg achieves a HD95 of 12.90, 21.34% better than state-of-the-art methods.

Conclusion: PADReg enhances registration accuracy and anatomical alignment by incorporating physical priors, offering a robust solution for ultrasound deformable registration.

Abstract: Ultrasound deformable registration estimates spatial transformations between
pairs of deformed ultrasound images, which is crucial for capturing
biomechanical properties and enhancing diagnostic accuracy in diseases such as
thyroid nodules and breast cancer. However, ultrasound deformable registration
remains highly challenging, especially under large deformation. The inherently
low contrast, heavy noise and ambiguous tissue boundaries in ultrasound images
severely hinder reliable feature extraction and correspondence matching.
Existing methods often suffer from poor anatomical alignment and lack physical
interpretability. To address the problem, we propose PADReg, a physics-aware
deformable registration framework guided by contact force. PADReg leverages
synchronized contact force measured by robotic ultrasound systems as a physical
prior to constrain the registration. Specifically, instead of directly
predicting deformation fields, we first construct a pixel-wise stiffness map
utilizing the multi-modal information from contact force and ultrasound images.
The stiffness map is then combined with force data to estimate a dense
deformation field, through a lightweight physics-aware module inspired by
Hooke's law. This design enables PADReg to achieve physically plausible
registration with better anatomical alignment than previous methods relying
solely on image similarity. Experiments on in-vivo datasets demonstrate that it
attains a HD95 of 12.90, which is 21.34\% better than state-of-the-art methods.
The source code is available at https://github.com/evelynskip/PADReg.

</details>


### [29] [ROD: RGB-Only Fast and Efficient Off-road Freespace Detection](https://arxiv.org/abs/2508.08697)
*Tong Sun,Hongliang Ye,Jilin Mei,Liang Chen,Fangzhou Zhao,Leiqiang Zong,Yu Hu*

Main category: cs.CV

TL;DR: A novel RGB-only method (ROD) for off-road freespace detection eliminates LiDAR reliance, using a Vision Transformer and lightweight decoder for high precision and speed (50 FPS).


<details>
  <summary>Details</summary>
Motivation: Off-road freespace detection is challenging due to blurred boundaries. LiDAR-based methods are slow, making them unsuitable for real-time applications.

Method: ROD uses a pre-trained Vision Transformer (ViT) for RGB feature extraction and a lightweight decoder for efficiency.

Result: ROD achieves state-of-the-art performance on ORFD and RELLIS-3D datasets with 50 FPS, outperforming prior models.

Conclusion: The RGB-only ROD method is efficient and effective for real-time off-road freespace detection, eliminating LiDAR dependency.

Abstract: Off-road freespace detection is more challenging than on-road scenarios
because of the blurred boundaries of traversable areas. Previous
state-of-the-art (SOTA) methods employ multi-modal fusion of RGB images and
LiDAR data. However, due to the significant increase in inference time when
calculating surface normal maps from LiDAR data, multi-modal methods are not
suitable for real-time applications, particularly in real-world scenarios where
higher FPS is required compared to slow navigation. This paper presents a novel
RGB-only approach for off-road freespace detection, named ROD, eliminating the
reliance on LiDAR data and its computational demands. Specifically, we utilize
a pre-trained Vision Transformer (ViT) to extract rich features from RGB
images. Additionally, we design a lightweight yet efficient decoder, which
together improve both precision and inference speed. ROD establishes a new SOTA
on ORFD and RELLIS-3D datasets, as well as an inference speed of 50 FPS,
significantly outperforming prior models.

</details>


### [30] [Subjective and Objective Quality Assessment of Banding Artifacts on Compressed Videos](https://arxiv.org/abs/2508.08700)
*Qi Zheng,Li-Heng Chen,Chenlong He,Neil Berkbeck,Yilin Wang,Balu Adsumilli,Alan C. Bovik,Yibo Fan,Zhengzhong Tu*

Main category: cs.CV

TL;DR: The paper addresses banding artifacts in compressed videos, introduces a new dataset (LIVE-YT-Banding), and proposes CBAND, an efficient no-reference quality evaluator that outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: Banding artifacts degrade video quality, especially in high-definition videos, but existing datasets lack temporal dynamics. A systematic study and new tools are needed.

Method: Created the LIVE-YT-Banding dataset with 160 AV1-compressed videos and 7,200 subjective opinions. Proposed CBAND, a no-reference evaluator using deep neural network embeddings.

Result: CBAND outperforms state-of-the-art models in banding prediction and is faster. It can also serve as a loss function for debanding models.

Conclusion: The study provides a valuable dataset and an effective tool (CBAND) for assessing and improving video quality by addressing banding artifacts.

Abstract: Although there have been notable advancements in video compression
technologies in recent years, banding artifacts remain a serious issue
affecting the quality of compressed videos, particularly on smooth regions of
high-definition videos. Noticeable banding artifacts can severely impact the
perceptual quality of videos viewed on a high-end HDTV or high-resolution
screen. Hence, there is a pressing need for a systematic investigation of the
banding video quality assessment problem for advanced video codecs. Given that
the existing publicly available datasets for studying banding artifacts are
limited to still picture data only, which cannot account for temporal banding
dynamics, we have created a first-of-a-kind open video dataset, dubbed
LIVE-YT-Banding, which consists of 160 videos generated by four different
compression parameters using the AV1 video codec. A total of 7,200 subjective
opinions are collected from a cohort of 45 human subjects. To demonstrate the
value of this new resources, we tested and compared a variety of models that
detect banding occurrences, and measure their impact on perceived quality.
Among these, we introduce an effective and efficient new no-reference (NR)
video quality evaluator which we call CBAND. CBAND leverages the properties of
the learned statistics of natural images expressed in the embeddings of deep
neural networks. Our experimental results show that the perceptual banding
prediction performance of CBAND significantly exceeds that of previous
state-of-the-art models, and is also orders of magnitude faster. Moreover,
CBAND can be employed as a differentiable loss function to optimize video
debanding models. The LIVE-YT-Banding database, code, and pre-trained model are
all publically available at https://github.com/uniqzheng/CBAND.

</details>


### [31] [SafeFix: Targeted Model Repair via Controlled Image Generation](https://arxiv.org/abs/2508.08701)
*Ouyang Xu,Baoming Zhang,Ruiyu Mao,Yunhui Guo*

Main category: cs.CV

TL;DR: A method to repair deep learning models by generating semantically faithful synthetic images for rare failure cases, filtered by a large vision-language model, improves robustness without introducing new errors.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail on underrepresented semantic subpopulations, and existing repair methods struggle with distribution shift and semantic errors.

Method: Uses a conditional text-to-image model to generate targeted images for failure cases, filtered by a large vision-language model to ensure quality and relevance.

Result: Significantly reduces errors associated with rare cases and improves model robustness.

Conclusion: The proposed repair strategy effectively addresses rare-case failures without introducing new bugs.

Abstract: Deep learning models for visual recognition often exhibit systematic errors
due to underrepresented semantic subpopulations. Although existing debugging
frameworks can pinpoint these failures by identifying key failure attributes,
repairing the model effectively remains difficult. Current solutions often rely
on manually designed prompts to generate synthetic training images -- an
approach prone to distribution shift and semantic errors. To overcome these
challenges, we introduce a model repair module that builds on an interpretable
failure attribution pipeline. Our approach uses a conditional text-to-image
model to generate semantically faithful and targeted images for failure cases.
To preserve the quality and relevance of the generated samples, we further
employ a large vision-language model (LVLM) to filter the outputs, enforcing
alignment with the original data distribution and maintaining semantic
consistency. By retraining vision models with this rare-case-augmented
synthetic dataset, we significantly reduce errors associated with rare cases.
Our experiments demonstrate that this targeted repair strategy improves model
robustness without introducing new bugs. Code is available at
https://github.com/oxu2/SafeFix

</details>


### [32] [Adaptive Confidence-Wise Loss for Improved Lens Structure Segmentation in AS-OCT](https://arxiv.org/abs/2508.08705)
*Zunjie Xiao,Xiao Wu,Tianhang Liu,Lingxi Hu,Yinling Zhang,Xiaoqing Zhang,Risa Higashita,Jiang Liu*

Main category: cs.CV

TL;DR: The paper introduces an Adaptive Confidence-Wise (ACW) loss for lens structure segmentation, addressing inhomogeneous sub-regions and boundary calibration errors, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep segmentation networks treat all pixels equally, ignoring inhomogeneous sub-regions and boundary calibration errors, which are clinically significant.

Method: Proposes ACW loss to group sub-regions by confidence, applies region-weighted loss, and introduces an adaptive threshold optimization algorithm. Also proposes Boundary Expected Calibration Error (BECE) metric.

Result: ACW outperforms CE with 6.13% IoU gain, 4.33% DSC increase, and 4.79% BECE reduction in lens structure segmentation.

Conclusion: ACW effectively leverages expert annotation confidence, improving segmentation accuracy and calibration, validated on clinical datasets.

Abstract: Precise lens structure segmentation is essential for the design of
intraocular lenses (IOLs) in cataract surgery. Existing deep segmentation
networks typically weight all pixels equally under cross-entropy (CE) loss,
overlooking the fact that sub-regions of lens structures are inhomogeneous
(e.g., some regions perform better than others) and that boundary regions often
suffer from poor segmentation calibration at the pixel level. Clinically,
experts annotate different sub-regions of lens structures with varying
confidence levels, considering factors such as sub-region proportions,
ambiguous boundaries, and lens structure shapes. Motivated by this observation,
we propose an Adaptive Confidence-Wise (ACW) loss to group each lens structure
sub-region into different confidence sub-regions via a confidence threshold
from the unique region aspect, aiming to exploit the potential of expert
annotation confidence prior. Specifically, ACW clusters each target region into
low-confidence and high-confidence groups and then applies a region-weighted
loss to reweigh each confidence group. Moreover, we design an adaptive
confidence threshold optimization algorithm to adjust the confidence threshold
of ACW dynamically. Additionally, to better quantify the miscalibration errors
in boundary region segmentation, we propose a new metric, termed Boundary
Expected Calibration Error (BECE). Extensive experiments on a clinical lens
structure AS-OCT dataset and other multi-structure datasets demonstrate that
our ACW significantly outperforms competitive segmentation loss methods across
different deep segmentation networks (e.g., MedSAM). Notably, our method
surpasses CE with 6.13% IoU gain, 4.33% DSC increase, and 4.79% BECE reduction
in lens structure segmentation under U-Net. The code of this paper is available
at https://github.com/XiaoLing12138/Adaptive-Confidence-Wise-Loss.

</details>


### [33] [Bridging the Gap: A Framework for Real-World Video Deepfake Detection via Social Network Compression Emulation](https://arxiv.org/abs/2508.08765)
*Andrea Montibeller,Dasara Shullani,Daniele Baracchi,Alessandro Piva,Giulia Boato*

Main category: cs.CV

TL;DR: A framework emulates social network video compression to improve deepfake detector generalization, achieving comparable performance to real-world data.


<details>
  <summary>Details</summary>
Motivation: AI-generated videos on social networks challenge deepfake detection due to proprietary compression, creating a gap between lab-trained detectors and real-world scenarios.

Method: Proposes a framework to estimate compression and resizing parameters from uploaded videos, enabling emulation of platform-specific artifacts without API access.

Result: Emulated data matches real upload degradation; detectors fine-tuned on emulated videos perform comparably to those trained on actual shared media.

Conclusion: The framework provides a scalable solution for bridging lab-based training and real-world deployment of deepfake detectors, especially for compressed video.

Abstract: The growing presence of AI-generated videos on social networks poses new
challenges for deepfake detection, as detectors trained under controlled
conditions often fail to generalize to real-world scenarios. A key factor
behind this gap is the aggressive, proprietary compression applied by platforms
like YouTube and Facebook, which launder low-level forensic cues. However,
replicating these transformations at scale is difficult due to API limitations
and data-sharing constraints. For these reasons, we propose a first framework
that emulates the video sharing pipelines of social networks by estimating
compression and resizing parameters from a small set of uploaded videos. These
parameters enable a local emulator capable of reproducing platform-specific
artifacts on large datasets without direct API access. Experiments on
FaceForensics++ videos shared via social networks demonstrate that our emulated
data closely matches the degradation patterns of real uploads. Furthermore,
detectors fine-tuned on emulated videos achieve comparable performance to those
trained on actual shared media. Our approach offers a scalable and practical
solution for bridging the gap between lab-based training and real-world
deployment of deepfake detectors, particularly in the underexplored domain of
compressed video content.

</details>


### [34] [SHREC 2025: Retrieval of Optimal Objects for Multi-modal Enhanced Language and Spatial Assistance (ROOMELSA)](https://arxiv.org/abs/2508.08781)
*Trong-Thuan Nguyen,Viet-Tham Huynh,Quang-Thuc Nguyen,Hoang-Phuc Nguyen,Long Le Bao,Thai Hoang Minh,Minh Nguyen Anh,Thang Nguyen Tien,Phat Nguyen Thuan,Huy Nguyen Phong,Bao Huynh Thai,Vinh-Tiep Nguyen,Duc-Vu Nguyen,Phu-Hoa Pham,Minh-Huy Le-Hoang,Nguyen-Khang Le,Minh-Chinh Nguyen,Minh-Quan Ho,Ngoc-Long Tran,Hien-Long Le-Hoang,Man-Khoi Tran,Anh-Duong Tran,Kim Nguyen,Quan Nguyen Hung,Dat Phan Thanh,Hoang Tran Van,Tien Huynh Viet,Nhan Nguyen Viet Thien,Dinh-Khoi Vo,Van-Loc Nguyen,Trung-Nghia Le,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: ROOMELSA is a benchmark for evaluating 3D retrieval systems in complex, real-world scenarios using natural language queries and panoramic room images.


<details>
  <summary>Details</summary>
Motivation: Existing 3D retrieval systems are limited to simple scenarios, while real-world applications require handling cluttered scenes and vague descriptions.

Method: ROOMELSA evaluates systems by retrieving 3D models from a large database based on targeted queries in panoramic room images.

Result: Coarse retrieval is largely solved, but only one model consistently ranked correct matches first. A CLIP-based model performed well but struggled with subtle variations.

Conclusion: ROOMELSA advances robust 3D recognition by integrating visual and language understanding, setting a new benchmark for real-world applications.

Abstract: Recent 3D retrieval systems are typically designed for simple, controlled
scenarios, such as identifying an object from a cropped image or a brief
description. However, real-world scenarios are more complex, often requiring
the recognition of an object in a cluttered scene based on a vague, free-form
description. To this end, we present ROOMELSA, a new benchmark designed to
evaluate a system's ability to interpret natural language. Specifically,
ROOMELSA attends to a specific region within a panoramic room image and
accurately retrieves the corresponding 3D model from a large database. In
addition, ROOMELSA includes over 1,600 apartment scenes, nearly 5,200 rooms,
and more than 44,000 targeted queries. Empirically, while coarse object
retrieval is largely solved, only one top-performing model consistently ranked
the correct match first across nearly all test cases. Notably, a lightweight
CLIP-based model also performed well, although it struggled with subtle
variations in materials, part structures, and contextual cues, resulting in
occasional errors. These findings highlight the importance of tightly
integrating visual and language understanding. By bridging the gap between
scene-level grounding and fine-grained 3D retrieval, ROOMELSA establishes a new
benchmark for advancing robust, real-world 3D recognition systems.

</details>


### [35] [DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation](https://arxiv.org/abs/2508.08783)
*Tianyu Xiong,Dayi Tan,Wei Tian*

Main category: cs.CV

TL;DR: DiffPose-Animal is a diffusion-based framework for animal pose estimation, leveraging LLMs for semantic guidance and a diffusion decoder for robust predictions.


<details>
  <summary>Details</summary>
Motivation: Animal pose estimation is challenging due to morphological diversity, complex structures, and limited data, necessitating innovative solutions.

Method: The framework reformulates pose estimation as a denoising process, using LLMs for anatomical priors and a diffusion decoder for refinement.

Result: Experiments show effectiveness in handling diverse species, cluttered backgrounds, and incomplete keypoints.

Conclusion: DiffPose-Animal advances animal pose estimation with improved robustness and generalization.

Abstract: Animal pose estimation is a fundamental task in computer vision, with growing
importance in ecological monitoring, behavioral analysis, and intelligent
livestock management. Compared to human pose estimation, animal pose estimation
is more challenging due to high interspecies morphological diversity, complex
body structures, and limited annotated data. In this work, we introduce
DiffPose-Animal, a novel diffusion-based framework for top-down animal pose
estimation. Unlike traditional heatmap regression methods, DiffPose-Animal
reformulates pose estimation as a denoising process under the generative
framework of diffusion models. To enhance semantic guidance during keypoint
generation, we leverage large language models (LLMs) to extract both global
anatomical priors and local keypoint-wise semantics based on species-specific
prompts. These textual priors are encoded and fused with image features via
cross-attention modules to provide biologically meaningful constraints
throughout the denoising process. Additionally, a diffusion-based keypoint
decoder is designed to progressively refine pose predictions, improving
robustness to occlusion and annotation sparsity. Extensive experiments on
public animal pose datasets demonstrate the effectiveness and generalization
capability of our method, especially under challenging scenarios with diverse
species, cluttered backgrounds, and incomplete keypoints.

</details>


### [36] [Region-Adaptive Video Sharpening via Rate-Perception Optimization](https://arxiv.org/abs/2508.08794)
*Yingxue Pang,Shijie Zhao,Mengxi Guo,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: Proposes RPO-AdaSharp, a region-adaptive video sharpening model for perceptual enhancement and bitrate savings, using CTU partition masks for bit allocation.


<details>
  <summary>Details</summary>
Motivation: Uniform sharpening degrades video quality and increases bitrate without optimal bit allocation across regions.

Method: Uses coding tree unit (CTU) partition masks to guide bit allocation in an end-to-end adaptive sharpening model.

Result: Demonstrates effectiveness in benchmarks, improving quality and saving bitrate.

Conclusion: RPO-AdaSharp successfully addresses the limitations of uniform sharpening with adaptive bit allocation.

Abstract: Sharpening is a widely adopted video enhancement technique. However, uniform
sharpening intensity ignores texture variations, degrading video quality.
Sharpening also increases bitrate, and there's a lack of techniques to
optimally allocate these additional bits across diverse regions. Thus, this
paper proposes RPO-AdaSharp, an end-to-end region-adaptive video sharpening
model for both perceptual enhancement and bitrate savings. We use the coding
tree unit (CTU) partition mask as prior information to guide and constrain the
allocation of increased bits. Experiments on benchmarks demonstrate the
effectiveness of the proposed model qualitatively and quantitatively.

</details>


### [37] [MonoPartNeRF:Human Reconstruction from Monocular Video via Part-Based Neural Radiance Fields](https://arxiv.org/abs/2508.08798)
*Yao Lu,Jiawei Li,Ming Jiang*

Main category: cs.CV

TL;DR: MonoPartNeRF improves monocular dynamic human rendering by addressing pose variations and occlusion issues with bidirectional deformation, part-based pose embedding, and dynamic texture modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex pose variations and occlusions, leading to unnatural transitions and inaccurate reconstructions in monocular settings.

Method: Proposes a bidirectional deformation model, part-based pose embedding, keyframe pose retrieval, and learnable appearance codes for dynamic texture changes.

Result: Outperforms prior methods on ZJU-MoCap and MonoCap datasets, achieving better joint alignment, texture fidelity, and structural continuity.

Conclusion: MonoPartNeRF effectively handles complex poses and occlusions, offering smoother transitions and robust reconstruction.

Abstract: In recent years, Neural Radiance Fields (NeRF) have achieved remarkable
progress in dynamic human reconstruction and rendering. Part-based rendering
paradigms, guided by human segmentation, allow for flexible parameter
allocation based on structural complexity, thereby enhancing representational
efficiency. However, existing methods still struggle with complex pose
variations, often producing unnatural transitions at part boundaries and
failing to reconstruct occluded regions accurately in monocular settings. We
propose MonoPartNeRF, a novel framework for monocular dynamic human rendering
that ensures smooth transitions and robust occlusion recovery. First, we build
a bidirectional deformation model that combines rigid and non-rigid
transformations to establish a continuous, reversible mapping between
observation and canonical spaces. Sampling points are projected into a
parameterized surface-time space (u, v, t) to better capture non-rigid motion.
A consistency loss further suppresses deformation-induced artifacts and
discontinuities. We introduce a part-based pose embedding mechanism that
decomposes global pose vectors into local joint embeddings based on body
regions. This is combined with keyframe pose retrieval and interpolation, along
three orthogonal directions, to guide pose-aware feature sampling. A learnable
appearance code is integrated via attention to model dynamic texture changes
effectively. Experiments on the ZJU-MoCap and MonoCap datasets demonstrate that
our method significantly outperforms prior approaches under complex pose and
occlusion conditions, achieving superior joint alignment, texture fidelity, and
structural continuity.

</details>


### [38] [Identity-Preserving Aging and De-Aging of Faces in the StyleGAN Latent Space](https://arxiv.org/abs/2508.08808)
*Luis S. Luevano,Pavel Korshunov,Sebastien Marcel*

Main category: cs.CV

TL;DR: The paper proposes a method for aging/de-aging faces by editing StyleGAN2's latent space using SVM and feature selection, ensuring identity preservation with empirical validation.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on complex conditioning, leading to training challenges and inconsistent identity preservation.

Method: Uses SVM to model aging/de-aging directions in StyleGAN2's latent space, with feature selection and identity-preserving subspace identification.

Result: Generated a public dataset of synthetic faces at different ages, validated for identity preservation.

Conclusion: Offers a simpler, practical approach for age synthesis with guaranteed identity preservation, useful for benchmarking.

Abstract: Face aging or de-aging with generative AI has gained significant attention
for its applications in such fields like forensics, security, and media.
However, most state of the art methods rely on conditional Generative
Adversarial Networks (GANs), Diffusion-based models, or Visual Language Models
(VLMs) to age or de-age faces based on predefined age categories and
conditioning via loss functions, fine-tuning, or text prompts. The reliance on
such conditioning leads to complex training requirements, increased data needs,
and challenges in generating consistent results. Additionally, identity
preservation is rarely taken into accountor evaluated on a single face
recognition system without any control or guarantees on whether identity would
be preserved in a generated aged/de-aged face. In this paper, we propose to
synthesize aged and de-aged faces via editing latent space of StyleGAN2 using a
simple support vector modeling of aging/de-aging direction and several feature
selection approaches. By using two state-of-the-art face recognition systems,
we empirically find the identity preserving subspace within the StyleGAN2
latent space, so that an apparent age of a given face can changed while
preserving the identity. We then propose a simple yet practical formula for
estimating the limits on aging/de-aging parameters that ensures identity
preservation for a given input face. Using our method and estimated parameters
we have generated a public dataset of synthetic faces at different ages that
can be used for benchmarking cross-age face recognition, age assurance systems,
or systems for detection of synthetic images. Our code and dataset are
available at the project page https://www.idiap.ch/paper/agesynth/

</details>


### [39] [Revisiting Efficient Semantic Segmentation: Learning Offsets for Better Spatial and Class Feature Alignment](https://arxiv.org/abs/2508.08811)
*Shi-Chen Zhang,Yunheng Li,Yu-Huan Wu,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: Proposes a dual-branch offset learning paradigm (OffSeg) to address misalignment in semantic segmentation, improving efficiency without extra architecture changes.


<details>
  <summary>Details</summary>
Motivation: Existing lightweight semantic segmentation methods suffer from misalignment between class representations and image features due to per-pixel classification.

Method: Introduces a coupled dual-branch offset learning paradigm to dynamically refine class representations and spatial features.

Result: Improves mIoU by 1.9-2.7% on datasets like ADE20K with minimal additional parameters (0.1-0.2M).

Conclusion: The offset learning paradigm effectively enhances semantic segmentation efficiency and can be integrated into existing methods.

Abstract: Semantic segmentation is fundamental to vision systems requiring pixel-level
scene understanding, yet deploying it on resource-constrained devices demands
efficient architectures. Although existing methods achieve real-time inference
through lightweight designs, we reveal their inherent limitation: misalignment
between class representations and image features caused by a per-pixel
classification paradigm. With experimental analysis, we find that this paradigm
results in a highly challenging assumption for efficient scenarios: Image pixel
features should not vary for the same category in different images. To address
this dilemma, we propose a coupled dual-branch offset learning paradigm that
explicitly learns feature and class offsets to dynamically refine both class
representations and spatial image features. Based on the proposed paradigm, we
construct an efficient semantic segmentation network, OffSeg. Notably, the
offset learning paradigm can be adopted to existing methods with no additional
architectural changes. Extensive experiments on four datasets, including
ADE20K, Cityscapes, COCO-Stuff-164K, and Pascal Context, demonstrate consistent
improvements with negligible parameters. For instance, on the ADE20K dataset,
our proposed offset learning paradigm improves SegFormer-B0, SegNeXt-T, and
Mask2Former-Tiny by 2.7%, 1.9%, and 2.6% mIoU, respectively, with only 0.1-0.2M
additional parameters required.

</details>


### [40] [TARA: Token-Aware LoRA for Composable Personalization in Diffusion Models](https://arxiv.org/abs/2508.08812)
*Yuqi Peng,Lingtao Zheng,Yufeng Yang,Yi Huang,Mingfu Yan,Jianzhuang Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: The paper introduces Token-Aware LoRA (TARA) to address issues in multi-concept text-to-image generation, such as identity loss and feature leakage, by using token masks and spatial alignment.


<details>
  <summary>Details</summary>
Motivation: Current methods using LoRA for multi-concept generation suffer from identity missing and visual feature leakage due to token-wise interference and spatial misalignment.

Method: Proposes TARA, which uses token masks to avoid interference and a training objective for spatial alignment of rare tokens.

Result: TARA enables efficient multi-concept inference while preserving visual identities by preventing module interference.

Conclusion: TARA effectively addresses the limitations of LoRA in multi-concept generation, offering a training-free solution for better identity preservation.

Abstract: Personalized text-to-image generation aims to synthesize novel images of a
specific subject or style using only a few reference images. Recent methods
based on Low-Rank Adaptation (LoRA) enable efficient single-concept
customization by injecting lightweight, concept-specific adapters into
pre-trained diffusion models. However, combining multiple LoRA modules for
multi-concept generation often leads to identity missing and visual feature
leakage. In this work, we identify two key issues behind these failures: (1)
token-wise interference among different LoRA modules, and (2) spatial
misalignment between the attention map of a rare token and its corresponding
concept-specific region. To address these issues, we propose Token-Aware LoRA
(TARA), which introduces a token mask to explicitly constrain each module to
focus on its associated rare token to avoid interference, and a training
objective that encourages the spatial attention of a rare token to align with
its concept region. Our method enables training-free multi-concept composition
by directly injecting multiple independently trained TARA modules at inference
time. Experimental results demonstrate that TARA enables efficient
multi-concept inference and effectively preserving the visual identity of each
concept by avoiding mutual interference between LoRA modules. The code and
models are available at https://github.com/YuqiPeng77/TARA.

</details>


### [41] [3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs](https://arxiv.org/abs/2508.08821)
*Noor Ahmed,Cameron Braunstein,Steffen Eger,Eddy Ilg*

Main category: cs.CV

TL;DR: 3DFroMLLM is a framework for generating 3D object prototypes from MLLMs, improving spatial reasoning and outperforming prior methods by 15% in image classification tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of spatial reasoning in Multi-Modal Large Language Models (MLLMs) by enabling 3D object prototype generation.

Method: Uses an agentic pipeline (designer, coder, visual inspector) in a refinement loop, requiring no extra training data or detailed user instructions.

Result: Outperforms previous methods by 15% in image classification tasks and improves CLIP fine-tuning for part segmentation by 55% accuracy.

Conclusion: 3DFroMLLM effectively enhances MLLMs' spatial reasoning and practical applications like part segmentation without additional human-labeled data.

Abstract: Recent Multi-Modal Large Language Models (MLLMs) have demonstrated strong
capabilities in learning joint representations from text and images. However,
their spatial reasoning remains limited. We introduce 3DFroMLLM, a novel
framework that enables the generation of 3D object prototypes directly from
MLLMs, including geometry and part labels. Our pipeline is agentic, comprising
a designer, coder, and visual inspector operating in a refinement loop.
Notably, our approach requires no additional training data or detailed user
instructions. Building on prior work in 2D generation, we demonstrate that
rendered images produced by our framework can be effectively used for image
classification pretraining tasks and outperforms previous methods by 15%. As a
compelling real-world use case, we show that the generated prototypes can be
leveraged to improve fine-grained vision-language models by using the rendered,
part-labeled prototypes to fine-tune CLIP for part segmentation and achieving a
55% accuracy improvement without relying on any additional human-labeled data.

</details>


### [42] [A Parametric Bi-Directional Curvature-Based Framework for Image Artifact Classification and Quantification](https://arxiv.org/abs/2508.08824)
*Diego Frias*

Main category: cs.CV

TL;DR: A novel NR-IQA framework using Anisotropic Texture Richness (ATR) achieves high accuracy in classifying and quantifying image distortions like blur and noise.


<details>
  <summary>Details</summary>
Motivation: To improve No-Reference Image Quality Assessment by leveraging directional image curvature and ATR for accurate artifact classification and quality prediction.

Method: A two-stage system: (1) Classifies artifacts using ATR signatures, (2) Maps ATR scores to quality ratings via regression.

Result: Achieves 97% artifact classification accuracy and high predictive performance (R2=0.892, RMSE=5.17 DMOS).

Conclusion: The framework is a robust tool for dual-purpose image degradation analysis.

Abstract: This work presents a novel framework for No-Reference Image Quality
Assessment (NR-IQA) founded on the analysis of directional image curvature.
Within this framework, we define a measure of Anisotropic Texture Richness
(ATR), which is computed at the pixel level using two tunable thresholds -- one
permissive and one restrictive -- that quantify orthogonal texture suppression.
When its parameters are optimized for a specific artifact, the resulting ATR
score serves as a high-performance quality metric, achieving Spearman
correlations with human perception of approximately -0.93 for Gaussian blur and
-0.95 for white noise on the LIVE dataset. The primary contribution is a
two-stage system that leverages the differential response of ATR to various
distortions. First, the system utilizes the signature from two specialist ATR
configurations to classify the primary artifact type (blur vs. noise) with over
97% accuracy. Second, following classification, it employs a dedicated
regression model mapping the relevant ATR score to a quality rating to quantify
the degradation. On a combined dataset, the complete system predicts human
scores with a coefficient of determination (R2) of 0.892 and a Root Mean Square
Error (RMSE) of 5.17 DMOS points. This error corresponds to just 7.4% of the
dataset's total quality range, demonstrating high predictive accuracy. This
establishes our framework as a robust, dual-purpose tool for the classification
and subsequent quantification of image degradation.

</details>


### [43] [Adaptive High-Frequency Preprocessing for Video Coding](https://arxiv.org/abs/2508.08849)
*Yingxue Pang,Shijie Zhao,Junlin Li,Li Zhang*

Main category: cs.CV

TL;DR: A learning-based framework (FFPN) optimizes high-frequency preprocessing in video coding to balance quality and bitrate, validated by improved RD performance.


<details>
  <summary>Details</summary>
Motivation: High-frequency components impact video clarity and bitrate, necessitating a solution to enhance quality while reducing bandwidth/storage costs.

Method: Uses FFPN to predict optimal preprocessing strategies, trained with pseudo-labels based on RD performance and quality metrics.

Result: Demonstrates visually enhanced quality and bitrate savings across multiple datasets.

Conclusion: The framework effectively balances video quality and compression efficiency.

Abstract: High-frequency components are crucial for maintaining video clarity and
realism, but they also significantly impact coding bitrate, resulting in
increased bandwidth and storage costs. This paper presents an end-to-end
learning-based framework for adaptive high-frequency preprocessing to enhance
subjective quality and save bitrate in video coding. The framework employs the
Frequency-attentive Feature pyramid Prediction Network (FFPN) to predict the
optimal high-frequency preprocessing strategy, guiding subsequent filtering
operators to achieve the optimal tradeoff between bitrate and quality after
compression. For training FFPN, we pseudo-label each training video with the
optimal strategy, determined by comparing the rate-distortion (RD) performance
across different preprocessing types and strengths. Distortion is measured
using the latest quality assessment metric. Comprehensive evaluations on
multiple datasets demonstrate the visually appealing enhancement capabilities
and bitrate savings achieved by our framework.

</details>


### [44] [GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments](https://arxiv.org/abs/2508.08867)
*Lin Zeng,Boming Zhao,Jiarui Hu,Xujie Shen,Ziqiang Dang,Hujun Bao,Zhaopeng Cui*

Main category: cs.CV

TL;DR: GaussianUpdate combines 3D Gaussian representation with continual learning for efficient novel view synthesis, adapting to scene changes without extensive retraining.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting neural models to scene changes are either labor-intensive or fail to capture detailed changes.

Method: Uses 3D Gaussian representation and continual learning with a multi-stage update strategy and visibility-aware generative replay.

Result: Achieves superior, real-time rendering and visualizes changes over time on benchmark datasets.

Conclusion: GaussianUpdate effectively updates scenes while preserving past information, addressing limitations of existing methods.

Abstract: Novel view synthesis with neural models has advanced rapidly in recent years,
yet adapting these models to scene changes remains an open problem. Existing
methods are either labor-intensive, requiring extensive model retraining, or
fail to capture detailed types of changes over time. In this paper, we present
GaussianUpdate, a novel approach that combines 3D Gaussian representation with
continual learning to address these challenges. Our method effectively updates
the Gaussian radiance fields with current data while preserving information
from past scenes. Unlike existing methods, GaussianUpdate explicitly models
different types of changes through a novel multi-stage update strategy.
Additionally, we introduce a visibility-aware continual learning approach with
generative replay, enabling self-aware updating without the need to store
images. The experiments on the benchmark dataset demonstrate our method
achieves superior and real-time rendering with the capability of visualizing
changes over different times

</details>


### [45] [Preview WB-DH: Towards Whole Body Digital Human Bench for the Generation of Whole-body Talking Avatar Videos](https://arxiv.org/abs/2508.08891)
*Chaoyi Wang,Yifan Yang,Jun Pei,Lijie Xia,Jianpo Liu,Xiaobing Yuan,Xinhan Di*

Main category: cs.CV

TL;DR: A new benchmark dataset (WB-DH) is introduced to evaluate whole-body animatable avatar generation, addressing gaps in current datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: Current datasets and metrics lack the ability to capture subtle expressions, body movements, and dynamic backgrounds in avatar generation.

Method: The WB-DH dataset includes multi-modal annotations and a versatile evaluation framework.

Result: The dataset is open-source and publicly available, providing tools for fine-grained guidance.

Conclusion: WB-DH bridges the gap in evaluating realistic, animatable whole-body avatars from single portraits.

Abstract: Creating realistic, fully animatable whole-body avatars from a single
portrait is challenging due to limitations in capturing subtle expressions,
body movements, and dynamic backgrounds. Current evaluation datasets and
metrics fall short in addressing these complexities. To bridge this gap, we
introduce the Whole-Body Benchmark Dataset (WB-DH), an open-source, multi-modal
benchmark designed for evaluating whole-body animatable avatar generation. Key
features include: (1) detailed multi-modal annotations for fine-grained
guidance, (2) a versatile evaluation framework, and (3) public access to the
dataset and tools at https://github.com/deepreasonings/WholeBodyBenchmark.

</details>


### [46] [A Robust Epipolar-Domain Regularization Algorithm for Light Field Depth Estimation](https://arxiv.org/abs/2508.08900)
*Noor Islam S. Mohammad*

Main category: cs.CV

TL;DR: A lightweight depth estimation method for light field imaging combines disparity information with a random walk algorithm, offering efficiency and robustness without heavy reliance on deep learning.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for depth estimation in light field imaging are computationally expensive and struggle in noisy environments, prompting the need for a more efficient and robust solution.

Method: The proposed pipeline integrates light field-based disparity information with a directed random walk refinement algorithm, avoiding extensive training or large datasets.

Result: The method maintains low computational complexity and competitive accuracy, though performance slightly declines in uncontrolled conditions.

Conclusion: This work presents a robust and efficient alternative to deep learning for depth estimation, with potential for practical applications and future integration of probabilistic graph models.

Abstract: Robust depth estimation in light field imaging remains a critical challenge
for pattern recognition applications such as augmented reality, biomedical
imaging, and scene reconstruction. While existing approaches often rely heavily
on deep convolutional neural networks, they tend to incur high computational
costs and struggle in noisy real-world environments. This paper proposes a
novel lightweight depth estimation pipeline that integrates light field-based
disparity information with a directed random walk refinement algorithm. Unlike
traditional CNN-based methods, our approach enhances depth map consistency
without requiring extensive training or large-scale datasets. The proposed
method was evaluated on the 4D Light Field Benchmark dataset and a diverse set
of real-world images. Experimental results indicate that while performance
slightly declines under uncontrolled conditions, the algorithm consistently
maintains low computational complexity and competitive accuracy compared to
state-of-the-art deep learning models. These findings highlight the potential
of our method as a robust and efficient alternative for depth estimation and
segmentation in light field imaging. The work provides insights into practical
algorithm design for light field-based pattern recognition and opens new
directions for integrating probabilistic graph models with depth sensing
frameworks.

</details>


### [47] [Masked Clustering Prediction for Unsupervised Point Cloud Pre-training](https://arxiv.org/abs/2508.08910)
*Bin Ren,Xiaoshui Huang,Mengyuan Liu,Hong Liu,Fabio Poiesi,Nicu Sebe,Guofeng Mei*

Main category: cs.CV

TL;DR: MaskClu is an unsupervised pre-training method for ViTs on 3D point clouds, combining masked point modeling with clustering-based learning and global contrastive learning to enhance semantic feature extraction.


<details>
  <summary>Details</summary>
Motivation: Standard ViTs struggle to learn dense and informative semantic features from 3D point clouds, prompting the need for a more effective pre-training method.

Method: MaskClu integrates masked point modeling with clustering-based learning, reconstructing cluster assignments and centers, and employs global contrastive learning for instance-level feature enhancement.

Result: MaskClu achieves competitive results in 3D tasks like part segmentation, semantic segmentation, object detection, and classification.

Conclusion: MaskClu successfully learns richer and more semantically meaningful representations from 3D point clouds, outperforming existing methods.

Abstract: Vision transformers (ViTs) have recently been widely applied to 3D point
cloud understanding, with masked autoencoding as the predominant pre-training
paradigm. However, the challenge of learning dense and informative semantic
features from point clouds via standard ViTs remains underexplored. We propose
MaskClu, a novel unsupervised pre-training method for ViTs on 3D point clouds
that integrates masked point modeling with clustering-based learning. MaskClu
is designed to reconstruct both cluster assignments and cluster centers from
masked point clouds, thus encouraging the model to capture dense semantic
information. Additionally, we introduce a global contrastive learning mechanism
that enhances instance-level feature learning by contrasting different masked
views of the same point cloud. By jointly optimizing these complementary
objectives, i.e., dense semantic reconstruction, and instance-level contrastive
learning. MaskClu enables ViTs to learn richer and more semantically meaningful
representations from 3D point clouds. We validate the effectiveness of our
method via multiple 3D tasks, including part segmentation, semantic
segmentation, object detection, and classification, where MaskClu sets new
competitive results. The code and models will be released
at:https://github.com/Amazingren/maskclu.

</details>


### [48] [Automatic and standardized surgical reporting for central nervous system tumors](https://arxiv.org/abs/2508.08916)
*David Bouget,Mathilde Gajda Faanes,Asgeir Store Jakola,Frederik Barkhof,Hilko Ardon,Lorenzo Bello,Mitchel S. Berger,Shawn L. Hervey-Jumper,Julia Furtner,Albert J. S. Idema,Barbara Kiesel,Georg Widhalm,Rishi Nandoe Tewarie,Emmanuel Mandonnet,Pierre A. Robe,Michiel Wagemakers,Timothy R. Smith,Philip C. De Witt Hamer,Ole solheim,Ingerid Reinertsen*

Main category: cs.CV

TL;DR: The study introduces a pipeline for automated postoperative CNS tumor analysis using Attention U-Net and DenseNet models, achieving high accuracy in segmentation and classification, integrated into Raidionics software.


<details>
  <summary>Details</summary>
Motivation: Postoperative imaging analysis for CNS tumors is understudied compared to preoperative data, necessitating standardized tools for improved clinical decision-making.

Method: Attention U-Net for segmentation (tumor core, residual tumor, resection cavity) and DenseNet for MR sequence and tumor type classification, trained on multicentric datasets with 5-fold cross-validation.

Result: Segmentation Dice scores: 87% (tumor core), 66% (non-enhancing core), 70% (residual tumor), 77% (resection cavity). Classification: 99.5% (MR sequence), 80% (tumor type).

Conclusion: The pipeline enhances postoperative evaluation, integrates with RANO 2.0 guidelines, and is available in Raidionics for clinical use.

Abstract: Magnetic resonance (MR) imaging is essential for evaluating central nervous
system (CNS) tumors, guiding surgical planning, treatment decisions, and
assessing postoperative outcomes and complication risks. While recent work has
advanced automated tumor segmentation and report generation, most efforts have
focused on preoperative data, with limited attention to postoperative imaging
analysis. This study introduces a comprehensive pipeline for standardized
postsurtical reporting in CNS tumors. Using the Attention U-Net architecture,
segmentation models were trained for the preoperative (non-enhancing) tumor
core, postoperative contrast-enhancing residual tumor, and resection cavity.
Additionally, MR sequence classification and tumor type identification for
contrast-enhancing lesions were explored using the DenseNet architecture. The
models were integrated into a reporting pipeline, following the RANO 2.0
guidelines. Training was conducted on multicentric datasets comprising 2000 to
7000 patients, using a 5-fold cross-validation. Evaluation included patient-,
voxel-, and object-wise metrics, with benchmarking against the latest BraTS
challenge results. The segmentation models achieved average voxel-wise Dice
scores of 87%, 66%, 70%, and 77% for the tumor core, non-enhancing tumor core,
contrast-enhancing residual tumor, and resection cavity, respectively.
Classification models reached 99.5% balanced accuracy in MR sequence
classification and 80% in tumor type classification. The pipeline presented in
this study enables robust, automated segmentation, MR sequence classification,
and standardized report generation aligned with RANO 2.0 guidelines, enhancing
postoperative evaluation and clinical decision-making. The proposed models and
methods were integrated into Raidionics, open-source software platform for CNS
tumor analysis, now including a dedicated module for postsurgical analysis.

</details>


### [49] [A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition](https://arxiv.org/abs/2508.08917)
*Jintao Cheng,Jiehao Luo,Xieyuanli Chen,Jin Wu,Rui Fan,Xiaoyu Tang,Wei Zhang*

Main category: cs.CV

TL;DR: A novel cross-view network for LiDAR-based Place Recognition (LPR) improves performance by addressing limitations of Euclidean-centric methods through a pseudo-global information guidance mechanism and a Manifold Adaptation and Pairwise Variance-Locality Learning Metric.


<details>
  <summary>Details</summary>
Motivation: Existing LPR methods rely on Euclidean distance-based metric learning, which fails to capture nonlinear data distributions and intrinsic feature structures, leading to suboptimal performance in complex environments.

Method: Proposes a cross-view network with a pseudo-global information guidance mechanism and a Manifold Adaptation and Pairwise Variance-Locality Learning Metric, using a Symmetric Positive Definite (SPD) matrix for Mahalanobis distance computation.

Result: The proposed algorithm achieves competitive performance, especially in complex environmental conditions.

Conclusion: The framework effectively addresses the limitations of Euclidean-centric methods, enhancing LPR performance in GPS-denied and dynamic environments.

Abstract: LiDAR-based Place Recognition (LPR) remains a critical task in Embodied
Artificial Intelligence (AI) and Autonomous Driving, primarily addressing
localization challenges in GPS-denied environments and supporting loop closure
detection. Existing approaches reduce place recognition to a Euclidean
distance-based metric learning task, neglecting the feature space's intrinsic
structures and intra-class variances. Such Euclidean-centric formulation
inherently limits the model's capacity to capture nonlinear data distributions,
leading to suboptimal performance in complex environments and temporal-varying
scenarios. To address these challenges, we propose a novel cross-view network
based on an innovative fusion paradigm. Our framework introduces a
pseudo-global information guidance mechanism that coordinates multi-modal
branches to perform feature learning within a unified semantic space.
Concurrently, we propose a Manifold Adaptation and Pairwise Variance-Locality
Learning Metric that constructs a Symmetric Positive Definite (SPD) matrix to
compute Mahalanobis distance, superseding traditional Euclidean distance
metrics. This geometric formulation enables the model to accurately
characterize intrinsic data distributions and capture complex inter-class
dependencies within the feature space. Experimental results demonstrate that
the proposed algorithm achieves competitive performance, particularly excelling
in complex environmental conditions.

</details>


### [50] [Shape Completion and Real-Time Visualization in Robotic Ultrasound Spine Acquisitions](https://arxiv.org/abs/2508.08923)
*Miruna-Alexandra Gafencu,Reem Shaban,Yordanka Velikova,Mohammad Farid Azampour,Nassir Navab*

Main category: cs.CV

TL;DR: A novel robotic ultrasound system with real-time shape completion improves spinal visualization by addressing shadowing artifacts and registration limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Ultrasound imaging in spinal procedures is hindered by shadowing artifacts and the limitations of CT-to-US registration, such as complex requirements and spine curvature differences.

Method: The system combines robotic ultrasound with deep learning-based shape completion to autonomously acquire US sweeps, extract vertebral surfaces, and reconstruct anatomy in real-time.

Result: Quantitative experiments show accurate shape completion, and qualitative results demonstrate effective visualization on a phantom and volunteer.

Conclusion: The integrated system enhances consistency, reproducibility, and anatomical understanding in spinal ultrasound procedures.

Abstract: Ultrasound (US) imaging is increasingly used in spinal procedures due to its
real-time, radiation-free capabilities; however, its effectiveness is hindered
by shadowing artifacts that obscure deeper tissue structures. Traditional
approaches, such as CT-to-US registration, incorporate anatomical information
from preoperative CT scans to guide interventions, but they are limited by
complex registration requirements, differences in spine curvature, and the need
for recent CT imaging. Recent shape completion methods can offer an alternative
by reconstructing spinal structures in US data, while being pretrained on large
set of publicly available CT scans. However, these approaches are typically
offline and have limited reproducibility. In this work, we introduce a novel
integrated system that combines robotic ultrasound with real-time shape
completion to enhance spinal visualization. Our robotic platform autonomously
acquires US sweeps of the lumbar spine, extracts vertebral surfaces from
ultrasound, and reconstructs the complete anatomy using a deep learning-based
shape completion network. This framework provides interactive, real-time
visualization with the capability to autonomously repeat scans and can enable
navigation to target locations. This can contribute to better consistency,
reproducibility, and understanding of the underlying anatomy. We validate our
approach through quantitative experiments assessing shape completion accuracy
and evaluations of multiple spine acquisition protocols on a phantom setup.
Additionally, we present qualitative results of the visualization on a
volunteer scan.

</details>


### [51] [Accelerated Volumetric Compression without Hierarchies: A Fourier Feature Based Implicit Neural Representation Approach](https://arxiv.org/abs/2508.08937)
*Leona rkov,Petr Strako,Michal Kravenko,Tom Brzobohat,Lubomr ha*

Main category: cs.CV

TL;DR: A neural compression method for volumetric data using Fourier feature encoding and selective voxel sampling, reducing training time by 63.7% with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Volumetric data compression is essential in fields like medical imaging and scientific simulation, but traditional methods often involve redundant computations and hierarchical metadata.

Method: Combines Fourier feature encoding with dynamic voxel selection (using morphological dilation) to prioritize active regions, eliminating redundant computation and hierarchical metadata.

Result: Reduced training time by 63.7% (30 to 11 minutes) with minor quality loss (PSNR drop of 0.59 dB, SSIM drop of 0.008). Achieved a compression rate of 14.

Conclusion: The method offers a scalable, structure-free solution for efficient volumetric compression, connecting coordinate-based neural representation with practical applications.

Abstract: Volumetric data compression is critical in fields like medical imaging,
scientific simulation, and entertainment. We introduce a structure-free neural
compression method combining Fourierfeature encoding with selective voxel
sampling, yielding compact volumetric representations and faster convergence.
Our dynamic voxel selection uses morphological dilation to prioritize active
regions, reducing redundant computation without any hierarchical metadata. In
the experiment, sparse training reduced training time by 63.7 % (from 30 to 11
minutes) with only minor quality loss: PSNR dropped 0.59 dB (from 32.60 to
32.01) and SSIM by 0.008 (from 0.948 to 0.940). The resulting neural
representation, stored solely as network weights, achieves a compression rate
of 14 and eliminates traditional data-loading overhead. This connects
coordinate-based neural representation with efficient volumetric compression,
offering a scalable, structure-free solution for practical applications.

</details>


### [52] [MADPromptS: Unlocking Zero-Shot Morphing Attack Detection with Multiple Prompt Aggregation](https://arxiv.org/abs/2508.08939)
*Eduarda Caldeira,Fadi Boutros,Naser Damer*

Main category: cs.CV

TL;DR: The paper explores a zero-shot approach for Face Morphing Attack Detection (MAD) using CLIP without fine-tuning, focusing on prompt design and aggregation to improve detection performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of MAD in face recognition security, where attackers exploit identity interpolation to fool systems, by leveraging the zero-shot capabilities of multimodal foundation models like CLIP.

Method: Uses CLIP without fine-tuning, designing and aggregating multiple textual prompts per class to align the model's representations with MAD tasks.

Result: Prompt aggregation significantly enhances zero-shot MAD detection performance.

Conclusion: Efficient prompt engineering can effectively exploit foundation models' multimodal knowledge for generalizable deployment in MAD.

Abstract: Face Morphing Attack Detection (MAD) is a critical challenge in face
recognition security, where attackers can fool systems by interpolating the
identity information of two or more individuals into a single face image,
resulting in samples that can be verified as belonging to multiple identities
by face recognition systems. While multimodal foundation models (FMs) like CLIP
offer strong zero-shot capabilities by jointly modeling images and text, most
prior works on FMs for biometric recognition have relied on fine-tuning for
specific downstream tasks, neglecting their potential for direct, generalizable
deployment. This work explores a pure zero-shot approach to MAD by leveraging
CLIP without any additional training or fine-tuning, focusing instead on the
design and aggregation of multiple textual prompts per class. By aggregating
the embeddings of diverse prompts, we better align the model's internal
representations with the MAD task, capturing richer and more varied cues
indicative of bona-fide or attack samples. Our results show that prompt
aggregation substantially improves zero-shot detection performance,
demonstrating the effectiveness of exploiting foundation models' built-in
multimodal knowledge through efficient prompt engineering.

</details>


### [53] [UniSTFormer: Unified Spatio-Temporal Lightweight Transformer for Efficient Skeleton-Based Action Recognition](https://arxiv.org/abs/2508.08944)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Aidong Lu*

Main category: cs.CV

TL;DR: A lightweight transformer framework for skeleton-based action recognition integrates spatial and temporal modeling in one module, reducing redundancy and computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods for skeleton-based action recognition are complex, heavy, and inefficient, limiting scalability.

Method: Proposes a unified spatio-temporal lightweight transformer with a single attention module and a multi-scale pooling fusion module for local and global motion capture.

Result: Reduces parameters by 58% and computational costs by 60% while maintaining competitive accuracy.

Conclusion: The framework offers an efficient, scalable solution for skeleton-based action recognition without sacrificing performance.

Abstract: Skeleton-based action recognition (SAR) has achieved impressive progress with
transformer architectures. However, existing methods often rely on complex
module compositions and heavy designs, leading to increased parameter counts,
high computational costs, and limited scalability. In this paper, we propose a
unified spatio-temporal lightweight transformer framework that integrates
spatial and temporal modeling within a single attention module, eliminating the
need for separate temporal modeling blocks. This approach reduces redundant
computations while preserving temporal awareness within the spatial modeling
process. Furthermore, we introduce a simplified multi-scale pooling fusion
module that combines local and global pooling pathways to enhance the model's
ability to capture fine-grained local movements and overarching global motion
patterns. Extensive experiments on benchmark datasets demonstrate that our
lightweight model achieves a superior balance between accuracy and efficiency,
reducing parameter complexity by over 58% and lowering computational cost by
over 60% compared to state-of-the-art transformer-based baselines, while
maintaining competitive recognition performance.

</details>


### [54] [Lay2Story: Extending Diffusion Transformers for Layout-Togglable Story Generation](https://arxiv.org/abs/2508.08949)
*Ao Ma,Jiasong Feng,Ke Cao,Jing Wang,Yun Wang,Quanwei Zhang,Zhanjie Zhang*

Main category: cs.CV

TL;DR: The paper introduces Layout-Togglable Storytelling, a method using layout conditions to enhance subject consistency and control in storytelling tasks, supported by the Lay2Story-1M dataset and Lay2Story-Bench benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with subject consistency and fine-grained control due to lack of guidance and high-quality data.

Method: Proposes Lay2Story, a framework using Diffusion Transformers (DiTs) and layout conditions for precise subject control.

Result: Outperforms SOTA techniques in consistency, semantic correlation, and aesthetic quality.

Conclusion: Layout conditions and the proposed framework significantly improve storytelling tasks, validated by extensive experiments.

Abstract: Storytelling tasks involving generating consistent subjects have gained
significant attention recently. However, existing methods, whether
training-free or training-based, continue to face challenges in maintaining
subject consistency due to the lack of fine-grained guidance and inter-frame
interaction. Additionally, the scarcity of high-quality data in this field
makes it difficult to precisely control storytelling tasks, including the
subject's position, appearance, clothing, expression, and posture, thereby
hindering further advancements. In this paper, we demonstrate that layout
conditions, such as the subject's position and detailed attributes, effectively
facilitate fine-grained interactions between frames. This not only strengthens
the consistency of the generated frame sequence but also allows for precise
control over the subject's position, appearance, and other key details.
Building on this, we introduce an advanced storytelling task: Layout-Togglable
Storytelling, which enables precise subject control by incorporating layout
conditions. To address the lack of high-quality datasets with layout
annotations for this task, we develop Lay2Story-1M, which contains over 1
million 720p and higher-resolution images, processed from approximately 11,300
hours of cartoon videos. Building on Lay2Story-1M, we create Lay2Story-Bench, a
benchmark with 3,000 prompts designed to evaluate the performance of different
methods on this task. Furthermore, we propose Lay2Story, a robust framework
based on the Diffusion Transformers (DiTs) architecture for Layout-Togglable
Storytelling tasks. Through both qualitative and quantitative experiments, we
find that our method outperforms the previous state-of-the-art (SOTA)
techniques, achieving the best results in terms of consistency, semantic
correlation, and aesthetic quality.

</details>


### [55] [Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering](https://arxiv.org/abs/2508.08974)
*Elman Ghazaei,Erchan Aptoula*

Main category: cs.CV

TL;DR: The paper introduces a new dataset (BrightVQA) and a Text-Conditioned State Space Model (TCSSM) to address domain shift in Change Detection Visual Question Answering (CDVQA), achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Traditional change detection methods require expert knowledge, and existing CDVQA methods assume similar training-testing distributions, which is unrealistic due to domain shifts.

Method: Proposes TCSSM, a model leveraging bi-temporal imagery and geo-disaster-related text to extract domain-invariant features, with dynamically predicted input-dependent parameters.

Result: TCSSM outperforms state-of-the-art models in experiments, demonstrating effective domain generalization.

Conclusion: The work advances CDVQA by addressing domain shift, with the dataset and model made publicly available.

Abstract: The Earth's surface is constantly changing, and detecting these changes
provides valuable insights that benefit various aspects of human society. While
traditional change detection methods have been employed to detect changes from
bi-temporal images, these approaches typically require expert knowledge for
accurate interpretation. To enable broader and more flexible access to change
information by non-expert users, the task of Change Detection Visual Question
Answering (CDVQA) has been introduced. However, existing CDVQA methods have
been developed under the assumption that training and testing datasets share
similar distributions. This assumption does not hold in real-world
applications, where domain shifts often occur. In this paper, the CDVQA task is
revisited with a focus on addressing domain shift. To this end, a new
multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate
domain generalization research in CDVQA. Furthermore, a novel state space
model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The
TCSSM framework is designed to leverage both bi-temporal imagery and
geo-disaster-related textual information in an unified manner to extract
domain-invariant features across domains. Input-dependent parameters existing
in TCSSM are dynamically predicted by using both bi-temporal images and
geo-disaster-related description, thereby facilitating the alignment between
bi-temporal visual data and the associated textual descriptions. Extensive
experiments are conducted to evaluate the proposed method against
state-of-the-art models, and superior performance is consistently demonstrated.
The code and dataset will be made publicly available upon acceptance at
https://github.com/Elman295/TCSSM.

</details>


### [56] [TaoCache: Structure-Maintained Video Generation Acceleration](https://arxiv.org/abs/2508.08978)
*Zhentao Fan,Zongzuo Wang,Weiwei Zhang*

Main category: cs.CV

TL;DR: TaoCache is a training-free caching strategy for video diffusion models that improves visual quality by predicting noise output in late denoising stages, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing cache-based methods skip early/mid denoising steps, causing structural issues and hindering consistency. TaoCache addresses this by focusing on late stages.

Method: TaoCache uses a fixed-point perspective to predict noise output, calibrating cosine similarities and norm ratios of noise deltas for aggressive skipping while preserving structure.

Result: TaoCache achieves higher visual quality (LPIPS, SSIM, PSNR) than prior methods on Latte-1, OpenSora-Plan v110, and Wan2.1 under the same speedups.

Conclusion: TaoCache is a plug-and-play solution that enhances video diffusion model acceleration without compromising quality, compatible with complementary methods like PAB and TeaCache.

Abstract: Existing cache-based acceleration methods for video diffusion models
primarily skip early or mid denoising steps, which often leads to structural
discrepancies relative to full-timestep generation and can hinder instruction
following and character consistency. We present TaoCache, a training-free,
plug-and-play caching strategy that, instead of residual-based caching, adopts
a fixed-point perspective to predict the model's noise output and is
specifically effective in late denoising stages. By calibrating cosine
similarities and norm ratios of consecutive noise deltas, TaoCache preserves
high-resolution structure while enabling aggressive skipping. The approach is
orthogonal to complementary accelerations such as Pyramid Attention Broadcast
(PAB) and TeaCache, and it integrates seamlessly into DiT-based frameworks.
Across Latte-1, OpenSora-Plan v110, and Wan2.1, TaoCache attains substantially
higher visual quality (LPIPS, SSIM, PSNR) than prior caching methods under the
same speedups.

</details>


### [57] [ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation](https://arxiv.org/abs/2508.08987)
*Ding Xia,Naoto Inoue,Qianru Qiu,Kotaro Kikuchi*

Main category: cs.CV

TL;DR: The paper explores using pretrained LLMs for color recommendation, proposing ColorGPT, a pipeline that outperforms traditional methods in accuracy and diversity.


<details>
  <summary>Details</summary>
Motivation: Colors are vital in design, but traditional methods struggle with complexity and data limitations. The study investigates if LLMs can excel in color recommendation.

Method: Developed ColorGPT, testing color representations and prompt engineering for palette completion and full generation tasks.

Result: ColorGPT outperformed existing methods in accuracy and color distribution for palette completion, and improved diversity/similarity in full generation.

Conclusion: Pretrained LLMs, like ColorGPT, can effectively serve as superior designers for color recommendation tasks.

Abstract: Colors play a crucial role in the design of vector graphic documents by
enhancing visual appeal, facilitating communication, improving usability, and
ensuring accessibility. In this context, color recommendation involves
suggesting appropriate colors to complete or refine a design when one or more
colors are missing or require alteration. Traditional methods often struggled
with these challenges due to the complex nature of color design and the limited
data availability. In this study, we explored the use of pretrained Large
Language Models (LLMs) and their commonsense reasoning capabilities for color
recommendation, raising the question: Can pretrained LLMs serve as superior
designers for color recommendation tasks? To investigate this, we developed a
robust, rigorously validated pipeline, ColorGPT, that was built by
systematically testing multiple color representations and applying effective
prompt engineering techniques. Our approach primarily targeted color palette
completion by recommending colors based on a set of given colors and
accompanying context. Moreover, our method can be extended to full palette
generation, producing an entire color palette corresponding to a provided
textual description. Experimental results demonstrated that our LLM-based
pipeline outperformed existing methods in terms of color suggestion accuracy
and the distribution of colors in the color palette completion task. For the
full palette generation task, our approach also yielded improvements in color
diversity and similarity compared to current techniques.

</details>


### [58] [KFFocus: Highlighting Keyframes for Enhanced Video Understanding](https://arxiv.org/abs/2508.08989)
*Ming Nie,Chunwei Wang,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: KFFocus improves video LLMs by efficiently compressing video tokens and emphasizing keyframes, outperforming existing methods in computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current video LLMs use uniform sampling for compression, risking omission of keyframes with essential details due to uneven temporal information distribution.

Method: KFFocus replaces uniform sampling with a refined approach to identify keyframes based on temporal redundancy, assigns varying condensation ratios, and includes a spatiotemporal modeling module.

Result: KFFocus outperforms existing methods on video understanding benchmarks, especially in long videos, achieving better computational efficiency and accuracy.

Conclusion: KFFocus effectively addresses the limitations of current video LLMs by optimizing token compression and enhancing spatiotemporal understanding.

Abstract: Recently, with the emergence of large language models, multimodal LLMs have
demonstrated exceptional capabilities in image and video modalities. Despite
advancements in video comprehension, the substantial computational demands of
long video sequences lead current video LLMs (Vid-LLMs) to employ compression
strategies at both the inter-frame level (e.g., uniform sampling of video
frames) and intra-frame level (e.g., condensing all visual tokens of each frame
into a limited number). However, this approach often neglects the uneven
temporal distribution of critical information across frames, risking the
omission of keyframes that contain essential temporal and semantic details. To
tackle these challenges, we propose KFFocus, a method designed to efficiently
compress video tokens and emphasize the informative context present within
video frames. We substitute uniform sampling with a refined approach inspired
by classic video compression principles to identify and capture keyframes based
on their temporal redundancy. By assigning varying condensation ratios to
frames based on their contextual relevance, KFFocus efficiently reduces token
redundancy while preserving informative content details. Additionally, we
introduce a spatiotemporal modeling module that encodes both the temporal
relationships between video frames and the spatial structure within each frame,
thus providing Vid-LLMs with a nuanced understanding of spatial-temporal
dynamics. Extensive experiments on widely recognized video understanding
benchmarks, especially long video scenarios, demonstrate that KFFocus
significantly outperforms existing methods, achieving substantial computational
efficiency and enhanced accuracy.

</details>


### [59] [Spatial-Temporal Multi-Scale Quantization for Flexible Motion Generation](https://arxiv.org/abs/2508.08991)
*Zan Wang,Jingze Zhang,Yixin Chen,Baoxiong Jia,Wei Liang,Siyuan Huang*

Main category: cs.CV

TL;DR: MSQ introduces a multi-scale quantization method for human motion generation, addressing limitations in capturing complex patterns and compositional flexibility. It outperforms baselines in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current motion representations lack multi-scale perspective and compositional flexibility, limiting complex pattern modeling and generalization.

Method: MSQ compresses motion sequences into multi-scale discrete tokens using distinct encoders for spatial granularities and temporal interpolation. A generative mask modeling model supports editing, control, and generation.

Result: MSQ enables seamless token composition without specialized design or re-training and outperforms existing methods in benchmarks.

Conclusion: MSQ effectively addresses motion representation limitations, offering improved flexibility and performance in generation tasks.

Abstract: Despite significant advancements in human motion generation, current motion
representations, typically formulated as discrete frame sequences, still face
two critical limitations: (i) they fail to capture motion from a multi-scale
perspective, limiting the capability in complex patterns modeling; (ii) they
lack compositional flexibility, which is crucial for model's generalization in
diverse generation tasks. To address these challenges, we introduce MSQ, a
novel quantization method that compresses the motion sequence into multi-scale
discrete tokens across spatial and temporal dimensions. MSQ employs distinct
encoders to capture body parts at varying spatial granularities and temporally
interpolates the encoded features into multiple scales before quantizing them
into discrete tokens. Building on this representation, we establish a
generative mask modeling model to effectively support motion editing, motion
control, and conditional motion generation. Through quantitative and
qualitative analysis, we show that our quantization method enables the seamless
composition of motion tokens without requiring specialized design or
re-training. Furthermore, extensive evaluations demonstrate that our approach
outperforms existing baseline methods on various benchmarks.

</details>


### [60] [UniConvNet: Expanding Effective Receptive Field while Maintaining Asymptotically Gaussian Distribution for ConvNets of Any Scale](https://arxiv.org/abs/2508.09000)
*Yuhao Wang,Wei Xi*

Main category: cs.CV

TL;DR: The paper proposes UniConvNet, a universal ConvNet model that expands the effective receptive field (ERF) while maintaining asymptotically Gaussian distribution (AGD) by combining smaller kernels. It outperforms state-of-the-art CNNs and ViTs on various tasks.


<details>
  <summary>Details</summary>
Motivation: Large ERF in ConvNets is promising but constrained by high computational costs and disrupted AGD. The paper aims to address this by efficiently expanding ERF while preserving AGD.

Method: Introduces a Three-layer Receptive Field Aggregator and a Layer Operator to expand ERF using smaller kernels (e.g., 77, 99, 1111).

Result: UniConvNet achieves top performance on ImageNet-1K, COCO2017, and ADE20K, with UniConvNet-T reaching 84.2% accuracy (30M params, 5.1G FLOPs) and UniConvNet-XL achieving 88.4%.

Conclusion: UniConvNet is a scalable and efficient alternative to large-kernel ConvNets, demonstrating superior performance across vision tasks.

Abstract: Convolutional neural networks (ConvNets) with large effective receptive field
(ERF), still in their early stages, have demonstrated promising effectiveness
while constrained by high parameters and FLOPs costs and disrupted
asymptotically Gaussian distribution (AGD) of ERF. This paper proposes an
alternative paradigm: rather than merely employing extremely large ERF, it is
more effective and efficient to expand the ERF while maintaining AGD of ERF by
proper combination of smaller kernels, such as $7\times{7}$, $9\times{9}$,
$11\times{11}$. This paper introduces a Three-layer Receptive Field Aggregator
and designs a Layer Operator as the fundamental operator from the perspective
of receptive field. The ERF can be expanded to the level of existing
large-kernel ConvNets through the stack of proposed modules while maintaining
AGD of ERF. Using these designs, we propose a universal model for ConvNet of
any scale, termed UniConvNet. Extensive experiments on ImageNet-1K, COCO2017,
and ADE20K demonstrate that UniConvNet outperforms state-of-the-art CNNs and
ViTs across various vision recognition tasks for both lightweight and
large-scale models with comparable throughput. Surprisingly, UniConvNet-T
achieves $84.2\%$ ImageNet top-1 accuracy with $30M$ parameters and $5.1G$
FLOPs. UniConvNet-XL also shows competitive scalability to big data and large
models, acquiring $88.4\%$ top-1 accuracy on ImageNet. Code and models are
publicly available at https://github.com/ai-paperwithcode/UniConvNet.

</details>


### [61] [Towards Perfection: Building Inter-component Mutual Correction for Retinex-based Low-light Image Enhancement](https://arxiv.org/abs/2508.09009)
*Luyang Cao,Han Xu,Jian Zhang,Lei Qi,Jiayi Ma,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: The paper introduces Inter-component residuals (ICR) in Retinex-based low-light image enhancement, proposes the IRetinex model to reduce ICR, and shows superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Previous Retinex-based methods underestimate residuals (ICR) after decomposition, which degrade decomposition accuracy and final image quality.

Method: Proposes IRetinex with inter-component residual reduction in decomposition and feature similarity-based mitigation in enhancement.

Result: Outperforms state-of-the-art methods on three benchmark datasets by reducing ICR.

Conclusion: Addressing ICR improves decomposition and enhancement, leading to better low-light image quality.

Abstract: In low-light image enhancement, Retinex-based deep learning methods have
garnered significant attention due to their exceptional interpretability. These
methods decompose images into mutually independent illumination and reflectance
components, allows each component to be enhanced separately. In fact, achieving
perfect decomposition of illumination and reflectance components proves to be
quite challenging, with some residuals still existing after decomposition. In
this paper, we formally name these residuals as inter-component residuals
(ICR), which has been largely underestimated by previous methods. In our
investigation, ICR not only affects the accuracy of the decomposition but also
causes enhanced components to deviate from the ideal outcome, ultimately
reducing the final synthesized image quality. To address this issue, we propose
a novel Inter-correction Retinex model (IRetinex) to alleviate ICR during the
decomposition and enhancement stage. In the decomposition stage, we leverage
inter-component residual reduction module to reduce the feature similarity
between illumination and reflectance components. In the enhancement stage, we
utilize the feature similarity between the two components to detect and
mitigate the impact of ICR within each enhancement unit. Extensive experiments
on three low-light benchmark datasets demonstrated that by reducing ICR, our
method outperforms state-of-the-art approaches both qualitatively and
quantitatively.

</details>


### [62] [Uncertainty-aware Cross-training for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2508.09014)
*Kaiwen Huang,Tao Zhou,Huazhu Fu,Yizhe Zhang,Yi Zhou,Xiao-Jun Wu*

Main category: cs.CV

TL;DR: UC-Seg is a semi-supervised medical image segmentation framework that addresses cognitive biases and pseudo-label challenges by using cross-subnet consistency and uncertainty-aware pseudo-label generation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on expert annotations and address cognitive biases and pseudo-label generation challenges in semi-supervised medical image segmentation.

Method: Proposes UC-Seg with Cross-subnet Consistency Preservation (CCP) and Uncertainty-aware Pseudo-label Generation (UPG) to enhance feature representation and generate high-confidence pseudo-labels.

Result: Achieves superior segmentation accuracy and generalization across various medical image modalities (MRI, CT, ultrasound, etc.).

Conclusion: UC-Seg effectively mitigates biases and improves segmentation performance, offering a robust solution for semi-supervised medical image segmentation.

Abstract: Semi-supervised learning has gained considerable popularity in medical image
segmentation tasks due to its capability to reduce reliance on expert-examined
annotations. Several mean-teacher (MT) based semi-supervised methods utilize
consistency regularization to effectively leverage valuable information from
unlabeled data. However, these methods often heavily rely on the student model
and overlook the potential impact of cognitive biases within the model.
Furthermore, some methods employ co-training using pseudo-labels derived from
different inputs, yet generating high-confidence pseudo-labels from perturbed
inputs during training remains a significant challenge. In this paper, we
propose an Uncertainty-aware Cross-training framework for semi-supervised
medical image Segmentation (UC-Seg). Our UC-Seg framework incorporates two
distinct subnets to effectively explore and leverage the correlation between
them, thereby mitigating cognitive biases within the model. Specifically, we
present a Cross-subnet Consistency Preservation (CCP) strategy to enhance
feature representation capability and ensure feature consistency across the two
subnets. This strategy enables each subnet to correct its own biases and learn
shared semantics from both labeled and unlabeled data. Additionally, we propose
an Uncertainty-aware Pseudo-label Generation (UPG) component that leverages
segmentation results and corresponding uncertainty maps from both subnets to
generate high-confidence pseudo-labels. We extensively evaluate the proposed
UC-Seg on various medical image segmentation tasks involving different modality
images, such as MRI, CT, ultrasound, colonoscopy, and so on. The results
demonstrate that our method achieves superior segmentation accuracy and
generalization performance compared to other state-of-the-art semi-supervised
methods. Our code will be released at https://github.com/taozh2017/UCSeg.

</details>


### [63] [When Deepfakes Look Real: Detecting AI-Generated Faces with Unlabeled Data due to Annotation Challenges](https://arxiv.org/abs/2508.09022)
*Zhiqiang Yang,Renshuai Tao,Xiaolong Zheng,Guodong Yang,Chunjie Zhang*

Main category: cs.CV

TL;DR: DPGNet introduces a dual-path approach to improve deepfake detection by leveraging unlabeled data and bridging domain gaps, outperforming state-of-the-art methods by 6.3%.


<details>
  <summary>Details</summary>
Motivation: Human annotators struggle to label deepfakes due to their realism, making labeled data scarce and unreliable. There's a need for methods that utilize unlabeled data effectively.

Method: DPGNet uses text-guided cross-domain alignment and curriculum-driven pseudo label generation to unify features and exploit unlabeled samples, aided by cross-domain knowledge distillation.

Result: DPGNet achieves a 6.3% improvement over state-of-the-art methods across 11 datasets.

Conclusion: DPGNet effectively addresses annotation challenges by leveraging unlabeled data and bridging domain gaps, proving robust against increasingly realistic deepfakes.

Abstract: Existing deepfake detection methods heavily depend on labeled training data.
However, as AI-generated content becomes increasingly realistic, even
\textbf{human annotators struggle to distinguish} between deepfakes and
authentic images. This makes the labeling process both time-consuming and less
reliable. Specifically, there is a growing demand for approaches that can
effectively utilize large-scale unlabeled data from online social networks.
Unlike typical unsupervised learning tasks, where categories are distinct,
AI-generated faces closely mimic real image distributions and share strong
similarities, causing performance drop in conventional strategies. In this
paper, we introduce the Dual-Path Guidance Network (DPGNet), to tackle two key
challenges: (1) bridging the domain gap between faces from different generation
models, and (2) utilizing unlabeled image samples. The method features two core
modules: text-guided cross-domain alignment, which uses learnable prompts to
unify visual and textual embeddings into a domain-invariant feature space, and
curriculum-driven pseudo label generation, which dynamically exploit more
informative unlabeled samples. To prevent catastrophic forgetting, we also
facilitate bridging between domains via cross-domain knowledge distillation.
Extensive experiments on \textbf{11 popular datasets}, show that DPGNet
outperforms SoTA approaches by \textbf{6.3\%}, highlighting its effectiveness
in leveraging unlabeled data to address the annotation challenges posed by the
increasing realism of deepfakes.

</details>


### [64] [Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding](https://arxiv.org/abs/2508.09032)
*Maxim A. Patratskiy,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.CV

TL;DR: The paper introduces a Vision-Language-Action model integrating spatial and temporal understanding via visual prompting, improving task success rates with minimal training data.


<details>
  <summary>Details</summary>
Motivation: To enhance agent movement prediction by combining spatial and temporal understanding, addressing limitations of prior independent approaches.

Method: Projects visual traces of key points onto depth maps to simultaneously capture spatial and temporal information.

Result: Experiments show a 4% improvement over SpatialVLA and 19% over TraceVLA in task success rates.

Conclusion: The method effectively integrates spatial-temporal understanding and is practical for real-world applications due to low data requirements.

Abstract: Vision-Language-Action models have demonstrated remarkable capabilities in
predicting agent movements within virtual environments and real-world scenarios
based on visual observations and textual instructions. Although recent research
has focused on enhancing spatial and temporal understanding independently, this
paper presents a novel approach that integrates both aspects through visual
prompting. We introduce a method that projects visual traces of key points from
observations onto depth maps, enabling models to capture both spatial and
temporal information simultaneously. The experiments in SimplerEnv show that
the mean number of tasks successfully solved increased for 4% compared to
SpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this
enhancement can be achieved with minimal training data, making it particularly
valuable for real-world applications where data collection is challenging. The
project page is available at https://ampiromax.github.io/ST-VLA.

</details>


### [65] [Per-Query Visual Concept Learning](https://arxiv.org/abs/2508.09045)
*Ori Malca,Dvir Samuel,Gal Chechik*

Main category: cs.CV

TL;DR: The paper introduces a personalization step for text-to-image models, using PDM features and attention-based losses to improve concept learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing visual concept learning in text-to-image models for applications like product placement and personalized design.

Method: Adds a prompt- and noise-specific personalization step with self- and cross-attention losses, leveraging PDM features for identity capture.

Result: Significant improvements over six existing personalization methods and various base models (UNet- and DiT-based).

Conclusion: The proposed method effectively augments existing approaches, achieving superior personalized semantic similarity.

Abstract: Visual concept learning, also known as Text-to-image personalization, is the
process of teaching new concepts to a pretrained model. This has numerous
applications from product placement to entertainment and personalized design.
Here we show that many existing methods can be substantially augmented by
adding a personalization step that is (1) specific to the prompt and noise
seed, and (2) using two loss terms based on the self- and cross- attention,
capturing the identity of the personalized concept. Specifically, we leverage
PDM features -- previously designed to capture identity -- and show how they
can be used to improve personalized semantic similarity. We evaluate the
benefit that our method gains on top of six different personalization methods,
and several base text-to-image models (both UNet- and DiT-based). We find
significant improvements even over previous per-query personalization methods.

</details>


### [66] [ALFred: An Active Learning Framework for Real-world Semi-supervised Anomaly Detection with Adaptive Thresholds](https://arxiv.org/abs/2508.09058)
*Shanle Yao,Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: An active learning framework for Video Anomaly Detection (VAD) improves adaptability in dynamic environments by leveraging human-in-the-loop feedback and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: VAD struggles in real-world settings due to dynamic human actions, environmental changes, and domain shifts. Traditional metrics fail to adapt, necessitating a more flexible approach.

Method: The framework uses active learning to select informative data for labeling, incorporates human feedback to refine pseudo-labels, and defines adaptive thresholds for different environments.

Result: Achieves an EBI of 68.91 in simulated real-world scenarios, demonstrating practical effectiveness.

Conclusion: The approach enhances VAD's applicability in dynamic settings by improving adaptability and accuracy.

Abstract: Video Anomaly Detection (VAD) can play a key role in spotting unusual
activities in video footage. VAD is difficult to use in real-world settings due
to the dynamic nature of human actions, environmental variations, and domain
shifts. Traditional evaluation metrics often prove inadequate for such
scenarios, as they rely on static assumptions and fall short of identifying a
threshold that distinguishes normal from anomalous behavior in dynamic
settings. To address this, we introduce an active learning framework tailored
for VAD, designed for adapting to the ever-changing real-world conditions. Our
approach leverages active learning to continuously select the most informative
data points for labeling, thereby enhancing model adaptability. A critical
innovation is the incorporation of a human-in-the-loop mechanism, which enables
the identification of actual normal and anomalous instances from
pseudo-labeling results generated by AI. This collected data allows the
framework to define an adaptive threshold tailored to different environments,
ensuring that the system remains effective as the definition of 'normal' shifts
across various settings. Implemented within a lab-based framework that
simulates real-world conditions, our approach allows rigorous testing and
refinement of VAD algorithms with a new metric. Experimental results show that
our method achieves an EBI (Error Balance Index) of 68.91 for Q3 in real-world
simulated scenarios, demonstrating its practical effectiveness and
significantly enhancing the applicability of VAD in dynamic environments.

</details>


### [67] [VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception](https://arxiv.org/abs/2508.09061)
*Fuhao Chang,Shuxin Li,Yabei Li,Lei He*

Main category: cs.CV

TL;DR: VLM-3D is an end-to-end framework using Visual Language Models (VLMs) for 3D geometric perception in autonomous driving, improving accuracy by 12.8% with a joint semantic-geometric loss.


<details>
  <summary>Details</summary>
Motivation: Open-set perception in traffic environments is challenging, especially for unseen objects. VLMs offer potential but existing methods suffer from multi-stage error propagation.

Method: VLM-3D integrates LoRA for efficient VLM adaptation and uses a joint semantic-geometric loss (token-level semantic loss early, 3D IoU loss later) for stable convergence and refined accuracy.

Result: 12.8% improvement in perception accuracy on the nuScenes dataset.

Conclusion: VLM-3D effectively addresses the limitations of existing methods, demonstrating significant accuracy improvements.

Abstract: Open-set perception in complex traffic environments poses a critical
challenge for autonomous driving systems, particularly in identifying
previously unseen object categories, which is vital for ensuring safety. Visual
Language Models (VLMs), with their rich world knowledge and strong semantic
reasoning capabilities, offer new possibilities for addressing this task.
However, existing approaches typically leverage VLMs to extract visual features
and couple them with traditional object detectors, resulting in multi-stage
error propagation that hinders perception accuracy. To overcome this
limitation, we propose VLM-3D, the first end-to-end framework that enables VLMs
to perform 3D geometric perception in autonomous driving scenarios. VLM-3D
incorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving
tasks with minimal computational overhead, and introduces a joint
semantic-geometric loss design: token-level semantic loss is applied during
early training to ensure stable convergence, while 3D IoU loss is introduced in
later stages to refine the accuracy of 3D bounding box predictions. Evaluations
on the nuScenes dataset demonstrate that the proposed joint semantic-geometric
loss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully
validating the effectiveness and advancement of our method.

</details>


### [68] [Scaling Learned Image Compression Models up to 1 Billion](https://arxiv.org/abs/2508.09075)
*Yuqi Li,Haotian Zhang,Li Li,Dong Liu,Feng Wu*

Main category: cs.CV

TL;DR: The paper explores scaling up learned image compression models, revealing performance trends via scaling laws, and demonstrates improved rate-distortion performance with larger models.


<details>
  <summary>Details</summary>
Motivation: Current learned image compression models are limited in scale, and the impact of scaling on performance is unexplored. This study aims to investigate this relationship.

Method: The study scales the HPCM model from 68.5 million to 1 billion parameters, fitting power-law relations between test loss and scaling variables like model size and training compute.

Result: The scaled-up HPCM-1B model achieves state-of-the-art rate-distortion performance, revealing a scaling trend for future models.

Conclusion: This work encourages further exploration of large-scale compression models and deeper understanding of the link between compression and intelligence.

Abstract: Recent advances in large language models (LLMs) highlight a strong connection
between intelligence and compression. Learned image compression, a fundamental
task in modern data compression, has made significant progress in recent years.
However, current models remain limited in scale, restricting their
representation capacity, and how scaling model size influences compression
performance remains unexplored. In this work, we present a pioneering study on
scaling up learned image compression models and revealing the performance
trends through scaling laws. Using the recent state-of-the-art HPCM model as
baseline, we scale model parameters from 68.5 millions to 1 billion and fit
power-law relations between test loss and key scaling variables, including
model size and optimal training compute. The results reveal a scaling trend,
enabling extrapolation to larger scale models. Experimental results demonstrate
that the scaled-up HPCM-1B model achieves state-of-the-art rate-distortion
performance. We hope this work inspires future exploration of large-scale
compression models and deeper investigations into the connection between
compression and intelligence.

</details>


### [69] [Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision](https://arxiv.org/abs/2508.09087)
*Ahsan Habib Akash,Greg Murray,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: The paper introduces an attribute-agnostic debiasing method for Vision-Language Models (VLMs) in glaucoma screening, reducing demographic biases without explicit protected attributes.


<details>
  <summary>Details</summary>
Motivation: VLMs exhibit demographic biases in tasks like glaucoma screening, which disproportionately affects underserved populations. Addressing these biases is critical for equitable healthcare applications.

Method: The method uses unsupervised clustering to infer proxy subgroups, computes gradient-similarity weights between multimodal and image-pair contrastive losses, and applies a joint weighted objective to upweight underperforming clusters.

Result: Evaluated on the Harvard FairVLMed dataset, the method improves fairness metrics like Equalized Odds Distance (EOD) and Equalized Subgroup AUC (ES AUC).

Conclusion: The proposed label-free debiasing approach effectively reduces subgroup disparities in glaucoma screening, demonstrating equitable performance across inferred demographic subgroups.

Abstract: Vision-Language Models (VLMs) have achieved remarkable success on multimodal
tasks such as image-text retrieval and zero-shot classification, yet they can
exhibit demographic biases even when explicit protected attributes are absent
during training. In this work, we focus on automated glaucoma screening from
retinal fundus images, a critical application given that glaucoma is a leading
cause of irreversible blindness and disproportionately affects underserved
populations. Building on a reweighting-based contrastive learning framework, we
introduce an attribute-agnostic debiasing method that (i) infers proxy
subgroups via unsupervised clustering of image-image embeddings, (ii) computes
gradient-similarity weights between the CLIP-style multimodal loss and a
SimCLR-style image-pair contrastive loss, and (iii) applies these weights in a
joint, top-$k$ weighted objective to upweight underperforming clusters. This
label-free approach adaptively targets the hardest examples, thereby reducing
subgroup disparities. We evaluate our method on the Harvard FairVLMed glaucoma
subset, reporting Equalized Odds Distance (EOD), Equalized Subgroup AUC (ES
AUC), and Groupwise AUC to demonstrate equitable performance across inferred
demographic subgroups.

</details>


### [70] [Deep Learning Models for Robust Facial Liveness Detection](https://arxiv.org/abs/2508.09094)
*Oleksandr Kuznetsov,Emanuele Frontoni,Luca Romeo,Riccardo Rosati,Andrea Maranesi,Alessandro Muscatello*

Main category: cs.CV

TL;DR: The paper introduces a deep learning-based solution (AttackNet V2.2) to improve liveness detection in facial recognition systems, achieving 99.9% accuracy against advanced spoofing attacks like deepfakes.


<details>
  <summary>Details</summary>
Motivation: Current liveness detection methods fail against advanced spoofing attacks, necessitating a more robust solution to secure biometric authentication.

Method: The study employs novel deep learning models integrating texture analysis and reflective properties to distinguish genuine traits from replicas.

Result: The best model (AttackNet V2.2) achieved 99.9% average accuracy across diverse datasets, outperforming existing systems.

Conclusion: The research not only enhances biometric security but also provides insights into evolving spoofing tactics, boosting confidence in authentication systems.

Abstract: In the rapidly evolving landscape of digital security, biometric
authentication systems, particularly facial recognition, have emerged as
integral components of various security protocols. However, the reliability of
these systems is compromised by sophisticated spoofing attacks, where imposters
gain unauthorized access by falsifying biometric traits. Current literature
reveals a concerning gap: existing liveness detection methodologies - designed
to counteract these breaches - fall short against advanced spoofing tactics
employing deepfakes and other artificial intelligence-driven manipulations.
This study introduces a robust solution through novel deep learning models
addressing the deficiencies in contemporary anti-spoofing techniques. By
innovatively integrating texture analysis and reflective properties associated
with genuine human traits, our models distinguish authentic presence from
replicas with remarkable precision. Extensive evaluations were conducted across
five diverse datasets, encompassing a wide range of attack vectors and
environmental conditions. Results demonstrate substantial advancement over
existing systems, with our best model (AttackNet V2.2) achieving 99.9% average
accuracy when trained on combined data. Moreover, our research unveils critical
insights into the behavioral patterns of impostor attacks, contributing to a
more nuanced understanding of their evolving nature. The implications are
profound: our models do not merely fortify the authentication processes but
also instill confidence in biometric systems across various sectors reliant on
secure access.

</details>


### [71] [Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices](https://arxiv.org/abs/2508.09136)
*Ya Zou,Jingfeng Yao,Siyuan Yu,Shuai Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Proposes Turbo-VAED, a mobile-optimized VAE decoder for real-time 720p video decoding, reducing parameters and improving speed with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Deploying large generative AI models on mobile devices is challenging due to computational bottlenecks in VAEs, causing memory issues and slow inference.

Method: Analyzes VAE redundancy, integrates 3D depthwise separable convolutions, proposes decoupled 3D pixel shuffle, and distills the decoder for mobile adaptation.

Result: Achieves 84.5x speedup, 17.5% parameter count, 96.9% reconstruction quality retention, and 2.9x FPS improvement on iPhone 16 Pro.

Conclusion: Turbo-VAED enables efficient mobile deployment of video VAEs with low training cost and high performance.

Abstract: There is a growing demand for deploying large generative AI models on mobile
devices. For recent popular video generative models, however, the Variational
AutoEncoder (VAE) represents one of the major computational bottlenecks. Both
large parameter sizes and mismatched kernels cause out-of-memory errors or
extremely slow inference on mobile devices. To address this, we propose a
low-cost solution that efficiently transfers widely used video VAEs to mobile
devices. (1) We analyze redundancy in existing VAE architectures and get
empirical design insights. By integrating 3D depthwise separable convolutions
into our model, we significantly reduce the number of parameters. (2) We
observe that the upsampling techniques in mainstream video VAEs are poorly
suited to mobile hardware and form the main bottleneck. In response, we propose
a decoupled 3D pixel shuffle scheme that slashes end-to-end delay. Building
upon these, we develop a universal mobile-oriented VAE decoder, Turbo-VAED. (3)
We propose an efficient VAE decoder training method. Since only the decoder is
used during deployment, we distill it to Turbo-VAED instead of retraining the
full VAE, enabling fast mobile adaptation with minimal performance loss. To our
knowledge, our method enables real-time 720p video VAE decoding on mobile
devices for the first time. This approach is widely applicable to most video
VAEs. When integrated into four representative models, with training cost as
low as $95, it accelerates original VAEs by up to 84.5x at 720p resolution on
GPUs, uses as low as 17.5% of original parameter count, and retains 96.9% of
the original reconstruction quality. Compared to mobile-optimized VAEs,
Turbo-VAED achieves a 2.9x speedup in FPS and better reconstruction quality on
the iPhone 16 Pro. The code and models will soon be available at
https://github.com/hustvl/Turbo-VAED.

</details>


### [72] [HumanOLAT: A Large-Scale Dataset for Full-Body Human Relighting and Novel-View Synthesis](https://arxiv.org/abs/2508.09137)
*Timo Teufel,Pulkit Gera,Xilong Zhou,Umar Iqbal,Pramod Rao,Jan Kautz,Vladislav Golyanik,Christian Theobalt*

Main category: cs.CV

TL;DR: The paper introduces HumanOLAT, the first large-scale public dataset for full-body human relighting and novel-view synthesis, addressing the lack of high-quality data in this field.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available, high-quality datasets for full-body human captures has hindered progress in relighting and novel-view rendering.

Method: The authors present the HumanOLAT dataset, featuring multi-view OLAT captures with HDR RGB frames under diverse illuminations.

Result: Evaluations show the dataset's value but highlight challenges in modeling human-centric appearance and lighting interactions.

Conclusion: HumanOLAT is expected to advance research by enabling rigorous benchmarking and improvements in relighting and rendering techniques.

Abstract: Simultaneous relighting and novel-view rendering of digital human
representations is an important yet challenging task with numerous
applications. Progress in this area has been significantly limited due to the
lack of publicly available, high-quality datasets, especially for full-body
human captures. To address this critical gap, we introduce the HumanOLAT
dataset, the first publicly accessible large-scale dataset of multi-view
One-Light-at-a-Time (OLAT) captures of full-body humans. The dataset includes
HDR RGB frames under various illuminations, such as white light, environment
maps, color gradients and fine-grained OLAT illuminations. Our evaluations of
state-of-the-art relighting and novel-view synthesis methods underscore both
the dataset's value and the significant challenges still present in modeling
complex human-centric appearance and lighting interactions. We believe
HumanOLAT will significantly facilitate future research, enabling rigorous
benchmarking and advancements in both general and human-specific relighting and
rendering techniques.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [73] [SharpXR: Structure-Aware Denoising for Pediatric Chest X-Rays](https://arxiv.org/abs/2508.08518)
*Ilerioluwakiiye Abolade,Emmanuel Idoko,Solomon Odelola,Promise Omoigui,Adetola Adebanwo,Aondana Iorumbur,Udunna Anazodo,Alessandro Crimi,Raymond Confidence*

Main category: eess.IV

TL;DR: SharpXR is a dual-decoder U-Net for denoising low-dose pediatric X-rays, preserving critical details and improving diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Pediatric chest X-rays in low-resource settings suffer from noise due to low-dose protocols, compromising diagnostic accuracy.

Method: SharpXR uses a Laplacian-guided edge-preserving decoder and a learnable fusion module to balance noise suppression and detail retention, trained on simulated noise data.

Result: SharpXR outperforms baselines, improving pneumonia classification accuracy from 88.8% to 92.5%.

Conclusion: SharpXR is effective for denoising pediatric X-rays, enhancing diagnostic accuracy in resource-limited settings.

Abstract: Pediatric chest X-ray imaging is essential for early diagnosis, particularly
in low-resource settings where advanced imaging modalities are often
inaccessible. Low-dose protocols reduce radiation exposure in children but
introduce substantial noise that can obscure critical anatomical details.
Conventional denoising methods often degrade fine details, compromising
diagnostic accuracy. In this paper, we present SharpXR, a structure-aware
dual-decoder U-Net designed to denoise low-dose pediatric X-rays while
preserving diagnostically relevant features. SharpXR combines a
Laplacian-guided edge-preserving decoder with a learnable fusion module that
adaptively balances noise suppression and structural detail retention. To
address the scarcity of paired training data, we simulate realistic
Poisson-Gaussian noise on the Pediatric Pneumonia Chest X-ray dataset. SharpXR
outperforms state-of-the-art baselines across all evaluation metrics while
maintaining computational efficiency suitable for resource-constrained
settings. SharpXR-denoised images improved downstream pneumonia classification
accuracy from 88.8% to 92.5%, underscoring its diagnostic value in low-resource
pediatric care.

</details>
