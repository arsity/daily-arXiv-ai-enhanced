<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 242]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Simulator Dataset to Support the Study of Impaired Driving](https://arxiv.org/abs/2507.02867)
*John Gideon,Kimimasa Tamura,Emily Sumner,Laporsha Dees,Patricio Reyes Gomez,Bassamul Haq,Todd Rowell,Avinash Balachandran,Simon Stent,Guy Rosman*

Main category: cs.CV

TL;DR: A dataset for studying driver impairment (alcohol intoxication and cognitive distraction) with 23.7 hours of simulated urban driving data from 52 subjects, including vehicle and driver-facing metrics.


<details>
  <summary>Details</summary>
Motivation: Address the societal cost of impaired driving by providing a dataset to study alcohol intoxication and cognitive distraction effects.

Method: Collected data from 52 subjects in simulated urban driving under normal and impaired conditions (alcohol, cognitive tasks, and combinations). Includes vehicle and driver-facing metrics.

Result: Dataset supports analysis of behavior changes due to impairment and responses to road hazards.

Conclusion: The dataset will aid research on impaired driving and is publicly available.

Abstract: Despite recent advances in automated driving technology, impaired driving
continues to incur a high cost to society. In this paper, we present a driving
dataset designed to support the study of two common forms of driver impairment:
alcohol intoxication and cognitive distraction. Our dataset spans 23.7 hours of
simulated urban driving, with 52 human subjects under normal and impaired
conditions, and includes both vehicle data (ground truth perception, vehicle
pose, controls) and driver-facing data (gaze, audio, surveys). It supports
analysis of changes in driver behavior due to alcohol intoxication (0.10\%
blood alcohol content), two forms of cognitive distraction (audio n-back and
sentence parsing tasks), and combinations thereof, as well as responses to a
set of eight controlled road hazards, such as vehicle cut-ins. The dataset will
be made available at https://toyotaresearchinstitute.github.io/IDD/.

</details>


### [2] [Learning to Generate Vectorized Maps at Intersections with Multiple Roadside Cameras](https://arxiv.org/abs/2507.02899)
*Miao Fan,Quanxin Zheng,Shengtong Xu,Linghe Kong,Haoyi Xiong*

Main category: cs.CV

TL;DR: MRC-VMap is a vision-centric neural network for generating high-definition vectorized maps at intersections using roadside cameras, outperforming online methods and matching LiDAR-based accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for vectorized maps are either costly (LiDAR-based) or limited in performance (camera-based). MRC-VMap bridges this gap.

Method: Uses time-aligned, multi-directional images from roadside cameras to directly generate vectorized maps, eliminating intermediate steps like feature extraction and BEV conversion.

Result: Outperforms online methods and matches LiDAR-based accuracy in experiments on 4,000 intersections across 4 Chinese cities.

Conclusion: MRC-VMap offers a scalable, efficient solution for autonomous navigation, reducing costs and improving performance.

Abstract: Vectorized maps are indispensable for precise navigation and the safe
operation of autonomous vehicles. Traditional methods for constructing these
maps fall into two categories: offline techniques, which rely on expensive,
labor-intensive LiDAR data collection and manual annotation, and online
approaches that use onboard cameras to reduce costs but suffer from limited
performance, especially at complex intersections. To bridge this gap, we
introduce MRC-VMap, a cost-effective, vision-centric, end-to-end neural network
designed to generate high-definition vectorized maps directly at intersections.
Leveraging existing roadside surveillance cameras, MRC-VMap directly converts
time-aligned, multi-directional images into vectorized map representations.
This integrated solution lowers the need for additional intermediate
modules--such as separate feature extraction and Bird's-Eye View (BEV)
conversion steps--thus reducing both computational overhead and error
propagation. Moreover, the use of multiple camera views enhances mapping
completeness, mitigates occlusions, and provides robust performance under
practical deployment constraints. Extensive experiments conducted on 4,000
intersections across 4 major metropolitan areas in China demonstrate that
MRC-VMap not only outperforms state-of-the-art online methods but also achieves
accuracy comparable to high-cost LiDAR-based approaches, thereby offering a
scalable and efficient solution for modern autonomous navigation systems.

</details>


### [3] [Advancing Talking Head Generation: A Comprehensive Survey of Multi-Modal Methodologies, Datasets, Evaluation Metrics, and Loss Functions](https://arxiv.org/abs/2507.02900)
*Vineet Kumar Rakesh,Soumya Mazumdar,Research Pratim Maity,Sarbajit Pal,Amitabha Das,Tapas Samanta*

Main category: cs.CV

TL;DR: A comprehensive review of talking head generation (THG) technologies, categorizing methods into 2D, 3D, NeRF, diffusion, and parameter-driven techniques, while addressing challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To synthesize realistic human faces synchronized with various inputs (image, audio, text, video) for applications like digital avatars, video dubbing, and online education.

Method: Categorizes THG approaches into 2D, 3D, NeRF, diffusion, and parameter-driven techniques, evaluating algorithms, datasets, and metrics.

Result: Highlights advancements in perceptual realism and technical efficiency, identifies challenges (e.g., extreme pose handling, multilingual synthesis), and suggests future directions (e.g., modular architectures, hybrid models).

Conclusion: Provides actionable insights for researchers and practitioners, with resources available on GitHub.

Abstract: Talking Head Generation (THG) has emerged as a transformative technology in
computer vision, enabling the synthesis of realistic human faces synchronized
with image, audio, text, or video inputs. This paper provides a comprehensive
review of methodologies and frameworks for talking head generation,
categorizing approaches into 2D--based, 3D--based, Neural Radiance Fields
(NeRF)--based, diffusion--based, parameter-driven techniques and many other
techniques. It evaluates algorithms, datasets, and evaluation metrics while
highlighting advancements in perceptual realism and technical efficiency
critical for applications such as digital avatars, video dubbing, ultra-low
bitrate video conferencing, and online education. The study identifies
challenges such as reliance on pre--trained models, extreme pose handling,
multilingual synthesis, and temporal consistency. Future directions include
modular architectures, multilingual datasets, hybrid models blending
pre--trained and task-specific layers, and innovative loss functions. By
synthesizing existing research and exploring emerging trends, this paper aims
to provide actionable insights for researchers and practitioners in the field
of talking head generation. For the complete survey, code, and curated resource
list, visit our GitHub repository: https://github.com/VineetKumarRakesh/thg.

</details>


### [4] [Enhancing Sports Strategy with Video Analytics and Data Mining: Assessing the effectiveness of Multimodal LLMs in tennis video analysis](https://arxiv.org/abs/2507.02904)
*Charlton Teo*

Main category: cs.CV

TL;DR: The paper evaluates Multimodal LLMs (MLLMs) for analyzing tennis videos, focusing on classifying actions and sequencing events in rallies, while exploring performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the gap in models capable of understanding and sequencing events in tennis rallies, which is valuable for sports analytics.

Method: Assessed MLLMs on classifying tennis actions and sequencing events, explored training methods, and combined MLLMs with traditional models.

Result: Not explicitly stated in the abstract, but the focus is on evaluating and improving MLLM performance for tennis video analysis.

Conclusion: MLLMs show potential for tennis video analysis, with further improvements possible through training and hybrid approaches.

Abstract: The use of Large Language Models (LLMs) in recent years has also given rise
to the development of Multimodal LLMs (MLLMs). These new MLLMs allow us to
process images, videos and even audio alongside textual inputs. In this
project, we aim to assess the effectiveness of MLLMs in analysing sports
videos, focusing mainly on tennis videos. Despite research done on tennis
analysis, there remains a gap in models that are able to understand and
identify the sequence of events in a tennis rally, which would be useful in
other fields of sports analytics. As such, we will mainly assess the MLLMs on
their ability to fill this gap - to classify tennis actions, as well as their
ability to identify these actions in a sequence of tennis actions in a rally.
We further looked into ways we can improve the MLLMs' performance, including
different training methods and even using them together with other traditional
models.

</details>


### [5] [Enhancing Sports Strategy with Video Analytics and Data Mining: Automated Video-Based Analytics Framework for Tennis Doubles](https://arxiv.org/abs/2507.02906)
*Jia Wei Chen*

Main category: cs.CV

TL;DR: A video-based analytics framework for tennis doubles using machine learning to automate player positioning, shot types, and formations, reducing manual effort and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses the lack of automated tools for analyzing the strategic complexity of tennis doubles.

Method: Combines GroundingDINO for player localization and YOLO-Pose for pose estimation, with CNN models for predicting shot types and formations.

Result: CNN-based models outperform pose-based methods, capturing essential visual and contextual features for doubles tennis analysis.

Conclusion: The framework enables automated tactical analysis and performance evaluation in professional tennis doubles.

Abstract: We present a comprehensive video-based analytics framework for tennis doubles
that addresses the lack of automated analysis tools for this strategically
complex sport. Our approach introduces a standardised annotation methodology
encompassing player positioning, shot types, court formations, and match
outcomes, coupled with a specialised annotation tool designed to meet the
unique requirements of tennis video labelling. The framework integrates
advanced machine learning techniques including GroundingDINO for precise player
localisation through natural language grounding and YOLO-Pose for robust pose
estimation. This combination significantly reduces manual annotation effort
whilst improving data consistency and quality. We evaluate our approach on
doubles tennis match data and demonstrate that CNN-based models with transfer
learning substantially outperform pose-based methods for predicting shot types,
player positioning, and formations. The CNN models effectively capture complex
visual and contextual features essential for doubles tennis analysis. Our
integrated system bridges advanced analytical capabilities with the strategic
complexities of tennis doubles, providing a foundation for automated tactical
analysis, performance evaluation, and strategic modelling in professional
tennis.

</details>


### [6] [Modeling Urban Food Insecurity with Google Street View Images](https://arxiv.org/abs/2507.02924)
*David Li*

Main category: cs.CV

TL;DR: The paper explores using street-level images to model food insecurity at the census tract level, proposing a two-step feature extraction and gated attention method. While predictive power is slightly limited, the approach shows potential for supplementing existing methods.


<details>
  <summary>Details</summary>
Motivation: Food insecurity is a critical issue, but current survey-based methods are hard to scale. The study aims to leverage street-level images for scalable identification.

Method: A two-step process involving feature extraction and gated attention for image aggregation, evaluated against other models and case studies.

Result: The model's predictive power is slightly below expectations, but it offers interpretable insights and potential for practical use.

Conclusion: The approach can supplement existing food insecurity identification methods, aiding urban planners and policymakers.

Abstract: Food insecurity is a significant social and public health issue that plagues
many urban metropolitan areas around the world. Existing approaches to
identifying food insecurity rely primarily on qualitative and quantitative
survey data, which is difficult to scale. This project seeks to explore the
effectiveness of using street-level images in modeling food insecurity at the
census tract level. To do so, we propose a two-step process of feature
extraction and gated attention for image aggregation. We evaluate the
effectiveness of our model by comparing against other model architectures,
interpreting our learned weights, and performing a case study. While our model
falls slightly short in terms of its predictive power, we believe our approach
still has the potential to supplement existing methods of identifying food
insecurity for urban planners and policymakers.

</details>


### [7] [OBSER: Object-Based Sub-Environment Recognition for Zero-Shot Environmental Inference](https://arxiv.org/abs/2507.02929)
*Won-Seok Choi,Dong-Sig Han,Suhyung Choi,Hyeonseo Yang,Byoung-Tak Zhang*

Main category: cs.CV

TL;DR: The OBSER framework uses Bayesian methods to infer relationships between sub-environments and objects, validated by the EDS function, and outperforms scene-based methods in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: To achieve autonomous environment understanding by recognizing sub-environments and their object relationships.

Method: Combines metric and self-supervised learning to estimate object distributions on latent space, using the EDS function for validation.

Result: Reliable inference in open-world and photorealistic environments, outperforming scene-based methods in retrieval tasks.

Conclusion: OBSER enables zero-shot environment recognition, advancing autonomous environment understanding.

Abstract: We present the Object-Based Sub-Environment Recognition (OBSER) framework, a
novel Bayesian framework that infers three fundamental relationships between
sub-environments and their constituent objects. In the OBSER framework, metric
and self-supervised learning models estimate the object distributions of
sub-environments on the latent space to compute these measures. Both
theoretically and empirically, we validate the proposed framework by
introducing the ($\epsilon,\delta$) statistically separable (EDS) function
which indicates the alignment of the representation. Our framework reliably
performs inference in open-world and photorealistic environments and
outperforms scene-based methods in chained retrieval tasks. The OBSER framework
enables zero-shot recognition of environments to achieve autonomous environment
understanding.

</details>


### [8] [GameTileNet: A Semantic Dataset for Low-Resolution Game Art in Procedural Content Generation](https://arxiv.org/abs/2507.02941)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.CV

TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles designed to support narrative-driven procedural content generation through visual-language alignment.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of aligning AI-generated visuals with game narratives and improving diversity in procedural content generation due to imbalanced training data.

Method: Collects artist-created game tiles from OpenGameArt.org, provides semantic annotations, and introduces a pipeline for object detection in low-resolution tile-based game art.

Result: A dataset supporting narrative-driven content generation and establishing a baseline for object detection in low-resolution, non-photorealistic images.

Conclusion: GameTileNet advances procedural content generation and AI research by addressing visual-language alignment and improving diversity in game content.

Abstract: GameTileNet is a dataset designed to provide semantic labels for
low-resolution digital game art, advancing procedural content generation (PCG)
and related AI research as a vision-language alignment task. Large Language
Models (LLMs) and image-generative AI models have enabled indie developers to
create visual assets, such as sprites, for game interactions. However,
generating visuals that align with game narratives remains challenging due to
inconsistent AI outputs, requiring manual adjustments by human artists. The
diversity of visual representations in automatically generated game content is
also limited because of the imbalance in distributions across styles for
training data. GameTileNet addresses this by collecting artist-created game
tiles from OpenGameArt.org under Creative Commons licenses and providing
semantic annotations to support narrative-driven content generation. The
dataset introduces a pipeline for object detection in low-resolution tile-based
game art (e.g., 32x32 pixels) and annotates semantics, connectivity, and object
classifications. GameTileNet is a valuable resource for improving PCG methods,
supporting narrative-rich game content, and establishing a baseline for object
detection in low-resolution, non-photorealistic images.
  TL;DR: GameTileNet is a semantic dataset of low-resolution game tiles
designed to support narrative-driven procedural content generation through
visual-language alignment.

</details>


### [9] [Iterative Zoom-In: Temporal Interval Exploration for Long Video Understanding](https://arxiv.org/abs/2507.02946)
*Chenglin Li,Qianglong Chen,fengtao,Yin Zhang*

Main category: cs.CV

TL;DR: Temporal Search (TS) is a training-free framework for Multimodal Large Language Models (MLLMs) to improve long-form video understanding by iteratively refining temporal focus and sampling frames based on confidence scores.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs inefficiently handle long videos due to uniform temporal sampling, risking missed crucial information and high memory use. Humans dynamically adjust focus, inspiring a need for similar adaptability in models.

Method: TS iteratively proposes temporal intervals, samples frames, and refines responses using confidence scores. TS-BFS, a best-first search strategy, expands candidate intervals via self-driven proposals and uniform partitioning, selecting nodes based on confidence.

Result: TS improves MLLMs' long video understanding by dynamically focusing on relevant temporal intervals, reducing memory use, and avoiding information loss.

Conclusion: Temporal Search enhances MLLMs' efficiency and accuracy in long-form video tasks by mimicking human-like dynamic temporal focus, validated by TS-BFS for optimal interval exploration.

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
video understanding tasks. However, they continue to struggle with long-form
videos because of an inefficient perception of temporal intervals. Unlike
humans, who can dynamically adjust their temporal focus to locate
query-relevant moments, current MLLMs often rely on dense, uniform sampling
across the video timeline, leading to high memory consumption and a risk of
missing crucial information. To address this challenge, we introduce Temporal
Search, a training-free framework that enables MLLMs to explore temporal
regions for improved long video understanding iteratively. TS is based on a key
observation: the model's generation confidence across different temporal
intervals is highly correlated with prediction accuracy. TS operates through
two main iterative stages. First, the MLLM proposes a temporal interval that is
likely to contain task-relevant information. Then, it samples a fixed number of
frames from the interval, regardless of length, and feeds them into the model
to produce a refined response and confidence score. TS refines the focus of the
model by iteratively shifting attention to more fine-grained temporal
intervals, improving its understanding of long videos. Additionally,
keyframe-level descriptions are collected to facilitate cross-interval
perception throughout the video. To further improve efficiency, we introduce
TS-BFS, a best-first search strategy over a tree. Each node represents a
candidate interval and is expanded via two methods: self-driven proposals and
uniform partitioning. Nodes are scored based on confidence and self-evaluation,
and the most promising one is selected for continued exploration.

</details>


### [10] [DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction](https://arxiv.org/abs/2507.02948)
*Zhiyi Hou,Enhui Ma,Fang Li,Zhiyi Lai,Kalok Ho,Zhanqian Wu,Lijun Zhou,Long Chen,Chitian Sun,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Kaicheng Yu*

Main category: cs.CV

TL;DR: The paper proposes a method to enhance motion risk prediction in autonomous driving by synthesizing high-risk motion data and introducing a VLM-agnostic framework, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Accurate motion risk prediction in long-tail scenarios is challenging due to environmental uncertainties and limited data coverage. The work explores enhancing Vision-Language Models (VLMs) with synthesized high-risk data.

Method: A BEV-based motion simulation method synthesizes high-risk data (DriveMRP-10K). A VLM-agnostic framework (DriveMRP-Agent) integrates global context, ego-vehicle perspective, and trajectory projection for improved reasoning.

Result: Fine-tuning with DriveMRP-10K boosts accident recognition accuracy from 27.13% to 88.03%. Zero-shot evaluation on real-world data improves accuracy from 29.42% to 68.50%.

Conclusion: The method significantly enhances VLM performance in motion risk prediction and generalizes well to real-world scenarios.

Abstract: Autonomous driving has seen significant progress, driven by extensive
real-world data. However, in long-tail scenarios, accurately predicting the
safety of the ego vehicle's future motion remains a major challenge due to
uncertainties in dynamic environments and limitations in data coverage. In this
work, we aim to explore whether it is possible to enhance the motion risk
prediction capabilities of Vision-Language Models (VLM) by synthesizing
high-risk motion data. Specifically, we introduce a Bird's-Eye View (BEV) based
motion simulation method to model risks from three aspects: the ego-vehicle,
other vehicles, and the environment. This allows us to synthesize
plug-and-play, high-risk motion data suitable for VLM training, which we call
DriveMRP-10K. Furthermore, we design a VLM-agnostic motion risk estimation
framework, named DriveMRP-Agent. This framework incorporates a novel
information injection strategy for global context, ego-vehicle perspective, and
trajectory projection, enabling VLMs to effectively reason about the spatial
relationships between motion waypoints and the environment. Extensive
experiments demonstrate that by fine-tuning with DriveMRP-10K, our
DriveMRP-Agent framework can significantly improve the motion risk prediction
performance of multiple VLM baselines, with the accident recognition accuracy
soaring from 27.13% to 88.03%. Moreover, when tested via zero-shot evaluation
on an in-house real-world high-risk motion dataset, DriveMRP-Agent achieves a
significant performance leap, boosting the accuracy from base_model's 29.42% to
68.50%, which showcases the strong generalization capabilities of our method in
real-world scenarios.

</details>


### [11] [Multimodal image registration for effective thermographic fever screening](https://arxiv.org/abs/2507.02955)
*C. Y. N. Dwith,Pejhman Ghassemi,Joshua Pfefer,Jon Casamento,Quanzeng Wang*

Main category: cs.CV

TL;DR: The paper proposes a method for accurate fever screening using infrared thermographs (IRTs) by localizing canthi regions through multi-modal registration of IR and white-light images.


<details>
  <summary>Details</summary>
Motivation: Fever screening via IRTs is crucial during pandemics for non-invasive, quick temperature monitoring in public spaces. Accurate localization of canthi regions improves screening reliability.

Method: A coarse-fine registration strategy using landmarks and edge detection on eye contours for multi-modal registration of IR and white-light images.

Result: Registration accuracy achieved within 2.7 mm, enabling precise canthi region localization.

Conclusion: The proposed method enhances fever screening accuracy, making IRTs more reliable for public health monitoring.

Abstract: Fever screening based on infrared thermographs (IRTs) is a viable mass
screening approach during infectious disease pandemics, such as Ebola and SARS,
for temperature monitoring in public places like hospitals and airports. IRTs
have found to be powerful, quick and non-invasive methods to detect elevated
temperatures. Moreover, regions medially adjacent to the inner canthi (called
the canthi regions in this paper) are preferred sites for fever screening.
Accurate localization of the canthi regions can be achieved through multi-modal
registration of infrared (IR) and white-light images. We proposed a
registration method through a coarse-fine registration strategy using different
registration models based on landmarks and edge detection on eye contours. We
evaluated the registration accuracy to be within 2.7 mm, which enables accurate
localization of the canthi regions.

</details>


### [12] [CS-VLM: Compressed Sensing Attention for Efficient Vision-Language Representation Learning](https://arxiv.org/abs/2507.02957)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.CV

TL;DR: CSAT introduces compressed sensing to reduce attention complexity in Vision-Language Models (vLLMs), maintaining performance while lowering computational costs.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of standard attention in vLLMs creates computational bottlenecks, especially for longer sequences and cross-modal tasks.

Method: CSAT projects high-dimensional key/value representations into a lower-dimensional subspace using random matrices and reconstructs attention outputs with sparse recovery.

Result: CSAT reduces attention complexity and resource usage while preserving semantic fidelity, validated on standard benchmarks.

Conclusion: CSAT offers a scalable, efficient, and interpretable solution for next-generation multimodal transformers.

Abstract: Vision-Language Models (vLLMs) have emerged as powerful architectures for
joint reasoning over visual and textual inputs, enabling breakthroughs in image
captioning, cross modal retrieval, and multimodal dialogue. However, as these
models scale to longer video sequences and richer language descriptions, the
quadratic complexity of the standard attention mechanism presents a fundamental
computational bottleneck. This challenge is exacerbated in vLLMs, where
attention must be computed not only within modalities but also across them,
leading to prohibitive memory and latency costs. In this work, we introduce the
Compressed Sensing Attention Transformer (CSAT), a novel architecture that
reimagines attention computation through the lens of compressed sensing. By
projecting high dimensional key and value representations into a
lower-dimensional subspace via random measurement matrices and reconstructing
the attention outputs using sparse recovery algorithms, CSAT significantly
reduces attention complexity while maintaining semantic fidelity. Applied to
vLLMs, CSAT exploits the inherent compressibility of both visual and textual
representations especially evident in video, where temporal redundancy is high,
and in language, where cross-modal grounding is often sparse. In contrast to
LLMs, which must often model entangled symbolic dependencies, vLLMs benefit
from structured sparsity in alignment and scene composition, making them
particularly well-suited to compressed attention. We provide a formal
mathematical treatment of CSAT, demonstrate its integration into vision
language pipelines, and validate its performance on standard benchmarks,
highlighting its promise as a scalable, interpretable, and resource efficient
solution for next generation multimodal transformers.

</details>


### [13] [VR-YOLO: Enhancing PCB Defect Detection with Viewpoint Robustness Based on YOLO](https://arxiv.org/abs/2507.02963)
*Hengyi Zhu,Linye Wei,He Li*

Main category: cs.CV

TL;DR: The paper introduces VR-YOLO, an enhanced PCB defect detection algorithm based on YOLOv8, improving generalization and viewpoint robustness through diversified scene enhancement and a key object focus scheme.


<details>
  <summary>Details</summary>
Motivation: Automated defect detection in electronics is crucial, but conventional methods struggle with angle, orientation, and clarity constraints.

Method: Proposes VR-YOLO with diversified scene enhancement (DSE) for dataset expansion and key object focus (KOF) for fine-grained feature learning.

Result: Achieves 98.9% mAP on original images and 94.7% on viewpoint-shifted images, outperforming baseline YOLO.

Conclusion: VR-YOLO significantly improves defect detection robustness with minimal computational overhead.

Abstract: The integration of large-scale circuits and systems emphasizes the importance
of automated defect detection of electronic components. The YOLO image
detection model has been used to detect PCB defects and it has become a typical
AI-assisted case of traditional industrial production. However, conventional
detection algorithms have stringent requirements for the angle, orientation,
and clarity of target images. In this paper, we propose an enhanced PCB defect
detection algorithm, named VR-YOLO, based on the YOLOv8 model. This algorithm
aims to improve the model's generalization performance and enhance viewpoint
robustness in practical application scenarios. We first propose a diversified
scene enhancement (DSE) method by expanding the PCB defect dataset by
incorporating diverse scenarios and segmenting samples to improve target
diversity. A novel key object focus (KOF) scheme is then presented by
considering angular loss and introducing an additional attention mechanism to
enhance fine-grained learning of small target features. Experimental results
demonstrate that our improved PCB defect detection approach achieves a mean
average precision (mAP) of 98.9% for the original test images, and 94.7% for
the test images with viewpoint shifts (horizontal and vertical shear
coefficients of $\pm 0.06$ and rotation angle of $\pm 10$ degrees), showing
significant improvements compared to the baseline YOLO model with negligible
additional computational cost.

</details>


### [14] [Concept-based Adversarial Attack: a Probabilistic Perspective](https://arxiv.org/abs/2507.02965)
*Andi Zhang,Xuan Ding,Steven McDonagh,Samuel Kaski*

Main category: cs.CV

TL;DR: A concept-based adversarial attack framework that generates diverse adversarial examples by modifying entire concepts, not just single images, while preserving the original concept.


<details>
  <summary>Details</summary>
Motivation: Extend adversarial attacks beyond single-image perturbations to operate on entire concepts, ensuring adversarial examples remain identifiable as instances of the original category.

Method: Uses a probabilistic generative model or image set to sample diverse adversarial examples, maintaining the concept but varying attributes like pose or background.

Result: Produces more diverse adversarial examples, preserves the underlying concept, and achieves higher attack efficiency.

Conclusion: Concept-based adversarial attacks are effective, diverse, and efficient, aligning with traditional adversarial attack principles.

Abstract: We propose a concept-based adversarial attack framework that extends beyond
single-image perturbations by adopting a probabilistic perspective. Rather than
modifying a single image, our method operates on an entire concept --
represented by a probabilistic generative model or a set of images -- to
generate diverse adversarial examples. Preserving the concept is essential, as
it ensures that the resulting adversarial images remain identifiable as
instances of the original underlying category or identity. By sampling from
this concept-based adversarial distribution, we generate images that maintain
the original concept but vary in pose, viewpoint, or background, thereby
misleading the classifier. Mathematically, this framework remains consistent
with traditional adversarial attacks in a principled manner. Our theoretical
and empirical results demonstrate that concept-based adversarial attacks yield
more diverse adversarial examples and effectively preserve the underlying
concept, while achieving higher attack efficiency.

</details>


### [15] [YOLO-Based Pipeline Monitoring in Challenging Visual Environments](https://arxiv.org/abs/2507.02967)
*Pragya Dhungana,Matteo Fresta,Niraj Tamrakar,Hariom Dhungana*

Main category: cs.CV

TL;DR: The study compares YOLOv8 and YOLOv11 for subsea pipeline inspection in low-visibility conditions, finding YOLOv11 superior.


<details>
  <summary>Details</summary>
Motivation: Challenges in subsea pipeline monitoring due to turbidity and image degradation drive the need for advanced AI solutions.

Method: Comparative analysis of YOLOv8 and YOLOv11 variants for image segmentation in low-visibility underwater environments.

Result: YOLOv11 outperformed YOLOv8 in delineating pipeline structures under challenging conditions.

Conclusion: YOLOv11 is more effective for subsea pipeline inspection in low-visibility environments.

Abstract: Condition monitoring subsea pipelines in low-visibility underwater
environments poses significant challenges due to turbidity, light distortion,
and image degradation. Traditional visual-based inspection systems often fail
to provide reliable data for mapping, object recognition, or defect detection
in such conditions. This study explores the integration of advanced artificial
intelligence (AI) techniques to enhance image quality, detect pipeline
structures, and support autonomous fault diagnosis. This study conducts a
comparative analysis of two most robust versions of YOLOv8 and Yolov11 and
their three variants tailored for image segmentation tasks in complex and
low-visibility subsea environments. Using pipeline inspection datasets captured
beneath the seabed, it evaluates model performance in accurately delineating
target structures under challenging visual conditions. The results indicated
that YOLOv11 outperformed YOLOv8 in overall performance.

</details>


### [16] [Farm-Level, In-Season Crop Identification for India](https://arxiv.org/abs/2507.02972)
*Ishan Deshpande,Amandeep Kaur Reehal,Chandan Nath,Renu Singh,Aayush Patel,Aishwarya Jayagopal,Gaurav Singh,Gaurav Aggarwal,Amit Agarwal,Prathmesh Bele,Sridhar Reddy,Tanya Warrier,Kinjal Singh,Ashish Tendulkar,Luis Pazos Outon,Nikita Saxena,Agata Dondzik,Dinesh Tewari,Shruti Garg,Avneet Singh,Harsh Dhand,Vaibhav Rajan,Alok Talekar*

Main category: cs.CV

TL;DR: A framework using deep learning and satellite data for in-season, farm-level crop identification in India, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in crop monitoring for food security and policy-making, especially in agriculturally significant regions like India.

Method: Leverages Sentinel-1 and Sentinel-2 satellite imagery with farm boundary data, incorporating an automated season detection algorithm.

Result: Identifies 12 major crops with 94% agreement in winter and 75% in monsoon, validated against national statistics.

Conclusion: The scalable system provides actionable insights for agricultural monitoring and management in India.

Abstract: Accurate, timely, and farm-level crop type information is paramount for
national food security, agricultural policy formulation, and economic planning,
particularly in agriculturally significant nations like India. While remote
sensing and machine learning have become vital tools for crop monitoring,
existing approaches often grapple with challenges such as limited geographical
scalability, restricted crop type coverage, the complexities of mixed-pixel and
heterogeneous landscapes, and crucially, the robust in-season identification
essential for proactive decision-making.
  We present a framework designed to address the critical data gaps for
targeted data driven decision making which generates farm-level, in-season,
multi-crop identification at national scale (India) using deep learning. Our
methodology leverages the strengths of Sentinel-1 and Sentinel-2 satellite
imagery, integrated with national-scale farm boundary data. The model
successfully identifies 12 major crops (which collectively account for nearly
90% of India's total cultivated area showing an agreement with national crop
census 2023-24 of 94% in winter, and 75% in monsoon season). Our approach
incorporates an automated season detection algorithm, which estimates crop
sowing and harvest periods. This allows for reliable crop identification as
early as two months into the growing season and facilitates rigorous in-season
performance evaluation. Furthermore, we have engineered a highly scalable
inference pipeline, culminating in what is, to our knowledge, the first
pan-India, in-season, farm-level crop type data product. The system's
effectiveness and scalability are demonstrated through robust validation
against national agricultural statistics, showcasing its potential to deliver
actionable, data-driven insights for transformative agricultural monitoring and
management across India.

</details>


### [17] [Mimesis, Poiesis, and Imagination: Exploring Text-to-Image Generation of Biblical Narratives](https://arxiv.org/abs/2507.02973)
*Willem Th. van Peursen,Samuel E. Entsua-Mensah*

Main category: cs.CV

TL;DR: AI-generated images of Exodus 2:5-9 are analyzed using MidJourney, comparing them to classical art and Google images. AI excels in aesthetics but shows biases, raising questions about creativity and theological depth.


<details>
  <summary>Details</summary>
Motivation: To explore how AI reproduces or reimagines sacred narratives, focusing on mimesis and poiesis in text-to-image models.

Method: Comparative visual analysis of AI-generated images, Google results, and classical paintings to evaluate stylistic, theological, and cultural dimensions.

Result: AI produces rich visuals but reflects training biases, excelling in aesthetics but lacking in genuine creativity and theological depth.

Conclusion: AI can aid in reinterpreting biblical texts as a creative partner, but its role in sacred art is complex and contested.

Abstract: This study explores the intersection of artificial intelligence and the
visualization of Biblical narratives by analyzing AI-generated images of Exodus
2:5-9 (Moses found in River Nile) using MidJourney. Drawing on the classical
concepts of mimesis (imitation) and poiesis (creative generation), the authors
investigate how text-to-image (T2I) models reproduce or reimagine sacred
narratives. Through comparative visual analysis, including Google image results
and classical paintings, the research evaluates the stylistic, theological, and
cultural dimensions of AI-generated depictions. Findings show that while AI
excels in producing aesthetically rich and imaginative visuals, it also
reflects the biases and limitations of its training data. The study highlights
AI's potential to augment human imagination but questions its capacity for
genuine creativity, authorial intent, and theological depth. It concludes by
suggesting that AI can serve as a creative partner in reinterpreting biblical
texts, though its role in sacred art remains complex and contested.

</details>


### [18] [Ascending the Infinite Ladder: Benchmarking Spatial Deformation Reasoning in Vision-Language Models](https://arxiv.org/abs/2507.02978)
*Jiahuan Zhang,Shunwen Bai,Tianheng Wang,Kaiwen Guo,Kai Han,Guozheng Rao,Kaicheng Yu*

Main category: cs.CV

TL;DR: A new framework evaluates Vision-Language Models (VLMs) for spatial deformation reasoning, revealing their poor performance even after enhancements.


<details>
  <summary>Details</summary>
Motivation: To determine if VLMs truly understand spatial objects by assessing their spatial deformation reasoning abilities.

Method: Proposed a benchmark for 2D-to-3D spatial deformation reasoning, using forward and reverse reasoning tasks with a ladder competition format.

Result: Almost no models showed plausible spatial deformation reasoning, even after targeted training and reasoning enhancements.

Conclusion: Current VLMs lack effective spatial deformation reasoning capabilities, highlighting a significant gap in their understanding of spatial objects.

Abstract: Humans naturally possess the spatial reasoning ability to form and manipulate
images and structures of objects in space. There is an increasing effort to
endow Vision-Language Models (VLMs) with similar spatial reasoning
capabilities. However, it remains unclear whether these models truly understand
and manipulate spatial objects or not. To address this question, we propose a
new evaluation framework aimed at assessing the performance of VLMs in spatial
deformation reasoning tasks. Specifically, we construct a benchmark for spatial
deformation reasoning from 2D to 3D. Leveraging our data engine, we can
generate unlimited evaluation problem pairs with infinite steps, without any
data leakage. We explore whether the model can effectively perform spatial
deformation reasoning from two directions: forward reasoning (given the
operations, find the final state) and reverse reasoning (given the final state,
determine the operations). We adopt a ladder competition format, using the
number of deformation steps as the level classification criterion, with the
goal of exploring the boundaries of the model's deformation reasoning
capabilities. Interestingly, the benchmarking results reveal that almost no
model demonstrates plausible spatial deformation reasoning abilities.
Furthermore, even after applying targeted training and mainstream reasoning
enhancement methods, the models are still unable to perform well on 3D spatial
deformation reasoning.

</details>


### [19] [Iterative Misclassification Error Training (IMET): An Optimized Neural Network Training Technique for Image Classification](https://arxiv.org/abs/2507.02979)
*Ruhaan Singh,Sreelekha Guggilam*

Main category: cs.CV

TL;DR: The paper introduces IMET, a framework combining curriculum learning and coreset selection to improve medical image classification by focusing on misclassified samples and edge cases.


<details>
  <summary>Details</summary>
Motivation: Medical datasets often have noisy or small samples, risking overfitting and misdiagnosis. Existing methods like coreset selection and curriculum learning have limitations in generalizability and computational efficiency.

Method: IMET identifies misclassified samples to streamline training, prioritizing edge cases and rare outcomes, evaluated on benchmark datasets with ResNet architectures.

Result: IMET enhances model robustness and accuracy in medical image analysis compared to state-of-the-art methods.

Conclusion: IMET offers a promising approach to address challenges in medical image classification, improving accuracy and generalizability.

Abstract: Deep learning models have proven to be effective on medical datasets for
accurate diagnostic predictions from images. However, medical datasets often
contain noisy, mislabeled, or poorly generalizable images, particularly for
edge cases and anomalous outcomes. Additionally, high quality datasets are
often small in sample size that can result in overfitting, where models
memorize noise rather than learn generalizable patterns. This in particular,
could pose serious risks in medical diagnostics where the risk associated with
mis-classification can impact human life. Several data-efficient training
strategies have emerged to address these constraints. In particular, coreset
selection identifies compact subsets of the most representative samples,
enabling training that approximates full-dataset performance while reducing
computational overhead. On the other hand, curriculum learning relies on
gradually increasing training difficulty and accelerating convergence. However,
developing a generalizable difficulty ranking mechanism that works across
diverse domains, datasets, and models while reducing the computational tasks
and remains challenging. In this paper, we introduce Iterative
Misclassification Error Training (IMET), a novel framework inspired by
curriculum learning and coreset selection. The IMET approach is aimed to
identify misclassified samples in order to streamline the training process,
while prioritizing the model's attention to edge case senarious and rare
outcomes. The paper evaluates IMET's performance on benchmark medical image
classification datasets against state-of-the-art ResNet architectures. The
results demonstrating IMET's potential for enhancing model robustness and
accuracy in medical image analysis are also presented in the paper.

</details>


### [20] [Gated Recursive Fusion: A Stateful Approach to Scalable Multimodal Transformers](https://arxiv.org/abs/2507.02985)
*Yusuf Shihata*

Main category: cs.CV

TL;DR: GRF introduces a linearly scalable, recurrent architecture for multimodal learning, balancing fine-grained fusion and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the tension between deep fusion and scalability in multimodal learning, where cross-attention models are powerful but computationally expensive.

Method: Uses Gated Recurrent Fusion (GRF), a recurrent pipeline with Transformer Decoder layers for symmetric cross-attention and a Gated Fusion Unit (GFU) for dynamic information flow.

Result: GRF achieves competitive performance on CMU-MOSI benchmark and creates structured, class-separable representations.

Conclusion: GRF provides an efficient and scalable solution for multimodal learning, suitable for high-modality environments.

Abstract: Multimodal learning faces a fundamental tension between deep, fine-grained
fusion and computational scalability. While cross-attention models achieve
strong performance through exhaustive pairwise fusion, their quadratic
complexity is prohibitive for settings with many modalities. We address this
challenge with Gated Recurrent Fusion (GRF), a novel architecture that captures
the power of cross-modal attention within a linearly scalable, recurrent
pipeline. Our method processes modalities sequentially, updating an evolving
multimodal context vector at each step. The core of our approach is a fusion
block built on Transformer Decoder layers that performs symmetric
cross-attention, mutually enriching the shared context and the incoming
modality. This enriched information is then integrated via a Gated Fusion Unit
(GFU) a GRU-inspired mechanism that dynamically arbitrates information flow,
enabling the model to selectively retain or discard features. This stateful,
recurrent design scales linearly with the number of modalities, O(n), making it
ideal for high-modality environments. Experiments on the CMU-MOSI benchmark
demonstrate that GRF achieves competitive performance compared to more complex
baselines. Visualizations of the embedding space further illustrate that GRF
creates structured, class-separable representations through its progressive
fusion mechanism. Our work presents a robust and efficient paradigm for
powerful, scalable multimodal representation learning.

</details>


### [21] [Leveraging the Structure of Medical Data for Improved Representation Learning](https://arxiv.org/abs/2507.02987)
*Andrea Agostini,Sonia Laguna,Alain Ryser,Samuel Ruiperez-Campillo,Moritz Vandenhirtz,Nicolas Deperrois,Farhad Nooralahzadeh,Michael Krauthammer,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.CV

TL;DR: A self-supervised framework leverages multi-view chest X-rays for pretraining medical AI, requiring no textual supervision and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of data-efficient and domain-aware pretraining for medical AI, given limited clinical datasets like MIMIC-CXR.

Method: Uses paired chest X-rays (frontal and lateral views) as positive pairs, learning to reconstruct views from sparse patches and aligning latent embeddings.

Result: Demonstrates strong performance on MIMIC-CXR compared to supervised methods and baselines ignoring structure.

Conclusion: Provides a lightweight, modality-agnostic approach for domain-specific pretraining in structured but scarce data settings.

Abstract: Building generalizable medical AI systems requires pretraining strategies
that are data-efficient and domain-aware. Unlike internet-scale corpora,
clinical datasets such as MIMIC-CXR offer limited image counts and scarce
annotations, but exhibit rich internal structure through multi-view imaging. We
propose a self-supervised framework that leverages the inherent structure of
medical datasets. Specifically, we treat paired chest X-rays (i.e., frontal and
lateral views) as natural positive pairs, learning to reconstruct each view
from sparse patches while aligning their latent embeddings. Our method requires
no textual supervision and produces informative representations. Evaluated on
MIMIC-CXR, we show strong performance compared to supervised objectives and
baselines being trained without leveraging structure. This work provides a
lightweight, modality-agnostic blueprint for domain-specific pretraining where
data is structured but scarce

</details>


### [22] [Enabling Robust, Real-Time Verification of Vision-Based Navigation through View Synthesis](https://arxiv.org/abs/2507.02993)
*Marius Neuhalfen,Jonathan Grzymisch,Manuel Sanchez-Gestido*

Main category: cs.CV

TL;DR: VISY-REVE is a pipeline for validating vision-based navigation algorithms by augmenting image datasets in real-time with synthesized views, avoiding traditional slow methods.


<details>
  <summary>Details</summary>
Motivation: Traditional validation methods like synthetic rendering or robotic testbeds are cumbersome and slow, prompting the need for a more efficient solution.

Method: The pipeline augments sparse image datasets with synthesized views at novel poses in real-time, creating continuous trajectories. It introduces the Boresight Deviation Distance metric for better view synthesis.

Result: The approach enables efficient validation and increases dataset density using the new metric.

Conclusion: VISY-REVE offers a faster, more flexible alternative to traditional validation methods for vision-based navigation.

Abstract: This work introduces VISY-REVE: a novel pipeline to validate image processing
algorithms for Vision-Based Navigation. Traditional validation methods such as
synthetic rendering or robotic testbed acquisition suffer from difficult setup
and slow runtime. Instead, we propose augmenting image datasets in real-time
with synthesized views at novel poses. This approach creates continuous
trajectories from sparse, pre-existing datasets in open or closed-loop. In
addition, we introduce a new distance metric between camera poses, the
Boresight Deviation Distance, which is better suited for view synthesis than
existing metrics. Using it, a method for increasing the density of image
datasets is developed.

</details>


### [23] [FreqCross: A Multi-Modal Frequency-Spatial Fusion Network for Robust Detection of Stable Diffusion 3.5 Generated Images](https://arxiv.org/abs/2507.02995)
*Guang Yang*

Main category: cs.CV

TL;DR: FreqCross, a multi-modal fusion network, detects AI-generated images by combining spatial, frequency, and radial energy features, achieving 97.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to detect photorealistic synthetic images from advanced diffusion models like Stable Diffusion 3.5.

Method: Uses a three-branch architecture: ResNet-18 for spatial features, a CNN for 2D FFT spectra, and an MLP for radial energy analysis. Features are fused via concatenation.

Result: Achieves 97.8% accuracy, outperforming baselines by 5.2%, and identifies distinct spectral signatures in synthetic images.

Conclusion: FreqCross provides robust detection of AI-generated images with theoretical and empirical validation, and promotes reproducibility with public code.

Abstract: The rapid advancement of diffusion models, particularly Stable Diffusion 3.5,
has enabled the generation of highly photorealistic synthetic images that pose
significant challenges to existing detection methods. This paper presents
FreqCross, a novel multi-modal fusion network that combines spatial RGB
features, frequency domain artifacts, and radial energy distribution patterns
to achieve robust detection of AI-generated images. Our approach leverages a
three-branch architecture: (1) a ResNet-18 backbone for spatial feature
extraction, (2) a lightweight CNN for processing 2D FFT magnitude spectra, and
(3) a multi-layer perceptron for analyzing radial energy profiles. We introduce
a novel radial energy distribution analysis that captures characteristic
frequency artifacts inherent in diffusion-generated images, and fuse it with
spatial and spectral cues via simple feature concatenation followed by a
compact classification head. Extensive experiments on a dataset of 10,000
paired real (MS-COCO) and synthetic (Stable Diffusion 3.5) images demonstrate
that FreqCross achieves 97.8\% accuracy, outperforming state-of-the-art
baselines by 5.2\%. The frequency analysis further reveals that synthetic
images exhibit distinct spectral signatures in the 0.1--0.4 normalised
frequency range, providing theoretical foundation for our approach. Code and
pre-trained models are publicly available to facilitate reproducible research.

</details>


### [24] [Text-Guided Multi-Instance Learning for Scoliosis Screening via Gait Video Analysis](https://arxiv.org/abs/2507.02996)
*Haiqing Li,Yuzhi Guo,Feng Jiang,Thao M. Dang,Hehuan Ma,Qifeng Zhou,Jean Gao,Junzhou Huang*

Main category: cs.CV

TL;DR: TG-MILNet is a non-invasive method for early-stage scoliosis detection using gait videos, addressing challenges like temporal misalignment and borderline cases with DTW clustering, IBTA, BAM, and textual guidance.


<details>
  <summary>Details</summary>
Motivation: Early-stage scoliosis detection is challenging, especially in adolescents, with traditional methods like X-rays being invasive and expertise-dependent. A non-invasive, scalable solution is needed.

Method: TG-MILNet uses gait videos, DTW clustering for temporal alignment, IBTA for feature focus, BAM for borderline cases, and textual guidance for enhanced representation.

Result: TG-MILNet achieves state-of-the-art performance on the Scoliosis1K dataset, excelling in class imbalance and borderline case detection.

Conclusion: TG-MILNet offers a robust, non-invasive solution for scoliosis detection, leveraging advanced techniques and expert guidance for improved accuracy and interpretability.

Abstract: Early-stage scoliosis is often difficult to detect, particularly in
adolescents, where delayed diagnosis can lead to serious health issues.
Traditional X-ray-based methods carry radiation risks and rely heavily on
clinical expertise, limiting their use in large-scale screenings. To overcome
these challenges, we propose a Text-Guided Multi-Instance Learning Network
(TG-MILNet) for non-invasive scoliosis detection using gait videos. To handle
temporal misalignment in gait sequences, we employ Dynamic Time Warping (DTW)
clustering to segment videos into key gait phases. To focus on the most
relevant diagnostic features, we introduce an Inter-Bag Temporal Attention
(IBTA) mechanism that highlights critical gait phases. Recognizing the
difficulty in identifying borderline cases, we design a Boundary-Aware Model
(BAM) to improve sensitivity to subtle spinal deviations. Additionally, we
incorporate textual guidance from domain experts and large language models
(LLM) to enhance feature representation and improve model interpretability.
Experiments on the large-scale Scoliosis1K gait dataset show that TG-MILNet
achieves state-of-the-art performance, particularly excelling in handling class
imbalance and accurately detecting challenging borderline cases. The code is
available at https://github.com/lhqqq/TG-MILNet

</details>


### [25] [Topological Signatures vs. Gradient Histograms: A Comparative Study for Medical Image Classification](https://arxiv.org/abs/2507.03006)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: Comparative study of HOG and TDA for medical image classification on retinal fundus images, showing competitive performance with XGBoost.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of HOG (local texture) and TDA (global topology) for medical image classification tasks.

Method: Extracted 26244 HOG and 800 TDA features from retinal images, trained seven ML models using 10-fold cross-validation.

Result: XGBoost performed best: 94.29% (HOG) and 94.18% (TDA) for binary classification; 74.41% (HOG) and 74.69% (TDA) for multi-class.

Conclusion: Both methods are competitive but capture different image aspects; they are interpretable and suitable for integration into deep learning.

Abstract: We present the first comparative study of two fundamentally distinct feature
extraction techniques: Histogram of Oriented Gradients (HOG) and Topological
Data Analysis (TDA), for medical image classification using retinal fundus
images. HOG captures local texture and edge patterns through gradient
orientation histograms, while TDA, using cubical persistent homology, extracts
high-level topological signatures that reflect the global structure of pixel
intensities. We evaluate both methods on the large APTOS dataset for two
classification tasks: binary detection (normal versus diabetic retinopathy) and
five-class diabetic retinopathy severity grading. From each image, we extract
26244 HOG features and 800 TDA features, using them independently to train
seven classical machine learning models with 10-fold cross-validation. XGBoost
achieved the best performance in both cases: 94.29 percent accuracy (HOG) and
94.18 percent (TDA) on the binary task; 74.41 percent (HOG) and 74.69 percent
(TDA) on the multi-class task. Our results show that both methods offer
competitive performance but encode different structural aspects of the images.
This is the first work to benchmark gradient-based and topological features on
retinal imagery. The techniques are interpretable, applicable to other medical
imaging domains, and suitable for integration into deep learning pipelines.

</details>


### [26] [Markerless Stride Length estimation in Athletic using Pose Estimation with monocular vision](https://arxiv.org/abs/2507.03016)
*Patryk Skorupski,Cosimo Distante,Pier Luigi Mazzeo*

Main category: cs.CV

TL;DR: A computer vision-based method estimates stride length and speed from race videos using image processing and human pose detection, proving useful for athlete training.


<details>
  <summary>Details</summary>
Motivation: Monitoring individual athletic performance is crucial for tailored training, requiring accurate stride length and speed measurement.

Method: Combines probabilistic Hough transform and human pose detection to estimate leg joint positions, then uses homography transformation to calculate stride length.

Result: Tests on race videos with three runners showed the system's effectiveness for coaching and training.

Conclusion: The approach holds potential for monitoring gait parameters in athletes, aiding performance analysis.

Abstract: Performance measures such as stride length in athletics and the pace of
runners can be estimated using different tricks such as measuring the number of
steps divided by the running length or helping with markers printed on the
track. Monitoring individual performance is essential for supporting staff
coaches in establishing a proper training schedule for each athlete. The aim of
this paper is to investigate a computer vision-based approach for estimating
stride length and speed transition from video sequences and assessing video
analysis processing among athletes. Using some well-known image processing
methodologies such as probabilistic hough transform combined with a human pose
detection algorithm, we estimate the leg joint position of runners. In this
way, applying a homography transformation, we can estimate the runner stride
length. Experiments on various race videos with three different runners
demonstrated that the proposed system represents a useful tool for coaching and
training. This suggests its potential value in measuring and monitoring the
gait parameters of athletes.

</details>


### [27] [Look-Back: Implicit Visual Re-focusing in MLLM Reasoning](https://arxiv.org/abs/2507.03019)
*Shuo Yang,Yuwei Niu,Yuyang Liu,Yang Ye,Bin Lin,Li Yuan*

Main category: cs.CV

TL;DR: Look-Back is an implicit method that guides MLLMs to autonomously refocus on visual inputs during reasoning, enhancing multimodal performance without explicit constraints.


<details>
  <summary>Details</summary>
Motivation: MLLMs overly rely on text in later reasoning stages, neglecting visual input integration, despite its importance.

Method: Analyzed MLLM attention patterns, observed spontaneous visual refocusing, and introduced Look-Back to guide this process implicitly.

Result: Look-Back improves reasoning and perception, validated by empirical evaluations on multimodal benchmarks.

Conclusion: MLLMs intrinsically support visual fusion reasoning; Look-Back leverages this capability effectively.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in
multimodal reasoning. However, they often excessively rely on textual
information during the later stages of inference, neglecting the crucial
integration of visual input. Current methods typically address this by
explicitly injecting visual information to guide the reasoning process. In this
work, through an analysis of MLLM attention patterns, we made an intriguing
observation: with appropriate guidance, MLLMs can spontaneously re-focus their
attention on visual inputs during the later stages of reasoning, even without
explicit visual information injection. This spontaneous shift in focus suggests
that MLLMs are intrinsically capable of performing visual fusion reasoning.
Building on this insight, we introduce Look-Back, an implicit approach designed
to guide MLLMs to ``look back" at visual information in a self-directed manner
during reasoning. Look-Back empowers the model to autonomously determine when,
where, and how to re-focus on visual inputs, eliminating the need for explicit
model-structure constraints or additional input. We demonstrate that Look-Back
significantly enhances the model's reasoning and perception capabilities, as
evidenced by extensive empirical evaluations on multiple multimodal benchmarks.

</details>


### [28] [Intelligent Histology for Tumor Neurosurgery](https://arxiv.org/abs/2507.03037)
*Xinhai Hou,Akhil Kondepudi,Cheng Jiang,Yiwei Lyu,Samir Harake,Asadur Chowdury,Anna-Katharina Meiner,Volker Neuschmelting,David Reinecke,Gina Furtjes,Georg Widhalm,Lisa Irina Koerner,Jakob Straehle,Nicolas Neidert,Pierre Scheffler,Juergen Beck,Michael Ivan,Ashish Shah,Aditya Pandey,Sandra Camelo-Piragua,Dieter Henrik Heiland,Oliver Schnell,Chris Freudiger,Jacob Young,Melike Pekmezci,Katie Scotford,Shawn Hervey-Jumper,Daniel Orringer,Mitchel Berger,Todd Hollon*

Main category: cs.CV

TL;DR: Intelligent Histology combines AI and SRH for rapid, real-time tumor analysis in neurosurgery, improving accuracy and efficiency over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional intraoperative pathology workflows are slow, resource-heavy, and lack digital capabilities, necessitating innovation for real-time tumor analysis.

Method: Integrates AI with Stimulated Raman Histology (SRH), a label-free digital imaging technique, to enable rapid tumor histologic analysis and molecular classification.

Result: Demonstrates transformative potential in neurosurgical specialties, offering high-resolution imaging and AI-driven analysis within seconds.

Conclusion: Intelligent Histology is a groundbreaking workflow for 21st-century neurosurgery, with future potential in multimodal learning and outcome prediction.

Abstract: The importance of rapid and accurate histologic analysis of surgical tissue
in the operating room has been recognized for over a century. Our
standard-of-care intraoperative pathology workflow is based on light microscopy
and H\&E histology, which is slow, resource-intensive, and lacks real-time
digital imaging capabilities. Here, we present an emerging and innovative
method for intraoperative histologic analysis, called Intelligent Histology,
that integrates artificial intelligence (AI) with stimulated Raman histology
(SRH). SRH is a rapid, label-free, digital imaging method for real-time
microscopic tumor tissue analysis. SRH generates high-resolution digital images
of surgical specimens within seconds, enabling AI-driven tumor histologic
analysis, molecular classification, and tumor infiltration detection. We review
the scientific background, clinical translation, and future applications of
intelligent histology in tumor neurosurgery. We focus on the major scientific
and clinical studies that have demonstrated the transformative potential of
intelligent histology across multiple neurosurgical specialties, including
neurosurgical oncology, skull base, spine oncology, pediatric tumors, and
periperal nerve tumors. Future directions include the development of AI
foundation models through multi-institutional datasets, incorporating clinical
and radiologic data for multimodal learning, and predicting patient outcomes.
Intelligent histology represents a transformative intraoperative workflow that
can reinvent real-time tumor analysis for 21st century neurosurgery.

</details>


### [29] [Detection of Rail Line Track and Human Beings Near the Track to Avoid Accidents](https://arxiv.org/abs/2507.03040)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CV

TL;DR: A YOLOv5-based system detects rail lines and nearby humans in real-time to prevent accidents, improving accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance railway safety by identifying humans near tracks in real-time and preventing accidents.

Method: Uses YOLOv5 for real-time video analysis to detect rail lines and humans within a one-meter range, with extended distance detection capabilities.

Result: Demonstrates improved accuracy over existing methods, enabling effective real-time alerts for human presence near tracks.

Conclusion: The approach significantly advances railway safety, offering a robust solution for accident prevention.

Abstract: This paper presents an approach for rail line detection and the
identification of human beings in proximity to the track, utilizing the YOLOv5
deep learning model to mitigate potential accidents. The technique incorporates
real-time video data to identify railway tracks with impressive accuracy and
recognizes nearby moving objects within a one-meter range, specifically
targeting the identification of humans. This system aims to enhance safety
measures in railway environments by providing real-time alerts for any detected
human presence close to the track. The integration of a functionality to
identify objects at a longer distance further fortifies the preventative
capabilities of the system. With a precise focus on real-time object detection,
this method is poised to deliver significant contributions to the existing
technologies in railway safety. The effectiveness of the proposed method is
demonstrated through a comprehensive evaluation, yielding a remarkable
improvement in accuracy over existing methods. These results underscore the
potential of this approach to revolutionize safety measures in railway
environments, providing a substantial contribution to accident prevention
strategies.

</details>


### [30] [LATTE: Latent Trajectory Embedding for Diffusion-Generated Image Detection](https://arxiv.org/abs/2507.03054)
*Ana Vasilcoiu,Ivona Najdenkoska,Zeno Geradts,Marcel Worring*

Main category: cs.CV

TL;DR: LATTE introduces a novel method for detecting generated images by modeling latent trajectory embeddings across denoising timesteps, outperforming baselines on benchmarks like GenImage and DiffusionFake.


<details>
  <summary>Details</summary>
Motivation: The challenge of distinguishing real from generated images due to advanced diffusion models threatens trust in digital media, necessitating robust detectors.

Method: LATTE models latent embeddings' evolution over denoising steps, refines them with a latent-visual feature module, and uses a lightweight classifier for detection.

Result: LATTE outperforms existing methods on benchmarks and shows strong cross-generator and cross-dataset generalization.

Conclusion: LATTE's trajectory-based approach effectively detects generated images, offering a promising solution for digital media verification.

Abstract: The rapid advancement of diffusion-based image generators has made it
increasingly difficult to distinguish generated from real images. This can
erode trust in digital media, making it critical to develop generalizable
detectors for generated images. Recent methods leverage diffusion denoising
cues, but mainly focus on single-step reconstruction errors, ignoring the
inherent sequential nature of the denoising process. In this work, we propose
LATTE - Latent Trajectory Embedding - a novel approach that models the
evolution of latent embeddings across several denoising timesteps. By modeling
the trajectory of such embeddings rather than single-step errors, LATTE
captures subtle, discriminative patterns that distinguish real from generated
images. Each latent is refined by employing our latent-visual feature
refinement module and aggregated into a unified representation. Afterwards, it
is fused with the visual features and finally passed into a lightweight
classifier. Our experiments demonstrate that LATTE surpasses the baselines on
several established benchmarks, such as GenImage and DiffusionFake. Moreover,
it demonstrates strong performance in cross-generator and cross-datasets
settings, highlighting the potential of using the trajectory of latent
embeddings for generated image detection. The code is available on the
following link: https://github.com/AnaMVasilcoiu/LATTE-Diffusion-Detector.

</details>


### [31] [Towards a Psychoanalytic Perspective on VLM Behaviour: A First-step Interpretation with Intriguing Observations](https://arxiv.org/abs/2507.03123)
*Xiangrui Liu,Man Luo,Agneet Chatterjee,Hua Wei,Yezhou Yang*

Main category: cs.CV

TL;DR: The paper investigates hallucination in Vision-Language Models (VLMs) through a psychological lens, introducing a taxonomy (sycophancy, logical inconsistency, authority bias) and a benchmark (AIpsych) to analyze these behaviors. Findings show larger models exhibit stronger sycophancy but reduced authority bias, suggesting trade-offs between competence and integrity.


<details>
  <summary>Details</summary>
Motivation: Existing explanations for VLM hallucinations focus on technical or external factors, overlooking potential parallels to human cognitive biases. This work aims to bridge this gap by analyzing hallucination behaviors through psychological principles.

Method: The authors introduce a psychological taxonomy for VLM hallucinations and design AIpsych, a scalable benchmark, to analyze these behaviors. They study the impact of model architecture and size on hallucination tendencies using strategically manipulated questions.

Result: Larger VLMs show increased sycophantic tendencies but reduced authority bias, indicating a trade-off between competence and response integrity. Human studies validate these findings and highlight differences between VLMs and humans.

Conclusion: The work offers a psychological perspective on VLM hallucinations, emphasizing the need to integrate psychological principles into model evaluation. The AIpsych benchmark provides a tool for further research.

Abstract: Hallucination is a long-standing problem that has been actively investigated
in Vision-Language Models (VLMs). Existing research commonly attributes
hallucinations to technical limitations or sycophancy bias, where the latter
means the models tend to generate incorrect answers to align with user
expectations. However, these explanations primarily focus on technical or
externally driven factors, may have neglected the possibility that
hallucination behaviours might mirror cognitive biases observed in human
psychology. In this work, we introduce a psychological taxonomy, categorizing
VLMs' hallucination behaviours, including sycophancy, logical inconsistency,
and a newly identified VLMs behaviour: authority bias. To systematically
analyze these behaviours, we design AIpsych, a scalable benchmark that reveals
psychological tendencies in model response patterns. Leveraging this benchmark,
we investigate how variations in model architecture and parameter size
influence model behaviour when responding to strategically manipulated
questions. Our experiments reveal that as model size increases, VLMs exhibit
stronger sycophantic tendencies but reduced authority bias, suggesting
increasing competence but a potential erosion of response integrity. A human
subject study further validates our hypotheses and highlights key behavioural
differences between VLMs and human respondents. This work suggests a new
perspective for understanding hallucination in VLMs and highlights the
importance of integrating psychological principles into model evaluation.The
benchmark is available at https://github.com/lxrswdd/AIpsych.

</details>


### [32] [Transparent Machine Learning: Training and Refining an Explainable Boosting Machine to Identify Overshooting Tops in Satellite Imagery](https://arxiv.org/abs/2507.03183)
*Nathan Mitchell,Lander Ver Hoef,Imme Ebert-Uphoff,Kristina Moen,Kyle Hilburn,Yoonjin Lee,Emily J. King*

Main category: cs.CV

TL;DR: The paper explores using Explainable Boosting Machines (EBMs) for interpretable ML in atmospheric science, specifically for detecting overshooting tops (OTs) in satellite imagery. It combines feature engineering with EBMs to create physics-based, interpretable models.


<details>
  <summary>Details</summary>
Motivation: To develop interpretable ML algorithms for high-risk meteorological applications, addressing the gap in using EBMs in atmospheric science.

Method: Feature extraction (e.g., cloud texture) followed by EBM classification using satellite imagery. Training labels are derived from convection flags.

Result: A fully interpretable ML model was created, though less accurate than complex methods, it performs well and aligns with domain scientists' strategies.

Conclusion: EBMs offer a viable path for interpretable ML in meteorology, balancing accuracy and transparency, and enabling human-machine collaboration.

Abstract: An Explainable Boosting Machine (EBM) is an interpretable machine learning
(ML) algorithm that has benefits in high risk applications but has not yet
found much use in atmospheric science. The overall goal of this work is
twofold: (1) explore the use of EBMs, in combination with feature engineering,
to obtain interpretable, physics-based machine learning algorithms for
meteorological applications; (2) illustrate these methods for the detection of
overshooting top (OTs) in satellite imagery.
  Specifically, we seek to simplify the process of OT detection by first using
mathematical methods to extract key features, such as cloud texture using
Gray-Level Co-occurrence Matrices, followed by applying an EBM. Our EBM focuses
on the classification task of predicting OT regions, utilizing Channel 2
(visible imagery) and Channel 13 (infrared imagery) of the Advanced Baseline
Imager sensor of the Geostationary Operational Environmental Satellite 16.
Multi-Radar/Multi-Sensor system convection flags are used as labels to train
the EBM model. Note, however, that detecting convection, while related, is
different from detecting OTs.
  Once trained, the EBM was examined and minimally altered to more closely
match strategies used by domain scientists to identify OTs. The result of our
efforts is a fully interpretable ML algorithm that was developed in a
human-machine collaboration. While the final model does not reach the accuracy
of more complex approaches, it performs well and represents a significant step
toward building fully interpretable ML algorithms for this and other
meteorological applications.

</details>


### [33] [AI-driven Web Application for Early Detection of Sudden Death Syndrome (SDS) in Soybean Leaves Using Hyperspectral Images and Genetic Algorithm](https://arxiv.org/abs/2507.03198)
*Pappu Kumar Yadav,Rishik Aggarwal,Supriya Paudel,Amee Parmar,Hasan Mirzakhaninafchi,Zain Ul Abideen Usmani,Dhe Yeong Tchalla,Shyam Solanki,Ravi Mural,Sachin Sharma,Thomas F. Burks,Jianwei Qin,Moon S. Kim*

Main category: cs.CV

TL;DR: AI-driven web app uses hyperspectral imaging and machine learning for early SDS detection in soybeans, achieving >98% accuracy.


<details>
  <summary>Details</summary>
Motivation: SDS threatens soybean production; early detection is critical for mitigation.

Method: Hyperspectral imaging (398-1011 nm) and Genetic Algorithm-selected wavelengths fed into a CNN and classical ML models.

Result: Ensemble classifiers (Random Forest, AdaBoost), Linear SVM, and Neural Net achieved >98% accuracy.

Conclusion: The system enables rapid, accessible SDS diagnostics; future work will expand datasets and crop applicability.

Abstract: Sudden Death Syndrome (SDS), caused by Fusarium virguliforme, poses a
significant threat to soybean production. This study presents an AI-driven web
application for early detection of SDS on soybean leaves using hyperspectral
imaging, enabling diagnosis prior to visible symptom onset. Leaf samples from
healthy and inoculated plants were scanned using a portable hyperspectral
imaging system (398-1011 nm), and a Genetic Algorithm was employed to select
five informative wavelengths (505.4, 563.7, 712.2, 812.9, and 908.4 nm)
critical for discriminating infection status. These selected bands were fed
into a lightweight Convolutional Neural Network (CNN) to extract
spatial-spectral features, which were subsequently classified using ten
classical machine learning models. Ensemble classifiers (Random Forest,
AdaBoost), Linear SVM, and Neural Net achieved the highest accuracy (>98%) and
minimal error across all folds, as confirmed by confusion matrices and
cross-validation metrics. Poor performance by Gaussian Process and QDA
highlighted their unsuitability for this dataset. The trained models were
deployed within a web application that enables users to upload hyperspectral
leaf images, visualize spectral profiles, and receive real-time classification
results. This system supports rapid and accessible plant disease diagnostics,
contributing to precision agriculture practices. Future work will expand the
training dataset to encompass diverse genotypes, field conditions, and disease
stages, and will extend the system for multiclass disease classification and
broader crop applicability.

</details>


### [34] [Development of an Improved Capsule-Yolo Network for Automatic Tomato Plant Disease Early Detection and Diagnosis](https://arxiv.org/abs/2507.03219)
*Idris Ochijenu,Monday Abutu Idakwo,Sani Felix*

Main category: cs.CV

TL;DR: An enhanced Capsule-YOLO network improves tomato disease detection with high accuracy, aiding farmers in early diagnosis and treatment.


<details>
  <summary>Details</summary>
Motivation: Tomato diseases threaten yields and food security in Nigeria; visual identification is possible but needs automation.

Method: Enhanced Capsule-YOLO architecture segments and identifies diseased tomato leaves using YOLO, achieving high accuracy.

Result: 99.31% accuracy, 98.78% recall, 99.09% precision, and 98.93% F1-score, outperforming existing methods.

Conclusion: The system enhances crop yields and food security by enabling early disease detection and treatment recommendations.

Abstract: Like many countries, Nigeria is naturally endowed with fertile agricultural
soil that supports large-scale tomato production. However, the prevalence of
disease causing pathogens poses a significant threat to tomato health, often
leading to reduced yields and, in severe cases, the extinction of certain
species. These diseases jeopardise both the quality and quantity of tomato
harvests, contributing to food insecurity. Fortunately, tomato diseases can
often be visually identified through distinct forms, appearances, or textures,
typically first visible on leaves and fruits. This study presents an enhanced
Capsule-YOLO network architecture designed to automatically segment overlapping
and occluded tomato leaf images from complex backgrounds using the YOLO
framework. It identifies disease symptoms with impressive performance metrics:
99.31% accuracy, 98.78% recall, and 99.09% precision, and a 98.93% F1-score
representing improvements of 2.91%, 1.84%, 5.64%, and 4.12% over existing
state-of-the-art methods. Additionally, a user-friendly interface was developed
to allow farmers and users to upload images of affected tomato plants and
detect early disease symptoms. The system also provides recommendations for
appropriate diagnosis and treatment. The effectiveness of this approach
promises significant benefits for the agricultural sector by enhancing crop
yields and strengthening food security.

</details>


### [35] [A Vision-Based Closed-Form Solution for Measuring the Rotation Rate of an Object by Tracking One Point](https://arxiv.org/abs/2507.03237)
*Daniel Raviv,Juan D. Yepes,Eiki M. Martinson*

Main category: cs.CV

TL;DR: The paper shows that under orthographic projection, a camera fixated on a rigid body's point can analytically determine the body's rotation by tracking just one additional feature, with most points yielding the same rotation rate.


<details>
  <summary>Details</summary>
Motivation: To simplify the process of determining rigid body rotation from video data without needing prior scene knowledge or object shape details.

Method: Analytical derivation under orthographic projection, tracking one additional feature on the rigid body, and validating with simulations and real video data.

Result: The method successfully estimates rotation rates and can segment scenes by identifying non-rigid points.

Conclusion: The approach is effective for rigid body rotation analysis and scene segmentation, requiring minimal tracking and no prior information.

Abstract: We demonstrate that, under orthographic projection and with a camera fixated
on a point located on a rigid body, the rotation of that body can be
analytically obtained by tracking only one other feature in the image. With
some exceptions, any tracked point, regardless of its location on the body,
yields the same value of the instantaneous rotation rate.
  The proposed method is independent of the shape of the 3D object and does not
require a priori knowledge about the scene. This algorithm is suited for
parallel processing and can achieve segmentation of the scene by distinguishing
points that do not belong to the same rigid body, simply because they do not
produce the same value of the rotation. This paper presents an analytical
derivation, simulation results, and results from real video data.

</details>


### [36] [Subject Invariant Contrastive Learning for Human Activity Recognition](https://arxiv.org/abs/2507.03250)
*Yavuz Yarici,Kiran Kokilepersaud,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: SICL, a new loss function for HAR, improves generalization by re-weighting negative pairs to suppress subject-specific cues, boosting performance by up to 11%.


<details>
  <summary>Details</summary>
Motivation: High annotation costs and domain shifts in HAR sensor data hinder model generalization, prompting the need for better self-supervised methods.

Method: Introduces Subject-Invariant Contrastive Learning (SICL), which re-weights negative pairs from the same subject to focus on activity-specific features.

Result: SICL outperforms traditional contrastive learning by up to 11% on benchmarks like UTD-MHAD, MMAct, and DARai.

Conclusion: SICL is effective and adaptable across various HAR settings, enhancing generalization and performance.

Abstract: The high cost of annotating data makes self-supervised approaches, such as
contrastive learning methods, appealing for Human Activity Recognition (HAR).
Effective contrastive learning relies on selecting informative positive and
negative samples. However, HAR sensor signals are subject to significant domain
shifts caused by subject variability. These domain shifts hinder model
generalization to unseen subjects by embedding subject-specific variations
rather than activity-specific features. As a result, human activity recognition
models trained with contrastive learning often struggle to generalize to new
subjects. We introduce Subject-Invariant Contrastive Learning (SICL), a simple
yet effective loss function to improve generalization in human activity
recognition. SICL re-weights negative pairs drawn from the same subject to
suppress subject-specific cues and emphasize activity-specific information. We
evaluate our loss function on three public benchmarks: UTD-MHAD, MMAct, and
DARai. We show that SICL improves performance by up to 11% over traditional
contrastive learning methods. Additionally, we demonstrate the adaptability of
our loss function across various settings, including multiple self-supervised
methods, multimodal scenarios, and supervised learning frameworks.

</details>


### [37] [LACONIC: A 3D Layout Adapter for Controllable Image Creation](https://arxiv.org/abs/2507.03257)
*Lopold Maillard,Tom Durand,Adrien Ramanana Rahary,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: A novel method enhances 3D-awareness in text-to-image diffusion models, enabling camera control, 3D geometry conditioning, and scene-wide context synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack 3D geometric consistency in multi-object scene synthesis, limiting realism and control.

Method: Proposes a conditioning approach, training method, and adapter network for pretrained diffusion models, supporting 3D-aware synthesis and editing.

Result: Lightweight, data-efficient, and generalizes well, enabling intuitive editing (e.g., object positioning, resizing) and richer applications.

Conclusion: The method advances 3D-aware image synthesis and editing, integrating seamlessly into workflows and outperforming prior approaches.

Abstract: Existing generative approaches for guided image synthesis of multi-object
scenes typically rely on 2D controls in the image or text space. As a result,
these methods struggle to maintain and respect consistent three-dimensional
geometric structure, underlying the scene. In this paper, we propose a novel
conditioning approach, training method and adapter network that can be plugged
into pretrained text-to-image diffusion models. Our approach provides a way to
endow such models with 3D-awareness, while leveraging their rich prior
knowledge. Our method supports camera control, conditioning on explicit 3D
geometries and, for the first time, accounts for the entire context of a scene,
i.e., both on and off-screen items, to synthesize plausible and semantically
rich images. Despite its multi-modal nature, our model is lightweight, requires
a reasonable number of data for supervised learning and shows remarkable
generalization power. We also introduce methods for intuitive and consistent
image editing and restyling, e.g., by positioning, rotating or resizing
individual objects in a scene. Our method integrates well within various image
creation workflows and enables a richer set of applications compared to
previous approaches.

</details>


### [38] [Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders](https://arxiv.org/abs/2507.03262)
*Song Mao,Yang Chen,Pinglong Cai,Ding Wang,Guohang Yan,Zhi Yu,Botian Shi*

Main category: cs.CV

TL;DR: The paper investigates encoder redundancy in Multimodal Large Language Models (MLLMs), where adding multiple vision encoders often yields diminishing or negative returns. It introduces metrics (CUR and IG) to quantify redundancy and confirms inefficiencies in current designs.


<details>
  <summary>Details</summary>
Motivation: To address the diminishing returns and performance degradation caused by redundant vision encoders in MLLMs.

Method: Conducts ablation studies on multi-encoder MLLMs, proposes Conditional Utilization Rate (CUR) and Information Gap (IG) metrics to quantify redundancy.

Result: Empirical evidence shows significant redundancy, with some encoders contributing minimally or negatively to performance.

Conclusion: The study highlights inefficiencies in multi-encoder designs and validates the proposed metrics as diagnostic tools for improving multimodal architectures.

Abstract: Multimodal Large Language Models (MLLMs) increasingly adopt multiple vision
encoders to capture diverse visual information, ranging from coarse semantics
to fine grained details. While this approach is intended to enhance visual
understanding capability, we observe that the performance gains from adding
encoders often diminish and can even lead to performance degradation, a
phenomenon we term encoder redundancy. This paper presents a systematic
investigation into this issue. Through comprehensive ablation studies on state
of the art multi encoder MLLMs, we empirically demonstrate that significant
redundancy exists. To quantify each encoder's unique contribution, we propose a
principled metric: the Conditional Utilization Rate (CUR). Building on CUR, we
introduce the Information Gap (IG) to capture the overall disparity in encoder
utility within a model.Our experiments reveal that certain vision encoders
contribute little, or even negatively, to overall performance, confirming
substantial redundancy. Our experiments reveal that certain vision encoders
contribute minimally, or even negatively, to the model's performance,
confirming the prevalence of redundancy. These findings highlight critical
inefficiencies in current multi encoder designs and establish that our proposed
metrics can serve as valuable diagnostic tools for developing more efficient
and effective multimodal architectures.

</details>


### [39] [Dual-frequency Selected Knowledge Distillation with Statistical-based Sample Rectification for PolSAR Image Classification](https://arxiv.org/abs/2507.03268)
*Xinyue Xin,Ming Li,Yan Wu,Xiang Li,Peng Zhang,Dazhi Xu*

Main category: cs.CV

TL;DR: SKDNet-SSR improves dual-frequency PolSAR image classification by addressing regional consistency and dual-frequency data utilization via dynamic sample rectification and gate-selected distillation.


<details>
  <summary>Details</summary>
Motivation: Challenges in dual-frequency PolSAR classification include regional consistency effects and dual-frequency data utilization.

Method: Proposes SKDNet-SSR with SDSR for sample purity evaluation and DGSD for dual-frequency complementary learning.

Result: Outperforms other methods on four measured dual-frequency PolSAR datasets.

Conclusion: SKDNet-SSR effectively enhances classification by mitigating noise and leveraging dual-frequency data.

Abstract: The collaborative classification of dual-frequency PolSAR images is a
meaningful but also challenging research. The effect of regional consistency on
classification information learning and the rational use of dual-frequency data
are two main difficulties for dual-frequency collaborative classification. To
tackle these problems, a selected knowledge distillation network with
statistical-based sample rectification (SKDNet-SSR) is proposed in this
article. First, in addition to applying CNN and ViT as local and global feature
extractors, a statistical-based dynamic sample rectification (SDSR) module is
designed to avoid the impact of poor regional consistency on spatial
information learning process. Specifically, based on the fact that the PolSAR
covariance matrix conforms to the complex Wishart distribution, SDSR first
dynamically evaluates the sample purity, and then performs pixel selection and
pixel generation to remove noisy pixels, thereby avoiding the feature
interaction between informative pixels and noisy pixels and improving the
classification feature extraction process. Next, a dual-frequency gate-selected
distillation (DGSD) module is constructed to emphasize the advantages of
different frequency bands and perform complementary learning on dual-frequency
data. It uses the dominant single-frequency branch on each sample as teacher
model to train the dual-frequency student model, enabling the student model to
learn the optimal results and realizing complementary utilization of
dual-frequency data on different terrain objects. Comprehensive experiments on
four measured dual-frequency PolSAR data demonstrate that the proposed
SKDNet-SSR outperforms other related methods.

</details>


### [40] [ConceptMix++: Leveling the Playing Field in Text-to-Image Benchmarking via Iterative Prompt Optimization](https://arxiv.org/abs/2507.03275)
*Haosheng Gan,Berk Tinaz,Mohammad Shahab Sepehri,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: ConceptMix++ introduces iterative prompt optimization to improve text-to-image model evaluations, revealing hidden capabilities and enabling fairer comparisons.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks use rigid prompts, underestimating model capabilities and introducing biases.

Method: ConceptMix++ employs a multimodal optimization pipeline with vision-language feedback to refine prompts systematically.

Result: Optimized prompts enhance compositional generation, especially for spatial relationships and shapes, and show cross-model transferability.

Conclusion: Rigid benchmarks underestimate model performance; ConceptMix++ offers a more accurate and insightful evaluation framework.

Abstract: Current text-to-image (T2I) benchmarks evaluate models on rigid prompts,
potentially underestimating true generative capabilities due to prompt
sensitivity and creating biases that favor certain models while disadvantaging
others. We introduce ConceptMix++, a framework that disentangles prompt
phrasing from visual generation capabilities by applying iterative prompt
optimization. Building on ConceptMix, our approach incorporates a multimodal
optimization pipeline that leverages vision-language model feedback to refine
prompts systematically. Through extensive experiments across multiple diffusion
models, we show that optimized prompts significantly improve compositional
generation performance, revealing previously hidden model capabilities and
enabling fairer comparisons across T2I models. Our analysis reveals that
certain visual concepts -- such as spatial relationships and shapes -- benefit
more from optimization than others, suggesting that existing benchmarks
systematically underestimate model performance in these categories.
Additionally, we find strong cross-model transferability of optimized prompts,
indicating shared preferences for effective prompt phrasing across models.
These findings demonstrate that rigid benchmarking approaches may significantly
underrepresent true model capabilities, while our framework provides more
accurate assessment and insights for future development.

</details>


### [41] [NOVO: Unlearning-Compliant Vision Transformers](https://arxiv.org/abs/2507.03281)
*Soumya Roy,Soumya Banerjee,Vinay Verma,Soumik Dasgupta,Deepak Gupta,Piyush Rai*

Main category: cs.CV

TL;DR: The paper introduces an unlearning-aware vision transformer architecture, {\pname}, that enables selective forgetting of training data without fine-tuning, avoiding performance degradation.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods are expensive, impractical, and degrade model performance. The goal is to develop a more efficient and effective solution.

Method: The model simulates unlearning during training by splitting mini-batches into proxy forget and retain sets, optimizing to forget the forget-set. Forgetting is achieved by withdrawing keys.

Result: Extensive experiments show {\pname} outperforms fine-tuning-free and fine-tuning-based methods across datasets, architectures, and resolutions.

Conclusion: {\pname} provides a practical and efficient solution for machine unlearning without performance degradation.

Abstract: Machine unlearning (MUL) refers to the problem of making a pre-trained model
selectively forget some training instances or class(es) while retaining
performance on the remaining dataset. Existing MUL research involves
fine-tuning using a forget and/or retain set, making it expensive and/or
impractical, and often causing performance degradation in the unlearned model.
We introduce {\pname}, an unlearning-aware vision transformer-based
architecture that can directly perform unlearning for future unlearning
requests without any fine-tuning over the requested set. The proposed model is
trained by simulating unlearning during the training process itself. It
involves randomly separating class(es)/sub-class(es) present in each mini-batch
into two disjoint sets: a proxy forget-set and a retain-set, and the model is
optimized so that it is unable to predict the forget-set. Forgetting is
achieved by withdrawing keys, making unlearning on-the-fly and avoiding
performance degradation. The model is trained jointly with learnable keys and
original weights, ensuring withholding a key irreversibly erases information,
validated by membership inference attack scores. Extensive experiments on
various datasets, architectures, and resolutions confirm {\pname}'s superiority
over both fine-tuning-free and fine-tuning-based methods.

</details>


### [42] [MolVision: Molecular Property Prediction with Vision Language Models](https://arxiv.org/abs/2507.03283)
*Deepan Adak,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: MolVision integrates molecular structure images and textual descriptions using Vision-Language Models (VLMs) to improve molecular property prediction, outperforming text-only methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on ambiguous textual representations (SMILES/SELFIES). MolVision aims to enhance prediction by incorporating visual molecular structure data.

Method: Uses VLMs to combine molecular images and text. Evaluates nine VLMs across ten datasets in zero-shot, few-shot, and fine-tuned settings, employing LoRA for efficient fine-tuning.

Result: Visual data alone is insufficient, but multimodal fusion (images + text) significantly improves prediction performance, especially with LoRA fine-tuning.

Conclusion: MolVision demonstrates the value of multimodal approaches in molecular property prediction, with potential for broader applications in drug discovery and materials science.

Abstract: Molecular property prediction is a fundamental task in computational
chemistry with critical applications in drug discovery and materials science.
While recent works have explored Large Language Models (LLMs) for this task,
they primarily rely on textual molecular representations such as
SMILES/SELFIES, which can be ambiguous and structurally less informative. In
this work, we introduce MolVision, a novel approach that leverages
Vision-Language Models (VLMs) by integrating both molecular structure as images
and textual descriptions to enhance property prediction. We construct a
benchmark spanning ten diverse datasets, covering classification, regression
and description tasks. Evaluating nine different VLMs in zero-shot, few-shot,
and fine-tuned settings, we find that visual information improves prediction
performance, particularly when combined with efficient fine-tuning strategies
such as LoRA. Our results reveal that while visual information alone is
insufficient, multimodal fusion significantly enhances generalization across
molecular properties. Adaptation of vision encoder for molecular images in
conjunction with LoRA further improves the performance. The code and data is
available at :
$\href{https://molvision.github.io/MolVision/}{https://molvision.github.io/MolVision/}$.

</details>


### [43] [Zero-shot Inexact CAD Model Alignment from a Single Image](https://arxiv.org/abs/2507.03292)
*Pattaramanee Arsomngern,Sasikarn Khwanmuang,Matthias Niener,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: A weakly supervised 9-DoF alignment method for inexact 3D models is proposed, requiring no pose annotations and generalizing to unseen categories. It outperforms SOTA baselines and supervised methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on supervised training with pose annotations, limiting them to narrow object categories. The goal is to generalize to unseen categories without pose annotations.

Method: The approach uses a novel feature space for multi-view consistency and a self-supervised triplet loss to address symmetry ambiguities. It also introduces a texture-invariant pose refinement technique.

Result: Outperforms SOTA weakly supervised baselines by +4.3% and surpasses supervised ROCA by +2.7%. Achieves SOTA on SUN2CAD, a novel test set.

Conclusion: The method successfully generalizes to unseen categories and outperforms existing approaches without requiring pose annotations.

Abstract: One practical approach to infer 3D scene structure from a single image is to
retrieve a closely matching 3D model from a database and align it with the
object in the image. Existing methods rely on supervised training with images
and pose annotations, which limits them to a narrow set of object categories.
To address this, we propose a weakly supervised 9-DoF alignment method for
inexact 3D models that requires no pose annotations and generalizes to unseen
categories. Our approach derives a novel feature space based on foundation
features that ensure multi-view consistency and overcome symmetry ambiguities
inherent in foundation features using a self-supervised triplet loss.
Additionally, we introduce a texture-invariant pose refinement technique that
performs dense alignment in normalized object coordinates, estimated through
the enhanced feature space. We conduct extensive evaluations on the real-world
ScanNet25k dataset, where our method outperforms SOTA weakly supervised
baselines by +4.3% mean alignment accuracy and is the only weakly supervised
approach to surpass the supervised ROCA by +2.7%. To assess generalization, we
introduce SUN2CAD, a real-world test set with 20 novel object categories, where
our method achieves SOTA results without prior training on them.

</details>


### [44] [CPKD: Clinical Prior Knowledge-Constrained Diffusion Models for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.03295)
*Xiangning Zhang,Jinnan Chen,Qingwei Zhang,Yaqi Wang,Chengfeng Zhou,Xiaobo Li,Dahong Qian*

Main category: cs.CV

TL;DR: The paper introduces CPKD, a generative framework for surgical phase recognition in ESD, leveraging diffusion principles and clinical knowledge to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the bottleneck of reliable surgical phase recognition in endoscopic workflows, crucial for computer-assisted ESD systems.

Method: Proposes CPKD, a diffusion-based generative framework with conditional masking and clinical prior knowledge integration.

Result: CPKD achieves superior or comparable performance on ESD820, Cholec80, and multi-center datasets.

Conclusion: CPKD validates the effectiveness of diffusion-based generative models for surgical phase recognition in ESD.

Abstract: Gastrointestinal malignancies constitute a leading cause of cancer-related
mortality worldwide, with advanced-stage prognosis remaining particularly
dismal. Originating as a groundbreaking technique for early gastric cancer
treatment, Endoscopic Submucosal Dissection has evolved into a versatile
intervention for diverse gastrointestinal lesions. While computer-assisted
systems significantly enhance procedural precision and safety in ESD, their
clinical adoption faces a critical bottleneck: reliable surgical phase
recognition within complex endoscopic workflows. Current state-of-the-art
approaches predominantly rely on multi-stage refinement architectures that
iteratively optimize temporal predictions. In this paper, we present Clinical
Prior Knowledge-Constrained Diffusion (CPKD), a novel generative framework that
reimagines phase recognition through denoising diffusion principles while
preserving the core iterative refinement philosophy. This architecture
progressively reconstructs phase sequences starting from random noise and
conditioned on visual-temporal features. To better capture three
domain-specific characteristics, including positional priors, boundary
ambiguity, and relation dependency, we design a conditional masking strategy.
Furthermore, we incorporate clinical prior knowledge into the model training to
improve its ability to correct phase logical errors. Comprehensive evaluations
on ESD820, Cholec80, and external multi-center demonstrate that our proposed
CPKD achieves superior or comparable performance to state-of-the-art
approaches, validating the effectiveness of diffusion-based generative
paradigms for surgical phase recognition.

</details>


### [45] [Leveraging Out-of-Distribution Unlabeled Images: Semi-Supervised Semantic Segmentation with an Open-Vocabulary Model](https://arxiv.org/abs/2507.03302)
*Wooseok Shin,Jisu Kang,Hyeonki Jeong,Jin Sob Kim,Sung Won Han*

Main category: cs.CV

TL;DR: The paper proposes SemiOVS, a semi-supervised semantic segmentation framework using open-vocabulary segmentation to leverage unlabeled out-of-distribution (OOD) images, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised methods struggle with unlabeled OOD images, which can misguide training. The goal is to effectively utilize abundant unlabeled OOD data.

Method: Introduces SemiOVS, combining semi-supervised learning with open-vocabulary segmentation to pseudo-label OOD images.

Result: SemiOVS outperforms PrevMatch and SemiVL by +3.5 and +3.0 mIoU on Pascal VOC, showing significant gains with OOD images.

Conclusion: The framework successfully leverages unlabeled OOD images, inspiring future research and applications.

Abstract: In semi-supervised semantic segmentation, existing studies have shown
promising results in academic settings with controlled splits of benchmark
datasets. However, the potential benefits of leveraging significantly larger
sets of unlabeled images remain unexplored. In real-world scenarios, abundant
unlabeled images are often available from online sources (web-scraped images)
or large-scale datasets. However, these images may have different distributions
from those of the target dataset, a situation known as out-of-distribution
(OOD). Using these images as unlabeled data in semi-supervised learning can
lead to inaccurate pseudo-labels, potentially misguiding network training. In
this paper, we propose a new semi-supervised semantic segmentation framework
with an open-vocabulary segmentation model (SemiOVS) to effectively utilize
unlabeled OOD images. Extensive experiments on Pascal VOC and Context datasets
demonstrate two key findings: (1) using additional unlabeled images improves
the performance of semi-supervised learners in scenarios with few labels, and
(2) using the open-vocabulary segmentation (OVS) model to pseudo-label OOD
images leads to substantial performance gains. In particular, SemiOVS
outperforms existing PrevMatch and SemiVL methods by +3.5 and +3.0 mIoU,
respectively, on Pascal VOC with a 92-label setting, achieving state-of-the-art
performance. These findings demonstrate that our approach effectively utilizes
abundant unlabeled OOD images for semantic segmentation tasks. We hope this
work can inspire future research and real-world applications. The code is
available at https://github.com/wooseok-shin/SemiOVS

</details>


### [46] [Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations](https://arxiv.org/abs/2507.03304)
*Hai Huang,Yan Xia,Sashuai Zhou,Hanting Wang,Shulei Wang,Zhou Zhao*

Main category: cs.CV

TL;DR: The paper proposes a novel approach for Multi-modal Domain Generalization (MMDG) using unified representations and supervised disentanglement to enhance generalization across unseen target domains.


<details>
  <summary>Details</summary>
Motivation: Existing DG methods focus on single-modal data, but multi-modal tasks require generalization across modalities, which current methods fail to address effectively.

Method: The approach leverages unified representations to map paired modalities together and introduces a supervised disentanglement framework to separate modal-general and modal-specific information.

Result: Experiments on datasets like EPIC-Kitchens and Human-Animal-Cartoon show the method's effectiveness in improving multi-modal domain generalization.

Conclusion: The proposed method outperforms existing DG techniques by addressing inter-modal consistency and synchronization in multi-modal settings.

Abstract: Domain Generalization (DG) aims to enhance model robustness in unseen or
distributionally shifted target domains through training exclusively on source
domains. Although existing DG techniques, such as data manipulation, learning
strategies, and representation learning, have shown significant progress, they
predominantly address single-modal data. With the emergence of numerous
multi-modal datasets and increasing demand for multi-modal tasks, a key
challenge in Multi-modal Domain Generalization (MMDG) has emerged: enabling
models trained on multi-modal sources to generalize to unseen target
distributions within the same modality set. Due to the inherent differences
between modalities, directly transferring methods from single-modal DG to MMDG
typically yields sub-optimal results. These methods often exhibit randomness
during generalization due to the invisibility of target domains and fail to
consider inter-modal consistency. Applying these methods independently to each
modality in the MMDG setting before combining them can lead to divergent
generalization directions across different modalities, resulting in degraded
generalization capabilities. To address these challenges, we propose a novel
approach that leverages Unified Representations to map different paired
modalities together, effectively adapting DG methods to MMDG by enabling
synchronized multi-modal improvements within the unified space. Additionally,
we introduce a supervised disentanglement framework that separates
modal-general and modal-specific information, further enhancing the alignment
of unified representations. Extensive experiments on benchmark datasets,
including EPIC-Kitchens and Human-Animal-Cartoon, demonstrate the effectiveness
and superiority of our method in enhancing multi-modal domain generalization.

</details>


### [47] [MGSfM: Multi-Camera Geometry Driven Global Structure-from-Motion](https://arxiv.org/abs/2507.03306)
*Peilin Tao,Hainan Cui,Diantao Tu,Shuhan Shen*

Main category: cs.CV

TL;DR: A novel global motion averaging framework for multi-camera systems improves robustness and efficiency in Structure-from-Motion (SfM) by decoupling rotation and hybrid translation averaging.


<details>
  <summary>Details</summary>
Motivation: Traditional global SfM systems lack robustness due to optimization challenges, especially in multi-camera setups with fixed relative pose constraints.

Method: Proposes a decoupled rotation averaging module (hierarchical strategy) and a hybrid translation averaging module (combining camera-to-camera and camera-to-point constraints).

Result: Matches or exceeds incremental SfM accuracy while significantly improving efficiency, outperforming existing global SfM methods.

Conclusion: The framework is a robust solution for real-world multi-camera SfM applications, with code available for public use.

Abstract: Multi-camera systems are increasingly vital in the environmental perception
of autonomous vehicles and robotics. Their physical configuration offers
inherent fixed relative pose constraints that benefit Structure-from-Motion
(SfM). However, traditional global SfM systems struggle with robustness due to
their optimization framework. We propose a novel global motion averaging
framework for multi-camera systems, featuring two core components: a decoupled
rotation averaging module and a hybrid translation averaging module. Our
rotation averaging employs a hierarchical strategy by first estimating relative
rotations within rigid camera units and then computing global rigid unit
rotations. To enhance the robustness of translation averaging, we incorporate
both camera-to-camera and camera-to-point constraints to initialize camera
positions and 3D points with a convex distance-based objective function and
refine them with an unbiased non-bilinear angle-based objective function.
Experiments on large-scale datasets show that our system matches or exceeds
incremental SfM accuracy while significantly improving efficiency. Our
framework outperforms existing global SfM methods, establishing itself as a
robust solution for real-world multi-camera SfM applications. The code is
available at https://github.com/3dv-casia/MGSfM/.

</details>


### [48] [Personalized Image Generation from an Author Writing Style](https://arxiv.org/abs/2507.03313)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CV

TL;DR: A pipeline uses Author Writing Sheets (AWS) and an LLM to generate text-to-image prompts, rendered by Stable Diffusion, for visualizing authorial styles. Evaluated with Reddit data, results show good style alignment and moderate distinctiveness.


<details>
  <summary>Details</summary>
Motivation: To translate nuanced authorial writing styles into compelling visual representations using generative AI.

Method: Leverages AWS and an LLM to generate text-to-image prompts, rendered by Stable Diffusion. Evaluated with human assessments on style match and distinctiveness.

Result: Good perceived alignment (mean style match: 4.08/5) and moderate distinctiveness. Captures mood well but struggles with abstract elements.

Conclusion: Introduces a novel methodology for visual authorial style personalization, validated empirically, with potential applications in creative assistance and cross-modal understanding.

Abstract: Translating nuanced, textually-defined authorial writing styles into
compelling visual representations presents a novel challenge in generative AI.
This paper introduces a pipeline that leverages Author Writing Sheets (AWS) -
structured summaries of an author's literary characteristics - as input to a
Large Language Model (LLM, Claude 3.7 Sonnet). The LLM interprets the AWS to
generate three distinct, descriptive text-to-image prompts, which are then
rendered by a diffusion model (Stable Diffusion 3.5 Medium). We evaluated our
approach using 49 author styles from Reddit data, with human evaluators
assessing the stylistic match and visual distinctiveness of the generated
images. Results indicate a good perceived alignment between the generated
visuals and the textual authorial profiles (mean style match: $4.08/5$), with
images rated as moderately distinctive. Qualitative analysis further
highlighted the pipeline's ability to capture mood and atmosphere, while also
identifying challenges in representing highly abstract narrative elements. This
work contributes a novel end-to-end methodology for visual authorial style
personalization and provides an initial empirical validation, opening avenues
for applications in creative assistance and cross-modal understanding.

</details>


### [49] [Source-Free Domain Adaptation via Multi-view Contrastive Learning](https://arxiv.org/abs/2507.03321)
*Amirfarhad Farhadi,Naser Mozayani,Azadeh Zamanifar*

Main category: cs.CV

TL;DR: The paper proposes a method for Source-Free Unsupervised Domain Adaptation (SFUDA) to address domain discrepancies without labeled target data, improving prototype quality and pseudo-label accuracy.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns restrict access to sensitive data, making SFUDA essential for domain adaptation without labeled target data.

Method: Three-phase approach: Reliable Sample Memory (RSM) for better prototypes, Multi-View Contrastive Learning (MVCL) for pseudo-label quality, and noisy label filtering.

Result: Achieves ~2% and 6% accuracy improvements over the second-best and average of 13 state-of-the-art methods on benchmark datasets.

Conclusion: The proposed method effectively addresses SFUDA challenges, enhancing domain adaptation performance.

Abstract: Domain adaptation has become a widely adopted approach in machine learning
due to the high costs associated with labeling data. It is typically applied
when access to a labeled source domain is available. However, in real-world
scenarios, privacy concerns often restrict access to sensitive information,
such as fingerprints, bank account details, and facial images. A promising
solution to this issue is Source-Free Unsupervised Domain Adaptation (SFUDA),
which enables domain adaptation without requiring access to labeled target
domain data. Recent research demonstrates that SFUDA can effectively address
domain discrepancies; however, two key challenges remain: (1) the low quality
of prototype samples, and (2) the incorrect assignment of pseudo-labels. To
tackle these challenges, we propose a method consisting of three main phases.
In the first phase, we introduce a Reliable Sample Memory (RSM) module to
improve the quality of prototypes by selecting more representative samples. In
the second phase, we employ a Multi-View Contrastive Learning (MVCL) approach
to enhance pseudo-label quality by leveraging multiple data augmentations. In
the final phase, we apply a noisy label filtering technique to further refine
the pseudo-labels. Our experiments on three benchmark datasets - VisDA 2017,
Office-Home, and Office-31 - demonstrate that our method achieves approximately
2 percent and 6 percent improvements in classification accuracy over the
second-best method and the average of 13 well-known state-of-the-art
approaches, respectively.

</details>


### [50] [Mirror in the Model: Ad Banner Image Generation via Reflective Multi-LLM and Multi-modal Agents](https://arxiv.org/abs/2507.03326)
*Zhao Wang,Bowen Chen,Yotaro Shimose,Sota Moriyama,Heng Wang,Shingo Takamatsu*

Main category: cs.CV

TL;DR: MIMO is an agentic framework for ad banner generation, combining hierarchical agents and iterative refinement to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Commercial design tasks require structured layouts and branding, which current generative models lack.

Method: MIMO uses a hierarchical multi-modal agent system (MIMO-Core) and a coordination loop (MIMO-Loop) for iterative improvement.

Result: MIMO outperforms diffusion and LLM-based baselines in real-world banner design.

Conclusion: MIMO effectively addresses the limitations of current models for commercial design tasks.

Abstract: Recent generative models such as GPT-4o have shown strong capabilities in
producing high-quality images with accurate text rendering. However, commercial
design tasks like advertising banners demand more than visual fidelity -- they
require structured layouts, precise typography, consistent branding, and more.
In this paper, we introduce MIMO (Mirror In-the-Model), an agentic refinement
framework for automatic ad banner generation. MIMO combines a hierarchical
multi-modal agent system (MIMO-Core) with a coordination loop (MIMO-Loop) that
explores multiple stylistic directions and iteratively improves design quality.
Requiring only a simple natural language based prompt and logo image as input,
MIMO automatically detects and corrects multiple types of errors during
generation. Experiments show that MIMO significantly outperforms existing
diffusion and LLM-based baselines in real-world banner design scenarios.

</details>


### [51] [Task-Specific Generative Dataset Distillation with Difficulty-Guided Sampling](https://arxiv.org/abs/2507.03331)
*Mingzhuo Li,Guang Li,Jiafeng Mao,Linfeng Ye,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: A task-specific sampling strategy for generative dataset distillation is proposed, focusing on classification tasks by incorporating difficulty alignment with the original dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook task-specific information in dataset distillation, which is critical for downstream performance.

Method: A task-specific sampling strategy aligns difficulty distributions between the distilled and original datasets, using logarithmic transformation to correct bias.

Result: Extensive experiments show the method's effectiveness, with potential for other downstream tasks.

Conclusion: The proposed approach improves dataset distillation by better aligning with task-specific requirements, enhancing downstream performance.

Abstract: To alleviate the reliance of deep neural networks on large-scale datasets,
dataset distillation aims to generate compact, high-quality synthetic datasets
that can achieve comparable performance to the original dataset. The
integration of generative models has significantly advanced this field.
However, existing approaches primarily focus on aligning the distilled dataset
with the original one, often overlooking task-specific information that can be
critical for optimal downstream performance. In this paper, focusing on the
downstream task of classification, we propose a task-specific sampling strategy
for generative dataset distillation that incorporates the concept of difficulty
to consider the requirements of the target task better. The final dataset is
sampled from a larger image pool with a sampling distribution obtained by
matching the difficulty distribution of the original dataset. A logarithmic
transformation is applied as a pre-processing step to correct for
distributional bias. The results of extensive experiments demonstrate the
effectiveness of our method and suggest its potential for enhancing performance
on other downstream tasks.

</details>


### [52] [De-Fake: Style based Anomaly Deepfake Detection](https://arxiv.org/abs/2507.03334)
*Sudev Kumar Padhi,Harshit Kumar,Umesh Kashyap,Sk. Subidh Ali*

Main category: cs.CV

TL;DR: SafeVision detects face-swap deepfakes by analyzing style discrepancies without needing real facial data, offering privacy-preserving and scalable detection.


<details>
  <summary>Details</summary>
Motivation: The rise of face-swap deepfakes poses threats like misinformation, reputational damage, and privacy violations, necessitating robust detection methods.

Method: SafeVision identifies style inconsistencies in face-swapped images without accessing real facial data, evaluated across diverse datasets and swapping methods.

Result: SafeVision effectively detects face-swap deepfakes in various scenarios, outperforming existing methods reliant on facial landmarks or pixel-level features.

Conclusion: SafeVision provides a privacy-preserving, scalable solution for detecting face-swap deepfakes, addressing real-world challenges and privacy concerns.

Abstract: Detecting deepfakes involving face-swaps presents a significant challenge,
particularly in real-world scenarios where anyone can perform face-swapping
with freely available tools and apps without any technical knowledge. Existing
deepfake detection methods rely on facial landmarks or inconsistencies in
pixel-level features and often struggle with face-swap deepfakes, where the
source face is seamlessly blended into the target image or video. The
prevalence of face-swap is evident in everyday life, where it is used to spread
false information, damage reputations, manipulate political opinions, create
non-consensual intimate deepfakes (NCID), and exploit children by enabling the
creation of child sexual abuse material (CSAM). Even prominent public figures
are not immune to its impact, with numerous deepfakes of them circulating
widely across social media platforms. Another challenge faced by deepfake
detection methods is the creation of datasets that encompass a wide range of
variations, as training models require substantial amounts of data. This raises
privacy concerns, particularly regarding the processing and storage of personal
facial data, which could lead to unauthorized access or misuse. Our key idea is
to identify these style discrepancies to detect face-swapped images effectively
without accessing the real facial image. We perform comprehensive evaluations
using multiple datasets and face-swapping methods, which showcases the
effectiveness of SafeVision in detecting face-swap deepfakes across diverse
scenarios. SafeVision offers a reliable and scalable solution for detecting
face-swaps in a privacy preserving manner, making it particularly effective in
challenging real-world applications. To the best of our knowledge, SafeVision
is the first deepfake detection using style features while providing inherent
privacy protection.

</details>


### [53] [DESign: Dynamic Context-Aware Convolution and Efficient Subnet Regularization for Continuous Sign Language Recognition](https://arxiv.org/abs/2507.03339)
*Sheng Liu,Yiheng Yu,Yuan Feng,Min Xu,Zhelun Jin,Yining Jiang,Tiantian Yuan*

Main category: cs.CV

TL;DR: DESign improves CSLR by combining DCAC for dynamic context-aware modeling and SR-CTC to prevent CTC overfitting, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current CSLR methods lack temporal dynamics and contextual modeling, and CTC training overfits to dominant paths.

Method: Proposes DESign with DCAC for fine-grained motion capture and SR-CTC for diverse alignment paths via subnet regularization.

Result: Achieves top performance on PHOENIX14, PHOENIX14-T, and CSL-Daily datasets.

Conclusion: DESign effectively addresses CSLR challenges with dynamic modeling and robust training, enhancing recognition accuracy.

Abstract: Current continuous sign language recognition (CSLR) methods struggle with
handling diverse samples. Although dynamic convolutions are ideal for this
task, they mainly focus on spatial modeling and fail to capture the temporal
dynamics and contextual dependencies. To address this, we propose DESign, a
novel framework that incorporates Dynamic Context-Aware Convolution (DCAC) and
Subnet Regularization Connectionist Temporal Classification (SR-CTC). DCAC
dynamically captures the inter-frame motion cues that constitute signs and
uniquely adapts convolutional weights in a fine-grained manner based on
contextual information, enabling the model to better generalize across diverse
signing behaviors and boost recognition accuracy. Furthermore, we observe that
existing methods still rely on only a limited number of frames for parameter
updates during training, indicating that CTC learning overfits to a dominant
path. To address this, SR-CTC regularizes training by applying supervision to
subnetworks, encouraging the model to explore diverse CTC alignment paths and
effectively preventing overfitting. A classifier-sharing strategy in SR-CTC
further strengthens multi-scale consistency. Notably, SR-CTC introduces no
inference overhead and can be seamlessly integrated into existing CSLR models
to boost performance. Extensive ablations and visualizations further validate
the effectiveness of the proposed methods. Results on mainstream CSLR datasets
(i.e., PHOENIX14, PHOENIX14-T, CSL-Daily) demonstrate that DESign achieves
state-of-the-art performance.

</details>


### [54] [Be the Change You Want to See: Revisiting Remote Sensing Change Detection Practices](https://arxiv.org/abs/2507.03367)
*Bla Rolih,Matic Fuka,Filip Wolf,Luka ehovin Zajc*

Main category: cs.CV

TL;DR: The paper argues that fundamental design choices (e.g., backbone selection, pre-training, training configurations) often outperform new architectural components in remote sensing change detection. A well-optimized baseline can match or exceed state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To highlight the underappreciated impact of fundamental design choices in change detection models, rather than relying on complex architectural additions.

Method: Systematically revisits the design space of change detection models, optimizing core components like backbone selection and training strategies.

Result: Demonstrates that a simple, well-designed model can achieve state-of-the-art performance on six datasets. The findings generalize to other methods.

Conclusion: Optimizing fundamental design choices is as crucial as architectural novelty for advancing change detection performance. The work provides guidelines for future methods.

Abstract: Remote sensing change detection aims to localize semantic changes between
images of the same location captured at different times. In the past few years,
newer methods have attributed enhanced performance to the additions of new and
complex components to existing architectures. Most fail to measure the
performance contribution of fundamental design choices such as backbone
selection, pre-training strategies, and training configurations. We claim that
such fundamental design choices often improve performance even more
significantly than the addition of new architectural components. Due to that,
we systematically revisit the design space of change detection models and
analyse the full potential of a well-optimised baseline. We identify a set of
fundamental design choices that benefit both new and existing architectures.
Leveraging this insight, we demonstrate that when carefully designed, even an
architecturally simple model can match or surpass state-of-the-art performance
on six challenging change detection datasets. Our best practices generalise
beyond our architecture and also offer performance improvements when applied to
related methods, indicating that the space of fundamental design choices has
been underexplored. Our guidelines and architecture provide a strong foundation
for future methods, emphasizing that optimizing core components is just as
important as architectural novelty in advancing change detection performance.
Code: https://github.com/blaz-r/BTC-change-detection

</details>


### [55] [MRC-DETR: An Adaptive Multi-Residual Coupled Transformer for Bare Board PCB Defect Detection](https://arxiv.org/abs/2507.03386)
*Jiangzhong Cao,Huanqi Wu,Xu Zhang,Lianghong Tan,Huan Zhang*

Main category: cs.CV

TL;DR: Proposes MRC-DETR, an efficient PCB defect detection framework with improved feature representation, reduced redundancy, and a new dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing PCB defect detection methods, such as poor feature representation, computational inefficiency, and lack of training data.

Method: Introduces Multi-Residual Directional Coupled Block (MRDCB) for better feature interaction and Adaptive Screening Pyramid Network (ASPN) for efficient feature fusion. Also creates a new dataset.

Result: Improved accuracy and efficiency in PCB defect detection, supported by a high-quality dataset.

Conclusion: MRC-DETR effectively tackles industrial challenges in PCB defect inspection, offering a robust solution with enhanced performance.

Abstract: In modern electronic manufacturing, defect detection on Printed Circuit
Boards (PCBs) plays a critical role in ensuring product yield and maintaining
the reliability of downstream assembly processes. However, existing methods
often suffer from limited feature representation, computational redundancy, and
insufficient availability of high-quality training data -- challenges that
hinder their ability to meet industrial demands for both accuracy and
efficiency. To address these limitations, we propose MRC-DETR, a novel and
efficient detection framework tailored for bare PCB defect inspection, built
upon the foundation of RT-DETR. Firstly, to enhance feature representation
capability, we design a Multi-Residual Directional Coupled Block (MRDCB). This
module improves channel-wise feature interaction through a multi-residual
structure. Moreover, a cross-spatial learning strategy is integrated to capture
fine-grained pixel-level relationships, further enriching the representational
power of the extracted features. Secondly, to reduce computational redundancy
caused by inefficient cross-layer information fusion, we introduce an Adaptive
Screening Pyramid Network (ASPN). This component dynamically filters and
aggregates salient low-level features, selectively fusing them with high-level
semantic features. By focusing on informative regions and suppressing redundant
computations, ASPN significantly improves both efficiency and detection
accuracy. Finally, to tackle the issue of insufficient training data,
particularly in the context of bare PCBs, we construct a new, high-quality
dataset that fills a critical gap in current public resources. Our dataset not
only supports the training and evaluation of our proposed framework but also
serves as a valuable benchmark for future research in this domain.

</details>


### [56] [Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos](https://arxiv.org/abs/2507.03393)
*Yufan Zhou,Zhaobo Qi,Lingshuai Lin,Junqi Jing,Tingting Chai,Beichen Zhang,Shuhui Wang,Weigang Zhang*

Main category: cs.CV

TL;DR: The paper introduces the Masked Temporal Interpolation Diffusion (MTID) model for procedure planning in instructional videos, enhancing temporal coherence and task alignment in action sequences.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating coherent action sequences from visual observations, previous methods struggle with temporal relationships. MTID aims to improve this by leveraging enriched visual supervision.

Method: MTID uses a latent space temporal interpolation module within a diffusion model, integrating a learnable interpolation matrix and action-aware mask projection for task-specific training.

Result: MTID achieves promising performance on three benchmark datasets, demonstrating improved temporal coherence and task alignment.

Conclusion: The MTID model effectively enhances procedure planning in instructional videos by integrating enriched supervision and task-specific mechanisms.

Abstract: In this paper, we address the challenge of procedure planning in
instructional videos, aiming to generate coherent and task-aligned action
sequences from start and end visual observations. Previous work has mainly
relied on text-level supervision to bridge the gap between observed states and
unobserved actions, but it struggles with capturing intricate temporal
relationships among actions. Building on these efforts, we propose the Masked
Temporal Interpolation Diffusion (MTID) model that introduces a latent space
temporal interpolation module within the diffusion model. This module leverages
a learnable interpolation matrix to generate intermediate latent features,
thereby augmenting visual supervision with richer mid-state details. By
integrating this enriched supervision into the model, we enable end-to-end
training tailored to task-specific requirements, significantly enhancing the
model's capacity to predict temporally coherent action sequences. Additionally,
we introduce an action-aware mask projection mechanism to restrict the action
generation space, combined with a task-adaptive masked proximity loss to
prioritize more accurate reasoning results close to the given start and end
states over those in intermediate steps. Simultaneously, it filters out
task-irrelevant action predictions, leading to contextually aware action
sequences. Experimental results across three widely used benchmark datasets
demonstrate that our MTID achieves promising action planning performance on
most metrics. The code is available at https://github.com/WiserZhou/MTID.

</details>


### [57] [Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering](https://arxiv.org/abs/2507.03394)
*Qing Li,Huifang Feng,Xun Gong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: A novel method for learning normals from noisy point clouds using local gradient-aware surface filtering, achieving state-of-the-art results in normal estimation, surface reconstruction, and denoising.


<details>
  <summary>Details</summary>
Motivation: Existing methods for normal estimation in noisy point clouds rely on supervised priors and clean data, limiting their effectiveness. This paper aims to address noisy data challenges.

Method: The approach projects noisy points onto surfaces using normals and distances from an implicit function, incorporating local gradient constraints to avoid over-smoothing and gradient degradation.

Result: The method outperforms existing techniques in normal estimation, surface reconstruction, and point cloud denoising, as demonstrated by comprehensive experiments.

Conclusion: The proposed local gradient-aware surface filtering method effectively handles noisy point clouds, offering superior performance and practical applicability.

Abstract: Estimating normals for noisy point clouds is a persistent challenge in 3D
geometry processing, particularly for end-to-end oriented normal estimation.
Existing methods generally address relatively clean data and rely on supervised
priors to fit local surfaces within specific neighborhoods. In this paper, we
propose a novel approach for learning normals from noisy point clouds through
local gradient-aware surface filtering. Our method projects noisy points onto
the underlying surface by utilizing normals and distances derived from an
implicit function constrained by local gradients. We start by introducing a
distance measurement operator for global surface fitting on noisy data, which
integrates projected distances along normals. Following this, we develop an
implicit field-based filtering approach for surface point construction, adding
projection constraints on these points during filtering. To address issues of
over-smoothing and gradient degradation, we further incorporate local gradient
consistency constraints, as well as local gradient orientation and aggregation.
Comprehensive experiments on normal estimation, surface reconstruction, and
point cloud denoising demonstrate the state-of-the-art performance of our
method. The source code and trained models are available at
https://github.com/LeoQLi/LGSF.

</details>


### [58] [Pose-Star: Anatomy-Aware Editing for Open-World Fashion Images](https://arxiv.org/abs/2507.03402)
*Yuran Dong,Mang Ye*

Main category: cs.CV

TL;DR: Pose-Star improves fashion image editing by addressing poor mask controllability in existing pipelines, enhancing user flexibility and pose robustness through anatomy-aware masks and attention calibration.


<details>
  <summary>Details</summary>
Motivation: Existing two-stage pipelines for fashion image editing prioritize generator optimization over mask controllability, leading to poor user-defined flexibility and weak pose robustness.

Method: Pose-Star dynamically recomposes body structures into anatomy-aware masks, calibrates diffusion-derived attention via skeletal keypoints, and refines edges with cross-self attention merging and Canny alignment.

Result: The framework enhances rare structure localization, suppresses noise, and improves edge refinement, bridging controlled benchmarks and open-world demands.

Conclusion: Pose-Star pioneers anatomy-aware, pose-robust editing, laying the foundation for industrial fashion image editing.

Abstract: To advance real-world fashion image editing, we analyze existing two-stage
pipelines(mask generation followed by diffusion-based editing)which overly
prioritize generator optimization while neglecting mask controllability. This
results in two critical limitations: I) poor user-defined flexibility
(coarse-grained human masks restrict edits to predefined regions like upper
torso; fine-grained clothes masks preserve poses but forbid style/length
customization). II) weak pose robustness (mask generators fail due to
articulated poses and miss rare regions like waist, while human parsers remain
limited by predefined categories). To address these gaps, we propose Pose-Star,
a framework that dynamically recomposes body structures (e.g., neck, chest,
etc.) into anatomy-aware masks (e.g., chest-length) for user-defined edits. In
Pose-Star, we calibrate diffusion-derived attention (Star tokens) via skeletal
keypoints to enhance rare structure localization in complex poses, suppress
noise through phase-aware analysis of attention dynamics
(Convergence,Stabilization,Divergence) with threshold masking and
sliding-window fusion, and refine edges via cross-self attention merging and
Canny alignment. This work bridges controlled benchmarks and open-world
demands, pioneering anatomy-aware, pose-robust editing and laying the
foundation for industrial fashion image editing.

</details>


### [59] [Rectifying Adversarial Sample with Low Entropy Prior for Test-Time Defense](https://arxiv.org/abs/2507.03427)
*Lina Ma,Xiaowei Fu,Fuxiang Huang,Xinbo Gao,Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces the low entropy (LE) prior in adversarial samples, proposing a two-stage REAL approach to rectify adversarial samples for improved robustness against unseen attacks.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods lack generalization against unknown attacks, prompting the exploration of common characteristics like the LE prior in adversarial samples.

Method: The REAL approach involves two stages: 1) reverse maximizing prediction entropy to eliminate adversarial nature, and 2) forward minimizing entropy for correct classification, forming a Max-Min entropy optimization. An attack-aware weighting mechanism adjusts objectives.

Result: Experiments show REAL significantly enhances the performance of existing sample rectification models.

Conclusion: The LE prior and REAL approach offer a universal solution for adversarial robustness, addressing generalization issues in defense methods.

Abstract: Existing defense methods fail to defend against unknown attacks and thus
raise generalization issue of adversarial robustness. To remedy this problem,
we attempt to delve into some underlying common characteristics among various
attacks for generality. In this work, we reveal the commonly overlooked low
entropy prior (LE) implied in various adversarial samples, and shed light on
the universal robustness against unseen attacks in inference phase. LE prior is
elaborated as two properties across various attacks as shown in Fig. 1 and Fig.
2: 1) low entropy misclassification for adversarial samples and 2) lower
entropy prediction for higher attack intensity. This phenomenon stands in stark
contrast to the naturally distributed samples. The LE prior can instruct
existing test-time defense methods, thus we propose a two-stage REAL approach:
Rectify Adversarial sample based on LE prior for test-time adversarial
rectification. Specifically, to align adversarial samples more closely with
clean samples, we propose to first rectify adversarial samples misclassified
with low entropy by reverse maximizing prediction entropy, thereby eliminating
their adversarial nature. To ensure the rectified samples can be correctly
classified with low entropy, we carry out secondary rectification by forward
minimizing prediction entropy, thus creating a Max-Min entropy optimization
scheme. Further, based on the second property, we propose an attack-aware
weighting mechanism to adaptively adjust the strengths of Max-Min entropy
objectives. Experiments on several datasets show that REAL can greatly improve
the performance of existing sample rectification models.

</details>


### [60] [Unlearning the Noisy Correspondence Makes CLIP More Robust](https://arxiv.org/abs/2507.03434)
*Haochen Han,Alex Jinpeng Wang,Peijun Ye,Fangming Liu*

Main category: cs.CV

TL;DR: The paper introduces NCU, a framework to unlearn noisy correspondences in pre-trained VLMs, improving robustness without retraining from scratch.


<details>
  <summary>Details</summary>
Motivation: Address the harmful effects of noisy data in VLMs without the resource-intensive need for retraining.

Method: Proposes NCU, a fine-tuning framework that unlearns noisy knowledge by focusing on hardest negative information, formalized into an optimal transport objective.

Result: NCU outperforms robust pre-trained methods on zero-shot transfer tasks with lower computational costs.

Conclusion: NCU offers an efficient solution to enhance VLM robustness by directly mitigating noisy data effects.

Abstract: The data appetite for Vision-Language Models (VLMs) has continuously scaled
up from the early millions to billions today, which faces an untenable
trade-off with data quality and inevitably introduces Noisy Correspondence (NC)
samples. Undoubtedly, such semantically unrelated data significantly impairs
the performance of VLMs. Previous efforts mainly address this challenge by
estimating refined alignment for more precise guidance. However, such
resource-intensive pipelines that train VLMs from scratch struggle to meet
realistic data demands. In this paper, we present a brand new perspective that
seeks to directly eliminate the harmful effects of NC in pre-trained VLMs.
Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning
framework that efficiently enhances VLMs' robustness by forgetting learned
noisy knowledge. The key to NCU is learning the hardest negative information,
which can provide explicit unlearning direction for both false positives and
false negatives. Such twin goals unlearning process can be formalized into one
unified optimal transport objective for fast fine-tuning. We validate our
approach with the prevailing CLIP model over various downstream tasks.
Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer
while with lower computational overhead. The code will be released upon
acceptance.

</details>


### [61] [Radar Tracker: Moving Instance Tracking in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.03441)
*Matthias Zeller,Daniel Casado Herraez,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: A learning-based radar tracker for moving instance tracking in sparse radar point clouds, combining temporal offset predictions and attention-based tracking to improve performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing scene interpretation for robots and autonomous vehicles by improving segmentation and tracking of moving objects using radar sensing.

Method: Proposes a learning-based radar tracker with temporal offset predictions and attention-based tracking, combining geometric and appearance features for association.

Result: Improved performance on the moving instance tracking benchmark of the RadarScenes dataset compared to state-of-the-art.

Conclusion: The approach effectively enhances moving instance tracking in sparse radar point clouds, benefiting autonomous systems.

Abstract: Robots and autonomous vehicles should be aware of what happens in their
surroundings. The segmentation and tracking of moving objects are essential for
reliable path planning, including collision avoidance. We investigate this
estimation task for vehicles using radar sensing. We address moving instance
tracking in sparse radar point clouds to enhance scene interpretation. We
propose a learning-based radar tracker incorporating temporal offset
predictions to enable direct center-based association and enhance segmentation
performance by including additional motion cues. We implement attention-based
tracking for sparse radar scans to include appearance features and enhance
performance. The final association combines geometric and appearance features
to overcome the limitations of center-based tracking to associate instances
reliably. Our approach shows an improved performance on the moving instance
tracking benchmark of the RadarScenes dataset compared to the current state of
the art.

</details>


### [62] [Helping CLIP See Both the Forest and the Trees: A Decomposition and Description Approach](https://arxiv.org/abs/2507.03458)
*Leyan Xue,Zongbo Han,Guangyu Wang,Qinghua Hu,Mingyue Cheng,Changqing Zhang*

Main category: cs.CV

TL;DR: The paper addresses CLIP's bias toward global image patterns by proposing a multi-crop augmentation method to enhance its ability to recognize localized visual details.


<details>
  <summary>Details</summary>
Motivation: Traditional prompt engineering in VLMs like CLIP neglects fine-grained local semantics, and existing methods fail to address CLIP's bias toward global patterns.

Method: The authors introduce stochastic multi-crop augmentation to activate CLIP's latent capacity for localized feature analysis, recalibrating its attention mechanism.

Result: The proposed method, D&D, shows promising performance in zero-shot, few-shot, and test-time adaptation settings.

Conclusion: The approach effectively mitigates CLIP's global bias, enabling it to recognize both global and local visual details.

Abstract: Vision-Language Models (VLMs) like CLIP achieve cross-modal semantic
alignment through contrastive learning, exhibiting robust zero-shot
generalization. Traditional prompt engineering, however, predominantly relies
on coarse-grained category labels, neglecting fine-grained local semantics.
Existing approaches assume that VLMs inherently recognize localized visual
details and attempt to enhance classification by augmenting text prompts with
attribute descriptors generated by large language models. However, our
systematic experiments reveal critical limitations: CLIP's strong bias toward
global image patterns hinders its ability to process localized visual
descriptors. To address this fundamental constraint, we propose a simple,
effective, and plug-and-play solution that enables CLIP to ``See Both the
Forest and the Trees." Specifically, we employ stochastic multi-crop
augmentation to activate CLIP's latent capacity for localized feature analysis.
By cropping only partial regions, the approach effectively constrains the
model's receptive field and recalibrates its attention mechanism, thereby
mitigating its inherent bias. We evaluate the proposed method under zero-shot,
few-shot, and test-time adaptation settings, and extensive experiments
demonstrate that D&D achieves promising performance.

</details>


### [63] [Radar Velocity Transformer: Single-scan Moving Object Segmentation in Noisy Radar Point Clouds](https://arxiv.org/abs/2507.03463)
*Matthias Zeller,Vardeep S. Sandhu,Benedikt Mersch,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: A transformer-based approach for single-scan moving object segmentation in radar point clouds, leveraging Doppler velocity for accurate results.


<details>
  <summary>Details</summary>
Motivation: Enhancing autonomous navigation by improving scene understanding through single-scan radar data, avoiding reliance on temporal data accumulation.

Method: Develops a Radar Velocity Transformer incorporating velocity information and transformer-based upsampling for sparse point clouds.

Result: Outperforms state-of-the-art methods, runs faster than sensor frame rate, and achieves superior segmentation with single-scan data.

Conclusion: The proposed method effectively segments moving objects in radar scans, advancing autonomous vehicle perception.

Abstract: The awareness about moving objects in the surroundings of a self-driving
vehicle is essential for safe and reliable autonomous navigation. The
interpretation of LiDAR and camera data achieves exceptional results but
typically requires to accumulate and process temporal sequences of data in
order to extract motion information. In contrast, radar sensors, which are
already installed in most recent vehicles, can overcome this limitation as they
directly provide the Doppler velocity of the detections and, hence incorporate
instantaneous motion information within a single measurement. % In this paper,
we tackle the problem of moving object segmentation in noisy radar point
clouds. We also consider differentiating parked from moving cars, to enhance
scene understanding. Instead of exploiting temporal dependencies to identify
moving objects, we develop a novel transformer-based approach to perform
single-scan moving object segmentation in sparse radar scans accurately. The
key to our Radar Velocity Transformer is to incorporate the valuable velocity
information throughout each module of the network, thereby enabling the precise
segmentation of moving and non-moving objects. Additionally, we propose a
transformer-based upsampling, which enhances the performance by adaptively
combining information and overcoming the limitation of interpolation of sparse
point clouds. Finally, we create a new radar moving object segmentation
benchmark based on the RadarScenes dataset and compare our approach to other
state-of-the-art methods. Our network runs faster than the frame rate of the
sensor and shows superior segmentation results using only single-scan radar
data.

</details>


### [64] [Information-Bottleneck Driven Binary Neural Network for Change Detection](https://arxiv.org/abs/2507.03504)
*Kaijie Yin,Zhiyuan Zhang,Shu Kong,Tian Gao,Chengzhong Xu,Hui Kong*

Main category: cs.CV

TL;DR: BiCD introduces a binary neural network for change detection, addressing limitations of conventional binarization by enhancing representational power and feature separability using an Information Bottleneck-based auxiliary objective.


<details>
  <summary>Details</summary>
Motivation: Conventional binarization methods in change detection models reduce accuracy by limiting data representation and feature discrimination.

Method: BiCD uses an auxiliary objective based on the Information Bottleneck principle, with a learnable module to approximate mutual information, optimizing both reconstruction and detection loss.

Result: BiCD achieves state-of-the-art performance on street-view and remote sensing datasets, setting a new benchmark for BNN-based change detection.

Conclusion: BiCD effectively improves binary neural networks for change detection, outperforming existing methods.

Abstract: In this paper, we propose Binarized Change Detection (BiCD), the first binary
neural network (BNN) designed specifically for change detection. Conventional
network binarization approaches, which directly quantize both weights and
activations in change detection models, severely limit the network's ability to
represent input data and distinguish between changed and unchanged regions.
This results in significantly lower detection accuracy compared to real-valued
networks. To overcome these challenges, BiCD enhances both the representational
power and feature separability of BNNs, improving detection performance.
Specifically, we introduce an auxiliary objective based on the Information
Bottleneck (IB) principle, guiding the encoder to retain essential input
information while promoting better feature discrimination. Since directly
computing mutual information under the IB principle is intractable, we design a
compact, learnable auxiliary module as an approximation target, leading to a
simple yet effective optimization strategy that minimizes both reconstruction
loss and standard change detection loss. Extensive experiments on street-view
and remote sensing datasets demonstrate that BiCD establishes a new benchmark
for BNN-based change detection, achieving state-of-the-art performance in this
domain.

</details>


### [65] [Multimodal Alignment with Cross-Attentive GRUs for Fine-Grained Video Understanding](https://arxiv.org/abs/2507.03531)
*Namho Kim,Junhwa Kim*

Main category: cs.CV

TL;DR: A multimodal framework combining video, image, and text representations with GRU-based encoders and cross-modal attention outperforms unimodal baselines in fine-grained video classification.


<details>
  <summary>Details</summary>
Motivation: Fine-grained video classification requires understanding complex spatio-temporal and semantic cues beyond a single modality's capacity.

Method: The framework fuses video, image, and text representations using GRU-based sequence encoders and cross-modal attention, trained with classification/regression loss and regularized via feature augmentation and autoencoding.

Result: The model significantly outperforms unimodal baselines on DVD (violence detection) and Aff-Wild2 (valence-arousal estimation) datasets, with cross-attention and feature augmentation enhancing robustness.

Conclusion: The proposed multimodal fusion strategy is effective for fine-grained video classification, demonstrating improved performance and robustness.

Abstract: Fine-grained video classification requires understanding complex
spatio-temporal and semantic cues that often exceed the capacity of a single
modality. In this paper, we propose a multimodal framework that fuses video,
image, and text representations using GRU-based sequence encoders and
cross-modal attention mechanisms. The model is trained using a combination of
classification or regression loss, depending on the task, and is further
regularized through feature-level augmentation and autoencoding techniques. To
evaluate the generality of our framework, we conduct experiments on two
challenging benchmarks: the DVD dataset for real-world violence detection and
the Aff-Wild2 dataset for valence-arousal estimation. Our results demonstrate
that the proposed fusion strategy significantly outperforms unimodal baselines,
with cross-attention and feature augmentation contributing notably to
robustness and performance.

</details>


### [66] [PhenoBench: A Comprehensive Benchmark for Cell Phenotyping](https://arxiv.org/abs/2507.03532)
*Jerome Luescher,Nora Koreuber,Jannik Franzen,Fabian H. Reith,Claudia Winklmayr,Christian M. Schuerch,Dagmar Kainmueller,Josef Lorenz Rumberger*

Main category: cs.CV

TL;DR: PhenoBench is a new benchmark for cell phenotyping on H&E images, introducing PhenoCell dataset and evaluating foundational models (FMs) in diverse scenarios, revealing their limitations and the dataset's challenge.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for cell phenotyping lack unified evaluation of foundational models (FMs), prompting the need for PhenoBench to assess their performance systematically.

Method: PhenoBench includes PhenoCell, a new H&E dataset with 14 cell types, and provides code for fine-tuning and benchmarking FMs under technical and medical domain shifts.

Result: FMs perform poorly on PhenoCell (F1 scores as low as 0.20), highlighting its challenge compared to existing benchmarks like Lizard and PanNuke (F1 > 0.70).

Conclusion: PhenoCell is a valuable benchmark for evaluating FMs and supervised models, revealing gaps in current FM capabilities for cell phenotyping.

Abstract: Digital pathology has seen the advent of a wealth of foundational models
(FM), yet to date their performance on cell phenotyping has not been
benchmarked in a unified manner. We therefore propose PhenoBench: A
comprehensive benchmark for cell phenotyping on Hematoxylin and Eosin (H&E)
stained histopathology images. We provide both PhenoCell, a new H&E dataset
featuring 14 granular cell types identified by using multiplexed imaging, and
ready-to-use fine-tuning and benchmarking code that allows the systematic
evaluation of multiple prominent pathology FMs in terms of dense cell phenotype
predictions in different generalization scenarios. We perform extensive
benchmarking of existing FMs, providing insights into their generalization
behavior under technical vs. medical domain shifts. Furthermore, while FMs
achieve macro F1 scores > 0.70 on previously established benchmarks such as
Lizard and PanNuke, on PhenoCell, we observe scores as low as 0.20. This
indicates a much more challenging task not captured by previous benchmarks,
establishing PhenoCell as a prime asset for future benchmarking of FMs and
supervised models alike. Code and data are available on GitHub.

</details>


### [67] [CLOT: Closed Loop Optimal Transport for Unsupervised Action Segmentation](https://arxiv.org/abs/2507.03539)
*Elena Bueno-Benito,Mariella Dimiccoli*

Main category: cs.CV

TL;DR: CLOT improves unsupervised action segmentation by introducing a multi-level cyclic feature learning mechanism, refining frame embeddings and pseudo-labels through cross-attention and OT problems.


<details>
  <summary>Details</summary>
Motivation: ASOT lacks segment-level supervision, limiting feedback between frames and action representations. CLOT addresses this by integrating a multi-level cyclic learning approach.

Method: CLOT uses an encoder-decoder architecture to solve two OT problems for pseudo-labels and embeddings, then refines them via cross-attention and a third OT problem.

Result: Experiments on four datasets show CLOT's cyclical learning enhances unsupervised action segmentation.

Conclusion: CLOT effectively addresses ASOT's limitations, improving segmentation through cyclical learning and multi-level OT integration.

Abstract: Unsupervised action segmentation has recently pushed its limits with ASOT, an
optimal transport (OT)-based method that simultaneously learns action
representations and performs clustering using pseudo-labels. Unlike other
OT-based approaches, ASOT makes no assumptions on the action ordering, and it
is able to decode a temporally consistent segmentation from a noisy cost matrix
between video frames and action labels. However, the resulting segmentation
lacks segment-level supervision, which limits the effectiveness of the feedback
between frames and action representations. To address this limitation, we
propose Closed Loop Optimal Transport (CLOT), a novel OT-based framework that
introduces a multi-level cyclic feature learning mechanism. Leveraging its
encoder-decoder architecture, CLOT learns pseudo-labels alongside frame and
segment embeddings by solving two separate OT problems. It then refines both
frame embeddings and pseudo-labels through cross-attention between the learned
frame and segment embeddings, integrating a third OT problem. Experimental
results on four benchmark datasets demonstrate the benefits of cyclical
learning for unsupervised action segmentation.

</details>


### [68] [Foundation versus Domain-specific Models: Performance Comparison, Fusion, and Explainability in Face Recognition](https://arxiv.org/abs/2507.03541)
*Redwan Sony,Parisa Farmanifard,Arun Ross,Anil K. Jain*

Main category: cs.CV

TL;DR: Domain-specific face recognition models outperform zero-shot foundation models, but contextual clues and fusion with foundation models improve performance and explainability.


<details>
  <summary>Details</summary>
Motivation: To compare the performance of generic foundation models (e.g., CLIP, BLIP) with domain-specific face recognition models (e.g., AdaFace, ArcFace) and explore their combined potential.

Method: Conducted experiments using benchmark datasets, evaluated zero-shot performance, impact of image segmentation, score-level fusion, and explainability via foundation models like ChatGPT.

Result: Domain-specific models outperformed zero-shot foundation models. Contextual clues improved foundation models' performance. Fusion enhanced accuracy, and foundation models added explainability and resolved low-confidence decisions.

Conclusion: Combining domain-specific models with foundation models judiciously improves face recognition performance and explainability.

Abstract: In this paper, we address the following question: How do generic foundation
models (e.g., CLIP, BLIP, LLaVa, DINO) compare against a domain-specific face
recognition model (viz., AdaFace or ArcFace) on the face recognition task?
Through a series of experiments involving several foundation models and
benchmark datasets, we are able to report the following findings: (a) In all
datasets considered, domain-specific models outperformed zero-shot foundation
models. (b) The performance of zero-shot generic foundation models improves on
over-segmented face images than tightly cropped faces thereby suggesting the
importance of contextual clues. For example, at a False Match Rate (FMR) of
0.01%, the True Match Rate (TMR) of OpenCLIP improved from 64.97% to 81.73% on
the LFW dataset as the face crop increased from 112x112 to 250x250 while the
TMR of domain-specific AdaFace dropped from 99.09% to 77.31%. (c) A simple
score-level fusion of a foundation model with a domain-specific FR model
improved the accuracy at low FMRs. For example, the TMR of AdaFace when fused
with BLIP improved from 72.64% to 83.31% at an FMR of 0.0001% on the IJB-B
dataset and from 73.17% to 85.81% on the IJB-C dataset. (d) Foundation models,
such as ChatGPT, can be used to impart explainability to the FR pipeline (e.g.,
``Despite minor lighting and head tilt differences, the two left-profile images
show high consistency in forehead slope, nose shape, chin contour...''). In
some instances, foundation models are even able to resolve low-confidence
decisions made by AdaFace (e.g., ``Although AdaFace assigns a low similarity
score of 0.21, both images exhibit visual similarity...and the pair is likely
of the same person''), thereby reiterating the importance of combining
domain-specific FR models with generic foundation models in a judicious manner.

</details>


### [69] [Beyond Accuracy: Metrics that Uncover What Makes a `Good' Visual Descriptor](https://arxiv.org/abs/2507.03542)
*Ethan Lin,Linxi Zhao,Atharva Sehgal,Jennifer J. Sun*

Main category: cs.CV

TL;DR: The paper analyzes text-based visual descriptors' effectiveness in VLMs, focusing on representational capacity and alignment with pre-training data. It introduces metrics to evaluate descriptor quality beyond accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand how text-based descriptors impact visual concept discovery and image classification in VLMs, considering factors like semantic clarity and pre-training data alignment.

Method: Systematic analysis of descriptor quality using metrics like Global Alignment and CLIP Similarity, evaluating various descriptor generation methods (e.g., zero-shot LLM prompts, iteratively refined descriptors).

Result: The study reveals how descriptor generation strategies interact with VLM properties, providing insights beyond traditional accuracy evaluations.

Conclusion: Alignment-based metrics offer a deeper understanding of descriptor effectiveness, guiding better descriptor generation for VLMs.

Abstract: Text-based visual descriptors-ranging from simple class names to more
descriptive phrases-are widely used in visual concept discovery and image
classification with vision-language models (VLMs). Their effectiveness,
however, depends on a complex interplay of factors, including semantic clarity,
presence in the VLM's pre-training data, and how well the descriptors serve as
a meaningful representation space. In this work, we systematically analyze
descriptor quality along two key dimensions: (1) representational capacity, and
(2) relationship with VLM pre-training data. We evaluate a spectrum of
descriptor generation methods, from zero-shot LLM-generated prompts to
iteratively refined descriptors. Motivated by ideas from representation
alignment and language understanding, we introduce two alignment-based
metrics-Global Alignment and CLIP Similarity-that move beyond accuracy. These
metrics allow us to shed light on how different descriptor generation
strategies interact with foundation model properties, offering insights into
ways of studying descriptor effectiveness beyond accuracy evaluations.

</details>


### [70] [An Advanced Deep Learning Framework for Ischemic and Hemorrhagic Brain Stroke Diagnosis Using Computed Tomography (CT) Images](https://arxiv.org/abs/2507.03558)
*Md. Sabbir Hossen,Eshat Ahmed Shuvo,Shibbir Ahmed Arif,Pabon Shaha,Md. Saiduzzaman,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: The paper proposes a machine learning approach for early brain stroke detection using CT scans, combining pre-trained models, feature engineering, and classifiers, achieving 97.93% accuracy with MobileNetV2, LDA, and SVC.


<details>
  <summary>Details</summary>
Motivation: Brain stroke is a major cause of mortality and disability, necessitating fast and precise diagnostic methods. Machine learning can enhance traditional CT-based diagnosis.

Method: The study uses pre-trained models (DenseNet201, InceptionV3, etc.) for feature extraction, applies feature engineering (BFO, PCA, LDA), and classifies with ML algorithms (SVC, RF, etc.).

Result: The MobileNetV2-LDA-SVC combination achieved the highest accuracy of 97.93%.

Conclusion: Integrating lightweight pre-trained models with optimization and classification techniques is effective for brain stroke diagnosis.

Abstract: Brain stroke is one of the leading causes of mortality and long-term
disability worldwide, highlighting the need for precise and fast prediction
techniques. Computed Tomography (CT) scan is considered one of the most
effective methods for diagnosing brain strokes. The majority of stroke
classification techniques rely on a single slice-level prediction mechanism,
allowing the radiologist to manually choose the most critical CT slice from the
original CT volume. Although clinical evaluations are often used in traditional
diagnostic procedures, machine learning (ML) has opened up new avenues for
improving stroke diagnosis. To supplement traditional diagnostic techniques,
this study investigates the use of machine learning models, specifically
concerning the prediction of brain stroke at an early stage utilizing CT scan
images. In this research, we proposed a novel approach to brain stroke
detection leveraging machine learning techniques, focusing on optimizing
classification performance with pre-trained deep learning models and advanced
optimization strategies. Pre-trained models, including DenseNet201,
InceptionV3, MobileNetV2, ResNet50, and Xception, are utilized for feature
extraction. Additionally, we employed feature engineering techniques, including
BFO, PCA, and LDA, to enhance models' performance further. These features are
subsequently classified using machine learning algorithms such as SVC, RF, XGB,
DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of
MobileNetV2, LDA, and SVC achieved the highest classification accuracy of
97.93%, significantly outperforming other model-optimizer-classifier
combinations. The results underline the effectiveness of integrating
lightweight pre-trained models with robust optimization and classification
techniques for brain stroke diagnosis.

</details>


### [71] [Predicting Asphalt Pavement Friction Using Texture-Based Image Indicator](https://arxiv.org/abs/2507.03559)
*Bingjie Lu,Zhengyang Lu,Yijiashun Qi,Hanzhe Guo,Tianyao Sun,Zunduo Zhao*

Main category: cs.CV

TL;DR: A texture-based image indicator is proposed to predict pavement friction, validated with high accuracy (R-square > 0.90) across three asphalt types.


<details>
  <summary>Details</summary>
Motivation: Pavement skid resistance is crucial for road safety, but current friction measurement methods can be costly and complex. This study aims to provide an easy, inexpensive solution using digital images.

Method: Three asphalt surfaces were evaluated under tire polishing cycles. Images and friction measurements (using DFT) were analyzed, with aggregate protrusion area proposed as the indicator. Statistical models linked the indicator to friction coefficients.

Result: The proposed indicator achieved adjusted R-square values above 0.90, outperforming other image-based methods in tracking friction changes with polishing cycles.

Conclusion: The image indicator is cost-effective and accurate, suitable for integrating pavement friction assessment into mix design.

Abstract: Pavement skid resistance is of vital importance for road safety. The
objective of this study is to propose and validate a texture-based image
indicator to predict pavement friction. This index enables pavement friction to
be measured easily and inexpensively using digital images. Three different
types of asphalt surfaces (dense-graded asphalt mix, open-grade friction
course, and chip seal) were evaluated subject to various tire polishing cycles.
Images were taken with corresponding friction measured using Dynamic Friction
Tester (DFT) in the laboratory. The aggregate protrusion area is proposed as
the indicator. Statistical models are established for each asphalt surface type
to correlate the proposed indicator with friction coefficients. The results
show that the adjusted R-square values of all relationships are above 0.90.
Compared to other image-based indicators in the literature, the proposed image
indicator more accurately reflects the changes in pavement friction with the
number of polishing cycles, proving its cost-effective use for considering
pavement friction in mix design stage.

</details>


### [72] [2.5D Object Detection for Intelligent Roadside Infrastructure](https://arxiv.org/abs/2507.03564)
*Nikolai Polley,Yacin Boualili,Ferdinand Mtsch,Maximilian Zipfl,Tobias Fleck,J. Marius Zllner*

Main category: cs.CV

TL;DR: A 2.5D object detection framework is introduced for roadside infrastructure cameras to address domain shift issues in top-down views, improving autonomous vehicle perception.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicle sensors can be obstructed; roadside infrastructure offers complementary data but faces challenges with top-down perspectives.

Method: Uses a 2.5D approach to detect vehicle ground planes as parallelograms, leveraging real and synthetic data for training.

Result: High accuracy, strong generalization across viewpoints, and robustness to adverse weather and lighting.

Conclusion: The framework effectively enhances perception for autonomous vehicles via infrastructure cameras.

Abstract: On-board sensors of autonomous vehicles can be obstructed, occluded, or
limited by restricted fields of view, complicating downstream driving
decisions. Intelligent roadside infrastructure perception systems, installed at
elevated vantage points, can provide wide, unobstructed intersection coverage,
supplying a complementary information stream to autonomous vehicles via
vehicle-to-everything (V2X) communication. However, conventional 3D
object-detection algorithms struggle to generalize under the domain shift
introduced by top-down perspectives and steep camera angles. We introduce a
2.5D object detection framework, tailored specifically for infrastructure
roadside-mounted cameras. Unlike conventional 2D or 3D object detection, we
employ a prediction approach to detect ground planes of vehicles as
parallelograms in the image frame. The parallelogram preserves the planar
position, size, and orientation of objects while omitting their height, which
is unnecessary for most downstream applications. For training, a mix of
real-world and synthetically generated scenes is leveraged. We evaluate
generalizability on a held-out camera viewpoint and in adverse-weather
scenarios absent from the training set. Our results show high detection
accuracy, strong cross-viewpoint generalization, and robustness to diverse
lighting and weather conditions. Model weights and inference code are provided
at: https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection

</details>


### [73] [SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications](https://arxiv.org/abs/2507.03578)
*Yana Hasson,Pauline Luc,Liliane Momeni,Maks Ovsjanikov,Guillaume Le Moing,Alina Kuznetsova,Ira Ktena,Jennifer J. Sun,Skanda Koppula,Dilara Gokay,Joseph Heyward,Etienne Pot,Andrew Zisserman*

Main category: cs.CV

TL;DR: The paper introduces SciVid, a benchmark for evaluating video foundation models (ViFMs) across diverse scientific tasks, showing their potential for transfer learning and highlighting limitations.


<details>
  <summary>Details</summary>
Motivation: To assess whether pretrained ViFMs can effectively transfer knowledge across scientific disciplines and compete with domain-specific models.

Method: Adapt six leading ViFMs to SciVid using trainable readout modules and evaluate their performance on five scientific video tasks.

Result: State-of-the-art results in some applications, demonstrating effective transfer learning, but also revealing limitations of current ViFMs.

Conclusion: ViFMs show promise for general-purpose scientific applications, but further development is needed for broader generalizability.

Abstract: In recent years, there has been a proliferation of spatiotemporal foundation
models in different scientific disciplines. While promising, these models are
often domain-specific and are only assessed within the particular applications
for which they are designed. Given that many tasks can be represented as video
modeling problems, video foundation models (ViFMs) hold considerable promise as
general-purpose domain-agnostic approaches. However, it is not known whether
the knowledge acquired on large-scale but potentially out-of-domain data can be
effectively transferred across diverse scientific disciplines, and if a single,
pretrained ViFM can be competitive with domain-specific baselines. To address
this, we introduce SciVid, a comprehensive benchmark comprising five
*Sci*entific *Vid*eo tasks, across medical computer vision, animal behavior,
and weather forecasting. We adapt six leading ViFMs to SciVid using simple
trainable readout modules, establishing strong baselines and demonstrating the
potential for effective transfer learning. Specifically, we show that
state-of-the-art results can be obtained in several applications by leveraging
the general-purpose representations from ViFM backbones. Furthermore, our
results reveal the limitations of existing ViFMs, and highlight opportunities
for the development of generalizable models for high-impact scientific
applications. We release our code at https://github.com/google-deepmind/scivid
to facilitate further research in the development of ViFMs.

</details>


### [74] [Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation](https://arxiv.org/abs/2507.03585)
*Tao Tang,Shijie Xu,Yiting Wu,Zhixiang Lu*

Main category: cs.CV

TL;DR: Causal-SAM-LLM improves medical image segmentation by using LLMs for causal reasoning, disentangling spurious correlations, and enabling real-time error correction, achieving state-of-the-art OOD robustness.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for medical image segmentation often fail to generalize due to spurious correlations between anatomy and imaging styles.

Method: Introduces Causal-SAM-LLM with Linguistic Adversarial Disentanglement (LAD) to remove non-causal information and Test-Time Causal Intervention (TCI) for real-time error correction via LLMs.

Result: Achieves up to 6.2-point Dice score improvement and 15.8 mm reduction in Hausdorff Distance, using <9% trainable parameters.

Conclusion: Causal-SAM-LLM offers a robust, efficient, and interactive approach for medical AI systems.

Abstract: The clinical utility of deep learning models for medical image segmentation
is severely constrained by their inability to generalize to unseen domains.
This failure is often rooted in the models learning spurious correlations
between anatomical content and domain-specific imaging styles. To overcome this
fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that
elevates Large Language Models (LLMs) to the role of causal reasoners. Our
framework, built upon a frozen Segment Anything Model (SAM) encoder,
incorporates two synergistic innovations. First, Linguistic Adversarial
Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual
descriptions of confounding image styles. By training the segmentation model's
features to be contrastively dissimilar to these style descriptions, it learns
a representation robustly purged of non-causal information. Second, Test-Time
Causal Intervention (TCI) provides an interactive mechanism where an LLM
interprets a clinician's natural language command to modulate the segmentation
decoder's features in real-time, enabling targeted error correction. We conduct
an extensive empirical evaluation on a composite benchmark from four public
datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under
cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM
establishes a new state of the art in out-of-distribution (OOD) robustness,
improving the average Dice score by up to 6.2 points and reducing the Hausdorff
Distance by 15.8 mm over the strongest baseline, all while using less than 9%
of the full model's trainable parameters. Our work charts a new course for
building robust, efficient, and interactively controllable medical AI systems.

</details>


### [75] [From Video to EEG: Adapting Joint Embedding Predictive Architecture to Uncover Visual Concepts in Brain Signal Analysis](https://arxiv.org/abs/2507.03633)
*Amir Hojjati,Lu Li,Ibrahim Hameed,Anis Yazidi,Pedro G. Lind,Rabindra Khadka*

Main category: cs.CV

TL;DR: EEG-VJEPA, a novel adaptation of V-JEPA for EEG classification, outperforms state-of-the-art models by learning spatiotemporal representations from EEG signals treated as video-like sequences.


<details>
  <summary>Details</summary>
Motivation: Effective EEG analysis is hindered by limited labeled data, high dimensionality, and lack of scalable models capturing spatiotemporal dependencies. Existing SSL methods focus on either spatial or temporal features, leading to suboptimal representations.

Method: EEG-VJEPA adapts V-JEPA for EEG classification, treating EEG as video-like sequences to learn spatiotemporal representations using joint embeddings and adaptive masking.

Result: EEG-VJEPA outperforms state-of-the-art models on the TUH Abnormal EEG dataset and captures physiologically relevant patterns, offering interpretable embeddings.

Conclusion: EEG-VJEPA is a scalable, trustworthy framework for EEG analysis, supporting human-AI collaboration in clinical settings.

Abstract: EEG signals capture brain activity with high temporal and low spatial
resolution, supporting applications such as neurological diagnosis, cognitive
monitoring, and brain-computer interfaces. However, effective analysis is
hindered by limited labeled data, high dimensionality, and the absence of
scalable models that fully capture spatiotemporal dependencies. Existing
self-supervised learning (SSL) methods often focus on either spatial or
temporal features, leading to suboptimal representations. To this end, we
propose EEG-VJEPA, a novel adaptation of the Video Joint Embedding Predictive
Architecture (V-JEPA) for EEG classification. By treating EEG as video-like
sequences, EEG-VJEPA learns semantically meaningful spatiotemporal
representations using joint embeddings and adaptive masking. To our knowledge,
this is the first work that exploits V-JEPA for EEG classification and explores
the visual concepts learned by the model. Evaluations on the publicly available
Temple University Hospital (TUH) Abnormal EEG dataset show that EEG-VJEPA
outperforms existing state-of-the-art models in classification accuracy.Beyond
classification accuracy, EEG-VJEPA captures physiologically relevant spatial
and temporal signal patterns, offering interpretable embeddings that may
support human-AI collaboration in diagnostic workflows. These findings position
EEG-VJEPA as a promising framework for scalable, trustworthy EEG analysis in
real-world clinical settings.

</details>


### [76] [Dynamic Multimodal Prototype Learning in Vision-Language Models](https://arxiv.org/abs/2507.03657)
*Xingyu Zhu,Shuo Wang,Beier Zhu,Miaoge Li,Yunfan Li,Junfeng Fang,Zhicai Wang,Dongsheng Wang,Hanwang Zhang*

Main category: cs.CV

TL;DR: ProtoMM is a training-free framework for test-time adaptation of vision-language models, using multimodal prototypes to address ambiguities in class names and improve performance.


<details>
  <summary>Details</summary>
Motivation: Previous methods focus on textual prototypes, overlooking ambiguous semantics in class names, leading to limited performance.

Method: ProtoMM constructs multimodal prototypes combining textual descriptions and visual particles, dynamically updated during testing. It formulates semantic distance as an optimal transport problem.

Result: Achieves a 1.03% average accuracy improvement over state-of-the-art methods on 15 zero-shot benchmarks, including ImageNet.

Conclusion: ProtoMM effectively enhances prototype learning and generalizability in unseen scenarios by leveraging multimodal features.

Abstract: With the increasing attention to pre-trained vision-language models (VLMs),
\eg, CLIP, substantial efforts have been devoted to many downstream tasks,
especially in test-time adaptation (TTA). However, previous works focus on
learning prototypes only in the textual modality while overlooking the
ambiguous semantics in class names. These ambiguities lead to textual
prototypes that are insufficient to capture visual concepts, resulting in
limited performance. To address this issue, we introduce \textbf{ProtoMM}, a
training-free framework that constructs multimodal prototypes to adapt VLMs
during the test time. By viewing the prototype as a discrete distribution over
the textual descriptions and visual particles, ProtoMM has the ability to
combine the multimodal features for comprehensive prototype learning. More
importantly, the visual particles are dynamically updated as the testing stream
flows. This allows our multimodal prototypes to continually learn from the
data, enhancing their generalizability in unseen scenarios. In addition, we
quantify the importance of the prototypes and test images by formulating their
semantic distance as an optimal transport problem. Extensive experiments on 15
zero-shot benchmarks demonstrate the effectiveness of our method, achieving a
1.03\% average accuracy improvement over state-of-the-art methods on ImageNet
and its variant datasets.

</details>


### [77] [On the rankability of visual embeddings](https://arxiv.org/abs/2507.03683)
*Ankit Sonthalia,Arnas Uselis,Seong Joon Oh*

Main category: cs.CV

TL;DR: Visual embedding models often capture ordinal attributes linearly, termed 'rank axes,' with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: To investigate if visual embeddings inherently preserve continuous, ordinal attributes like age or aesthetics along linear directions.

Method: Analyzed 7 encoders and 9 datasets for rankability, using minimal samples (even two extremes) to recover rank axes.

Result: Many embeddings are inherently rankable, with minimal supervision needed for meaningful rank axes.

Conclusion: Findings enable new image ranking applications and motivate further research into rankable embeddings.

Abstract: We study whether visual embedding models capture continuous, ordinal
attributes along linear directions, which we term _rank axes_. We define a
model as _rankable_ for an attribute if projecting embeddings onto such an axis
preserves the attribute's order. Across 7 popular encoders and 9 datasets with
attributes like age, crowd count, head pose, aesthetics, and recency, we find
that many embeddings are inherently rankable. Surprisingly, a small number of
samples, or even just two extreme examples, often suffice to recover meaningful
rank axes, without full-scale supervision. These findings open up new use cases
for image ranking in vector databases and motivate further study into the
structure and learning of rankable embeddings. Our code is available at
https://github.com/aktsonthalia/rankable-vision-embeddings.

</details>


### [78] [SAMed-2: Selective Memory Enhanced Medical Segment Anything Model](https://arxiv.org/abs/2507.03698)
*Zhiling Yan,Sifan Song,Dingjie Song,Yiwei Li,Rong Zhou,Weixiang Sun,Zhennong Chen,Sekeun Kim,Hui Ren,Tianming Liu,Quanzheng Li,Xiang Li,Lifang He,Lichao Sun*

Main category: cs.CV

TL;DR: SAMed-2 is a foundation model for medical image segmentation, addressing challenges like noisy data and continual learning with a temporal adapter and confidence-driven memory mechanism. It outperforms baselines on multi-task scenarios.


<details>
  <summary>Details</summary>
Motivation: Adapting general segmentation models to medical images is difficult due to data complexity, noisy annotations, and continual learning needs across diverse modalities.

Method: Introduces a temporal adapter for image correlations and a confidence-driven memory mechanism to handle noise and prevent forgetting. Trained on MedBank-100k, a large multi-modality dataset.

Result: SAMed-2 shows superior performance on internal benchmarks and 10 external datasets compared to state-of-the-art baselines.

Conclusion: SAMed-2 effectively addresses medical segmentation challenges and sets a new benchmark for multi-task scenarios.

Abstract: Recent "segment anything" efforts show promise by learning from large-scale
data, but adapting such models directly to medical images remains challenging
due to the complexity of medical data, noisy annotations, and continual
learning requirements across diverse modalities and anatomical structures. In
this work, we propose SAMed-2, a new foundation model for medical image
segmentation built upon the SAM-2 architecture. Specifically, we introduce a
temporal adapter into the image encoder to capture image correlations and a
confidence-driven memory mechanism to store high-certainty features for later
retrieval. This memory-based strategy counters the pervasive noise in
large-scale medical datasets and mitigates catastrophic forgetting when
encountering new tasks or modalities. To train and evaluate SAMed-2, we curate
MedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21
medical segmentation tasks. Our experiments on both internal benchmarks and 10
external datasets demonstrate superior performance over state-of-the-art
baselines in multi-task scenarios. The code is available at:
https://github.com/ZhilingYan/Medical-SAM-Bench.

</details>


### [79] [Sign Spotting Disambiguation using Large Language Models](https://arxiv.org/abs/2507.03703)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: A training-free framework using LLMs improves sign spotting by combining spatio-temporal features, hand shape matching, and context-aware disambiguation, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and vocabulary inflexibility in sign language translation by automating frame-level annotations.

Method: Extracts global spatio-temporal and hand shape features, matches them to a sign dictionary using dynamic time warping and cosine similarity, and employs LLMs for context-aware disambiguation.

Result: Superior accuracy and sentence fluency on synthetic and real-world datasets compared to traditional approaches.

Conclusion: LLMs enhance sign spotting without training, offering flexibility and improved performance, advancing the field.

Abstract: Sign spotting, the task of identifying and localizing individual signs within
continuous sign language video, plays a pivotal role in scaling dataset
annotations and addressing the severe data scarcity issue in sign language
translation. While automatic sign spotting holds great promise for enabling
frame-level supervision at scale, it grapples with challenges such as
vocabulary inflexibility and ambiguity inherent in continuous sign streams.
Hence, we introduce a novel, training-free framework that integrates Large
Language Models (LLMs) to significantly enhance sign spotting quality. Our
approach extracts global spatio-temporal and hand shape features, which are
then matched against a large-scale sign dictionary using dynamic time warping
and cosine similarity. This dictionary-based matching inherently offers
superior vocabulary flexibility without requiring model retraining. To mitigate
noise and ambiguity from the matching process, an LLM performs context-aware
gloss disambiguation via beam search, notably without fine-tuning. Extensive
experiments on both synthetic and real-world sign language datasets demonstrate
our method's superior accuracy and sentence fluency compared to traditional
approaches, highlighting the potential of LLMs in advancing sign spotting.

</details>


### [80] [Computationally efficient non-Intrusive pre-impact fall detection system](https://arxiv.org/abs/2507.03705)
*Praveen Jesudhas,Raghuveera T,Shiney Jeyaraj*

Main category: cs.CV

TL;DR: A non-intrusive, computationally efficient pre-impact fall detection system using video data and minimal skeletal features with an LSTM network, achieving 88% accuracy and 18x lower computation than existing systems.


<details>
  <summary>Details</summary>
Motivation: Existing fall detection systems are either intrusive or computationally expensive, limiting global adoption. This work aims to address these issues.

Method: Utilizes video data and skeletal features to detect falls. Uses a lightweight LSTM network designed for minimal computational cost.

Result: Achieves 88% accuracy with 18x lower computation than existing systems.

Conclusion: The system is suitable for wider adoption due to its efficiency and accuracy, benefiting industrial and residential safety.

Abstract: Existing pre-impact fall detection systems have high accuracy, however they
are either intrusive to the subject or require heavy computational resources
for fall detection, resulting in prohibitive deployment costs. These factors
limit the global adoption of existing fall detection systems. In this work we
present a Pre-impact fall detection system that is both non-intrusive and
computationally efficient at deployment. Our system utilizes video data of the
locality available through cameras, thereby requiring no specialized equipment
to be worn by the subject. Further, the fall detection system utilizes minimal
fall specific features and simplistic neural network models, designed to reduce
the computational cost of the system. A minimal set of fall specific features
are derived from the skeletal data, post observing the relative position of
human skeleton during fall. These features are shown to have different
distributions for Fall and non-fall scenarios proving their discriminative
capability. A Long Short Term Memory (LSTM) based network is selected and the
network architecture and training parameters are designed after evaluation of
performance on standard datasets. In the Pre-impact fall detection system the
computation requirement is about 18 times lesser than existing modules with a
comparable accuracy of 88%. Given the low computation requirements and higher
accuracy levels, the proposed system is suitable for wider adoption in
engineering systems related to industrial and residential safety.

</details>


### [81] [Less is More: Empowering GUI Agent with Context-Aware Simplification](https://arxiv.org/abs/2507.03730)
*Gongwei Chen,Xurui Zhou,Rui Shao,Yibo Lyu,Kaiwen Zhou,Shuai Wang,Wentao Li,Yinchuan Li,Zhongang Qi,Liqiang Nie*

Main category: cs.CV

TL;DR: SimpAgent is a context-aware GUI agent framework addressing element and history context challenges, reducing FLOPs by 27% and improving navigation performance.


<details>
  <summary>Details</summary>
Motivation: Current GUI agents neglect contextual modeling challenges, focusing on pre-training data. This work addresses the high-density, loose-relation of elements and high redundancy in history context.

Method: Proposes a masking-based element pruning method and a consistency-guided history compression module to simplify context modeling.

Result: SimpAgent reduces FLOPs by 27% and achieves superior GUI navigation performance in diverse environments.

Conclusion: The framework effectively balances performance and efficiency, demonstrating potential for practical GUI agent applications.

Abstract: The research focus of GUI agents is shifting from text-dependent to
pure-vision-based approaches, which, though promising, prioritize comprehensive
pre-training data collection while neglecting contextual modeling challenges.
We probe the characteristics of element and history contextual modeling in GUI
agent and summarize: 1) the high-density and loose-relation of element context
highlight the existence of many unrelated elements and their negative
influence; 2) the high redundancy of history context reveals the inefficient
history modeling in current GUI agents. In this work, we propose a
context-aware simplification framework for building an efficient and effective
GUI Agent, termed SimpAgent. To mitigate potential interference from numerous
unrelated elements, we introduce a masking-based element pruning method that
circumvents the intractable relation modeling through an efficient masking
mechanism. To reduce the redundancy in historical information, we devise a
consistency-guided history compression module, which enhances implicit
LLM-based compression through innovative explicit guidance, achieving an
optimal balance between performance and efficiency. With the above components,
SimpAgent reduces 27% FLOPs and achieves superior GUI navigation performances.
Comprehensive navigation experiments across diverse web and mobile environments
demonstrate the effectiveness and potential of our agent.

</details>


### [82] [Outdoor Monocular SLAM with Global Scale-Consistent 3D Gaussian Pointmaps](https://arxiv.org/abs/2507.03737)
*Chong Cheng,Sicheng Yu,Zijian Wang,Yifan Zhou,Hao Wang*

Main category: cs.CV

TL;DR: S3PO-GS is a robust RGB-only outdoor 3D Gaussian Splatting SLAM method that addresses scale drift and lack of geometric priors, achieving state-of-the-art results in tracking accuracy and scene reconstruction.


<details>
  <summary>Details</summary>
Motivation: Previous 3DGS SLAM methods lack geometric priors in outdoor scenes or suffer from scale drift with significant camera movement.

Method: S3PO-GS uses a self-consistent tracking module anchored in the 3DGS pointmap and a patch-based dynamic mapping module to introduce geometric priors.

Result: The method outperforms other 3DGS SLAM methods in tracking accuracy and achieves high-fidelity scene reconstruction, as demonstrated on Waymo, KITTI, and DL3DV datasets.

Conclusion: S3PO-GS is particularly effective for complex outdoor environments, offering precise tracking and improved reconstruction quality.

Abstract: 3D Gaussian Splatting (3DGS) has become a popular solution in SLAM due to its
high-fidelity and real-time novel view synthesis performance. However, some
previous 3DGS SLAM methods employ a differentiable rendering pipeline for
tracking, \textbf{lack geometric priors} in outdoor scenes. Other approaches
introduce separate tracking modules, but they accumulate errors with
significant camera movement, leading to \textbf{scale drift}. To address these
challenges, we propose a robust RGB-only outdoor 3DGS SLAM method: S3PO-GS.
Technically, we establish a self-consistent tracking module anchored in the
3DGS pointmap, which avoids cumulative scale drift and achieves more precise
and robust tracking with fewer iterations. Additionally, we design a
patch-based pointmap dynamic mapping module, which introduces geometric priors
while avoiding scale ambiguity. This significantly enhances tracking accuracy
and the quality of scene reconstruction, making it particularly suitable for
complex outdoor environments. Our experiments on the Waymo, KITTI, and DL3DV
datasets demonstrate that S3PO-GS achieves state-of-the-art results in novel
view synthesis and outperforms other 3DGS SLAM methods in tracking accuracy.
Project page: https://3dagentworld.github.io/S3PO-GS/.

</details>


### [83] [Flow-Anchored Consistency Models](https://arxiv.org/abs/2507.03738)
*Yansong Peng,Kai Zhu,Yu Liu,Pingyu Wu,Hebei Li,Xiaoyan Sun,Feng Wu*

Main category: cs.CV

TL;DR: The paper introduces Flow-Anchored Consistency Model (FACM), a training strategy to stabilize Consistency Models (CMs) by anchoring them in the underlying probability flow, achieving state-of-the-art few-step generation performance.


<details>
  <summary>Details</summary>
Motivation: Training instability in CMs arises from losing grasp on the instantaneous velocity field when learning shortcuts.

Method: FACM uses Flow Matching (FM) as an anchor during CM training, requiring no architectural changes.

Result: Achieves FID of 1.32 (NFE=2) and 1.76 (NFE=1) on ImageNet 256x256, outperforming prior methods.

Conclusion: FACM provides a general and effective approach for high-performance few-step generative models.

Abstract: Continuous-time Consistency Models (CMs) promise efficient few-step
generation but face significant challenges with training instability. We argue
this instability stems from a fundamental conflict: by training a network to
learn only a shortcut across a probability flow, the model loses its grasp on
the instantaneous velocity field that defines the flow. Our solution is to
explicitly anchor the model in the underlying flow during training. We
introduce the Flow-Anchored Consistency Model (FACM), a simple but effective
training strategy that uses a Flow Matching (FM) task as an anchor for the
primary CM shortcut objective. This Flow-Anchoring approach requires no
architectural modifications and is broadly compatible with standard model
architectures. By distilling a pre-trained LightningDiT model, our method
achieves a state-of-the-art FID of 1.32 with two steps (NFE=2) and 1.76 with
just one step (NFE=1) on ImageNet 256x256, significantly outperforming previous
methods. This provides a general and effective recipe for building
high-performance, few-step generative models. Our code and pretrained models:
https://github.com/ali-vilab/FACM.

</details>


### [84] [ChestGPT: Integrating Large Language Models and Vision Transformers for Disease Detection and Localization in Chest X-Rays](https://arxiv.org/abs/2507.03739)
*Shehroz S. Khan,Petar Przulj,Ahmed Ashraf,Ali Abedi*

Main category: cs.CV

TL;DR: ChestGPT integrates EVA ViT and Llama 2 LLM to classify and localize diseases in chest X-rays, achieving strong performance and aiding radiologists.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for radiologists by leveraging AI to enhance diagnostic accuracy and efficiency.

Method: Combines EVA ViT to tokenize X-ray images and Llama 2 LLM for joint classification and localization using transfer learning and engineered prompts.

Result: Achieved an F1 score of 0.76 on the VinDr-CXR dataset and successfully localized pathologies with bounding boxes.

Conclusion: ChestGPT serves as an assistive tool to reduce radiologists' workload by providing preliminary findings and regions of interest.

Abstract: The global demand for radiologists is increasing rapidly due to a growing
reliance on medical imaging services, while the supply of radiologists is not
keeping pace. Advances in computer vision and image processing technologies
present significant potential to address this gap by enhancing radiologists'
capabilities and improving diagnostic accuracy. Large language models (LLMs),
particularly generative pre-trained transformers (GPTs), have become the
primary approach for understanding and generating textual data. In parallel,
vision transformers (ViTs) have proven effective at converting visual data into
a format that LLMs can process efficiently. In this paper, we present ChestGPT,
a deep-learning framework that integrates the EVA ViT with the Llama 2 LLM to
classify diseases and localize regions of interest in chest X-ray images. The
ViT converts X-ray images into tokens, which are then fed, together with
engineered prompts, into the LLM, enabling joint classification and
localization of diseases. This approach incorporates transfer learning
techniques to enhance both explainability and performance. The proposed method
achieved strong global disease classification performance on the VinDr-CXR
dataset, with an F1 score of 0.76, and successfully localized pathologies by
generating bounding boxes around the regions of interest. We also outline
several task-specific prompts, in addition to general-purpose prompts, for
scenarios radiologists might encounter. Overall, this framework offers an
assistive tool that can lighten radiologists' workload by providing preliminary
findings and regions of interest to facilitate their diagnostic process.

</details>


### [85] [StreamDiT: Real-Time Streaming Text-to-Video Generation](https://arxiv.org/abs/2507.03745)
*Akio Kodaira,Tingbo Hou,Ji Hou,Masayoshi Tomizuka,Yue Zhao*

Main category: cs.CV

TL;DR: StreamDiT is a streaming video generation model addressing real-time and interactive limitations of existing text-to-video models. It uses flow matching, mixed training, and adaLN DiT with distillation to achieve 16 FPS on one GPU.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models produce only short clips offline, limiting real-time and interactive applications.

Method: Proposes StreamDiT with flow matching, mixed training, adaLN DiT, and multistep distillation to reduce function evaluations.

Result: Achieves real-time performance at 16 FPS on one GPU, generating 512p video streams.

Conclusion: StreamDiT enables real-time applications like streaming and interactive generation, validated by metrics and human evaluation.

Abstract: Recently, great progress has been achieved in text-to-video (T2V) generation
by scaling transformer-based diffusion models to billions of parameters, which
can generate high-quality videos. However, existing models typically produce
only short clips offline, restricting their use cases in interactive and
real-time applications. This paper addresses these challenges by proposing
StreamDiT, a streaming video generation model. StreamDiT training is based on
flow matching by adding a moving buffer. We design mixed training with
different partitioning schemes of buffered frames to boost both content
consistency and visual quality. StreamDiT modeling is based on adaLN DiT with
varying time embedding and window attention. To practice the proposed method,
we train a StreamDiT model with 4B parameters. In addition, we propose a
multistep distillation method tailored for StreamDiT. Sampling distillation is
performed in each segment of a chosen partitioning scheme. After distillation,
the total number of function evaluations (NFEs) is reduced to the number of
chunks in a buffer. Finally, our distilled model reaches real-time performance
at 16 FPS on one GPU, which can generate video streams at 512p resolution. We
evaluate our method through both quantitative metrics and human evaluation. Our
model enables real-time applications, e.g. streaming generation, interactive
generation, and video-to-video. We provide video results and more examples in
our project website: <a href="https://cumulo-autumn.github.io/StreamDiT/">this
https URL.</a>

</details>


### [86] [Efficient Event-Based Semantic Segmentation via Exploiting Frame-Event Fusion: A Hybrid Neural Network Approach](https://arxiv.org/abs/2507.03765)
*Hebei Li,Yansong Peng,Jiahui Yuan,Peixi Wu,Jin Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: A hybrid framework combining Spiking Neural Networks for events and Artificial Neural Networks for frames improves semantic segmentation by integrating temporal and spatial data efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully utilize complementary frame and event data, leading to inefficiencies.

Method: Proposes three modules (ATW Injector, EDS Injector, CSF) to integrate event and frame features dynamically and selectively.

Result: Achieves state-of-the-art accuracy on three datasets and reduces energy consumption by 65% on DSEC-Semantic.

Conclusion: The framework efficiently leverages event and frame data for superior segmentation performance and energy savings.

Abstract: Event cameras have recently been introduced into image semantic segmentation,
owing to their high temporal resolution and other advantageous properties.
However, existing event-based semantic segmentation methods often fail to fully
exploit the complementary information provided by frames and events, resulting
in complex training strategies and increased computational costs. To address
these challenges, we propose an efficient hybrid framework for image semantic
segmentation, comprising a Spiking Neural Network branch for events and an
Artificial Neural Network branch for frames. Specifically, we introduce three
specialized modules to facilitate the interaction between these two branches:
the Adaptive Temporal Weighting (ATW) Injector, the Event-Driven Sparse (EDS)
Injector, and the Channel Selection Fusion (CSF) module. The ATW Injector
dynamically integrates temporal features from event data into frame features,
enhancing segmentation accuracy by leveraging critical dynamic temporal
information. The EDS Injector effectively combines sparse event data with rich
frame features, ensuring precise temporal and spatial information alignment.
The CSF module selectively merges these features to optimize segmentation
performance. Experimental results demonstrate that our framework not only
achieves state-of-the-art accuracy across the DDD17-Seg, DSEC-Semantic, and
M3ED-Semantic datasets but also significantly reduces energy consumption,
achieving a 65\% reduction on the DSEC-Semantic dataset.

</details>


### [87] [FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed](https://arxiv.org/abs/2507.03779)
*Jiaqi Zhang,Juntuo Wang,Zhixin Sun,John Zou,Randall Balestriero*

Main category: cs.CV

TL;DR: A novel pre-training strategy for DINOv2 accelerates convergence and enhances robustness using frequency filtering curriculum and Gaussian noise patching, reducing pre-training time and FLOPs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Address the computational demands of reproducing large-scale vision foundation models for scenarios like private data, new modalities, or scientific questioning.

Method: Proposes a frequency filtering curriculum (low-frequency first) and Gaussian noise patching augmentation for DINOv2 pre-training.

Result: Reduces pre-training time and FLOPs by 1.6x and 2.25x, achieves matching robustness on ImageNet-C, and maintains competitive linear probing performance.

Conclusion: The method makes large-scale self-supervised foundation modeling more efficient and robust, enabling further exploration of data curriculum and augmentation for model robustness.

Abstract: Large-scale vision foundation models such as DINOv2 boast impressive
performances by leveraging massive architectures and training datasets. But
numerous scenarios require practitioners to reproduce those pre-training
solutions, such as on private data, new modalities, or simply for scientific
questioning--which is currently extremely demanding computation-wise. We thus
propose a novel pre-training strategy for DINOv2 that simultaneously
accelerates convergence--and strengthens robustness to common corruptions as a
by-product. Our approach involves a frequency filtering
curriculum--low-frequency being seen first--and the Gaussian noise patching
augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while
pre-training time and FLOPs are reduced by 1.6x and 2.25x, our method still
achieves matching robustness in corruption benchmarks (ImageNet-C) and
maintains competitive linear probing performance compared with baseline. This
dual benefit of efficiency and robustness makes large-scale self-supervised
foundation modeling more attainable, while opening the door to novel
exploration around data curriculum and augmentation as means to improve
self-supervised learning models robustness. The code is available at
https://github.com/KevinZ0217/fast_dinov2

</details>


### [88] [Zero Memory Overhead Approach for Protecting Vision Transformer Parameters](https://arxiv.org/abs/2507.03816)
*Fereshteh Baradaran,Mohsen Raji,Azadeh Baradaran,Arezoo Baradaran,Reihaneh Akbarifard*

Main category: cs.CV

TL;DR: A zero-overhead fault tolerance technique for Vision Transformers (ViTs) detects and mitigates bit-flip faults by replacing least significant bits (LSBs) with parity bits and masking faulty parameters.


<details>
  <summary>Details</summary>
Motivation: Ensuring ViT reliability in safety-critical applications like autonomous driving, where bit-flip faults in memory can compromise functionality.

Method: Replace LSBs of parameters with parity bits for error detection and mask faulty parameters by zeroing them out.

Result: Improves robustness to bit-flips by up to three orders of magnitude without memory overhead.

Conclusion: The technique is an effective zero-overhead solution for enhancing ViT fault tolerance in critical applications.

Abstract: Vision Transformers (ViTs) have demonstrated superior performance over
Convolutional Neural Networks (CNNs) in various vision-related tasks such as
classification, object detection, and segmentation due to their use of
self-attention mechanisms. As ViTs become more popular in safety-critical
applications like autonomous driving, ensuring their correct functionality
becomes essential, especially in the presence of bit-flip faults in their
parameters stored in memory. In this paper, a fault tolerance technique is
introduced to protect ViT parameters against bit-flip faults with zero memory
overhead. Since the least significant bits of parameters are not critical for
model accuracy, replacing the LSB with a parity bit provides an error detection
mechanism without imposing any overhead on the model. When faults are detected,
affected parameters are masked by zeroing out, as most parameters in ViT models
are near zero, effectively preventing accuracy degradation. This approach
enhances reliability across ViT models, improving the robustness of parameters
to bit-flips by up to three orders of magnitude, making it an effective
zero-overhead solution for fault tolerance in critical applications.

</details>


### [89] [Query-Based Adaptive Aggregation for Multi-Dataset Joint Training Toward Universal Visual Place Recognition](https://arxiv.org/abs/2507.03831)
*Jiuhong Xiao,Yang Zhou,Giuseppe Loianno*

Main category: cs.CV

TL;DR: The paper introduces Query-based Adaptive Aggregation (QAA) to improve generalization in Visual Place Recognition (VPR) by leveraging learned queries as reference codebooks, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing VPR models suffer from dataset-specific biases and limited generalization due to single-dataset training. Multi-dataset training faces challenges like information saturation in feature aggregation layers.

Method: Proposes QAA, a feature aggregation technique using learned queries as reference codebooks to enhance information capacity. Introduces Cross-query Similarity (CS) for robust descriptor generation.

Result: QAA outperforms state-of-the-art models, achieving balanced generalization across datasets while maintaining peak performance. Ablation studies and visualizations validate its effectiveness.

Conclusion: QAA addresses dataset-specific biases and information saturation, offering a scalable and effective solution for universal VPR models. Code will be released publicly.

Abstract: Deep learning methods for Visual Place Recognition (VPR) have advanced
significantly, largely driven by large-scale datasets. However, most existing
approaches are trained on a single dataset, which can introduce
dataset-specific inductive biases and limit model generalization. While
multi-dataset joint training offers a promising solution for developing
universal VPR models, divergences among training datasets can saturate limited
information capacity in feature aggregation layers, leading to suboptimal
performance. To address these challenges, we propose Query-based Adaptive
Aggregation (QAA), a novel feature aggregation technique that leverages learned
queries as reference codebooks to effectively enhance information capacity
without significant computational or parameter complexity. We show that
computing the Cross-query Similarity (CS) between query-level image features
and reference codebooks provides a simple yet effective way to generate robust
descriptors. Our results demonstrate that QAA outperforms state-of-the-art
models, achieving balanced generalization across diverse datasets while
maintaining peak performance comparable to dataset-specific models. Ablation
studies further explore QAA's mechanisms and scalability. Visualizations reveal
that the learned queries exhibit diverse attention patterns across datasets.
Code will be publicly released.

</details>


### [90] [Interpretable Diffusion Models with B-cos Networks](https://arxiv.org/abs/2507.03846)
*Nicola Bernold,Moritz Vandenhirtz,Alice Bizeul,Julia E. Vogt*

Main category: cs.CV

TL;DR: A diffusion model with B-cos modules is introduced for interpretable text-to-image generation, highlighting how prompt tokens influence image regions.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models often fail to accurately reflect all semantic details in prompts, with errors hard to detect automatically.

Method: The paper proposes a diffusion model architecture using B-cos modules for inherent interpretability, showing how prompt tokens affect image regions.

Result: The B-cos diffusion model generates high-quality images while providing insights into prompt-image alignment.

Conclusion: The approach successfully combines interpretability with image generation quality, improving understanding of prompt-image relationships.

Abstract: Text-to-image diffusion models generate images by iteratively denoising
random noise, conditioned on a prompt. While these models have enabled
impressive progress in image generation, they often fail to accurately reflect
all semantic information described in the prompt -- failures that are difficult
to detect automatically. In this work, we introduce a diffusion model
architecture built with B-cos modules that offers inherent interpretability.
Our approach provides insight into how individual prompt tokens affect the
generated image by producing explanations that highlight the pixel regions
influenced by each token. We demonstrate that B-cos diffusion models can
produce high-quality images while providing meaningful insights into
prompt-image alignment.

</details>


### [91] [ArmGS: Composite Gaussian Appearance Refinement for Modeling Dynamic Urban Environments](https://arxiv.org/abs/2507.03886)
*Guile Wu,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: Proposes ArmGS, a method using multi-granularity Gaussian splatting for dynamic urban scene modeling in autonomous driving, improving rendering quality and detail.


<details>
  <summary>Details</summary>
Motivation: Current methods for photorealistic driving scene modeling lack efficiency and fine-grained detail, especially in dynamic environments.

Method: ArmGS employs multi-level appearance modeling to refine Gaussian splatting, optimizing transformations from local to global levels for better detail and scene variation.

Result: Outperforms state-of-the-art methods on datasets like Waymo, KITTI, NOTR, and VKITTI2.

Conclusion: ArmGS effectively addresses fine-grained and global scene variations, enhancing dynamic urban environment modeling for autonomous driving.

Abstract: This work focuses on modeling dynamic urban environments for autonomous
driving simulation. Contemporary data-driven methods using neural radiance
fields have achieved photorealistic driving scene modeling, but they suffer
from low rendering efficacy. Recently, some approaches have explored 3D
Gaussian splatting for modeling dynamic urban scenes, enabling high-fidelity
reconstruction and real-time rendering. However, these approaches often neglect
to model fine-grained variations between frames and camera viewpoints, leading
to suboptimal results. In this work, we propose a new approach named ArmGS that
exploits composite driving Gaussian splatting with multi-granularity appearance
refinement for autonomous driving scene modeling. The core idea of our approach
is devising a multi-level appearance modeling scheme to optimize a set of
transformation parameters for composite Gaussian refinement from multiple
granularities, ranging from local Gaussian level to global image level and
dynamic actor level. This not only models global scene appearance variations
between frames and camera viewpoints, but also models local fine-grained
changes of background and objects. Extensive experiments on multiple
challenging autonomous driving datasets, namely, Waymo, KITTI, NOTR and
VKITTI2, demonstrate the superiority of our approach over the state-of-the-art
methods.

</details>


### [92] [Hierarchical Semantic-Visual Fusion of Visible and Near-infrared Images for Long-range Haze Removal](https://arxiv.org/abs/2507.03893)
*Yi Li,Xiaoxiong Wang,Jiawei Wang,Yi Chang,Kai Cao,Luxin Yan*

Main category: cs.CV

TL;DR: The paper introduces a Hierarchical Semantic-Visual Fusion (HSVF) framework for long-range haze removal, leveraging near-infrared and visible images to achieve high-contrast and detailed results.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing methods focus on short-range scenarios and neglect residual haze in visible images, while near-infrared offers complementary cues. The paper aims to exploit semantic consistency between modalities for better haze removal.

Method: Proposes HSVF with two streams: a semantic stream for haze-free scene reconstruction and a visual stream for structural detail recovery from near-infrared. Uses modality-invariant semantic alignment and complementary cue fusion.

Result: HSVF outperforms state-of-the-art methods in long-range haze removal, producing high-contrast scenes with rich texture details.

Conclusion: The HSVF framework effectively combines semantic and visual cues for superior long-range dehazing, supported by a new dataset for benchmarking.

Abstract: While image dehazing has advanced substantially in the past decade, most
efforts have focused on short-range scenarios, leaving long-range haze removal
under-explored. As distance increases, intensified scattering leads to severe
haze and signal loss, making it impractical to recover distant details solely
from visible images. Near-infrared, with superior fog penetration, offers
critical complementary cues through multimodal fusion. However, existing
methods focus on content integration while often neglecting haze embedded in
visible images, leading to results with residual haze. In this work, we argue
that the infrared and visible modalities not only provide complementary
low-level visual features, but also share high-level semantic consistency.
Motivated by this, we propose a Hierarchical Semantic-Visual Fusion (HSVF)
framework, comprising a semantic stream to reconstruct haze-free scenes and a
visual stream to incorporate structural details from the near-infrared
modality. The semantic stream first acquires haze-robust semantic prediction by
aligning modality-invariant intrinsic representations. Then the shared
semantics act as strong priors to restore clear and high-contrast distant
scenes under severe haze degradation. In parallel, the visual stream focuses on
recovering lost structural details from near-infrared by fusing complementary
cues from both visible and near-infrared images. Through the cooperation of
dual streams, HSVF produces results that exhibit both high-contrast scenes and
rich texture details. Moreover, we introduce a novel pixel-aligned
visible-infrared haze dataset with semantic labels to facilitate benchmarking.
Extensive experiments demonstrate the superiority of our method over
state-of-the-art approaches in real-world long-range haze removal.

</details>


### [93] [Deconfounding Causal Inference through Two-Branch Framework with Early-Forking for Sensor-Based Cross-Domain Activity Recognition](https://arxiv.org/abs/2507.03898)
*Di Xiong,Lei Zhang,Shuoyuan Wang,Dongzhou Cheng,Wenbo Huang*

Main category: cs.CV

TL;DR: A causality-inspired representation learning algorithm is proposed for cross-domain human activity recognition (HAR), outperforming state-of-the-art baselines by disentangling causal and non-causal features.


<details>
  <summary>Details</summary>
Motivation: Existing domain generalization (DG) methods in HAR overlook intrinsic causal mechanisms, focusing only on statistical dependence. This work addresses this gap by modeling causal factors explicitly.

Method: An early-forking two-branch framework separates causal and non-causal features, using Hilbert-Schmidt Information Criterion for disentanglement. Domain sampling and perturbation layers enhance robustness.

Result: The method outperforms eleven baselines in cross-person, cross-dataset, and cross-position HAR scenarios, validated by extensive experiments.

Conclusion: The causality-inspired approach is effective, efficient, and universal for cross-domain HAR, with ablation studies confirming its underlying causal mechanism.

Abstract: Recently, domain generalization (DG) has emerged as a promising solution to
mitigate distribution-shift issue in sensor-based human activity recognition
(HAR) scenario. However, most existing DG-based works have merely focused on
modeling statistical dependence between sensor data and activity labels,
neglecting the importance of intrinsic casual mechanism. Intuitively, every
sensor input can be viewed as a mixture of causal (category-aware) and
non-causal factors (domain-specific), where only the former affects activity
classification judgment. In this paper, by casting such DG-based HAR as a
casual inference problem, we propose a causality-inspired representation
learning algorithm for cross-domain activity recognition. To this end, an
early-forking two-branch framework is designed, where two separate branches are
respectively responsible for learning casual and non-causal features, while an
independence-based Hilbert-Schmidt Information Criterion is employed to
implicitly disentangling them. Additionally, an inhomogeneous domain sampling
strategy is designed to enhance disentanglement, while a category-aware domain
perturbation layer is performed to prevent representation collapse. Extensive
experiments on several public HAR benchmarks demonstrate that our
causality-inspired approach significantly outperforms eleven related
state-of-the-art baselines under cross-person, cross-dataset, and
cross-position settings. Detailed ablation and visualizations analyses reveal
underlying casual mechanism, indicating its effectiveness, efficiency, and
universality in cross-domain activity recognition scenario.

</details>


### [94] [Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving Reconstruction for 3D Anomaly Detection](https://arxiv.org/abs/2507.03903)
*Hanzhe Liang,Jie Zhang,Tao Dai,Linlin Shen,Jinbao Wang,Can Gao*

Main category: cs.CV

TL;DR: DUS-Net, a Down-Up Sampling Network, is proposed for high-precision 3D point cloud reconstruction in anomaly detection, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Reconstruction-based methods struggle with high-precision point clouds due to scale and complexity.

Method: DUS-Net uses noise generation, Down-Net for anomaly-free center point clouds, and Up-Net for multi-scale reconstruction.

Result: Achieves 79.9% and 79.5% Object-level AUROC, 71.2% and 84.7% Point-level AUROC on Real3D-AD and Anomaly-ShapeNet.

Conclusion: DUS-Net effectively preserves geometric structure and outperforms existing methods in 3D anomaly detection.

Abstract: Reconstruction-based methods have demonstrated very promising results for 3D
anomaly detection. However, these methods face great challenges in handling
high-precision point clouds due to the large scale and complex structure. In
this study, a Down-Up Sampling Network (DUS-Net) is proposed to reconstruct
high-precision point clouds for 3D anomaly detection by preserving the group
center geometric structure. The DUS-Net first introduces a Noise Generation
module to generate noisy patches, which facilitates the diversity of training
data and strengthens the feature representation for reconstruction. Then, a
Down-sampling Network~(Down-Net) is developed to learn an anomaly-free center
point cloud from patches with noise injection. Subsequently, an Up-sampling
Network (Up-Net) is designed to reconstruct high-precision point clouds by
fusing multi-scale up-sampling features. Our method leverages group centers for
construction, enabling the preservation of geometric structure and providing a
more precise point cloud. Extensive experiments demonstrate the effectiveness
of our proposed method, achieving state-of-the-art (SOTA) performance with an
Object-level AUROC of 79.9% and 79.5%, and a Point-level AUROC of 71.2% and
84.7% on the Real3D-AD and Anomaly-ShapeNet datasets, respectively.

</details>


### [95] [EchoMimicV3: 1.3B Parameters are All You Need for Unified Multi-Modal and Multi-Task Human Animation](https://arxiv.org/abs/2507.03905)
*Rang Meng,Yan Wang,Weipeng Wu,Ruobing Zheng,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: Proposes a unified multi-task model for human animation, improving speed, quality, and generalization while integrating diverse tasks into one framework.


<details>
  <summary>Details</summary>
Motivation: Addresses the slow inference speed, high computational cost, and fragmentation of specialized models in human animation.

Method: Introduces a multi-task paradigm inspired by MAE, a multi-modal decoupled cross-attention module, and an SFT+Reward training paradigm.

Result: Achieves high-quality generation with a minimal 1.3B parameter model, outperforming larger models in facial and semi-body video generation.

Conclusion: Paves the way for efficient, versatile digital human generation, solving performance and practicality challenges.

Abstract: Human animation recently has advanced rapidly, achieving increasingly
realistic and vivid results, especially with the integration of large-scale
video generation models. However, the slow inference speed and high
computational cost of these large models bring significant challenges for
practical applications. Additionally, various tasks in human animation, such as
lip-syncing, audio-driven full-body animation, and video generation from start
and end frames, often require different specialized models. The introduction of
large video models has not alleviated this dilemma. This raises an important
question: Can we make human animation Faster, Higher in quality, Stronger in
generalization, and make various tasks Together in one model? To address this,
we dive into video generation models and discover that the devil lies in the
details: Inspired by MAE, we propose a novel unified Multi-Task paradigm for
human animation, treating diverse generation tasks as spatial-temporal local
reconstructions, requiring modifications only on the input side; Given the
interplay and division among multi-modal conditions including text, image, and
audio, we introduce a multi-modal decoupled cross-attention module to fuse
multi-modals in a divide-and-conquer manner; We propose a new SFT+Reward
alternating training paradigm, enabling the minimal model with 1.3B parameters
to achieve generation quality comparable to models with 10 times the parameters
count. Through these innovations, our work paves the way for efficient,
high-quality, and versatile digital human generation, addressing both
performance and practicality challenges in the field. Extensive experiments
demonstrate that EchoMimicV3 outperforms existing models in both facial and
semi-body video generation, providing precise text-based control for creating
videos in a wide range of scenarios.

</details>


### [96] [Bridging Vision and Language: Optimal Transport-Driven Radiology Report Generation via LLMs](https://arxiv.org/abs/2507.03908)
*Haifeng Zhao,Yufei Zhang,Leilei Ma,Shuo Xu,Dengdi Sun*

Main category: cs.CV

TL;DR: OTDRG uses Optimal Transport to align X-ray image features with disease labels, improving clinical accuracy in radiology report generation.


<details>
  <summary>Details</summary>
Motivation: General LLMs prioritize linguistic fluency over clinical effectiveness, failing to bridge the gap between X-ray images and text.

Method: OTDRG aligns image and label features using Optimal Transport, fine-tunes LLMs, and includes a disease prediction module.

Result: OTDRG achieves top performance in NLG and clinical efficacy on MIMIC-CXR and IU X-Ray datasets.

Conclusion: OTDRG enhances both linguistic coherence and clinical accuracy in radiology reports.

Abstract: Radiology report generation represents a significant application within
medical AI, and has achieved impressive results. Concurrently, large language
models (LLMs) have demonstrated remarkable performance across various domains.
However, empirical validation indicates that general LLMs tend to focus more on
linguistic fluency rather than clinical effectiveness, and lack the ability to
effectively capture the relationship between X-ray images and their
corresponding texts, thus resulting in poor clinical practicability. To address
these challenges, we propose Optimal Transport-Driven Radiology Report
Generation (OTDRG), a novel framework that leverages Optimal Transport (OT) to
align image features with disease labels extracted from reports, effectively
bridging the cross-modal gap. The core component of OTDRG is Alignment \&
Fine-Tuning, where OT utilizes results from the encoding of label features and
image visual features to minimize cross-modal distances, then integrating image
and text features for LLMs fine-tuning. Additionally, we design a novel disease
prediction module to predict disease labels contained in X-ray images during
validation and testing. Evaluated on the MIMIC-CXR and IU X-Ray datasets, OTDRG
achieves state-of-the-art performance in both natural language generation (NLG)
and clinical efficacy (CE) metrics, delivering reports that are not only
linguistically coherent but also clinically accurate.

</details>


### [97] [Learning Disentangled Stain and Structural Representations for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2507.03923)
*Ha-Hieu Pham,Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Ulas Bagci,Min Xu,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: CSDS is a semi-supervised framework for gland segmentation, using dual student networks and uncertainty modules to improve performance in low-label settings.


<details>
  <summary>Details</summary>
Motivation: Variability in H&E staining and tissue morphology, along with limited annotated data, challenges automated gland segmentation.

Method: CSDS employs two student networks (stain-augmented and structure-augmented) supervised by a shared teacher network with EMA. Uncertainty modules refine label reliability.

Result: Achieves state-of-the-art performance, with Dice score improvements of up to 1.2% on GlaS and 1.4% on CRAG at 5% and 10% labeled data.

Conclusion: CSDS effectively addresses challenges in gland segmentation, offering improved accuracy in low-label scenarios.

Abstract: Accurate gland segmentation in histopathology images is essential for cancer
diagnosis and prognosis. However, significant variability in Hematoxylin and
Eosin (H&E) staining and tissue morphology, combined with limited annotated
data, poses major challenges for automated segmentation. To address this, we
propose Color-Structure Dual-Student (CSDS), a novel semi-supervised
segmentation framework designed to learn disentangled representations of stain
appearance and tissue structure. CSDS comprises two specialized student
networks: one trained on stain-augmented inputs to model chromatic variation,
and the other on structure-augmented inputs to capture morphological cues. A
shared teacher network, updated via Exponential Moving Average (EMA),
supervises both students through pseudo-labels. To further improve label
reliability, we introduce stain-aware and structure-aware uncertainty
estimation modules that adaptively modulate the contribution of each student
during training. Experiments on the GlaS and CRAG datasets show that CSDS
achieves state-of-the-art performance in low-label settings, with Dice score
improvements of up to 1.2% on GlaS and 0.7% on CRAG at 5% labeled data, and
0.7% and 1.4% at 10%. Our code and pre-trained models are available at
https://github.com/hieuphamha19/CSDS.

</details>


### [98] [DNF-Intrinsic: Deterministic Noise-Free Diffusion for Indoor Inverse Rendering](https://arxiv.org/abs/2507.03924)
*Rongjia Zheng,Qing Zhang,Chengjiang Long,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: DNF-Intrinsic improves inverse rendering by using source images instead of noisy inputs, ensuring higher quality and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing noise-to-intrinsic methods struggle with deteriorated structure and appearance, which are crucial for inverse rendering.

Method: DNF-Intrinsic uses source images and flow matching for deterministic intrinsic prediction, paired with a generative renderer for physical faithfulness.

Result: Outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: DNF-Intrinsic offers a robust and efficient solution for high-quality inverse rendering.

Abstract: Recent methods have shown that pre-trained diffusion models can be fine-tuned
to enable generative inverse rendering by learning image-conditioned
noise-to-intrinsic mapping. Despite their remarkable progress, they struggle to
robustly produce high-quality results as the noise-to-intrinsic paradigm
essentially utilizes noisy images with deteriorated structure and appearance
for intrinsic prediction, while it is common knowledge that structure and
appearance information in an image are crucial for inverse rendering. To
address this issue, we present DNF-Intrinsic, a robust yet efficient inverse
rendering approach fine-tuned from a pre-trained diffusion model, where we
propose to take the source image rather than Gaussian noise as input to
directly predict deterministic intrinsic properties via flow matching.
Moreover, we design a generative renderer to constrain that the predicted
intrinsic properties are physically faithful to the source image. Experiments
on both synthetic and real-world datasets show that our method clearly
outperforms existing state-of-the-art methods.

</details>


### [99] [Learning Adaptive Node Selection with External Attention for Human Interaction Recognition](https://arxiv.org/abs/2507.03936)
*Chen Pang,Xuequan Lu,Qianyu Zhou,Lei Lyu*

Main category: cs.CV

TL;DR: ASEA dynamically models interactions without predefined assumptions, using GCN for intra-personal relationships and AT-NAC for node selection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect inter-dependencies and use static interaction matrices, failing to adapt to dynamic contexts.

Method: Combines GCN for individual modeling, AT-NAC for adaptive node selection, and EA for interaction dynamics.

Result: Effectively captures interactions, outperforming existing methods.

Conclusion: ASEA offers a flexible and adaptive solution for interaction modeling, setting a new benchmark.

Abstract: Most GCN-based methods model interacting individuals as independent graphs,
neglecting their inherent inter-dependencies. Although recent approaches
utilize predefined interaction adjacency matrices to integrate participants,
these matrices fail to adaptively capture the dynamic and context-specific
joint interactions across different actions. In this paper, we propose the
Active Node Selection with External Attention Network (ASEA), an innovative
approach that dynamically captures interaction relationships without predefined
assumptions. Our method models each participant individually using a GCN to
capture intra-personal relationships, facilitating a detailed representation of
their actions. To identify the most relevant nodes for interaction modeling, we
introduce the Adaptive Temporal Node Amplitude Calculation (AT-NAC) module,
which estimates global node activity by combining spatial motion magnitude with
adaptive temporal weighting, thereby highlighting salient motion patterns while
reducing irrelevant or redundant information. A learnable threshold,
regularized to prevent extreme variations, is defined to selectively identify
the most informative nodes for interaction modeling. To capture interactions,
we design the External Attention (EA) module to operate on active nodes,
effectively modeling the interaction dynamics and semantic relationships
between individuals. Extensive evaluations show that our method captures
interaction relationships more effectively and flexibly, achieving
state-of-the-art performance.

</details>


### [100] [VISC: mmWave Radar Scene Flow Estimation using Pervasive Visual-Inertial Supervision](https://arxiv.org/abs/2507.03938)
*Kezhong Liu,Yiwen Zhou,Mozi Chen,Jianhua He,Jingao Xu,Zheng Yang,Chris Xiaoxuan Lu,Shengkai Zhang*

Main category: cs.CV

TL;DR: A mmWave radar scene flow estimation framework supervised by VI sensor data, addressing limitations of LiDAR and VI data, outperforming SOTA in smoke-filled environments.


<details>
  <summary>Details</summary>
Motivation: Current mmWave radar scene flow methods rely on expensive LiDARs, while VI data lacks 3D motion capture. The proposed method leverages accessible VI data to overcome these issues.

Method: A drift-free rigid transformation estimator fuses kinematic ego-motions with neural network results, and an optical-mmWave module extracts supervision signals for dynamic points.

Result: Outperforms SOTA methods using costly LiDARs, especially in smoke-filled environments.

Conclusion: The framework effectively uses VI data for mmWave radar scene flow estimation, offering a cost-efficient and robust alternative to LiDAR-based methods.

Abstract: This work proposes a mmWave radar's scene flow estimation framework
supervised by data from a widespread visual-inertial (VI) sensor suite,
allowing crowdsourced training data from smart vehicles. Current scene flow
estimation methods for mmWave radar are typically supervised by dense point
clouds from 3D LiDARs, which are expensive and not widely available in smart
vehicles. While VI data are more accessible, visual images alone cannot capture
the 3D motions of moving objects, making it difficult to supervise their scene
flow. Moreover, the temporal drift of VI rigid transformation also degenerates
the scene flow estimation of static points. To address these challenges, we
propose a drift-free rigid transformation estimator that fuses kinematic
model-based ego-motions with neural network-learned results. It provides strong
supervision signals to radar-based rigid transformation and infers the scene
flow of static points. Then, we develop an optical-mmWave supervision
extraction module that extracts the supervision signals of radar rigid
transformation and scene flow. It strengthens the supervision by learning the
scene flow of dynamic points with the joint constraints of optical and mmWave
radar measurements. Extensive experiments demonstrate that, in smoke-filled
environments, our method even outperforms state-of-the-art (SOTA) approaches
using costly LiDARs.

</details>


### [101] [Evaluating Adversarial Protections for Diffusion Personalization: A Comprehensive Study](https://arxiv.org/abs/2507.03953)
*Kai Ye,Tianyi Chen,Zhen Wang*

Main category: cs.CV

TL;DR: A comparison of eight perturbation-based protection methods for diffusion models, evaluating their efficacy and visual imperceptibility under varying budgets.


<details>
  <summary>Details</summary>
Motivation: Address privacy breaches and content misuse concerns in diffusion models for image generation and personalization.

Method: Comprehensive evaluation of eight methods (AdvDM, ASPL, FSGM, MetaCloak, Mist, PhotoGuard, SDS, SimAC) across portrait and artwork domains with varying perturbation budgets.

Result: Provides practical guidance for selecting protection methods based on visual imperceptibility and protective efficacy.

Conclusion: The study aids in choosing effective perturbation-based protection methods for diffusion models, with code available for further use.

Abstract: With the increasing adoption of diffusion models for image generation and
personalization, concerns regarding privacy breaches and content misuse have
become more pressing. In this study, we conduct a comprehensive comparison of
eight perturbation based protection methods: AdvDM, ASPL, FSGM, MetaCloak,
Mist, PhotoGuard, SDS, and SimAC--across both portrait and artwork domains.
These methods are evaluated under varying perturbation budgets, using a range
of metrics to assess visual imperceptibility and protective efficacy. Our
results offer practical guidance for method selection. Code is available at:
https://github.com/vkeilo/DiffAdvPerturbationBench.

</details>


### [102] [Robust Low-light Scene Restoration via Illumination Transition](https://arxiv.org/abs/2507.03976)
*Ze Li,Feng Zhang,Xiatian Zhu,Meng Zhang,Yanghong Zhou,P. Y. Mok*

Main category: cs.CV

TL;DR: RoSe is a framework for synthesizing normal-light novel views from low-light multiview images by modeling illuminance transition in 3D space, achieving superior denoising and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with low-light preprocessing due to ignored multiview correlations and suffer from color distortions, artifacts, and poor denoising.

Method: RoSe formulates the task as an illuminance transition estimation in 3D space, uses low-rank illumination constraints, and employs a dual-branch architecture with a low-rank denoising module.

Result: RoSe outperforms state-of-the-art models in rendering quality and multiview consistency on benchmarks.

Conclusion: RoSe effectively addresses low-light multiview challenges, offering robust denoising and high-quality novel view synthesis.

Abstract: Synthesizing normal-light novel views from low-light multiview images is an
important yet challenging task, given the low visibility and high ISO noise
present in the input images. Existing low-light enhancement methods often
struggle to effectively preprocess such low-light inputs, as they fail to
consider correlations among multiple views. Although other state-of-the-art
methods have introduced illumination-related components offering alternative
solutions to the problem, they often result in drawbacks such as color
distortions and artifacts, and they provide limited denoising effectiveness. In
this paper, we propose a novel Robust Low-light Scene Restoration framework
(RoSe), which enables effective synthesis of novel views in normal lighting
conditions from low-light multiview image inputs, by formulating the task as an
illuminance transition estimation problem in 3D space, conceptualizing it as a
specialized rendering task. This multiview-consistent illuminance transition
field establishes a robust connection between low-light and normal-light
conditions. By further exploiting the inherent low-rank property of
illumination to constrain the transition representation, we achieve more
effective denoising without complex 2D techniques or explicit noise modeling.
To implement RoSe, we design a concise dual-branch architecture and introduce a
low-rank denoising module. Experiments demonstrate that RoSe significantly
outperforms state-of-the-art models in both rendering quality and multiview
consistency on standard benchmarks. The codes and data are available at
https://pegasus2004.github.io/RoSe.

</details>


### [103] [Flux-Sculptor: Text-Driven Rich-Attribute Portrait Editing through Decomposed Spatial Flow Control](https://arxiv.org/abs/2507.03979)
*Tianyao He,Runqi Wang,Yang Chen,Dejia Song,Nemo Chen,Xu Tang,Yao Hu*

Main category: cs.CV

TL;DR: Flux-Sculptor is a flux-based framework for precise text-driven portrait editing, balancing fidelity and flexibility with PASL and S2D-EC strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance reconstruction fidelity and editing flexibility in text-driven portrait editing.

Method: Introduces Prompt-Aligned Spatial Locator (PASL) for precise region identification and Structure-to-Detail Edit Control (S2D-EC) for guided denoising.

Result: Outperforms existing methods in rich-attribute editing and facial preservation.

Conclusion: Flux-Sculptor is a practical solution for text-driven portrait editing.

Abstract: Text-driven portrait editing holds significant potential for various
applications but also presents considerable challenges. An ideal text-driven
portrait editing approach should achieve precise localization and appropriate
content modification, yet existing methods struggle to balance reconstruction
fidelity and editing flexibility. To address this issue, we propose
Flux-Sculptor, a flux-based framework designed for precise text-driven portrait
editing. Our framework introduces a Prompt-Aligned Spatial Locator (PASL) to
accurately identify relevant editing regions and a Structure-to-Detail Edit
Control (S2D-EC) strategy to spatially guide the denoising process through
sequential mask-guided fusion of latent representations and attention values.
Extensive experiments demonstrate that Flux-Sculptor surpasses existing methods
in rich-attribute editing and facial information preservation, making it a
strong candidate for practical portrait editing applications. Project page is
available at https://flux-sculptor.github.io/.

</details>


### [104] [CoT-Segmenter: Enhancing OOD Detection in Dense Road Scenes via Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.03984)
*Jeonghyo Song,Kimin Yun,DaeUng Jo,Jinyoung Kim,Youngjoon Yoo*

Main category: cs.CV

TL;DR: The paper proposes a Chain-of-Thought (CoT)-based framework for Out-of-Distribution (OOD) detection in semantic segmentation, addressing challenges in complex road scenes. It leverages foundation models like GPT-4 to improve detection and reasoning.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliability of semantic segmentation in complex road environments is critical for safety. Current OOD methods struggle with specific challenging scenarios (e.g., overlapping objects, distant scenes).

Method: A novel CoT-based framework using foundation models (e.g., GPT-4) for enhanced image understanding and prompt-based reasoning to tackle OOD detection in road anomalies.

Result: The framework outperforms state-of-the-art methods on standard benchmarks and a challenging subset of the RoadAnomaly dataset.

Conclusion: The proposed solution offers a robust and interpretable approach for OOD semantic segmentation in complex driving environments.

Abstract: Effective Out-of-Distribution (OOD) detection is criti-cal for ensuring the
reliability of semantic segmentation models, particularly in complex road
environments where safety and accuracy are paramount. Despite recent
advancements in large language models (LLMs), notably GPT-4, which
significantly enhanced multimodal reasoning through Chain-of-Thought (CoT)
prompting, the application of CoT-based visual reasoning for OOD semantic
segmentation remains largely unexplored. In this paper, through extensive
analyses of the road scene anomalies, we identify three challenging scenarios
where current state-of-the-art OOD segmentation methods consistently struggle:
(1) densely packed and overlapping objects, (2) distant scenes with small
objects, and (3) large foreground-dominant objects. To address the presented
challenges, we propose a novel CoT-based framework targeting OOD detection in
road anomaly scenes. Our method leverages the extensive knowledge and reasoning
capabilities of foundation models, such as GPT-4, to enhance OOD detection
through improved image understanding and prompt-based reasoning aligned with
observed problematic scene attributes. Extensive experiments show that our
framework consistently outperforms state-of-the-art methods on both standard
benchmarks and our newly defined challenging subset of the RoadAnomaly dataset,
offering a robust and interpretable solution for OOD semantic segmentation in
complex driving environments.

</details>


### [105] [LEHA-CVQAD: Dataset To Enable Generalized Video Quality Assessment of Compression Artifacts](https://arxiv.org/abs/2507.03990)
*Aleksandr Gushchin,Maksim Smirnov,Dmitriy Vatolin,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: The paper introduces the LEHA-CVQAD dataset for video quality assessment and proposes the RDAE metric to evaluate VQA models' performance in preserving bitrate-quality order.


<details>
  <summary>Details</summary>
Motivation: To address the need for a comprehensive dataset and evaluation metric for compression-oriented video quality assessment, supporting codec parameter tuning.

Method: The LEHA-CVQAD dataset includes 6,240 clips from 59 source videos encoded with 186 codec-preset variants, with 1.8M pairwise and 1.5k MOS ratings. The RDAE metric is introduced to quantify bitrate-quality alignment.

Result: Popular VQA metrics show high RDAE and lower correlations, highlighting the dataset's challenges and utility.

Conclusion: The LEHA-CVQAD dataset and RDAE metric provide valuable tools for VQA research, with open access to part of the dataset and results.

Abstract: We propose the LEHA-CVQAD (Large-scale Enriched Human-Annotated) dataset,
which comprises 6,240 clips for compression-oriented video quality assessment.
59 source videos are encoded with 186 codec-preset variants, 1.8M pairwise, and
1.5k MOS ratings are fused into a single quality scale; part of the videos
remains hidden for blind evaluation. We also propose Rate-Distortion Alignment
Error (RDAE), a novel evaluation metric that quantifies how well VQA models
preserve bitrate-quality ordering, directly supporting codec parameter tuning.
Testing IQA/VQA methods reveals that popular VQA metrics exhibit high RDAE and
lower correlations, underscoring the dataset challenges and utility. The open
part and the results of LEHA-CVQAD are available at
https://aleksandrgushchin.github$.io/lcvqad/

</details>


### [106] [NRSeg: Noise-Resilient Learning for BEV Semantic Segmentation via Driving World Models](https://arxiv.org/abs/2507.04002)
*Siyu Li,Fei Teng,Yihong Cao,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.CV

TL;DR: NRSeg is a noise-resilient learning framework for BEV semantic segmentation, leveraging synthetic data and novel metrics to improve performance in unsupervised and semi-supervised tasks.


<details>
  <summary>Details</summary>
Motivation: Homogeneous labeled data limits performance in BEV semantic segmentation; synthetic data from driving world models offers diversity but introduces noise.

Method: Proposes PGCM for evaluating synthetic data guidance, BiDPP for robust learning via parallel prediction, and HLSE to handle non-mutual exclusivity.

Result: Achieves state-of-the-art performance with mIoU improvements of 13.8% (unsupervised) and 11.4% (semi-supervised).

Conclusion: NRSeg effectively harnesses synthetic data for robust BEV segmentation, outperforming existing methods.

Abstract: Birds' Eye View (BEV) semantic segmentation is an indispensable perception
task in end-to-end autonomous driving systems. Unsupervised and semi-supervised
learning for BEV tasks, as pivotal for real-world applications, underperform
due to the homogeneous distribution of the labeled data. In this work, we
explore the potential of synthetic data from driving world models to enhance
the diversity of labeled data for robustifying BEV segmentation. Yet, our
preliminary findings reveal that generation noise in synthetic data compromises
efficient BEV model learning. To fully harness the potential of synthetic data
from world models, this paper proposes NRSeg, a noise-resilient learning
framework for BEV semantic segmentation. Specifically, a Perspective-Geometry
Consistency Metric (PGCM) is proposed to quantitatively evaluate the guidance
capability of generated data for model learning. This metric originates from
the alignment measure between the perspective road mask of generated data and
the mask projected from the BEV labels. Moreover, a Bi-Distribution Parallel
Prediction (BiDPP) is designed to enhance the inherent robustness of the model,
where the learning process is constrained through parallel prediction of
multinomial and Dirichlet distributions. The former efficiently predicts
semantic probabilities, whereas the latter adopts evidential deep learning to
realize uncertainty quantification. Furthermore, a Hierarchical Local Semantic
Exclusion (HLSE) module is designed to address the non-mutual exclusivity
inherent in BEV semantic segmentation tasks. Experimental results demonstrate
that NRSeg achieves state-of-the-art performance, yielding the highest
improvements in mIoU of 13.8% and 11.4% in unsupervised and semi-supervised BEV
segmentation tasks, respectively. The source code will be made publicly
available at https://github.com/lynn-yu/NRSeg.

</details>


### [107] [Group-wise Scaling and Orthogonal Decomposition for Domain-Invariant Feature Extraction in Face Anti-Spoofing](https://arxiv.org/abs/2507.04006)
*Seungjin Jung,Kanghee Lee,Yonghyun Jeong,Haeun Noh,Jungmin Lee,Jongwon Choi*

Main category: cs.CV

TL;DR: A novel DGFAS framework aligns weights and biases using FOD and GS-RM, improving generalization on unseen domains.


<details>
  <summary>Details</summary>
Motivation: Existing DGFAS methods misalign bias terms, degrading performance on unseen domains.

Method: Uses Feature Orthogonal Decomposition (FOD) and Group-wise Scaling Risk Minimization (GS-RM) to align weights and biases.

Result: Achieves state-of-the-art performance, improving accuracy and reducing bias misalignment.

Conclusion: The proposed framework enhances generalization stability and performance on unseen domains.

Abstract: Domain Generalizable Face Anti-Spoofing (DGFAS) methods effectively capture
domain-invariant features by aligning the directions (weights) of local
decision boundaries across domains. However, the bias terms associated with
these boundaries remain misaligned, leading to inconsistent classification
thresholds and degraded performance on unseen target domains. To address this
issue, we propose a novel DGFAS framework that jointly aligns weights and
biases through Feature Orthogonal Decomposition (FOD) and Group-wise Scaling
Risk Minimization (GS-RM). Specifically, GS-RM facilitates bias alignment by
balancing group-wise losses across multiple domains. FOD employs the
Gram-Schmidt orthogonalization process to decompose the feature space
explicitly into domain-invariant and domain-specific subspaces. By enforcing
orthogonality between domain-specific and domain-invariant features during
training using domain labels, FOD ensures effective weight alignment across
domains without negatively impacting bias alignment. Additionally, we introduce
Expected Calibration Error (ECE) as a novel evaluation metric for
quantitatively assessing the effectiveness of our method in aligning bias terms
across domains. Extensive experiments on benchmark datasets demonstrate that
our approach achieves state-of-the-art performance, consistently improving
accuracy, reducing bias misalignment, and enhancing generalization stability on
unseen target domains.

</details>


### [108] [Habitat Classification from Ground-Level Imagery Using Deep Neural Networks](https://arxiv.org/abs/2507.04017)
*Hongrui Shi,Lisa Norton,Lucy Ridding,Simon Rolph,Tom August,Claire M Wood,Lan Qie,Petra Bosilj,James M Brown*

Main category: cs.CV

TL;DR: The paper explores AI-driven tools for habitat assessment using ground-level imagery, comparing CNNs and ViTs with supervised contrastive learning, showing ViTs outperform CNNs and match expert accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional habitat assessment is costly and relies on expert surveys, prompting the need for scalable, automated solutions using AI.

Method: The study uses deep neural networks (CNNs and ViTs) on ground-level imagery from the UK Countryside Survey, evaluating supervised and supervised contrastive learning paradigms.

Result: ViTs outperform CNNs (Top-3 accuracy = 91%, MCC = 0.66) and supervised contrastive learning reduces misclassification. The best model matches expert-level accuracy.

Conclusion: The research presents a scalable, cost-effective AI framework for ground-level habitat monitoring, aiding biodiversity conservation and land-use decisions.

Abstract: Habitat assessment at local scales -- critical for enhancing biodiversity and
guiding conservation priorities -- often relies on expert field survey that can
be costly, motivating the exploration of AI-driven tools to automate and refine
this process. While most AI-driven habitat mapping depends on remote sensing,
it is often constrained by sensor availability, weather, and coarse resolution.
In contrast, ground-level imagery captures essential structural and
compositional cues invisible from above and remains underexplored for robust,
fine-grained habitat classification. This study addresses this gap by applying
state-of-the-art deep neural network architectures to ground-level habitat
imagery. Leveraging data from the UK Countryside Survey covering 18 broad
habitat types, we evaluate two families of models -- convolutional neural
networks (CNNs) and vision transformers (ViTs) -- under both supervised and
supervised contrastive learning paradigms. Our results demonstrate that ViTs
consistently outperform state-of-the-art CNN baselines on key classification
metrics (Top-3 accuracy = 91\%, MCC = 0.66) and offer more interpretable scene
understanding tailored to ground-level images. Moreover, supervised contrastive
learning significantly reduces misclassification rates among visually similar
habitats (e.g., Improved vs. Neutral Grassland), driven by a more
discriminative embedding space. Finally, our best model performs on par with
experienced ecological experts in habitat classification from images,
underscoring the promise of expert-level automated assessment. By integrating
advanced AI with ecological expertise, this research establishes a scalable,
cost-effective framework for ground-level habitat monitoring to accelerate
biodiversity conservation and inform land-use decisions at the national scale.

</details>


### [109] [Exploring Kolmogorov-Arnold Network Expansions in Vision Transformers for Mitigating Catastrophic Forgetting in Continual Learning](https://arxiv.org/abs/2507.04020)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: Replacing MLPs in ViTs with KANs reduces catastrophic forgetting in continual learning, improving knowledge retention and task adaptation.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in vision transformers (ViTs) during continual learning by leveraging local plasticity in KANs.

Method: Replace MLPs in ViTs with KANs, which use spline-based activations to update only a subset of parameters per sample.

Result: KAN-based ViTs outperform MLP-based ViTs in retaining accuracy on earlier tasks while adapting to new ones.

Conclusion: Integrating KANs into ViTs offers a robust solution for continual learning in dynamic environments.

Abstract: Continual learning (CL), the ability of a model to learn new tasks without
forgetting previously acquired knowledge, remains a critical challenge in
artificial intelligence, particularly for vision transformers (ViTs) utilizing
Multilayer Perceptrons (MLPs) for global representation learning. Catastrophic
forgetting, where new information overwrites prior knowledge, is especially
problematic in these models. This research proposes replacing MLPs in ViTs with
Kolmogorov-Arnold Network (KANs) to address this issue. KANs leverage local
plasticity through spline-based activations, ensuring that only a subset of
parameters is updated per sample, thereby preserving previously learned
knowledge. The study investigates the efficacy of KAN-based ViTs in CL
scenarios across benchmark datasets (MNIST, CIFAR100), focusing on their
ability to retain accuracy on earlier tasks while adapting to new ones.
Experimental results demonstrate that KAN-based ViTs significantly mitigate
catastrophic forgetting, outperforming traditional MLP-based ViTs in knowledge
retention and task adaptation. This novel integration of KANs into ViTs
represents a promising step toward more robust and adaptable models for dynamic
environments.

</details>


### [110] [PresentAgent: Multimodal Agent for Presentation Video Generation](https://arxiv.org/abs/2507.04036)
*Jingwei Shi,Zeyu Zhang,Biao Wu,Yanjie Liang,Meng Fang,Ling Chen,Yang Zhao*

Main category: cs.CV

TL;DR: PresentAgent converts long documents into narrated videos with synchronized visuals and speech, outperforming static slides or summaries. It uses a modular pipeline and introduces PresentEval for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing methods only produce static slides or text summaries, lacking dynamic, human-like presentations. PresentAgent aims to bridge this gap.

Method: A modular pipeline segments documents, plans visuals, generates narration with LLMs and TTS, and aligns audio-visual content. PresentEval evaluates outputs.

Result: Experiments on 30 document-presentation pairs show PresentAgent achieves near-human quality in content fidelity, visual clarity, and audience comprehension.

Conclusion: PresentAgent demonstrates the potential of multimodal agents to transform static text into dynamic, accessible presentations.

Abstract: We present PresentAgent, a multimodal agent that transforms long-form
documents into narrated presentation videos. While existing approaches are
limited to generating static slides or text summaries, our method advances
beyond these limitations by producing fully synchronized visual and spoken
content that closely mimics human-style presentations. To achieve this
integration, PresentAgent employs a modular pipeline that systematically
segments the input document, plans and renders slide-style visual frames,
generates contextual spoken narration with large language models and
Text-to-Speech models, and seamlessly composes the final video with precise
audio-visual alignment. Given the complexity of evaluating such multimodal
outputs, we introduce PresentEval, a unified assessment framework powered by
Vision-Language Models that comprehensively scores videos across three critical
dimensions: content fidelity, visual clarity, and audience comprehension
through prompt-based evaluation. Our experimental validation on a curated
dataset of 30 document-presentation pairs demonstrates that PresentAgent
approaches human-level quality across all evaluation metrics. These results
highlight the significant potential of controllable multimodal agents in
transforming static textual materials into dynamic, effective, and accessible
presentation formats. Code will be available at
https://github.com/AIGeeksGroup/PresentAgent.

</details>


### [111] [T-SYNTH: A Knowledge-Based Dataset of Synthetic Breast Images](https://arxiv.org/abs/2507.04038)
*Christopher Wiedeman,Anastasiia Sarmakeeva,Elena Sizikova,Daniil Filienko,Miguel Lago,Jana G. Delfino,Aldo Badano*

Main category: cs.CV

TL;DR: Proposes T-SYNTH, a synthetic dataset for breast imaging, to address limited annotated medical data.


<details>
  <summary>Details</summary>
Motivation: Limited access to large-scale annotated medical datasets hinders robust algorithm development.

Method: Uses physics simulations to generate synthetic images with pixel-level annotations for breast imaging.

Result: T-SYNTH shows promise for augmenting real datasets in detection tasks for DM and DBT.

Conclusion: Synthetic data like T-SYNTH can mitigate data limitations in medical imaging research.

Abstract: One of the key impediments for developing and assessing robust medical
imaging algorithms is limited access to large-scale datasets with suitable
annotations. Synthetic data generated with plausible physical and biological
constraints may address some of these data limitations. We propose the use of
physics simulations to generate synthetic images with pixel-level segmentation
annotations, which are notoriously difficult to obtain. Specifically, we apply
this approach to breast imaging analysis and release T-SYNTH, a large-scale
open-source dataset of paired 2D digital mammography (DM) and 3D digital breast
tomosynthesis (DBT) images. Our initial experimental results indicate that
T-SYNTH images show promise for augmenting limited real patient datasets for
detection tasks in DM and DBT. Our data and code are publicly available at
https://github.com/DIDSR/tsynth-release.

</details>


### [112] [Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation](https://arxiv.org/abs/2507.04047)
*Ziyu Zhu,Xilin Wang,Yixuan Li,Zhuofan Zhang,Xiaojian Ma,Yixin Chen,Baoxiong Jia,Wei Liang,Qian Yu,Zhidong Deng,Siyuan Huang,Qing Li*

Main category: cs.CV

TL;DR: MTU3D integrates active perception with 3D vision-language learning, enabling embodied agents to explore and understand environments without explicit 3D reconstruction. It outperforms existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing 3D-VL models lack active perception and exploration capabilities. MTU3D aims to bridge this gap by combining grounding and exploration.

Method: MTU3D uses online query-based representation learning, a unified grounding-exploration objective, and end-to-end trajectory learning from diverse RGB-D sequences.

Result: MTU3D outperforms state-of-the-art methods by 14%, 23%, 9%, and 2% on benchmarks like HM3D-OVON, GOAT-Bench, SG3D, and A-EQA.

Conclusion: Bridging visual grounding and exploration is crucial for embodied intelligence, and MTU3D demonstrates this effectively.

Abstract: Embodied scene understanding requires not only comprehending visual-spatial
information that has been observed but also determining where to explore next
in the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarily
focus on grounding objects in static observations from 3D reconstruction, such
as meshes and point clouds, but lack the ability to actively perceive and
explore their environment. To address this limitation, we introduce
\underline{\textbf{M}}ove \underline{\textbf{t}}o
\underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework that
integrates active perception with \underline{\textbf{3D}} vision-language
learning, enabling embodied agents to effectively explore and understand their
environment. This is achieved by three key innovations: 1) Online query-based
representation learning, enabling direct spatial memory construction from RGB-D
frames, eliminating the need for explicit 3D reconstruction. 2) A unified
objective for grounding and exploring, which represents unexplored locations as
frontier queries and jointly optimizes object grounding and frontier selection.
3) End-to-end trajectory learning that combines
\textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over a
million diverse trajectories collected from both simulated and real-world RGB-D
sequences. Extensive evaluations across various embodied navigation and
question-answering benchmarks show that MTU3D outperforms state-of-the-art
reinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%,
and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA,
respectively. \model's versatility enables navigation using diverse input
modalities, including categories, language descriptions, and reference images.
These findings highlight the importance of bridging visual grounding and
exploration for embodied intelligence.

</details>


### [113] [Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation](https://arxiv.org/abs/2507.04049)
*Ziying Song,Lin Liu,Hongyu Pan,Bencheng Liao,Mingzhe Guo,Lei Yang,Yongchang Zhang,Shaoqing Xu,Caiyan Jia,Yadan Luo*

Main category: cs.CV

TL;DR: DIVER integrates reinforcement learning with diffusion-based generation to enhance trajectory diversity in autonomous driving, overcoming limitations of imitation learning.


<details>
  <summary>Details</summary>
Motivation: Imitation learning from single expert demonstrations leads to conservative behaviors and poor generalization in complex scenarios.

Method: DIVER combines reinforcement learning and diffusion-based generation to produce diverse trajectories, using reward-based supervision for safety and diversity.

Result: DIVER improves trajectory diversity on benchmarks like NAVSIM, Bench2Drive, and nuScenes, addressing mode collapse in imitation learning.

Conclusion: DIVER effectively enhances trajectory diversity and generalization, offering a robust solution for autonomous driving.

Abstract: Most end-to-end autonomous driving methods rely on imitation learning from
single expert demonstrations, often leading to conservative and homogeneous
behaviors that limit generalization in complex real-world scenarios. In this
work, we propose DIVER, an end-to-end driving framework that integrates
reinforcement learning with diffusion-based generation to produce diverse and
feasible trajectories. At the core of DIVER lies a reinforced diffusion-based
generation mechanism. First, the model conditions on map elements and
surrounding agents to generate multiple reference trajectories from a single
ground-truth trajectory, alleviating the limitations of imitation learning that
arise from relying solely on single expert demonstrations. Second,
reinforcement learning is employed to guide the diffusion process, where
reward-based supervision enforces safety and diversity constraints on the
generated trajectories, thereby enhancing their practicality and generalization
capability. Furthermore, to address the limitations of L2-based open-loop
metrics in capturing trajectory diversity, we propose a novel Diversity metric
to evaluate the diversity of multi-mode predictions.Extensive experiments on
the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop
nuScenes dataset, demonstrate that DIVER significantly improves trajectory
diversity, effectively addressing the mode collapse problem inherent in
imitation learning.

</details>


### [114] [Generate, Refine, and Encode: Leveraging Synthesized Novel Samples for On-the-Fly Fine-Grained Category Discovery](https://arxiv.org/abs/2507.04051)
*Xiao Liu,Nan Pu,Haiyang Zheng,Wenjing Li,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: The paper introduces DiffGRE, a diffusion-based framework for On-the-fly Category Discovery (OCD), improving transferability by generating, refining, and encoding synthetic data to enhance recognition of known and unknown categories.


<details>
  <summary>Details</summary>
Motivation: Existing OCD methods struggle with limited transferability due to insufficient labeled data, especially in fine-grained recognition. The paper aims to address this by leveraging synthetic data generation.

Method: Proposes DiffGRE, a multi-stage framework: 1) Attribute-composition generation via diffusion latent space interpolation, 2) Diversity-driven refinement to select novel samples, and 3) Semi-supervised leader encoding to inject synthetic data knowledge into OCD models.

Result: DiffGRE outperforms previous methods on six fine-grained datasets, demonstrating superior performance in discovering known and unknown categories.

Conclusion: The DiffGRE framework effectively enhances OCD by leveraging synthetic data generation and refinement, addressing limitations of existing methods.

Abstract: In this paper, we investigate a practical yet challenging task: On-the-fly
Category Discovery (OCD). This task focuses on the online identification of
newly arriving stream data that may belong to both known and unknown
categories, utilizing the category knowledge from only labeled data. Existing
OCD methods are devoted to fully mining transferable knowledge from only
labeled data. However, the transferability learned by these methods is limited
because the knowledge contained in known categories is often insufficient,
especially when few annotated data/categories are available in fine-grained
recognition. To mitigate this limitation, we propose a diffusion-based OCD
framework, dubbed DiffGRE, which integrates Generation, Refinement, and
Encoding in a multi-stage fashion. Specifically, we first design an
attribute-composition generation method based on cross-image interpolation in
the diffusion latent space to synthesize novel samples. Then, we propose a
diversity-driven refinement approach to select the synthesized images that
differ from known categories for subsequent OCD model training. Finally, we
leverage a semi-supervised leader encoding to inject additional category
knowledge contained in synthesized data into the OCD models, which can benefit
the discovery of both known and unknown categories during the on-the-fly
inference process. Extensive experiments demonstrate the superiority of our
DiffGRE over previous methods on six fine-grained datasets.

</details>


### [115] [Temporal Continual Learning with Prior Compensation for Human Motion Prediction](https://arxiv.org/abs/2507.04060)
*Jianwei Tang,Jiangxin Sun,Xiaotong Lin,Lifang Zhang,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TL;DR: The paper introduces Temporal Continual Learning (TCL), a multi-stage training framework for Human Motion Prediction (HMP), addressing limitations in prior approaches by preserving prior information and optimizing objectives.


<details>
  <summary>Details</summary>
Motivation: Previous HMP methods treated all prediction moments equally, hindering short-term prediction learning and limiting prior information use.

Method: Proposes TCL with Prior Compensation Factor (PCF) to preserve prior information and a derived optimization objective.

Result: Demonstrates effectiveness on four HMP benchmark datasets, showing flexibility and integration ease with various models.

Conclusion: TCL improves HMP by addressing prior limitations and is adaptable to different models and datasets.

Abstract: Human Motion Prediction (HMP) aims to predict future poses at different
moments according to past motion sequences. Previous approaches have treated
the prediction of various moments equally, resulting in two main limitations:
the learning of short-term predictions is hindered by the focus on long-term
predictions, and the incorporation of prior information from past predictions
into subsequent predictions is limited. In this paper, we introduce a novel
multi-stage training framework called Temporal Continual Learning (TCL) to
address the above challenges. To better preserve prior information, we
introduce the Prior Compensation Factor (PCF). We incorporate it into the model
training to compensate for the lost prior information. Furthermore, we derive a
more reasonable optimization objective through theoretical derivation. It is
important to note that our TCL framework can be easily integrated with
different HMP backbone models and adapted to various datasets and applications.
Extensive experiments on four HMP benchmark datasets demonstrate the
effectiveness and flexibility of TCL. The code is available at
https://github.com/hyqlat/TCL.

</details>


### [116] [Consistent and Invariant Generalization Learning for Short-video Misinformation Detection](https://arxiv.org/abs/2507.04061)
*Hanghui Guo,Weijie Shi,Mengze Li,Juncheng Li,Hao Chen,Yue Cui,Jiajie Xu,Jia Zhu,Jiawei Shen,Zhangze Chen,Sirui Han*

Main category: cs.CV

TL;DR: The paper proposes DOCTOR, a domain generalization model for short-video misinformation detection, addressing domain gaps via cross-modal consistency and invariance learning.


<details>
  <summary>Details</summary>
Motivation: Current models perform poorly on unseen domains due to domain gaps, necessitating better generalization across modalities.

Method: DOCTOR uses cross-modal feature interpolation, interpolation distillation, and a diffusion model for noise addition and denoising to enhance domain-invariant features.

Result: Extensive experiments show DOCTOR's effectiveness in improving domain generalization for misinformation detection.

Conclusion: DOCTOR successfully addresses domain gaps in short-video misinformation detection through innovative multi-modal learning techniques.

Abstract: Short-video misinformation detection has attracted wide attention in the
multi-modal domain, aiming to accurately identify the misinformation in the
video format accompanied by the corresponding audio. Despite significant
advancements, current models in this field, trained on particular domains
(source domains), often exhibit unsatisfactory performance on unseen domains
(target domains) due to domain gaps. To effectively realize such domain
generalization on the short-video misinformation detection task, we propose
deep insights into the characteristics of different domains: (1) The detection
on various domains may mainly rely on different modalities (i.e., mainly
focusing on videos or audios). To enhance domain generalization, it is crucial
to achieve optimal model performance on all modalities simultaneously. (2) For
some domains focusing on cross-modal joint fraud, a comprehensive analysis
relying on cross-modal fusion is necessary. However, domain biases located in
each modality (especially in each frame of videos) will be accumulated in this
fusion process, which may seriously damage the final identification of
misinformation. To address these issues, we propose a new DOmain generalization
model via ConsisTency and invariance learning for shORt-video misinformation
detection (named DOCTOR), which contains two characteristic modules: (1) We
involve the cross-modal feature interpolation to map multiple modalities into a
shared space and the interpolation distillation to synchronize multi-modal
learning; (2) We design the diffusion model to add noise to retain core
features of multi modal and enhance domain invariant features through
cross-modal guided denoising. Extensive experiments demonstrate the
effectiveness of our proposed DOCTOR model. Our code is public available at
https://github.com/ghh1125/DOCTOR.

</details>


### [117] [Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic](https://arxiv.org/abs/2507.04062)
*Jianwei Tang,Hong Yang,Tengyue Chen,Jian-Fang Hu*

Main category: cs.CV

TL;DR: The paper proposes STAB and ACB memory banks with AAA strategy to improve stochastic human motion prediction by addressing transition smoothness and action characteristic learning.


<details>
  <summary>Details</summary>
Motivation: Challenges in generating smooth transitions and learning action characteristics due to varying speeds and action similarities lead to unreasonable predictions.

Method: Introduces Soft-transition Action Bank (STAB) for transition information and Action Characteristic Bank (ACB) for action specifics, fused via Adaptive Attention Adjustment (AAA).

Result: Outperforms state-of-the-art on four datasets.

Conclusion: The proposed STAB and ACB with AAA effectively address key challenges in human motion prediction, achieving superior performance.

Abstract: Action-driven stochastic human motion prediction aims to generate future
motion sequences of a pre-defined target action based on given past observed
sequences performing non-target actions. This task primarily presents two
challenges. Firstly, generating smooth transition motions is hard due to the
varying transition speeds of different actions. Secondly, the action
characteristic is difficult to be learned because of the similarity of some
actions. These issues cause the predicted results to be unreasonable and
inconsistent. As a result, we propose two memory banks, the Soft-transition
Action Bank (STAB) and Action Characteristic Bank (ACB), to tackle the problems
above. The STAB stores the action transition information. It is equipped with
the novel soft searching approach, which encourages the model to focus on
multiple possible action categories of observed motions. The ACB records action
characteristic, which produces more prior information for predicting certain
actions. To fuse the features retrieved from the two banks better, we further
propose the Adaptive Attention Adjustment (AAA) strategy. Extensive experiments
on four motion prediction datasets demonstrate that our approach consistently
outperforms the previous state-of-the-art. The demo and code are available at
https://hyqlat.github.io/STABACB.github.io/.

</details>


### [118] [VICI: VLM-Instructed Cross-view Image-localisation](https://arxiv.org/abs/2507.04107)
*Xiaohan Zhang,Tavis Shore,Chen Chen,Oscar Mendez,Simon Hadfield,Safwan Wshah*

Main category: cs.CV

TL;DR: A high-performing solution for matching narrow FOV street-level images to satellite imagery, using a two-stage retrieval and re-ranking approach, achieves competitive retrieval rates.


<details>
  <summary>Details</summary>
Motivation: Addressing the practical challenge of geo-localisation with limited-FOV street-level images, as real-world scenarios rarely offer panoramic queries.

Method: A two-stage approach: retrieving candidate satellite image embeddings followed by selective re-ranking to enhance accuracy among top candidates.

Result: Achieves R@1 and R@10 retrieval rates of \topone\% and \topten\%, demonstrating competitive performance.

Conclusion: Optimised retrieval and re-ranking strategies can significantly advance practical geo-localisation performance.

Abstract: In this paper, we present a high-performing solution to the UAVM 2025
Challenge, which focuses on matching narrow FOV street-level images to
corresponding satellite imagery using the University-1652 dataset. As panoramic
Cross-View Geo-Localisation nears peak performance, it becomes increasingly
important to explore more practical problem formulations. Real-world scenarios
rarely offer panoramic street-level queries; instead, queries typically consist
of limited-FOV images captured with unknown camera parameters. Our work
prioritises discovering the highest achievable performance under these
constraints, pushing the limits of existing architectures. Our method begins by
retrieving candidate satellite image embeddings for a given query, followed by
a re-ranking stage that selectively enhances retrieval accuracy within the top
candidates. This two-stage approach enables more precise matching, even under
the significant viewpoint and scale variations inherent in the task. Through
experimentation, we demonstrate that our approach achieves competitive results
-specifically attaining R@1 and R@10 retrieval rates of \topone\% and \topten\%
respectively. This underscores the potential of optimised retrieval and
re-ranking strategies in advancing practical geo-localisation performance. Code
is available at https://github.com/tavisshore/VICI.

</details>


### [119] [Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking](https://arxiv.org/abs/2507.04116)
*Fred Lydeard,Bashar I. Ahmad,Simon Godsill*

Main category: cs.CV

TL;DR: A computationally efficient multi-object tracking method (GaPP-Class and GaPP-ReaCtion) minimizes track breaks, learns measurement models online, and infers object classes, outperforming state-of-the-art algorithms.


<details>
  <summary>Details</summary>
Motivation: To address challenges in tracking agile targets in dynamic environments and enable joint tracking and classification.

Method: Uses Gaussian processes for motion modeling and non-homogeneous Poisson processes for observation, combined with particle filtering and MCMC for track revival.

Result: GaPP-ReaCtion reduces track breaks by ~30% in real radar data and more in simulations, outperforming other trackers.

Conclusion: The proposed trackers (GaPP-Class and GaPP-ReaCtion) are robust, adaptive, and superior to existing methods in performance.

Abstract: This paper presents a computationally efficient multi-object tracking
approach that can minimise track breaks (e.g., in challenging environments and
against agile targets), learn the measurement model parameters on-line (e.g.,
in dynamically changing scenes) and infer the class of the tracked objects, if
joint tracking and kinematic behaviour classification is sought. It capitalises
on the flexibilities offered by the integrated Gaussian process as a motion
model and the convenient statistical properties of non-homogeneous Poisson
processes as a suitable observation model. This can be combined with the
proposed effective track revival / stitching mechanism. We accordingly
introduce the two robust and adaptive trackers, Gaussian and Poisson Process
with Classification (GaPP-Class) and GaPP with Revival and Classification
(GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme
that efficiently integrates track management and hyperparameter learning
(including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class
with the addition of a Markov Chain Monte Carlo kernel applied to each particle
permitting track revival and stitching (e.g., within a few time steps after
deleting a trajectory). Performance evaluation and benchmarking using synthetic
and real data show that GaPP-Class and GaPP-ReaCtion outperform other
state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly
reduces track breaks (e.g., by around 30% from real radar data and markedly
more from simulated data).

</details>


### [120] [PromptSR: Cascade Prompting for Lightweight Image Super-Resolution](https://arxiv.org/abs/2507.04118)
*Wenyang Liu,Chen Cai,Jianjun Gao,Kejun Wu,Yi Wang,Kim-Hui Yap,Lap-Pui Chau*

Main category: cs.CV

TL;DR: PromptSR introduces a lightweight image super-resolution method using cascade prompting blocks (CPB) to enhance global and local refinement efficiently.


<details>
  <summary>Details</summary>
Motivation: Address the limited receptive field and high computational cost of window-based self-attention in lightweight Vision Transformers for image super-resolution.

Method: Proposes CPB with global anchor prompting (GAPL) and local prompting layers (LPLs) to combine global priors and local details efficiently.

Result: Outperforms state-of-the-art lightweight SR methods in quantitative, qualitative, and complexity evaluations.

Conclusion: PromptSR effectively balances global perception and local refinement with low computational costs, advancing lightweight image super-resolution.

Abstract: Although the lightweight Vision Transformer has significantly advanced image
super-resolution (SR), it faces the inherent challenge of a limited receptive
field due to the window-based self-attention modeling. The quadratic
computational complexity relative to window size restricts its ability to use a
large window size for expanding the receptive field while maintaining low
computational costs. To address this challenge, we propose PromptSR, a novel
prompt-empowered lightweight image SR method. The core component is the
proposed cascade prompting block (CPB), which enhances global information
access and local refinement via three cascaded prompting layers: a global
anchor prompting layer (GAPL) and two local prompting layers (LPLs). The GAPL
leverages downscaled features as anchors to construct low-dimensional anchor
prompts (APs) through cross-scale attention, significantly reducing
computational costs. These APs, with enhanced global perception, are then used
to provide global prompts, efficiently facilitating long-range token
connections. The two LPLs subsequently combine category-based self-attention
and window-based self-attention to refine the representation in a
coarse-to-fine manner. They leverage attention maps from the GAPL as additional
global prompts, enabling them to perceive features globally at different
granularities for adaptive local refinement. In this way, the proposed CPB
effectively combines global priors and local details, significantly enlarging
the receptive field while maintaining the low computational costs of our
PromptSR. The experimental results demonstrate the superiority of our method,
which outperforms state-of-the-art lightweight SR methods in quantitative,
qualitative, and complexity evaluations. Our code will be released at
https://github.com/wenyang001/PromptSR.

</details>


### [121] [Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge](https://arxiv.org/abs/2507.04123)
*Linshen Liu,Boyan Su,Junyue Jiang,Guanlin Wu,Cong Guo,Ceyu Xu,Hao Frank Yang*

Main category: cs.CV

TL;DR: EMC2 is an edge-optimized system for AVs, combining LiDAR and camera data for low-latency, high-accuracy 3D object detection. It outperforms baselines with 3.58% accuracy gain and 159.06% speedup.


<details>
  <summary>Details</summary>
Motivation: To address the need for real-time, accurate 3D object detection in AVs by leveraging edge computing and multimodal data fusion.

Method: Uses a scenario-aware MoE architecture, adaptive multimodal data bridge, and joint hardware-software optimizations for efficient edge deployment.

Result: Achieves 3.58% higher accuracy and 159.06% faster inference on KITTI, with similar gains on nuScenes.

Conclusion: EMC2 advances reliable, real-time 3D object detection for AVs, proving its effectiveness on edge platforms.

Abstract: This paper presents Edge-based Mixture of Experts (MoE) Collaborative
Computing (EMC2), an optimal computing system designed for autonomous vehicles
(AVs) that simultaneously achieves low-latency and high-accuracy 3D object
detection. Unlike conventional approaches, EMC2 incorporates a scenario-aware
MoE architecture specifically optimized for edge platforms. By effectively
fusing LiDAR and camera data, the system leverages the complementary strengths
of sparse 3D point clouds and dense 2D images to generate robust multimodal
representations. To enable this, EMC2 employs an adaptive multimodal data
bridge that performs multi-scale preprocessing on sensor inputs, followed by a
scenario-aware routing mechanism that dynamically dispatches features to
dedicated expert models based on object visibility and distance. In addition,
EMC2 integrates joint hardware-software optimizations, including hardware
resource utilization optimization and computational graph simplification, to
ensure efficient and real-time inference on resource-constrained edge devices.
Experiments on open-source benchmarks clearly show the EMC2 advancements as a
end-to-end system. On the KITTI dataset, it achieves an average accuracy
improvement of 3.58% and a 159.06% inference speedup compared to 15 baseline
methods on Jetson platforms, with similar performance gains on the nuScenes
dataset, highlighting its capability to advance reliable, real-time 3D object
detection tasks for AVs.

</details>


### [122] [Driver-Net: Multi-Camera Fusion for Assessing Driver Take-Over Readiness in Automated Vehicles](https://arxiv.org/abs/2507.04139)
*Mahdi Rezaei,Mohsen Azarmi*

Main category: cs.CV

TL;DR: Driver-Net is a deep learning framework using multi-camera inputs to assess driver readiness for automated vehicles, achieving 95.8% accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate and timely assessment of driver readiness is crucial for safe control transitions in automated vehicles.

Method: Driver-Net fuses multi-camera inputs (head, hands, body posture) via a dual-path architecture (Context Block and Feature Block) with cross-modal fusion.

Result: Achieves 95.8% accuracy in driver readiness classification, outperforming conventional methods.

Conclusion: Driver-Net enhances safety in automated vehicles and aligns with regulatory standards, offering a real-time, non-intrusive solution.

Abstract: Ensuring safe transition of control in automated vehicles requires an
accurate and timely assessment of driver readiness. This paper introduces
Driver-Net, a novel deep learning framework that fuses multi-camera inputs to
estimate driver take-over readiness. Unlike conventional vision-based driver
monitoring systems that focus on head pose or eye gaze, Driver-Net captures
synchronised visual cues from the driver's head, hands, and body posture
through a triple-camera setup. The model integrates spatio-temporal data using
a dual-path architecture, comprising a Context Block and a Feature Block,
followed by a cross-modal fusion strategy to enhance prediction accuracy.
Evaluated on a diverse dataset collected from the University of Leeds Driving
Simulator, the proposed method achieves an accuracy of up to 95.8% in driver
readiness classification. This performance significantly enhances existing
approaches and highlights the importance of multimodal and multi-view fusion.
As a real-time, non-intrusive solution, Driver-Net contributes meaningfully to
the development of safer and more reliable automated vehicles and aligns with
new regulatory mandates and upcoming safety standards.

</details>


### [123] [Pedestrian Intention Prediction via Vision-Language Foundation Models](https://arxiv.org/abs/2507.04141)
*Mohsen Azarmi,Mahdi Rezaei,He Wang*

Main category: cs.CV

TL;DR: VLFMs outperform vision-based methods in predicting pedestrian crossing intentions by integrating multimodal data and hierarchical prompts, achieving up to 19.8% higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of conventional vision-based methods in generalizability, context understanding, and causal reasoning for pedestrian crossing intention prediction.

Method: Uses vision-language foundation models (VLFMs) with hierarchical prompt templates, integrating visual frames, physical cues, and ego-vehicle dynamics.

Result: Achieves up to 19.8% higher accuracy with vehicle speed and time-conscious prompts, and an additional 12.5% gain with optimized prompts.

Conclusion: VLFMs offer superior performance, generalizability, and contextual understanding for autonomous driving applications.

Abstract: Prediction of pedestrian crossing intention is a critical function in
autonomous vehicles. Conventional vision-based methods of crossing intention
prediction often struggle with generalizability, context understanding, and
causal reasoning. This study explores the potential of vision-language
foundation models (VLFMs) for predicting pedestrian crossing intentions by
integrating multimodal data through hierarchical prompt templates. The
methodology incorporates contextual information, including visual frames,
physical cues observations, and ego-vehicle dynamics, into systematically
refined prompts to guide VLFMs effectively in intention prediction. Experiments
were conducted on three common datasets-JAAD, PIE, and FU-PIP. Results
demonstrate that incorporating vehicle speed, its variations over time, and
time-conscious prompts significantly enhances the prediction accuracy up to
19.8%. Additionally, optimised prompts generated via an automatic prompt
engineering framework yielded 12.5% further accuracy gains. These findings
highlight the superior performance of VLFMs compared to conventional
vision-based models, offering enhanced generalisation and contextual
understanding for autonomous driving applications.

</details>


### [124] [Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation](https://arxiv.org/abs/2507.04151)
*Fernando Gabriela Garcia,Spencer Burns,Ryan Shaw,Hunter Young*

Main category: cs.CV

TL;DR: Hi-SSLVLM introduces a two-stage self-supervised learning model for text-to-image synthesis, improving control over visual attributes and spatial relationships without heavy reliance on labeled data.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional methods, such as high costs of curated datasets and poor control over fine-grained visual details.

Method: Two-stage approach: 1) Multi-Granularity Visual-Language Grounding for hierarchical caption alignment, 2) Self-Refinement and Guided Image Generation with Internal Compositional Planning and Semantic Consistency Loss.

Result: Outperforms leading baselines in fine-grained metrics and human evaluations, showing superior fidelity, compositional accuracy, and aesthetic quality.

Conclusion: Hi-SSLVLM advances controllable and semantically consistent text-to-image generation, validated by comprehensive experiments and human feedback.

Abstract: This paper introduces Hierarchical Self-Supervised LVLM (Hi-SSLVLM), a novel
generative model designed to significantly advance text-to-image synthesis,
particularly for complex and compositionally challenging prompts. Traditional
methods often grapple with the high cost of meticulously curated paired
image-text datasets and struggle with precise control over fine-grained visual
attributes and intricate spatial relationships. Our Hi-SSLVLM addresses these
limitations through a unique two-stage self-supervised learning strategy. The
first stage, Multi-Granularity Visual-Language Grounding, enables the Large
Vision-Language Model (LVLM) backbone to autonomously generate and align
hierarchical captions (global and local) to images, cultivating a deep internal
semantic understanding without reliance on extensive human annotation. The
second stage, Self-Refinement and Guided Image Generation, leverages this
acquired knowledge by an Internal Compositional Planning (ICP) mechanism, where
the LVLM first formulates detailed textual sub-prompts to guide the image
generation process, complemented by a novel Semantic Consistency Loss for
precise output alignment. Comprehensive experiments against leading baselines,
including Janus-Pro-1B, Stable Diffusion XL 1.0, DeepFloyd IF v1.0, and
ControlNet-XL, on multi-dimensional benchmarks such as Gemini-2.0-Flash and
InternVL3-78B, demonstrate Hi-SSLVLM's superior performance across all
fine-grained metrics. An in-depth ablation study confirms the critical role of
each proposed component. Furthermore, human evaluations corroborate our
quantitative findings, highlighting Hi-SSLVLM's enhanced fidelity to prompt,
compositional accuracy, and overall aesthetic quality, marking a significant
step towards more controllable and semantically consistent open-ended
text-to-image generation.

</details>


### [125] [LVLM-Composer's Explicit Planning for Image Generation](https://arxiv.org/abs/2507.04152)
*Spencer Ramsey,Jeffrey Lee,Amina Grant*

Main category: cs.CV

TL;DR: LVLM-Composer, a 10B-parameter LVLM, enhances compositional image synthesis with hierarchical semantic planning and fine-grained feature alignment, outperforming benchmarks like LongBench-T2I.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs struggle with complex textual descriptions requiring precise compositional understanding, impacting accurate rendering of objects, attributes, and spatial relationships.

Method: Introduces Hierarchical Semantic Planning Module and Fine-Grained Feature Alignment Mechanism, trained via multi-stage paradigm including Hierarchical Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement Learning.

Result: Superior performance on LongBench-T2I in object accuracy, composition fidelity, and pose accuracy, validated by automatic and human evaluations.

Conclusion: LVLM-Composer advances controllable and compositionally accurate text-to-image generation.

Abstract: The burgeoning field of generative artificial intelligence has fundamentally
reshaped our approach to content creation, with Large Vision-Language Models
(LVLMs) standing at its forefront. While current LVLMs have demonstrated
impressive capabilities in text-to-image generation, they often falter when
confronted with complex textual descriptions demanding precise compositional
understanding and visual planning. This limitation particularly impacts the
accurate rendering of multiple objects, their attributes, spatial
relationships, and specific poses within intricate scenes, as evidenced by
benchmarks like LongBench-T2I. To address these challenges, we introduce
LVLM-Composer, a novel 10-billion parameter scale LVLM specifically engineered
for enhanced compositional image synthesis. Our method incorporates a
Hierarchical Semantic Planning Module for structured prompt decomposition and a
Fine-Grained Feature Alignment Mechanism for precise visual guidance during
generation. We propose a multi-stage training paradigm, featuring Hierarchical
Semantic-Visual Grounding Pre-training and Compositional Planning Reinforcement
Learning with Self-Correction, to instill robust compositional reasoning.
Extensive experiments on the LongBench-T2I benchmark, utilizing automatic
evaluation by Gemini-2.0-Flash and InternVL3-78B, demonstrate LVLM-Composer's
superior performance across critical compositional dimensions including object
accuracy, composition fidelity, and pose accuracy, significantly outperforming
state-of-the-art baselines. An in-depth ablation study further validates the
indispensable contribution of our proposed modules, while human evaluations
confirm the perceptual superiority of our generated images. LVLM-Composer
represents a significant step towards truly controllable and compositionally
accurate open-ended text-to-image generation.

</details>


### [126] [Voyaging into Unbounded Dynamic Scenes from a Single View](https://arxiv.org/abs/2507.04183)
*Fengrui Tian,Tianjiao Ding,Jinqi Luo,Hancheng Min,Ren Vidal*

Main category: cs.CV

TL;DR: DynamicVoyager generates unbounded dynamic scenes from a single view by reformulating the task as scene outpainting, using ray context for 3D motion consistency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of bounded scene regions and limited camera movements in previous methods, enabling applications in AR/VR and robotics.

Method: Maps single-view video to dynamic point cloud, renders partial video at novel views, and outpaints with ray contexts for 3D consistency. Iteratively updates point cloud for future views.

Result: Generates unbounded scenes with consistent motions and controllable content via scene prompts.

Conclusion: DynamicVoyager effectively addresses the challenge of generating 3D consistent dynamic scenes from a single view.

Abstract: This paper studies the problem of generating an unbounded dynamic scene from
a single view, which has wide applications in augmented/virtual reality and
robotics. Since the scene is changing over time, different generated views need
to be consistent with the underlying 3D motions. While previous works learn
such consistency by training from multiple views, the generated scene regions
are bounded to be close to the training views with limited camera movements. To
address this issue, we propose DynamicVoyager that reformulates the dynamic
scene generation as a scene outpainting process for new dynamic content. As 2D
outpainting models can hardly generate 3D consistent motions from only 2D
pixels at a single view, we consider pixels as rays to enrich the pixel input
with the ray context, so that the 3D motion consistency can be learned from the
ray information. More specifically, we first map the single-view video input to
a dynamic point cloud with the estimated video depths. Then we render the
partial video at a novel view and outpaint the video with ray contexts from the
point cloud to generate 3D consistent motions. We employ the outpainted video
to update the point cloud, which is used for scene outpainting from future
novel views. Experiments show that our model is able to generate unbounded
scenes with consistent motions along fly-through cameras, and the generated
contents can be controlled with scene prompts.

</details>


### [127] [Towards Spatially-Varying Gain and Binning](https://arxiv.org/abs/2507.04190)
*Anqi Yang,Eunhee Kang,Wei Chen,Hyong-Euk Lee,Aswin C. Sankaranarayanan*

Main category: cs.CV

TL;DR: The paper proposes spatially-varying gain and binning to improve noise performance and dynamic range in image sensors, balancing resolution and noise based on local scene brightness.


<details>
  <summary>Details</summary>
Motivation: Smaller pixels in image sensors reduce light accumulation, worsening image quality. The study aims to enhance noise performance and dynamic range.

Method: Uses spatially-varying gain and binning, analyzing optimal binning size and comparing analog vs. digital binning.

Result: Spatially-varying gain expands dynamic range by an order of magnitude; digital binning outperforms analog with higher gain.

Conclusion: Combining spatially-varying gain and binning improves applications like HDR imaging, vignetting, and lens distortion.

Abstract: Pixels in image sensors have progressively become smaller, driven by the goal
of producing higher-resolution imagery. However, ceteris paribus, a smaller
pixel accumulates less light, making image quality worse. This interplay of
resolution, noise, and the dynamic range of the sensor and their impact on the
eventual quality of acquired imagery is a fundamental concept in photography.
In this paper, we propose spatially-varying gain and binning to enhance the
noise performance and dynamic range of image sensors. First, we show that by
varying gain spatially to local scene brightness, the read noise can be made
negligible, and the dynamic range of a sensor is expanded by an order of
magnitude. Second, we propose a simple analysis to find a binning size that
best balances resolution and noise for a given light level; this analysis
predicts a spatially-varying binning strategy, again based on local scene
brightness, to effectively increase the overall signal-to-noise ratio. %
without sacrificing resolution. We discuss analog and digital binning modes
and, perhaps surprisingly, show that digital binning outperforms its analog
counterparts when a larger gain is allowed. Finally, we demonstrate that
combining spatially-varying gain and binning in various applications, including
high dynamic range imaging, vignetting, and lens distortion.

</details>


### [128] [Quick Bypass Mechanism of Zero-Shot Diffusion-Based Image Restoration](https://arxiv.org/abs/2507.04207)
*Yu-Shan Tai,An-Yeu,Wu*

Main category: cs.CV

TL;DR: The paper introduces Quick Bypass Mechanism (QBM) and Revised Reverse Process (RRP) to accelerate diffusion models for image restoration tasks without compromising performance.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot diffusion models for image restoration suffer from slow denoising processes.

Method: Proposes QBM to bypass early denoising steps and RRP to adjust noise weighting for better stochasticity.

Result: Validated on ImageNet-1K and CelebA-HQ, showing faster performance while maintaining quality.

Conclusion: The proposed methods effectively speed up diffusion models for restoration tasks without losing original performance.

Abstract: Recent advancements in diffusion models have demonstrated remarkable success
in various image generation tasks. Building upon these achievements, diffusion
models have also been effectively adapted to image restoration tasks, e.g.,
super-resolution and deblurring, aiming to recover high-quality images from
degraded inputs. Although existing zero-shot approaches enable pretrained
diffusion models to perform restoration tasks without additional fine-tuning,
these methods often suffer from prolonged iteration times in the denoising
process. To address this limitation, we propose a Quick Bypass Mechanism (QBM),
a strategy that significantly accelerates the denoising process by initializing
from an intermediate approximation, effectively bypassing early denoising
steps. Furthermore, recognizing that approximation may introduce
inconsistencies, we introduce a Revised Reverse Process (RRP), which adjusts
the weighting of random noise to enhance the stochasticity and mitigate
potential disharmony. We validate proposed methods on ImageNet-1K and CelebA-HQ
across multiple image restoration tasks, e.g., super-resolution, deblurring,
and compressed sensing. Our experimental results show that the proposed methods
can effectively accelerate existing methods while maintaining original
performance.

</details>


### [129] [DreamPoster: A Unified Framework for Image-Conditioned Generative Poster Design](https://arxiv.org/abs/2507.04218)
*Xiwei Hu,Haokun Chen,Zhongqi Qi,Hui Zhang,Dexiang Hong,Jie Shao,Xinglong Wu*

Main category: cs.CV

TL;DR: DreamPoster is a Text-to-Image framework for high-quality poster generation, outperforming existing methods with 88.55% usability.


<details>
  <summary>Details</summary>
Motivation: To synthesize posters from user inputs while maintaining content fidelity and supporting flexible outputs.

Method: Built on Seedream3.0, uses systematic data annotation and progressive training for multi-task generation.

Result: Achieves 88.55% usability, surpassing GPT-4o (47.56%) and SeedEdit3.0 (25.96%).

Conclusion: DreamPoster is effective and will be deployed in Bytedance Apps.

Abstract: We present DreamPoster, a Text-to-Image generation framework that
intelligently synthesizes high-quality posters from user-provided images and
text prompts while maintaining content fidelity and supporting flexible
resolution and layout outputs. Specifically, DreamPoster is built upon our T2I
model, Seedream3.0 to uniformly process different poster generating types. For
dataset construction, we propose a systematic data annotation pipeline that
precisely annotates textual content and typographic hierarchy information
within poster images, while employing comprehensive methodologies to construct
paired datasets comprising source materials (e.g., raw graphics/text) and their
corresponding final poster outputs. Additionally, we implement a progressive
training strategy that enables the model to hierarchically acquire multi-task
generation capabilities while maintaining high-quality generation. Evaluations
on our testing benchmarks demonstrate DreamPoster's superiority over existing
methods, achieving a high usability rate of 88.55\%, compared to GPT-4o
(47.56\%) and SeedEdit3.0 (25.96\%). DreamPoster will be online in Jimeng and
other Bytedance Apps.

</details>


### [130] [Domain Generalizable Portrait Style Transfer](https://arxiv.org/abs/2507.04243)
*Xinbo Wang,Wenju Xu,Qing Zhang,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: A method for high-quality, semantic-aligned portrait style transfer across domains using dense semantic correspondence, AdaIN-Wavelet transform, and a dual-conditional diffusion model.


<details>
  <summary>Details</summary>
Motivation: To generalize portrait style transfer across diverse domains while ensuring semantic alignment and high-quality stylization.

Method: Uses a pre-trained model and semantic adapter for dense semantic correspondence, AdaIN-Wavelet transform for balancing content and style, and a dual-conditional diffusion model for final output.

Result: Demonstrates superior performance in experiments, with code and trained model available.

Conclusion: The proposed method effectively achieves high-quality, semantic-aligned portrait style transfer across various domains.

Abstract: This paper presents a portrait style transfer method that generalizes well to
various different domains while enabling high-quality semantic-aligned
stylization on regions including hair, eyes, eyelashes, skins, lips, and
background. To this end, we propose to establish dense semantic correspondence
between the given input and reference portraits based on a pre-trained model
and a semantic adapter, with which we obtain a warped reference semantically
aligned with the input. To ensure effective yet controllable style transfer, we
devise an AdaIN-Wavelet transform to balance content preservation and
stylization by blending low-frequency information of the warped reference with
high-frequency information of the input in the latent space. A style adapter is
also designed to provide style guidance from the warped reference. With the
stylized latent from AdaIN-Wavelet transform, we employ a dual-conditional
diffusion model that integrates a ControlNet recording high-frequency
information and the style guidance to generate the final result. Extensive
experiments demonstrate the superiority of our method. Our code and trained
model are available at https://github.com/wangxb29/DGPST.

</details>


### [131] [MoReMouse: Monocular Reconstruction of Laboratory Mouse](https://arxiv.org/abs/2507.04258)
*Yuan Zhong,Jingxiang Sun,Liang An,Yebin Liu*

Main category: cs.CV

TL;DR: MoReMouse is a monocular dense 3D reconstruction network for laboratory mice, addressing challenges in motion reconstruction with synthetic datasets, transformer-based architecture, and geodesic embeddings.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D mouse surface motion reconstruction is challenging due to non-rigid deformations and lack of structured datasets, hindering progress beyond sparse keypoint tracking.

Method: Constructs a synthetic dataset with a Gaussian mouse avatar, uses a transformer-based triplane architecture, and introduces geodesic-based embeddings for surface consistency.

Result: MoReMouse outperforms existing methods in accuracy and robustness, demonstrated through extensive experiments.

Conclusion: MoReMouse advances 3D mouse reconstruction with innovative designs, setting a new benchmark for future research.

Abstract: Laboratory mice play a crucial role in biomedical research, yet accurate 3D
mouse surface motion reconstruction remains challenging due to their complex
non-rigid geometric deformations and textureless appearance. Moreover, the
absence of structured 3D datasets severely hinders the progress beyond sparse
keypoint tracking. To narrow the gap, we present MoReMouse, the first monocular
dense 3D reconstruction network tailored for laboratory mice. To achieve this
goal, we highlight three key designs. First, we construct the first
high-fidelity dense-view synthetic dataset for mice, by rendering our
self-designed realistic Gaussian mouse avatar. Second, MoReMouse adopts a
transformer-based feedforward architecture with triplane representation,
achieving high-quality 3D surface generation from a single image. Third, we
create geodesic-based continuous correspondence embeddings on mouse surface,
which serve as strong semantic priors to improve reconstruction stability and
surface consistency. Extensive quantitative and qualitative experiments
demonstrate that MoReMouse significantly outperforms existing open-source
methods in accuracy and robustness. Video results are available at
https://zyyw-eric.github.io/MoreMouse-webpage/.

</details>


### [132] [Efficient Training of Deep Networks using Guided Spectral Data Selection: A Step Toward Learning What You Need](https://arxiv.org/abs/2507.04269)
*Mohammadreza Sharifi,Ahad Harati*

Main category: cs.CV

TL;DR: GSTDS is a spectral-based data selection algorithm that dynamically curates training data using a pre-trained model, reducing computational costs while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To optimize neural network training by efficiently selecting informative data points and reducing redundant computations, thereby saving resources.

Method: Uses a pre-trained reference model and spectral analysis (Fiedler vector-based scoring) to dynamically filter less beneficial data points per batch.

Result: Achieves up to 4x computational savings without performance loss and improves accuracy under limited resources, outperforming JEST and standard training.

Conclusion: GSTDS demonstrates the potential of spectral-based data selection for scalable, resource-efficient deep learning, encouraging further research in adaptive data curation.

Abstract: Effective data curation is essential for optimizing neural network training.
In this paper, we present the Guided Spectrally Tuned Data Selection (GSTDS)
algorithm, which dynamically adjusts the subset of data points used for
training using an off-the-shelf pre-trained reference model. Based on a
pre-scheduled filtering ratio, GSTDS effectively reduces the number of data
points processed per batch. The proposed method ensures an efficient selection
of the most informative data points for training while avoiding redundant or
less beneficial computations. Preserving data points in each batch is performed
based on spectral analysis. A Fiedler vector-based scoring mechanism removes
the filtered portion of the batch, lightening the resource requirements of the
learning. The proposed data selection approach not only streamlines the
training process but also promotes improved generalization and accuracy.
Extensive experiments on standard image classification benchmarks, including
CIFAR-10, Oxford-IIIT Pet, and Oxford-Flowers, demonstrate that GSTDS
outperforms standard training scenarios and JEST, a recent state-of-the-art
data curation method, on several key factors. It is shown that GSTDS achieves
notable reductions in computational requirements, up to four times, without
compromising performance. GSTDS exhibits a considerable growth in terms of
accuracy under the limited computational resource usage, in contrast to other
methodologies. These promising results underscore the potential of
spectral-based data selection as a scalable solution for resource-efficient
deep learning and motivate further exploration into adaptive data curation
strategies. You can find the code at https://github.com/rezasharifi82/GSTDS.

</details>


### [133] [ZERO: Multi-modal Prompt-based Visual Grounding](https://arxiv.org/abs/2507.04270)
*Sangbum Choi,Kyeongryeol Go*

Main category: cs.CV

TL;DR: ZERO is a zero-shot multi-prompt object detection model for industrial use, combining image and prompt inputs for scalable, adaptable detection with strong benchmark performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust, production-ready object detection models that can adapt to diverse industrial domains with minimal supervision.

Method: Integrates image and multi-prompt inputs (textual/visual) via dedicated encoders, optimized for scalability (1.033 TFLOPS, 622.346M parameters), and uses domain-specific fine-tuning with prompt diversity and pseudo-labeling.

Result: Achieves strong performance on the RF20VL-fsod benchmark, demonstrating flexibility, efficiency, and real-world applicability.

Conclusion: ZERO highlights the potential of prompt-driven, data-centric AI for scalable and adaptive object detection in dynamic industrial settings.

Abstract: Recent advances in artificial intelligence have led to the emergence of
foundation models, large-scale pre-trained neural networks that serve as
versatile starting points for a wide range of downstream tasks. In this work,
we present ZERO, a zero-shot multi-prompt object detection model specifically
designed for robust, production-ready deployment across diverse industrial
domains. ZERO integrates direct image input with multiple user-defined prompts,
which can include both textual and visual cues, and processes them through
dedicated encoders to generate accurate detection outputs. The model
architecture is optimized for scalability, with a total of 1.033 TFLOPS and
622.346 million parameters, and is trained using a domain-specific image
database exceeding one billion images. For the CVPR 2025 Foundational Few-Shot
Object Detection (FSOD) Challenge, we introduce a domain-specific fine-tuning
strategy that emphasizes prompt diversity and conservative pseudo-labeling,
enabling effective adaptation to new domains with minimal supervision. Our
approach demonstrates practical advantages in flexibility, efficiency, and
real-world applicability, achieving strong performance on the RF20VL-fsod
benchmark despite limited annotation budgets. The results highlight the
potential of prompt-driven, data-centric AI for scalable and adaptive object
detection in dynamic industrial environments.

</details>


### [134] [Towards Lightest Low-Light Image Enhancement Architecture for Mobile Devices](https://arxiv.org/abs/2507.04277)
*Guangrui Bai,Hailong Yan,Wenhai Liu,Yahui Deng,Erbao Dong*

Main category: cs.CV

TL;DR: LiteIE is an ultra-lightweight unsupervised framework for real-time low-light image enhancement on mobile devices, outperforming SOTA with minimal parameters.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for low-light enhancement are resource-heavy and rely on labeled data, limiting deployment on resource-constrained platforms.

Method: LiteIE uses a backbone-agnostic feature extractor with two convolutional layers and a parameter-free Iterative Restoration Module, trained with an unsupervised objective.

Result: Achieves 19.04 dB PSNR on LOL dataset (1.4 dB higher than SOTA) with only 0.07% of parameters, running at 30 FPS for 4K images on mobile.

Conclusion: LiteIE is efficient and practical for low-light enhancement on edge devices.

Abstract: Real-time low-light image enhancement on mobile and embedded devices requires
models that balance visual quality and computational efficiency. Existing deep
learning methods often rely on large networks and labeled datasets, limiting
their deployment on resource-constrained platforms. In this paper, we propose
LiteIE, an ultra-lightweight unsupervised enhancement framework that eliminates
dependence on large-scale supervision and generalizes well across diverse
conditions. We design a backbone-agnostic feature extractor with only two
convolutional layers to produce compact image features enhancement tensors. In
addition, we develop a parameter-free Iterative Restoration Module, which
reuses the extracted features to progressively recover fine details lost in
earlier enhancement steps, without introducing any additional learnable
parameters. We further propose an unsupervised training objective that
integrates exposure control, edge-aware smoothness, and multi-scale color
consistency losses. Experiments on the LOL dataset, LiteIE achieves 19.04 dB
PSNR, surpassing SOTA by 1.4 dB while using only 0.07\% of its parameters. On a
Snapdragon 8 Gen 3 mobile processor, LiteIE runs at 30 FPS for 4K images with
just 58 parameters, enabling real-time deployment on edge devices. These
results establish LiteIE as an efficient and practical solution for low-light
enhancement on resource-limited platforms.

</details>


### [135] [SeqTex: Generate Mesh Textures in Video Sequence](https://arxiv.org/abs/2507.04285)
*Ze Yuan,Xin Yu,Yangtian Sun,Yuan-Chen Guo,Yan-Pei Cao,Ding Liang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: SeqTex is an end-to-end framework for generating UV texture maps by leveraging pretrained video models, avoiding post-processing and improving 3D consistency.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale 3D texture datasets limits generalization, and existing methods suffer from error accumulation and inconsistencies.

Method: SeqTex reformulates texture generation as a sequence problem, using a decoupled multi-view and UV branch, geometry-informed attention, and adaptive token resolution.

Result: SeqTex achieves state-of-the-art performance in 3D texture generation with superior consistency and alignment.

Conclusion: SeqTex effectively transfers video priors to UV textures, eliminating post-processing and enhancing real-world generalization.

Abstract: Training native 3D texture generative models remains a fundamental yet
challenging problem, largely due to the limited availability of large-scale,
high-quality 3D texture datasets. This scarcity hinders generalization to
real-world scenarios. To address this, most existing methods finetune
foundation image generative models to exploit their learned visual priors.
However, these approaches typically generate only multi-view images and rely on
post-processing to produce UV texture maps -- an essential representation in
modern graphics pipelines. Such two-stage pipelines often suffer from error
accumulation and spatial inconsistencies across the 3D surface. In this paper,
we introduce SeqTex, a novel end-to-end framework that leverages the visual
knowledge encoded in pretrained video foundation models to directly generate
complete UV texture maps. Unlike previous methods that model the distribution
of UV textures in isolation, SeqTex reformulates the task as a sequence
generation problem, enabling the model to learn the joint distribution of
multi-view renderings and UV textures. This design effectively transfers the
consistent image-space priors from video foundation models into the UV domain.
To further enhance performance, we propose several architectural innovations: a
decoupled multi-view and UV branch design, geometry-informed attention to guide
cross-domain feature alignment, and adaptive token resolution to preserve fine
texture details while maintaining computational efficiency. Together, these
components allow SeqTex to fully utilize pretrained video priors and synthesize
high-fidelity UV texture maps without the need for post-processing. Extensive
experiments show that SeqTex achieves state-of-the-art performance on both
image-conditioned and text-conditioned 3D texture generation tasks, with
superior 3D consistency, texture-geometry alignment, and real-world
generalization.

</details>


### [136] [M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding](https://arxiv.org/abs/2507.04289)
*Shenxi Liu,Kan Li,Mingyang Zhao,Yuhang Tian,Bin Li,Shoujun Zhou,Hongliang Li,Fuxia Yang*

Main category: cs.CV

TL;DR: M3-Med is a new benchmark for multi-lingual, multi-modal, and multi-hop reasoning in medical video understanding, addressing limitations of existing benchmarks in linguistic diversity and deep reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for video comprehension lack multilingual support and deep reasoning, limiting their applicability in professional domains like medical education.

Method: M3-Med introduces multi-hop reasoning tasks (TAGSV and TAGVC) requiring models to locate textual entities, find visual evidence, and synthesize information across modalities.

Result: State-of-the-art models and LLMs perform significantly worse than human experts, especially on multi-hop questions, revealing gaps in deep cross-modal reasoning.

Conclusion: M3-Med highlights AI's current limitations in specialized domains and sets a new direction for future research in multi-modal understanding.

Abstract: With the rapid progress of artificial intelligence (AI) in multi-modal
understanding, there is increasing potential for video comprehension
technologies to support professional domains such as medical education.
However, existing benchmarks suffer from two primary limitations: (1)
Linguistic Singularity: they are largely confined to English, neglecting the
need for multilingual resources; and (2) Shallow Reasoning: their questions are
often designed for surface-level information retrieval, failing to properly
assess deep multi-modal integration. To address these limitations, we present
M3-Med, the first benchmark for Multi-lingual, Multi-modal, and Multi-hop
reasoning in Medical instructional video understanding. M3-Med consists of
medical questions paired with corresponding video segments, annotated by a team
of medical experts. A key innovation of M3-Med is its multi-hop reasoning task,
which requires a model to first locate a key entity in the text, then find
corresponding visual evidence in the video, and finally synthesize information
across both modalities to derive the answer. This design moves beyond simple
text matching and poses a substantial challenge to a model's deep cross-modal
understanding capabilities. We define two tasks: Temporal Answer Grounding in
Single Video (TAGSV) and Temporal Answer Grounding in Video Corpus (TAGVC). We
evaluated several state-of-the-art models and Large Language Models (LLMs) on
M3-Med. The results reveal a significant performance gap between all models and
human experts, especially on the complex multi-hop questions where model
performance drops sharply. M3-Med effectively highlights the current
limitations of AI models in deep cross-modal reasoning within specialized
domains and provides a new direction for future research.

</details>


### [137] [MPQ-DMv2: Flexible Residual Mixed Precision Quantization for Low-Bit Diffusion Models with Temporal Distillation](https://arxiv.org/abs/2507.04290)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiangqi Li,Zhulin An,Libo Huang,Boyu Diao,Fuzhen Zhuang,Michele Magno,Yongjun Xu,Yingli Tian,Tingwen Huang*

Main category: cs.CV

TL;DR: MPQ-DMv2 improves low-bit quantization for diffusion models by addressing outlier issues, optimizing initialization, and enhancing temporal consistency, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods degrade performance under extremely low-bit (2-4 bit) settings due to outlier-unfriendly design and suboptimal optimization.

Method: Proposes Flexible Z-Order Residual Mixed Quantization for handling outliers, Object-Oriented Low-Rank Initialization for better convergence, and Memory-based Temporal Relation Distillation for consistency.

Result: MPQ-DMv2 significantly outperforms current SOTA methods across architectures, especially at extremely low-bit widths.

Conclusion: The framework effectively addresses quantization challenges in diffusion models, enabling efficient deployment on edge devices without severe performance loss.

Abstract: Diffusion models have demonstrated remarkable performance on vision
generation tasks. However, the high computational complexity hinders its wide
application on edge devices. Quantization has emerged as a promising technique
for inference acceleration and memory reduction. However, existing quantization
methods do not generalize well under extremely low-bit (2-4 bit) quantization.
Directly applying these methods will cause severe performance degradation. We
identify that the existing quantization framework suffers from the
outlier-unfriendly quantizer design, suboptimal initialization, and
optimization strategy. We present MPQ-DMv2, an improved \textbf{M}ixed
\textbf{P}recision \textbf{Q}uantization framework for extremely low-bit
\textbf{D}iffusion \textbf{M}odels. For the quantization perspective, the
imbalanced distribution caused by salient outliers is quantization-unfriendly
for uniform quantizer. We propose \textit{Flexible Z-Order Residual Mixed
Quantization} that utilizes an efficient binary residual branch for flexible
quant steps to handle salient error. For the optimization framework, we
theoretically analyzed the convergence and optimality of the LoRA module and
propose \textit{Object-Oriented Low-Rank Initialization} to use prior
quantization error for informative initialization. We then propose
\textit{Memory-based Temporal Relation Distillation} to construct an online
time-aware pixel queue for long-term denoising temporal information
distillation, which ensures the overall temporal consistency between quantized
and full-precision model. Comprehensive experiments on various generation tasks
show that our MPQ-DMv2 surpasses current SOTA methods by a great margin on
different architectures, especially under extremely low-bit widths.

</details>


### [138] [Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization](https://arxiv.org/abs/2507.04302)
*Zuyu Zhang,Ning Chen,Yongshan Liu,Qinghua Zhang,Xu Zhang*

Main category: cs.CV

TL;DR: LEAwareSGD, a Lyapunov Exponent-guided optimization method, improves Single Domain Generalization by dynamically adjusting learning rates to train models near the edge of chaos, enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of Single Domain Generalization (SDG) with limited data diversity and large domain shifts, where existing data augmentation techniques fall short.

Method: Proposes LEAwareSGD, leveraging Lyapunov Exponent measurements to modulate learning rates, training models near the edge of chaos for better adaptability and stability.

Result: Achieves up to 9.47% improvement on PACS in low-data regimes, with notable gains on OfficeHome and DomainNet datasets.

Conclusion: Training near the edge of chaos using LEAwareSGD effectively enhances model generalization in SDG tasks.

Abstract: Single Domain Generalization (SDG) aims to develop models capable of
generalizing to unseen target domains using only one source domain, a task
complicated by substantial domain shifts and limited data diversity. Existing
SDG approaches primarily rely on data augmentation techniques, which struggle
to effectively adapt training dynamics to accommodate large domain shifts. To
address this, we propose LEAwareSGD, a novel Lyapunov Exponent (LE)-guided
optimization approach inspired by dynamical systems theory. By leveraging LE
measurements to modulate the learning rate, LEAwareSGD encourages model
training near the edge of chaos, a critical state that optimally balances
stability and adaptability. This dynamic adjustment allows the model to explore
a wider parameter space and capture more generalizable features, ultimately
enhancing the model's generalization capability. Extensive experiments on PACS,
OfficeHome, and DomainNet demonstrate that LEAwareSGD yields substantial
generalization gains, achieving up to 9.47\% improvement on PACS in low-data
regimes. These results underscore the effectiveness of training near the edge
of chaos for enhancing model generalization capability in SDG tasks.

</details>


### [139] [Exploring Remote Physiological Signal Measurement under Dynamic Lighting Conditions at Night: Dataset, Experiment, and Analysis](https://arxiv.org/abs/2507.04306)
*Zhipeng Li,Kegang Wang,Hanguang Xiao,Xingyue Liu,Feizhong Zhou,Jiaxin Jiang,Tianqi Liu*

Main category: cs.CV

TL;DR: The paper introduces DLCN, a large-scale dataset for remote photoplethysmography (rPPG) under dynamic nighttime lighting, addressing a research gap and evaluating algorithm robustness.


<details>
  <summary>Details</summary>
Motivation: Current rPPG methods lack effectiveness in realistic nighttime scenarios with dynamic lighting, and there's a shortage of datasets for such conditions.

Method: The authors present the DLCN dataset, collected under dynamic nighttime lighting, and use the Happy-rPPG Toolkit for experiments.

Result: DLCN includes 13 hours of video and physiological data from 98 participants, covering four nighttime scenarios, providing a resource for robustness evaluation.

Conclusion: The dataset and code are publicly available, facilitating research into rPPG algorithm performance in challenging nighttime conditions.

Abstract: Remote photoplethysmography (rPPG) is a non-contact technique for measuring
human physiological signals. Due to its convenience and non-invasiveness, it
has demonstrated broad application potential in areas such as health monitoring
and emotion recognition. In recent years, the release of numerous public
datasets has significantly advanced the performance of rPPG algorithms under
ideal lighting conditions. However, the effectiveness of current rPPG methods
in realistic nighttime scenarios with dynamic lighting variations remains
largely unknown. Moreover, there is a severe lack of datasets specifically
designed for such challenging environments, which has substantially hindered
progress in this area of research. To address this gap, we present and release
a large-scale rPPG dataset collected under dynamic lighting conditions at
night, named DLCN. The dataset comprises approximately 13 hours of video data
and corresponding synchronized physiological signals from 98 participants,
covering four representative nighttime lighting scenarios. DLCN offers high
diversity and realism, making it a valuable resource for evaluating algorithm
robustness in complex conditions. Built upon the proposed Happy-rPPG Toolkit,
we conduct extensive experiments and provide a comprehensive analysis of the
challenges faced by state-of-the-art rPPG methods when applied to DLCN. The
dataset and code are publicly available at
https://github.com/dalaoplan/Happp-rPPG-Toolkit.

</details>


### [140] [DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection](https://arxiv.org/abs/2507.04323)
*Paul Hill,Alin Achim,Dave Bull,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: A novel framework (DMAT) combines AT mitigation and object detection, improving performance by 15% on turbulence-corrupted datasets.


<details>
  <summary>Details</summary>
Motivation: AT degrades surveillance imagery, hindering visualization and object detection. Existing methods struggle with spatio-temporal distortions.

Method: Uses a 3D Mamba-based structure for AT mitigation, extracts pyramid features, and shares them with the detector. End-to-end optimization via back-propagation.

Result: DMAT outperforms state-of-the-art systems by up to 15% on turbulence-corrupted datasets.

Conclusion: The framework effectively compensates for AT distortions while enhancing visualization and detection.

Abstract: Atmospheric Turbulence (AT) degrades the clarity and accuracy of surveillance
imagery, posing challenges not only for visualization quality but also for
object classification and scene tracking. Deep learning-based methods have been
proposed to improve visual quality, but spatio-temporal distortions remain a
significant issue. Although deep learning-based object detection performs well
under normal conditions, it struggles to operate effectively on sequences
distorted by atmospheric turbulence. In this paper, we propose a novel
framework that learns to compensate for distorted features while simultaneously
improving visualization and object detection. This end-to-end framework
leverages and exchanges knowledge of low-level distorted features in the AT
mitigator with semantic features extracted in the object detector.
Specifically, in the AT mitigator a 3D Mamba-based structure is used to handle
the spatio-temporal displacements and blurring caused by turbulence. Features
are extracted in a pyramid manner during the mitigation stage and passed to the
detector. Optimization is achieved through back-propagation in both the AT
mitigator and object detector. Our proposed DMAT outperforms state-of-the-art
AT mitigation and object detection systems up to a 15% improvement on datasets
corrupted by generated turbulence.

</details>


### [141] [Computed Tomography Visual Question Answering with Cross-modal Feature Graphing](https://arxiv.org/abs/2507.04333)
*Yuanhe Tian,Chen Su,Junwen Duan,Yan Song*

Main category: cs.CV

TL;DR: A novel LLM-based framework for medical VQA uses a graph representation to integrate visual and textual features, outperforming baselines by addressing spatial continuity in CT data.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods in medical imaging neglect spatial continuity and inter-slice correlations in CT data, leading to fragmented responses.

Method: Proposes a graph-based framework integrating CT slices and question tokens as nodes, using an attentive graph convolutional network to fuse features and guide an LLM.

Result: Outperforms baselines on the M3D-VQA benchmark, showing robust reasoning capabilities.

Conclusion: The graph-based approach enhances accuracy in medical VQA by better capturing spatial and textual relationships.

Abstract: Visual question answering (VQA) in medical imaging aims to support clinical
diagnosis by automatically interpreting complex imaging data in response to
natural language queries. Existing studies typically rely on distinct visual
and textual encoders to independently extract features from medical images and
clinical questions, which are subsequently combined to generate answers.
Specifically, in computed tomography (CT), such approaches are similar to the
conventional practices in medical image analysis. However, these approaches pay
less attention to the spatial continuity and inter-slice correlations in the
volumetric CT data, leading to fragmented and imprecise responses. In this
paper, we propose a novel large language model (LLM)-based framework enhanced
by a graph representation of salient features. Different from conventional
multimodal encoding strategies, our approach constructs a cross-modal graph
integrating both visual and textual features, treating individual CT slices and
question tokens as nodes within the graph. We further leverage an attentive
graph convolutional network to dynamically fuse information within this
structure. The resulting aggregated graph features then serve as a soft prompt
to guide a large language model in generating accurate answers. Extensive
experiments on the M3D-VQA benchmark demonstrate that our approach consistently
outperforms baselines across multiple evaluation metrics, offering more robust
reasoning capabilities.

</details>


### [142] [MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection](https://arxiv.org/abs/2507.04369)
*Hanshi Wang,Jin Gao,Weiming Hu,Zhipeng Zhang*

Main category: cs.CV

TL;DR: A pure Mamba block enables efficient Dense Global Fusion for camera-LiDAR 3D object detection, addressing inefficiencies and information loss in existing fusion methods.


<details>
  <summary>Details</summary>
Motivation: Existing fusion strategies lack efficiency, long-range modeling, and complete scene information retention.

Method: Proposes height-fidelity LiDAR encoding and Hybrid Mamba Block for enhanced alignment and contextual learning.

Result: Achieves state-of-the-art performance (75.0 NDS on nuScenes) with faster inference speed.

Conclusion: The method outperforms others in performance and efficiency, leveraging height-informed features and Mamba blocks.

Abstract: We present the first work demonstrating that a pure Mamba block can achieve
efficient Dense Global Fusion, meanwhile guaranteeing top performance for
camera-LiDAR multi-modal 3D object detection. Our motivation stems from the
observation that existing fusion strategies are constrained by their inability
to simultaneously achieve efficiency, long-range modeling, and retaining
complete scene information. Inspired by recent advances in state-space models
(SSMs) and linear attention, we leverage their linear complexity and long-range
modeling capabilities to address these challenges. However, this is non-trivial
since our experiments reveal that simply adopting efficient linear-complexity
methods does not necessarily yield improvements and may even degrade
performance. We attribute this degradation to the loss of height information
during multi-modal alignment, leading to deviations in sequence order. To
resolve this, we propose height-fidelity LiDAR encoding that preserves precise
height information through voxel compression in continuous space, thereby
enhancing camera-LiDAR alignment. Subsequently, we introduce the Hybrid Mamba
Block, which leverages the enriched height-informed features to conduct local
and global contextual learning. By integrating these components, our method
achieves state-of-the-art performance with the top-tire NDS score of 75.0 on
the nuScenes validation benchmark, even surpassing methods that utilize
high-resolution inputs. Meanwhile, our method maintains efficiency, achieving
faster inference speed than most recent state-of-the-art methods.

</details>


### [143] [Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions](https://arxiv.org/abs/2507.04377)
*Xiao Zhang,Johan Bos*

Main category: cs.CV

TL;DR: A novel multi-modal framework for digitizing tombstones using vision-language models (VLMs) and retrieval-augmented generation (RAG) improves parsing accuracy and semantic enrichment, addressing preservation challenges.


<details>
  <summary>Details</summary>
Motivation: Tombstones are culturally significant but face preservation issues like erosion and vandalism. Digitization can aid in interpretation and retrieval of their content.

Method: The approach uses VLMs to create Tombstone Meaning Representations (TMRs) and RAG for semantic enrichment, outperforming traditional OCR methods.

Result: Parsing accuracy improved from an F1 score of 36.1 to 89.5, with robustness tested across diverse inscriptions and degraded conditions.

Conclusion: This work formalizes tombstone understanding with VLMs, offering significant potential for heritage preservation.

Abstract: Tombstones are historically and culturally rich artifacts, encapsulating
individual lives, community memory, historical narratives and artistic
expression. Yet, many tombstones today face significant preservation
challenges, including physical erosion, vandalism, environmental degradation,
and political shifts. In this paper, we introduce a novel multi-modal framework
for tombstones digitization, aiming to improve the interpretation, organization
and retrieval of tombstone content. Our approach leverages vision-language
models (VLMs) to translate tombstone images into structured Tombstone Meaning
Representations (TMRs), capturing both image and text information. To further
enrich semantic parsing, we incorporate retrieval-augmented generation (RAG)
for integrate externally dependent elements such as toponyms, occupation codes,
and ontological concepts. Compared to traditional OCR-based pipelines, our
method improves parsing accuracy from an F1 score of 36.1 to 89.5. We
additionally evaluate the model's robustness across diverse linguistic and
cultural inscriptions, and simulate physical degradation through image fusion
to assess performance under noisy or damaged conditions. Our work represents
the first attempt to formalize tombstone understanding using large
vision-language models, presenting implications for heritage preservation.

</details>


### [144] [Transferring Visual Explainability of Self-Explaining Models through Task Arithmetic](https://arxiv.org/abs/2507.04380)
*Yuya Yoshikawa,Ryotaro Shimizu,Takahiro Kawashima,Yuki Saito*

Main category: cs.CV

TL;DR: The paper proposes a method to transfer visual explainability from a source to a target domain using task arithmetic, reducing training costs while maintaining accuracy and improving explanation quality.


<details>
  <summary>Details</summary>
Motivation: To address the high labeling and computational costs of training self-explaining models for image classification by leveraging explainability learned in a source domain.

Method: Extends image classifiers with a vision-language pretrained model, defines an explainability vector, and applies it to a target domain model using task arithmetic.

Result: Successful transfer of explainability between domains, improved explanation quality without accuracy loss, and comparable performance to Kernel SHAP with fewer inferences.

Conclusion: The method effectively transfers explainability, demonstrating universality and robustness, and offers efficiency gains over traditional explanation methods.

Abstract: In scenarios requiring both prediction and explanation efficiency for image
classification, self-explaining models that perform both tasks in a single
inference are effective. However, their training incurs substantial labeling
and computational costs. This study aims to tackle the issue by proposing a
method to transfer the visual explainability of self-explaining models, learned
in a source domain, to a target domain based on a task arithmetic framework.
Specifically, we construct a self-explaining model by extending image
classifiers based on a vision-language pretrained model. We then define an
\emph{explainability vector} as the difference between model parameters trained
on the source domain with and without explanation supervision. Based on the
task arithmetic framework, we impart explainability to a model trained only on
the prediction task in the target domain by applying the explainability vector.
Experimental results on various image classification datasets demonstrate that,
except for transfers between some less-related domains, visual explainability
can be successfully transferred from source to target domains, improving
explanation quality in the target domain without sacrificing classification
accuracy. Furthermore, we show that the explainability vector learned on a
large and diverse dataset like ImageNet, extended with explanation supervision,
exhibits universality and robustness, improving explanation quality on nine out
of ten different target datasets. We also find that the explanation quality
achieved with a single model inference is comparable to that of Kernel SHAP,
which requires 150 model inferences.

</details>


### [145] [Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers](https://arxiv.org/abs/2507.04388)
*Jung-Ho Hong,Ho-Joong Kim,Kyu-Sung Jeon,Seong-Whan Lee*

Main category: cs.CV

TL;DR: The paper introduces CoIBA, a method using a shared parametric damping ratio across layers to improve feature attribution by capturing distributed decision-making evidence.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single layers, missing distributed decision clues. CoIBA aims to address this by analyzing multiple layers.

Method: CoIBA applies the information bottleneck principle across targeted layers, sharing a damping ratio to estimate comprehensive information and using a variational approach to bound layer-wise information.

Result: Experiments show CoIBA enhances the faithfulness of feature attributions by capturing omitted decision clues.

Conclusion: CoIBA effectively improves attribution accuracy by leveraging shared information across layers, ensuring discarded activations are unnecessary for decisions.

Abstract: The feature attribution method reveals the contribution of input variables to
the decision-making process to provide an attribution map for explanation.
Existing methods grounded on the information bottleneck principle compute
information in a specific layer to obtain attributions, compressing the
features by injecting noise via a parametric damping ratio. However, the
attribution obtained in a specific layer neglects evidence of the
decision-making process distributed across layers. In this paper, we introduce
a comprehensive information bottleneck (CoIBA), which discovers the relevant
information in each targeted layer to explain the decision-making process. Our
core idea is applying information bottleneck in multiple targeted layers to
estimate the comprehensive information by sharing a parametric damping ratio
across the layers. Leveraging this shared ratio complements the over-compressed
information to discover the omitted clues of the decision by sharing the
relevant information across the targeted layers. We suggest the variational
approach to fairly reflect the relevant information of each layer by upper
bounding layer-wise information. Therefore, CoIBA guarantees that the discarded
activation is unnecessary in every targeted layer to make a decision. The
extensive experimental results demonstrate the enhancement in faithfulness of
the feature attributions provided by CoIBA.

</details>


### [146] [RegistrationMamba: A Mamba-based Registration Framework Integrating Multi-Expert Feature Learning for Cross-Modal Remote Sensing Images](https://arxiv.org/abs/2507.04397)
*Wei Wang,Dou Quan,Chonghua Lv,Shuang Wang,Ning Huyan,Yunan Li,Licheng Jiao*

Main category: cs.CV

TL;DR: Proposes RegistrationMamba, a Mamba-based architecture for cross-modal remote sensing image registration, addressing challenges like nonlinear radiometric variations and limited textures. It integrates multi-expert feature learning and multi-level feature aggregation for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in CRSI registration include nonlinear radiometric variations and limited textures. Existing methods (CNNs, Transformers) have limitations like local receptive fields or high computational complexity.

Method: RegistrationMamba uses a multi-directional cross-scanning strategy for global context with linear complexity. Multi-expert feature learning (MEFL) captures diverse features, and multi-level feature aggregation (MFA) combines global and local features.

Result: RegistrationMamba outperforms state-of-the-art methods in accuracy and robustness across varying image resolutions.

Conclusion: RegistrationMamba effectively addresses CRSI registration challenges, offering superior performance and adaptability to texture-limited scenarios.

Abstract: Cross-modal remote sensing image (CRSI) registration is critical for
multi-modal image applications. However, CRSI mainly faces two challenges:
significant nonlinear radiometric variations between cross-modal images and
limited textures hindering the discriminative information extraction. Existing
methods mainly adopt convolutional neural networks (CNNs) or Transformer
architectures to extract discriminative features for registration. However,
CNNs with the local receptive field fail to capture global contextual features,
and Transformers have high computational complexity and restrict their
application to high-resolution CRSI. To solve these issues, this paper proposes
RegistrationMamba, a novel Mamba architecture based on state space models
(SSMs) integrating multi-expert feature learning for improving the accuracy of
CRSI registration. Specifically, RegistrationMamba employs a multi-directional
cross-scanning strategy to capture global contextual relationships with linear
complexity. To enhance the performance of RegistrationMamba under
texture-limited scenarios, we propose a multi-expert feature learning (MEFL)
strategy to capture features from various augmented image variants through
multiple feature experts. MEFL leverages a learnable soft router to dynamically
fuse the features from multiple experts, thereby enriching feature
representations and improving registration performance. Notably, MEFL can be
seamlessly integrated into various frameworks, substantially boosting
registration performance. Additionally, RegistrationMamba integrates a
multi-level feature aggregation (MFA) module to extract fine-grained local
information and enable effective interaction between global and local features.
Extensive experiments on CRSI with varying image resolutions have demonstrated
that RegistrationMamba has superior performance and robustness compared to
state-of-the-art methods.

</details>


### [147] [Sat2City: 3D City Generation from A Single Satellite Image with Cascaded Latent Diffusion](https://arxiv.org/abs/2507.04403)
*Tongyan Hua,Lutao Jiang,Ying-Cong Chen,Wufan Zhao*

Main category: cs.CV

TL;DR: Sat2City is a novel framework combining sparse voxel grids and latent diffusion models to generate detailed 3D urban scenes from satellite imagery, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing neural rendering techniques struggle with structural ambiguity and lack detail in 3D urban scene generation from 2D satellite images.

Method: Sat2City uses a cascaded latent diffusion framework, Re-Hash operation for multi-scale feature grids, and inverse sampling for smooth transitions.

Result: The framework generates high-fidelity 3D structures from single satellite images, validated on a synthesized city dataset.

Conclusion: Sat2City advances 3D urban scene generation by addressing structural ambiguity and scalability, with applications in gaming and digital twins.

Abstract: Recent advancements in generative models have enabled 3D urban scene
generation from satellite imagery, unlocking promising applications in gaming,
digital twins, and beyond. However, most existing methods rely heavily on
neural rendering techniques, which hinder their ability to produce detailed 3D
structures on a broader scale, largely due to the inherent structural ambiguity
derived from relatively limited 2D observations. To address this challenge, we
propose Sat2City, a novel framework that synergizes the representational
capacity of sparse voxel grids with latent diffusion models, tailored
specifically for our novel 3D city dataset. Our approach is enabled by three
key components: (1) A cascaded latent diffusion framework that progressively
recovers 3D city structures from satellite imagery, (2) a Re-Hash operation at
its Variational Autoencoder (VAE) bottleneck to compute multi-scale feature
grids for stable appearance optimization and (3) an inverse sampling strategy
enabling implicit supervision for smooth appearance transitioning.To overcome
the challenge of collecting real-world city-scale 3D models with high-quality
geometry and appearance, we introduce a dataset of synthesized large-scale 3D
cities paired with satellite-view height maps. Validated on this dataset, our
framework generates detailed 3D structures from a single satellite image,
achieving superior fidelity compared to existing city generation models.

</details>


### [148] [A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields](https://arxiv.org/abs/2507.04408)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Pascal Fua*

Main category: cs.CV

TL;DR: The paper proposes using view-consistent distributions for depth regularization in NeRF training to address issues with fixed depth estimations, improving novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Fixed depth estimations in NeRF training are error-prone, especially for outdoor scenes, due to generalization issues and expensive 3D supervision.

Method: Uses view-consistent distributions from color and high-level features, combined with a depth-pushing loss, to regularize NeRF training.

Result: Outperforms state-of-the-art NeRF variants and depth regularization methods in novel view synthesis.

Conclusion: The proposed method effectively eliminates failure modes and enhances NeRF performance for real-world scenes.

Abstract: Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene
representation and 3D recovery. To improve its performance on real-world data,
depth regularizations have proven to be the most effective ones. However, depth
estimation models not only require expensive 3D supervision in training, but
also suffer from generalization issues. As a result, the depth estimations can
be erroneous in practice, especially for outdoor unbounded scenes. In this
paper, we propose to employ view-consistent distributions instead of fixed
depth value estimations to regularize NeRF training. Specifically, the
distribution is computed by utilizing both low-level color features and
high-level distilled features from foundation models at the projected 2D
pixel-locations from per-ray sampled 3D points. By sampling from the
view-consistency distributions, an implicit regularization is imposed on the
training of NeRF. We also utilize a depth-pushing loss that works in
conjunction with the sampling technique to jointly provide effective
regularizations for eliminating the failure modes. Extensive experiments
conducted on various scenes from public datasets demonstrate that our proposed
method can generate significantly better novel view synthesis results than
state-of-the-art NeRF variants as well as different depth regularization
methods.

</details>


### [149] [MVNet: Hyperspectral Remote Sensing Image Classification Based on Hybrid Mamba-Transformer Vision Backbone Architecture](https://arxiv.org/abs/2507.04409)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: MVNet integrates 3D-CNN, Transformer, and Mamba for efficient HSI classification, addressing high-dimensional data and spectral redundancy with a dual-branch Mamba module and optimized HSI-MambaVision Mixer.


<details>
  <summary>Details</summary>
Motivation: Challenges in HSI classification like high-dimensional data, limited samples, and spectral redundancy lead to overfitting and poor generalization. MVNet aims to improve feature extraction and fusion.

Method: Proposes MVNet with a dual-branch Mamba module (SSM and non-SSM branches) and HSI-MambaVision Mixer for bidirectional spatial-spectral dependency capture, reducing computational latency.

Result: Outperforms mainstream methods on IN, UP, and KSC datasets in accuracy and efficiency, demonstrating robust handling of complex HSI data.

Conclusion: MVNet effectively addresses HSI classification challenges with innovative architecture, achieving superior performance and computational efficiency.

Abstract: Hyperspectral image (HSI) classification faces challenges such as
high-dimensional data, limited training samples, and spectral redundancy, which
often lead to overfitting and insufficient generalization capability. This
paper proposes a novel MVNet network architecture that integrates 3D-CNN's
local feature extraction, Transformer's global modeling, and Mamba's linear
complexity sequence modeling capabilities, achieving efficient spatial-spectral
feature extraction and fusion. MVNet features a redesigned dual-branch Mamba
module, including a State Space Model (SSM) branch and a non-SSM branch
employing 1D convolution with SiLU activation, enhancing modeling of both
short-range and long-range dependencies while reducing computational latency in
traditional Mamba. The optimized HSI-MambaVision Mixer module overcomes the
unidirectional limitation of causal convolution, capturing bidirectional
spatial-spectral dependencies in a single forward pass through decoupled
attention that focuses on high-value features, alleviating parameter redundancy
and the curse of dimensionality. On IN, UP, and KSC datasets, MVNet outperforms
mainstream hyperspectral image classification methods in both classification
accuracy and computational efficiency, demonstrating robust capability in
processing complex HSI data.

</details>


### [150] [Multimedia Verification Through Multi-Agent Deep Research Multimodal Large Language Models](https://arxiv.org/abs/2507.04410)
*Huy Hoan Le,Van Sy Thinh Nguyen,Thi Le Chi Dang,Vo Thanh Khang Nguyen,Truong Thanh Hung Nguyen,Hung Cao*

Main category: cs.CV

TL;DR: A multi-agent system combining MLLMs and verification tools effectively detects multimedia misinformation through six stages, demonstrating success in verifying authenticity, geolocation, timing, and source attribution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of multimedia misinformation by leveraging advanced tools and models for accurate verification.

Method: A six-stage system involving raw data processing, planning, information extraction, deep research, evidence collection, and report generation, utilizing tools like reverse image search and fact-checking databases.

Result: Successfully verified content authenticity, extracted precise geolocation and timing, and traced source attribution in complex multimedia scenarios.

Conclusion: The system effectively tackles real-world multimedia verification, proving its utility in combating misinformation.

Abstract: This paper presents our submission to the ACMMM25 - Grand Challenge on
Multimedia Verification. We developed a multi-agent verification system that
combines Multimodal Large Language Models (MLLMs) with specialized verification
tools to detect multimedia misinformation. Our system operates through six
stages: raw data processing, planning, information extraction, deep research,
evidence collection, and report generation. The core Deep Researcher Agent
employs four tools: reverse image search, metadata analysis, fact-checking
databases, and verified news processing that extracts spatial, temporal,
attribution, and motivational context. We demonstrate our approach on a
challenge dataset sample involving complex multimedia content. Our system
successfully verified content authenticity, extracted precise geolocation and
timing information, and traced source attribution across multiple platforms,
effectively addressing real-world multimedia verification scenarios.

</details>


### [151] [SFOOD: A Multimodal Benchmark for Comprehensive Food Attribute Analysis Beyond RGB with Spectral Insights](https://arxiv.org/abs/2507.04412)
*Zhenbo Xu,Jinghan Yang,Gong Huang,Jiqing Feng,Liu Liu,Ruihan Sun,Ajin Meng,Zhuo Zhang,Zhaofeng He*

Main category: cs.CV

TL;DR: The paper introduces SFOOD, the first large-scale spectral food benchmark, addressing the lack of comprehensive datasets for food attributes like sweetness and weight, and highlights the importance of spectral data in food analysis.


<details>
  <summary>Details</summary>
Motivation: Existing research on food attributes is limited due to the absence of large benchmarks and the challenge of accurately perceiving attributes like sweetness and weight using RGB cameras alone.

Method: The authors built the SFOOD benchmark by organizing existing datasets, collecting hyperspectral images of hundreds of foods, and experimentally determining attributes like sweetness and weight.

Result: The benchmark includes 3,266 food categories and 2,351k data points. Evaluations show large-scale models struggle with food digitization and spectral data is key for analyzing food properties.

Conclusion: The SFOOD benchmark fills a critical gap in food analysis, will be open source, and iterated for future tasks, emphasizing the difficulty of studying food compared to other objects.

Abstract: With the rise and development of computer vision and LLMs, intelligence is
everywhere, especially for people and cars. However, for tremendous food
attributes (such as origin, quantity, weight, quality, sweetness, etc.),
existing research still mainly focuses on the study of categories. The reason
is the lack of a large and comprehensive benchmark for food. Besides, many food
attributes (such as sweetness, weight, and fine-grained categories) are
challenging to accurately percept solely through RGB cameras. To fulfill this
gap and promote the development of intelligent food analysis, in this paper, we
built the first large-scale spectral food (SFOOD) benchmark suite. We spent a
lot of manpower and equipment costs to organize existing food datasets and
collect hyperspectral images of hundreds of foods, and we used instruments to
experimentally determine food attributes such as sweetness and weight. The
resulting benchmark consists of 3,266 food categories and 2,351 k data points
for 17 main food categories. Extensive evaluations find that: (i) Large-scale
models are still poor at digitizing food. Compared to people and cars, food has
gradually become one of the most difficult objects to study; (ii) Spectrum data
are crucial for analyzing food properties (such as sweetness). Our benchmark
will be open source and continuously iterated for different food analysis
tasks.

</details>


### [152] [DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge](https://arxiv.org/abs/2507.04447)
*Wenyao Zhang,Hongsi Liu,Zekun Qi,Yunnan Wang,XinQiang Yu,Jiazhao Zhang,Runpei Dong,Jiawei He,He Wang,Zhizheng Zhang,Li Yi,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: DreamVLA is a vision-language-action framework that integrates comprehensive world knowledge forecasting for robot manipulation, outperforming existing methods with a 76.7% success rate.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models lack comprehensive world knowledge (dynamic, spatial, semantic) and suffer from redundant information in image-based forecasting.

Method: DreamVLA uses dynamic-region-guided world knowledge prediction with spatial/semantic cues, a block-wise structured attention mechanism, and a diffusion-based transformer for action modeling.

Result: Achieves 76.7% success rate on real robot tasks and 4.44 average length on CALVIN ABC-D benchmarks.

Conclusion: DreamVLA effectively integrates world knowledge for improved robot manipulation, demonstrating superior performance over existing methods.

Abstract: Recent advances in vision-language-action (VLA) models have shown promise in
integrating image generation with action prediction to improve generalization
and reasoning in robot manipulation. However, existing methods are limited to
challenging image-based forecasting, which suffers from redundant information
and lacks comprehensive and critical world knowledge, including dynamic,
spatial and semantic information. To address these limitations, we propose
DreamVLA, a novel VLA framework that integrates comprehensive world knowledge
forecasting to enable inverse dynamics modeling, thereby establishing a
perception-prediction-action loop for manipulation tasks. Specifically,
DreamVLA introduces a dynamic-region-guided world knowledge prediction,
integrated with the spatial and semantic cues, which provide compact yet
comprehensive representations for action planning. This design aligns with how
humans interact with the world by first forming abstract multimodal reasoning
chains before acting. To mitigate interference among the dynamic, spatial and
semantic information during training, we adopt a block-wise structured
attention mechanism that masks their mutual attention, preventing information
leakage and keeping each representation clean and disentangled. Moreover, to
model the conditional distribution over future actions, we employ a
diffusion-based transformer that disentangles action representations from
shared latent features. Extensive experiments on both real-world and simulation
environments demonstrate that DreamVLA achieves 76.7% success rate on real
robot tasks and 4.44 average length on the CALVIN ABC-D benchmarks.

</details>


### [153] [CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step](https://arxiv.org/abs/2507.04451)
*Zheyuan Liu,Munan Ning,Qihui Zhang,Shuo Yang,Zhongrui Wang,Yiwei Yang,Xianzhe Xu,Yibing Song,Weihua Chen,Fan Wang,Li Yuan*

Main category: cs.CV

TL;DR: CoT-Diff integrates MLLM-driven 3D layout planning with diffusion models for better spatial alignment in text-to-image generation, outperforming state-of-the-art methods by 34.7%.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models struggle with spatial composition alignment, especially in complex scenes, due to decoupled layout planning and generation.

Method: CoT-Diff combines MLLM-driven 3D layout planning with diffusion models, using dynamic layout updates and condition-aware attention for precise control.

Result: CoT-Diff improves spatial alignment and compositional fidelity, outperforming state-of-the-art by 34.7% in complex scene accuracy.

Conclusion: The entangled generation paradigm of CoT-Diff validates its effectiveness for precise spatial control in T2I generation.

Abstract: Current text-to-image (T2I) generation models struggle to align spatial
composition with the input text, especially in complex scenes. Even
layout-based approaches yield suboptimal spatial control, as their generation
process is decoupled from layout planning, making it difficult to refine the
layout during synthesis. We present CoT-Diff, a framework that brings
step-by-step CoT-style reasoning into T2I generation by tightly integrating
Multimodal Large Language Model (MLLM)-driven 3D layout planning with the
diffusion process. CoT-Diff enables layout-aware reasoning inline within a
single diffusion round: at each denoising step, the MLLM evaluates intermediate
predictions, dynamically updates the 3D scene layout, and continuously guides
the generation process. The updated layout is converted into semantic
conditions and depth maps, which are fused into the diffusion model via a
condition-aware attention mechanism, enabling precise spatial control and
semantic injection. Experiments on 3D Scene benchmarks show that CoT-Diff
significantly improves spatial alignment and compositional fidelity, and
outperforms the state-of-the-art method by 34.7% in complex scene spatial
accuracy, thereby validating the effectiveness of this entangled generation
paradigm.

</details>


### [154] [BiVM: Accurate Binarized Neural Network for Efficient Video Matting](https://arxiv.org/abs/2507.04456)
*Haotong Qin,Xianglong Liu,Xudong Ma,Lei Ke,Yulun Zhang,Jie Luo,Michele Magno*

Main category: cs.CV

TL;DR: BiVM is a binarized neural network for video matting that improves accuracy and efficiency by optimizing encoder and decoder structures and using information-guided strategies.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for video matting face computational limitations on edge devices, and binarization, while efficient, suffers from accuracy and efficiency issues due to degraded encoders and redundant decoders.

Method: BiVM introduces binarized computation structures with elastic shortcuts and evolvable topologies for the encoder, and sparsifies decoder features by masking homogeneous parts. It also uses a localized binarization-aware mimicking framework.

Result: BiVM outperforms other binarized video matting networks, achieving 14.3x and 21.6x savings in computation and storage, respectively.

Conclusion: BiVM addresses the limitations of binarized video matting networks, offering a resource-efficient and accurate solution suitable for edge devices.

Abstract: Deep neural networks for real-time video matting suffer significant
computational limitations on edge devices, hindering their adoption in
widespread applications such as online conferences and short-form video
production. Binarization emerges as one of the most common compression
approaches with compact 1-bit parameters and efficient bitwise operations.
However, accuracy and efficiency limitations exist in the binarized video
matting network due to its degenerated encoder and redundant decoder. Following
a theoretical analysis based on the information bottleneck principle, the
limitations are mainly caused by the degradation of prediction-relevant
information in the intermediate features and the redundant computation in
prediction-irrelevant areas. We present BiVM, an accurate and
resource-efficient Binarized neural network for Video Matting. First, we
present a series of binarized computation structures with elastic shortcuts and
evolvable topologies, enabling the constructed encoder backbone to extract
high-quality representation from input videos for accurate prediction. Second,
we sparse the intermediate feature of the binarized decoder by masking
homogeneous parts, allowing the decoder to focus on representation with diverse
details while alleviating the computation burden for efficient inference.
Furthermore, we construct a localized binarization-aware mimicking framework
with the information-guided strategy, prompting matting-related representation
in full-precision counterparts to be accurately and fully utilized.
Comprehensive experiments show that the proposed BiVM surpasses alternative
binarized video matting networks, including state-of-the-art (SOTA)
binarization methods, by a substantial margin. Moreover, our BiVM achieves
significant savings of 14.3x and 21.6x in computation and storage costs,
respectively. We also evaluate BiVM on ARM CPU hardware.

</details>


### [155] [Visual Hand Gesture Recognition with Deep Learning: A Comprehensive Review of Methods, Datasets, Challenges and Future Research Directions](https://arxiv.org/abs/2507.04465)
*Konstantinos Foteinos,Jorgen Cani,Manousos Linardakis,Panagiotis Radoglou-Grammatikis,Vasileios Argyriou,Panagiotis Sarigiannidis,Iraklis Varlamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: The paper provides a comprehensive survey on vision-based hand gesture recognition (VHGR), organizing state-of-the-art methods, datasets, and metrics to guide researchers in selecting the right approach for specific tasks.


<details>
  <summary>Details</summary>
Motivation: The lack of a structured survey in VHGR makes it difficult for researchers to navigate the field, prompting this review to fill the gap.

Method: A systematic methodology is used to identify and categorize VHGR approaches, focusing on input modalities, application domains, and tasks like static, dynamic, and continuous gesture recognition.

Result: The survey presents a taxonomy of VHGR methods, analyzes architectural trends, reviews datasets, and evaluates performance metrics.

Conclusion: The paper highlights challenges in VHGR and suggests future research directions, serving as a guideline for researchers.

Abstract: The rapid evolution of deep learning (DL) models and the ever-increasing size
of available datasets have raised the interest of the research community in the
always important field of vision-based hand gesture recognition (VHGR), and
delivered a wide range of applications, such as sign language understanding and
human-computer interaction using cameras. Despite the large volume of research
works in the field, a structured and complete survey on VHGR is still missing,
leaving researchers to navigate through hundreds of papers in order to find the
right combination of data, model, and approach for each task. The current
survey aims to fill this gap by presenting a comprehensive overview of this
aspect of computer vision. With a systematic research methodology that
identifies the state-of-the-art works and a structured presentation of the
various methods, datasets, and evaluation metrics, this review aims to
constitute a useful guideline for researchers, helping them to choose the right
strategy for delving into a certain VHGR task. Starting with the methodology
used for study selection, literature retrieval, and the analytical framing, the
survey identifies and organizes key VHGR approaches using a taxonomy-based
format in various dimensions such as input modality and application domain. The
core of the survey provides an in-depth analysis of state-of-the-art techniques
across three primary VHGR tasks: static gesture recognition, isolated dynamic
gestures and continuous gesture recognition. For each task, the architectural
trends and learning strategies are listed. Additionally, the study reviews
commonly used datasets - emphasizing on annotation schemes - and evaluates
standard performance metrics. It concludes by identifying major challenges in
VHGR, including both general computer vision issues and domain-specific
obstacles, and outlines promising directions for future research.

</details>


### [156] [A Training-Free Style-Personalization via Scale-wise Autoregressive Model](https://arxiv.org/abs/2507.04482)
*Kyoungmin Lee,Jihun Park,Jongmin Gim,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Sunghoon Im*

Main category: cs.CV

TL;DR: A training-free framework for style-personalized image generation using a scale-wise autoregressive model with three-path design for flexible control.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and efficient control over image semantics without additional training.

Method: Uses a three-path design (content, style, generation) guided by text prompts, with step-wise and attention-wise intervention analysis. Introduces Key Stage Attention Sharing and Adaptive Query Sharing.

Result: Achieves competitive style and prompt fidelity compared to fine-tuned baselines, with faster inference and greater flexibility.

Conclusion: The framework effectively controls content and style during inference, offering practical advantages over traditional methods.

Abstract: We present a training-free framework for style-personalized image generation
that controls content and style information during inference using a scale-wise
autoregressive model. Our method employs a three-path design--content, style,
and generation--each guided by a corresponding text prompt, enabling flexible
and efficient control over image semantics without any additional training. A
central contribution of this work is a step-wise and attention-wise
intervention analysis. Through systematic prompt and feature injection, we find
that early-to-middle generation steps play a pivotal role in shaping both
content and style, and that query features predominantly encode
content-specific information. Guided by these insights, we introduce two
targeted mechanisms: Key Stage Attention Sharing, which aligns content and
style during the semantically critical steps, and Adaptive Query Sharing, which
reinforces content semantics in later steps through similarity-aware query
blending. Extensive experiments demonstrate that our method achieves
competitive style fidelity and prompt fidelity compared to fine-tuned
baselines, while offering faster inference and greater deployment flexibility.

</details>


### [157] [U-ViLAR: Uncertainty-Aware Visual Localization for Autonomous Driving via Differentiable Association and Registration](https://arxiv.org/abs/2507.04503)
*Xiaofan Li,Zhihao Xu,Chenming Wu,Zhao Yang,Yumeng Zhang,Jiang-Jiang Liu,Haibao Yu,Fan Duan,Xiaoqing Ye,Yuan Wang,Shirui Li,Xun Sun,Ji Wan,Jun Wang*

Main category: cs.CV

TL;DR: U-ViLAR is an uncertainty-aware visual localization framework for urban environments, improving accuracy by combining perceptual and localization uncertainty techniques with HD maps.


<details>
  <summary>Details</summary>
Motivation: GNSS signals are unreliable in urban areas due to buildings and construction, necessitating robust visual localization methods.

Method: Extracts visual features, maps to BEV space, and uses uncertainty-guided association and registration to balance coarse and fine localization.

Result: Achieves state-of-the-art performance in localization tasks and stable performance in real-world urban scenarios.

Conclusion: U-ViLAR provides robust and accurate visual localization, addressing challenges in GNSS-degraded environments.

Abstract: Accurate localization using visual information is a critical yet challenging
task, especially in urban environments where nearby buildings and construction
sites significantly degrade GNSS (Global Navigation Satellite System) signal
quality. This issue underscores the importance of visual localization
techniques in scenarios where GNSS signals are unreliable. This paper proposes
U-ViLAR, a novel uncertainty-aware visual localization framework designed to
address these challenges while enabling adaptive localization using
high-definition (HD) maps or navigation maps. Specifically, our method first
extracts features from the input visual data and maps them into Bird's-Eye-View
(BEV) space to enhance spatial consistency with the map input. Subsequently, we
introduce: a) Perceptual Uncertainty-guided Association, which mitigates errors
caused by perception uncertainty, and b) Localization Uncertainty-guided
Registration, which reduces errors introduced by localization uncertainty. By
effectively balancing the coarse-grained large-scale localization capability of
association with the fine-grained precise localization capability of
registration, our approach achieves robust and accurate localization.
Experimental results demonstrate that our method achieves state-of-the-art
performance across multiple localization tasks. Furthermore, our model has
undergone rigorous testing on large-scale autonomous driving fleets and has
demonstrated stable performance in various challenging urban scenarios.

</details>


### [158] [MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization](https://arxiv.org/abs/2507.04509)
*Zhendong Xiao,Wu Wei,Shujie Ji,Shan Yang,Changhao Chen*

Main category: cs.CV

TL;DR: MVL-Loc is a novel multi-scene 6-DoF camera relocalization framework that uses vision-language models and multimodal data for robust performance across diverse environments.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning methods lack generalization and robustness in diverse scenes, prompting the need for a more adaptable solution.

Method: MVL-Loc leverages pretrained vision-language models and multimodal data, using natural language to guide multi-scene learning and enhance semantic understanding.

Result: MVL-Loc achieves state-of-the-art performance on 7Scenes and Cambridge Landmarks datasets, improving accuracy in positional and orientational estimates.

Conclusion: MVL-Loc demonstrates robust and generalizable camera relocalization, making it suitable for real-world applications like AR, MR, and robotics.

Abstract: Camera relocalization, a cornerstone capability of modern computer vision,
accurately determines a camera's position and orientation (6-DoF) from images
and is essential for applications in augmented reality (AR), mixed reality
(MR), autonomous driving, delivery drones, and robotic navigation. Unlike
traditional deep learning-based methods that regress camera pose from images in
a single scene, which often lack generalization and robustness in diverse
environments, we propose MVL-Loc, a novel end-to-end multi-scene 6-DoF camera
relocalization framework. MVL-Loc leverages pretrained world knowledge from
vision-language models (VLMs) and incorporates multimodal data to generalize
across both indoor and outdoor settings. Furthermore, natural language is
employed as a directive tool to guide the multi-scene learning process,
facilitating semantic understanding of complex scenes and capturing spatial
relationships among objects. Extensive experiments on the 7Scenes and Cambridge
Landmarks datasets demonstrate MVL-Loc's robustness and state-of-the-art
performance in real-world multi-scene camera relocalization, with improved
accuracy in both positional and orientational estimates.

</details>


### [159] [FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection](https://arxiv.org/abs/2507.04511)
*Xinhua Lu,Runhe Lai,Yanqi Wu,Kanghao Chen,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: A CLIP-based framework using Forced prompt leArning (FA) improves OOD detection by leveraging ID knowledge without external datasets.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based OOD detection methods lack generalization or rely on external datasets. FA aims to enhance OOD detection by focusing on ID knowledge.

Method: FA learns a 'forced prompt' with diverse ID class descriptions, improving ID image discernment via semantic similarity. A forced coefficient ensures nuanced learning.

Result: FA outperforms state-of-the-art methods in OOD detection without external datasets, matching CoOp's trainable parameters.

Conclusion: FA effectively boosts OOD detection by utilizing ID knowledge, demonstrating superior performance and practicality.

Abstract: Pre-trained vision-language models (VLMs) have advanced out-of-distribution
(OOD) detection recently. However, existing CLIP-based methods often focus on
learning OOD-related knowledge to improve OOD detection, showing limited
generalization or reliance on external large-scale auxiliary datasets. In this
study, instead of delving into the intricate OOD-related knowledge, we propose
an innovative CLIP-based framework based on Forced prompt leArning (FA),
designed to make full use of the In-Distribution (ID) knowledge and ultimately
boost the effectiveness of OOD detection. Our key insight is to learn a prompt
(i.e., forced prompt) that contains more diversified and richer descriptions of
the ID classes beyond the textual semantics of class labels. Specifically, it
promotes better discernment for ID images, by forcing more notable semantic
similarity between ID images and the learnable forced prompt. Moreover, we
introduce a forced coefficient, encouraging the forced prompt to learn more
comprehensive and nuanced descriptions of the ID classes. In this way, FA is
capable of achieving notable improvements in OOD detection, even when trained
without any external auxiliary datasets, while maintaining an identical number
of trainable parameters as CoOp. Extensive empirical evaluations confirm our
method consistently outperforms current state-of-the-art methods. Code is
available at https://github.com/0xFAFA/FA.

</details>


### [160] [Grounded Gesture Generation: Language, Motion, and Space](https://arxiv.org/abs/2507.04522)
*Anna Deichler,Jim O'Regan,Teo Guichoux,David Johansson,Jonas Beskow*

Main category: cs.CV

TL;DR: The paper introduces a multimodal dataset and framework for grounded gesture generation, addressing the gap between motion generation and environmental grounding in human motion research.


<details>
  <summary>Details</summary>
Motivation: Existing models for human motion generation often treat motion and environmental grounding separately, limiting progress toward embodied, communicative agents. The paper aims to bridge this gap.

Method: The work combines a synthetic dataset of spatially grounded referential gestures and a VR-based dataset (MM-Conv) of two-party dialogues, providing synchronized motion, speech, and 3D scene data. The framework also connects to a physics-based simulator for synthetic data generation and evaluation.

Result: The contribution includes over 7.7 hours of standardized synchronized data (HumanML3D format) and a framework that integrates gesture modeling with spatial grounding.

Conclusion: The paper establishes a foundation for advancing research in situated gesture generation and grounded multimodal interaction.

Abstract: Human motion generation has advanced rapidly in recent years, yet the
critical problem of creating spatially grounded, context-aware gestures has
been largely overlooked. Existing models typically specialize either in
descriptive motion generation, such as locomotion and object interaction, or in
isolated co-speech gesture synthesis aligned with utterance semantics. However,
both lines of work often treat motion and environmental grounding separately,
limiting advances toward embodied, communicative agents. To address this gap,
our work introduces a multimodal dataset and framework for grounded gesture
generation, combining two key resources: (1) a synthetic dataset of spatially
grounded referential gestures, and (2) MM-Conv, a VR-based dataset capturing
two-party dialogues. Together, they provide over 7.7 hours of synchronized
motion, speech, and 3D scene information, standardized in the HumanML3D format.
Our framework further connects to a physics-based simulator, enabling synthetic
data generation and situated evaluation. By bridging gesture modeling and
spatial grounding, our contribution establishes a foundation for advancing
research in situated gesture generation and grounded multimodal interaction.
  Project page: https://groundedgestures.github.io/

</details>


### [161] [A Data-Driven Novelty Score for Diverse In-Vehicle Data Recording](https://arxiv.org/abs/2507.04529)
*Philipp Reis,Joshua Ransiek,David Petri,Jacob Langner,Eric Sax*

Main category: cs.CV

TL;DR: A real-time data selection method for autonomous driving datasets improves model performance by focusing on novel objects and reducing redundancy.


<details>
  <summary>Details</summary>
Motivation: Real-world data collection is biased toward common scenes, leaving novel cases underrepresented, which hinders model generalization and safety.

Method: Uses a dynamic Mean Shift algorithm to assign novelty scores to image frames, identifying and retaining novel objects while discarding redundant data.

Result: Reducing dataset size with this method improves model performance, while higher redundancy degrades it. Aggressive filtering is beneficial as redundancy increases.

Conclusion: The method efficiently detects novelties in real-time, supports continuous data streams, and improves dataset balance and diversity.

Abstract: High-quality datasets are essential for training robust perception systems in
autonomous driving. However, real-world data collection is often biased toward
common scenes and objects, leaving novel cases underrepresented. This imbalance
hinders model generalization and compromises safety. The core issue is the
curse of rarity. Over time, novel events occur infrequently, and standard
logging methods fail to capture them effectively. As a result, large volumes of
redundant data are stored, while critical novel cases are diluted, leading to
biased datasets. This work presents a real-time data selection method focused
on object-level novelty detection to build more balanced and diverse datasets.
The method assigns a data-driven novelty score to image frames using a novel
dynamic Mean Shift algorithm. It models normal content based on mean and
covariance statistics to identify frames with novel objects, discarding those
with redundant elements. The main findings show that reducing the training
dataset size with this method can improve model performance, whereas higher
redundancy tends to degrade it. Moreover, as data redundancy increases, more
aggressive filtering becomes both possible and beneficial. While random
sampling can offer some gains, it often leads to overfitting and
unpredictability in outcomes. The proposed method supports real-time deployment
with 32 frames per second and is constant over time. By continuously updating
the definition of normal content, it enables efficient detection of novelties
in a continuous data stream.

</details>


### [162] [MambaVideo for Discrete Video Tokenization with Channel-Split Quantization](https://arxiv.org/abs/2507.04559)
*Dawit Mureja Argaw,Xian Liu,Joon Son Chung,Ming-Yu Liu,Fitsum Reda*

Main category: cs.CV

TL;DR: A novel Mamba-based encoder-decoder architecture and channel-split quantization improve video tokenization for autoregressive modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficient video tokenization is needed due to the high dimensionality of video data for autoregressive generative modeling.

Method: Proposes a Mamba-based encoder-decoder architecture and channel-split quantization to enhance representational power without increasing token count.

Result: Sets a new state-of-the-art, outperforming causal 3D convolution and Transformer-based approaches on multiple datasets.

Conclusion: The model is robust for autoregressive video generation, offering superior performance and efficiency.

Abstract: Discrete video tokenization is essential for efficient autoregressive
generative modeling due to the high dimensionality of video data. This work
introduces a state-of-the-art discrete video tokenizer with two key
contributions. First, we propose a novel Mamba-based encoder-decoder
architecture that overcomes the limitations of previous sequencebased
tokenizers. Second, we introduce a new quantization scheme, channel-split
quantization, which significantly enhances the representational power of
quantized latents while preserving the token count. Our model sets a new
state-of-the-art, outperforming both causal 3D convolutionbased and
Transformer-based approaches across multiple datasets. Experimental results
further demonstrate its robustness as a tokenizer for autoregressive video
generation.

</details>


### [163] [S$^2$Edit: Text-Guided Image Editing with Precise Semantic and Spatial Control](https://arxiv.org/abs/2507.04584)
*Xudong Liu,Zikun Chen,Ruowei Jiang,Ziyi Wu,Kejia Yin,Han Zhao,Parham Aarabi,Igor Gilitschenski*

Main category: cs.CV

TL;DR: S$^2$Edit is a diffusion-based method for precise, localized image editing while preserving identity, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods for fine-grained image editing (e.g., face editing) often lose identity details or alter irrelevant regions due to entangled concepts.

Method: Fine-tunes a pre-trained diffusion model to embed identity into a learnable text token, disentangles it from editable attributes via orthogonality constraints, and uses object masks to guide cross-attention for localized editing.

Result: S$^2$Edit achieves superior performance in preserving identity and enabling precise editing, demonstrated through extensive experiments and applications like makeup transfer.

Conclusion: S$^2$Edit provides a robust solution for personalized, semantically and spatially controlled image editing.

Abstract: Recent advances in diffusion models have enabled high-quality generation and
manipulation of images guided by texts, as well as concept learning from
images. However, naive applications of existing methods to editing tasks that
require fine-grained control, e.g., face editing, often lead to suboptimal
solutions with identity information and high-frequency details lost during the
editing process, or irrelevant image regions altered due to entangled concepts.
In this work, we propose S$^2$Edit, a novel method based on a pre-trained
text-to-image diffusion model that enables personalized editing with precise
semantic and spatial control. We first fine-tune our model to embed the
identity information into a learnable text token. During fine-tuning, we
disentangle the learned identity token from attributes to be edited by
enforcing an orthogonality constraint in the textual feature space. To ensure
that the identity token only affects regions of interest, we apply object masks
to guide the cross-attention maps. At inference time, our method performs
localized editing while faithfully preserving the original identity with
semantically disentangled and spatially focused identity token learned.
Extensive experiments demonstrate the superiority of S$^2$Edit over
state-of-the-art methods both quantitatively and qualitatively. Additionally,
we showcase several compositional image editing applications of S$^2$Edit such
as makeup transfer.

</details>


### [164] [CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection](https://arxiv.org/abs/2507.04587)
*Hanzhi Zhong,Zhiyu Xiang,Ruoyu Xu,Jingyun Fu,Peng Xu,Shaohong Wang,Zhihao Yang,Tianyu Pu,Eryun Liu*

Main category: cs.CV

TL;DR: A cross-view two-stage fusion network (CVFusion) improves 3D object detection by integrating radar and camera data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: 4D radar's potential in autonomous driving is underexplored due to sparse data and noisy measurements, limiting performance.

Method: CVFusion uses a radar-guided iterative BEV fusion module for high-recall proposals, then aggregates multi-view features for refinement.

Result: Achieves 9.10% and 3.68% mAP improvements on VoD and TJ4DRadSet datasets.

Conclusion: CVFusion effectively leverages radar and camera fusion, significantly advancing 3D object detection performance.

Abstract: 4D radar has received significant attention in autonomous driving thanks to
its robustness under adverse weathers. Due to the sparse points and noisy
measurements of the 4D radar, most of the research finish the 3D object
detection task by integrating images from camera and perform modality fusion in
BEV space. However, the potential of the radar and the fusion mechanism is
still largely unexplored, hindering the performance improvement. In this study,
we propose a cross-view two-stage fusion network called CVFusion. In the first
stage, we design a radar guided iterative (RGIter) BEV fusion module to
generate high-recall 3D proposal boxes. In the second stage, we aggregate
features from multiple heterogeneous views including points, image, and BEV for
each proposal. These comprehensive instance level features greatly help refine
the proposals and generate high-quality predictions. Extensive experiments on
public datasets show that our method outperforms the previous state-of-the-art
methods by a large margin, with 9.10% and 3.68% mAP improvements on
View-of-Delft (VoD) and TJ4DRadSet, respectively. Our code will be made
publicly available.

</details>


### [165] [VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents](https://arxiv.org/abs/2507.04590)
*Rui Meng,Ziyan Jiang,Ye Liu,Mingyi Su,Xinyi Yang,Yuepeng Fu,Can Qin,Zeyuan Chen,Ran Xu,Caiming Xiong,Yingbo Zhou,Wenhu Chen,Semih Yavuz*

Main category: cs.CV

TL;DR: VLM2Vec-V2 is a unified framework for multimodal embeddings, extending support to videos and visual documents, outperforming prior models on new and existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal embeddings focus mainly on natural images, limiting their real-world applicability. VLM2Vec-V2 aims to bridge this gap by supporting diverse visual forms.

Method: Introduces MMEB-V2, a benchmark with new task types, and trains VLM2Vec-V2, a general-purpose embedding model for text, image, video, and visual documents.

Result: VLM2Vec-V2 excels in video/document retrieval tasks and improves performance on original image benchmarks.

Conclusion: The study provides insights into multimodal embedding generalizability and strategies for unified learning, advancing scalable representation learning.

Abstract: Multimodal embedding models have been crucial in enabling various downstream
tasks such as semantic similarity, information retrieval, and clustering over
different modalities. However, existing multimodal embeddings like VLM2Vec,
E5-V, GME are predominantly focused on natural images, with limited support for
other visual forms such as videos and visual documents. This restricts their
applicability in real-world scenarios, including AI agents, multi-modal search
and recommendation, and retrieval-augmented generation (RAG). To close this
gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across
diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark
that extends MMEB with five new task types: visual document retrieval, video
retrieval, temporal grounding, video classification and video question
answering - spanning text, image, video, and visual document inputs. Next, we
train VLM2Vec-V2, a general-purpose embedding model that supports text, image,
video, and visual document inputs. Extensive experiments show that VLM2Vec-V2
achieves strong performance not only on the newly introduced video and document
retrieval tasks, but also improves over prior baselines on the original image
benchmarks. Through extensive evaluation, our study offers insights into the
generalizability of various multimodal embedding models and highlights
effective strategies for unified embedding learning, laying the groundwork for
more scalable and adaptable representation learning in both research and
real-world settings.

</details>


### [166] [QR-LoRA: Efficient and Disentangled Fine-tuning via QR Decomposition for Customized Generation](https://arxiv.org/abs/2507.04599)
*Jiahui Yang,Yongjia Ma,Donglin Di,Hao Li,Wei Chen,Yan Xie,Jianxun Cui,Xun Yang,Wangmeng Zuo*

Main category: cs.CV

TL;DR: QR-LoRA introduces a structured fine-tuning framework using QR decomposition to separate content and style attributes in text-to-image models, reducing trainable parameters and improving disentanglement.


<details>
  <summary>Details</summary>
Motivation: Existing LoRA methods for text-to-image models suffer from feature entanglement when combining multiple adaptations, leading to undesired mixing of content and style attributes.

Method: QR-LoRA leverages QR decomposition, fixing the Q and R matrices and training only an additional task-specific R matrix. This structured approach minimizes interference and reduces trainable parameters.

Result: QR-LoRA achieves superior disentanglement in content-style fusion tasks, with half the trainable parameters of conventional LoRA methods.

Conclusion: QR-LoRA establishes a new paradigm for parameter-efficient, disentangled fine-tuning in generative models, addressing the limitations of existing methods.

Abstract: Existing text-to-image models often rely on parameter fine-tuning techniques
such as Low-Rank Adaptation (LoRA) to customize visual attributes. However,
when combining multiple LoRA models for content-style fusion tasks,
unstructured modifications of weight matrices often lead to undesired feature
entanglement between content and style attributes. We propose QR-LoRA, a novel
fine-tuning framework leveraging QR decomposition for structured parameter
updates that effectively separate visual attributes. Our key insight is that
the orthogonal Q matrix naturally minimizes interference between different
visual features, while the upper triangular R matrix efficiently encodes
attribute-specific transformations. Our approach fixes both Q and R matrices
while only training an additional task-specific $\Delta R$ matrix. This
structured design reduces trainable parameters to half of conventional LoRA
methods and supports effective merging of multiple adaptations without
cross-contamination due to the strong disentanglement properties between
$\Delta R$ matrices. Experiments demonstrate that QR-LoRA achieves superior
disentanglement in content-style fusion tasks, establishing a new paradigm for
parameter-efficient, disentangled fine-tuning in generative models.

</details>


### [167] [HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction](https://arxiv.org/abs/2507.04613)
*Jiaqi Cui,Lu Wen,Yuchen Fei,Bo Liu,Luping Zhou,Dinggang Shen,Yan Wang*

Main category: cs.CV

TL;DR: HiLa framework improves survival prediction in cancer research by leveraging hierarchical visual features and advanced language prompts for better vision-language alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on sparse slide-level labels and simple language prompts, limiting discriminative representation learning from gigapixel WSIs.

Method: HiLa uses pretrained feature extractors for hierarchical visual features, aligns them with survival-related language prompts via Optimal Prompt Learning (OPL), and enhances interactions with Cross-Level Propagation (CLP) and Mutual Contrastive Learning (MCL).

Result: Experiments on three TCGA datasets show state-of-the-art performance.

Conclusion: HiLa effectively addresses limitations of current methods by improving vision-language alignment and hierarchical feature modeling.

Abstract: Survival prediction using whole-slide images (WSIs) is crucial in cancer
re-search. Despite notable success, existing approaches are limited by their
reliance on sparse slide-level labels, which hinders the learning of
discriminative repre-sentations from gigapixel WSIs. Recently, vision language
(VL) models, which incorporate additional language supervision, have emerged as
a promising solu-tion. However, VL-based survival prediction remains largely
unexplored due to two key challenges. First, current methods often rely on only
one simple lan-guage prompt and basic cosine similarity, which fails to learn
fine-grained associ-ations between multi-faceted linguistic information and
visual features within WSI, resulting in inadequate vision-language alignment.
Second, these methods primarily exploit patch-level information, overlooking
the intrinsic hierarchy of WSIs and their interactions, causing ineffective
modeling of hierarchical interac-tions. To tackle these problems, we propose a
novel Hierarchical vision-Language collaboration (HiLa) framework for improved
survival prediction. Specifically, HiLa employs pretrained feature extractors
to generate hierarchical visual features from WSIs at both patch and region
levels. At each level, a series of language prompts describing various
survival-related attributes are constructed and aligned with visual features
via Optimal Prompt Learning (OPL). This ap-proach enables the comprehensive
learning of discriminative visual features cor-responding to different
survival-related attributes from prompts, thereby improv-ing vision-language
alignment. Furthermore, we introduce two modules, i.e., Cross-Level Propagation
(CLP) and Mutual Contrastive Learning (MCL) to maximize hierarchical
cooperation by promoting interactions and consistency be-tween patch and region
levels. Experiments on three TCGA datasets demonstrate our SOTA performance.

</details>


### [168] [Learn 3D VQA Better with Active Selection and Reannotation](https://arxiv.org/abs/2507.04630)
*Shengli Zhou,Yang Liu,Feng Zheng*

Main category: cs.CV

TL;DR: The paper proposes a multi-turn interactive active learning strategy for 3D VQA to address misleading annotations, improving model performance and reducing training costs.


<details>
  <summary>Details</summary>
Motivation: Misleading annotations in 3D VQA datasets negatively impact model training due to data scarcity. Active learning alone fails to resolve these issues.

Method: A multi-turn interactive active learning strategy selects data based on semantic uncertainty and requests reannotation for misleading labels. A variance-based metric assesses uncertainty.

Result: Experiments show improved model performance and a 50% reduction in training costs for high accuracy.

Conclusion: The proposed strategy effectively mitigates misleading annotations and enhances training efficiency in 3D VQA.

Abstract: 3D Visual Question Answering (3D VQA) is crucial for enabling models to
perceive the physical world and perform spatial reasoning. In 3D VQA, the
free-form nature of answers often leads to improper annotations that can
confuse or mislead models when training on the entire dataset. While other text
generation tasks can mitigate this issue by learning on large-scale datasets,
the scarcity of 3D scene data enlarges the negative effect of misleading
annotations. Although active learning strategies can select valuable instances
for training, they fail to identify and resolve misleading labels, which the
oracle inevitably provides in practice. To address this issue, we propose a
multi-turn interactive active learning strategy. This strategy selects data
based on models' semantic uncertainty to form a solid knowledge foundation more
effectively and actively requests reannotation from an oracle to resolve
potentially misleading labels. For uncertainty assessment, we utilize a
variance-based metric that takes semantic relationships between terms into
consideration, thus avoiding the uniform inter-class similarity assumption of
previous assessment metrics. Extensive experiments exhibit better model
performance and a substantial reduction in training costs, with a halving of
training costs for achieving relatively high accuracy. The code is available at
https://github.com/fz-zsl/AQuA.

</details>


### [169] [Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts](https://arxiv.org/abs/2507.04631)
*Yun Wang,Longguang Wang,Chenghao Zhang,Yongjian Zhang,Zhanjie Zhang,Ao Ma,Chenyou Fan,Tin Lun Lam,Junjie Hu*

Main category: cs.CV

TL;DR: SMoEStereo adapts Vision Foundation Models (VFMs) for stereo matching using LoRA and MoE modules, improving cross-domain robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing stereo matching networks lack robustness and struggle with cross-domain performance due to domain shifts and imbalanced disparity distributions.

Method: Proposes SMoEStereo, combining LoRA and MoE modules (MoE-LoRA and MoE-Adapter) for scene-specific adaptation, with a lightweight decision network for efficiency.

Result: Achieves state-of-the-art cross-domain and joint generalization across benchmarks without dataset-specific adaptation.

Conclusion: SMoEStereo effectively enhances stereo matching robustness and efficiency by leveraging VFMs and adaptive modules.

Abstract: Recently, learning-based stereo matching networks have advanced
significantly. However, they often lack robustness and struggle to achieve
impressive cross-domain performance due to domain shifts and imbalanced
disparity distributions among diverse datasets. Leveraging Vision Foundation
Models (VFMs) can intuitively enhance the model's robustness, but integrating
such a model into stereo matching cost-effectively to fully realize their
robustness remains a key challenge. To address this, we propose SMoEStereo, a
novel framework that adapts VFMs for stereo matching through a tailored,
scene-specific fusion of Low-Rank Adaptation (LoRA) and Mixture-of-Experts
(MoE) modules. SMoEStereo introduces MoE-LoRA with adaptive ranks and
MoE-Adapter with adaptive kernel sizes. The former dynamically selects optimal
experts within MoE to adapt varying scenes across domains, while the latter
injects inductive bias into frozen VFMs to improve geometric feature
extraction. Importantly, to mitigate computational overhead, we further propose
a lightweight decision network that selectively activates MoE modules based on
input complexity, balancing efficiency with accuracy. Extensive experiments
demonstrate that our method exhibits state-of-the-art cross-domain and joint
generalization across multiple benchmarks without dataset-specific adaptation.
The code is available at
\textcolor{red}{https://github.com/cocowy1/SMoE-Stereo}.

</details>


### [170] [LTMSformer: A Local Trend-Aware Attention and Motion State Encoding Transformer for Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2507.04634)
*Yixin Yan,Yang Li,Yuanfan Wang,Xiaozhou Zhou,Beihao Xia,Manjiang Hu,Hongmao Qin*

Main category: cs.CV

TL;DR: The paper proposes LTMSformer, a lightweight framework for multi-modal trajectory prediction, addressing overlooked local temporal dependencies and high-order motion state attributes.


<details>
  <summary>Details</summary>
Motivation: Existing methods often miss local temporal dependencies and high-order motion state attributes, which are crucial for accurate trajectory prediction.

Method: LTMSformer uses a Local Trend-Aware Attention mechanism for temporal dependency and a Motion State Encoder for spatial interaction. A Lightweight Proposal Refinement Module refines predictions.

Result: On Argoverse 1, LTMSformer outperforms HiVT-64 (minADE 4.35%, minFDE 8.74%, MR 20%) and matches HiVT-128 with 68% fewer parameters.

Conclusion: LTMSformer effectively captures temporal-spatial dependencies and achieves superior performance with reduced model size.

Abstract: It has been challenging to model the complex temporal-spatial dependencies
between agents for trajectory prediction. As each state of an agent is closely
related to the states of adjacent time steps, capturing the local temporal
dependency is beneficial for prediction, while most studies often overlook it.
Besides, learning the high-order motion state attributes is expected to enhance
spatial interaction modeling, but it is rarely seen in previous works. To
address this, we propose a lightweight framework, LTMSformer, to extract
temporal-spatial interaction features for multi-modal trajectory prediction.
Specifically, we introduce a Local Trend-Aware Attention mechanism to capture
the local temporal dependency by leveraging a convolutional attention mechanism
with hierarchical local time boxes. Next, to model the spatial interaction
dependency, we build a Motion State Encoder to incorporate high-order motion
state attributes, such as acceleration, jerk, heading, etc. To further refine
the trajectory prediction, we propose a Lightweight Proposal Refinement Module
that leverages Multi-Layer Perceptrons for trajectory embedding and generates
the refined trajectories with fewer model parameters. Experiment results on the
Argoverse 1 dataset demonstrate that our method outperforms the baseline
HiVT-64, reducing the minADE by approximately 4.35%, the minFDE by 8.74%, and
the MR by 20%. We also achieve higher accuracy than HiVT-128 with a 68%
reduction in model size.

</details>


### [171] [MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding](https://arxiv.org/abs/2507.04635)
*Zhicheng Zhang,Wuyou Xia,Chenxi Zhao,Zhou Yan,Xiaoqiang Liu,Yongjie Zhu,Wenyu Qin,Pengfei Wan,Di Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: The paper introduces MODA, a novel attention mechanism addressing attention deficit in multimodal learning by refining inner-modal and inter-modal interactions.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on language-centric tuning, neglecting multimodal token mixing, which limits performance in tasks requiring fine-grained cognition and emotion understanding.

Method: MODA uses a correct-after-align strategy, mapping tokens to duplex modality spaces and ensuring attention correctness with adaptive masked attention.

Result: Experiments on 21 datasets show MODA's effectiveness in perception, cognition, and emotion tasks.

Conclusion: MODA improves multimodal learning by addressing attention deficits, offering better performance in high-level tasks.

Abstract: Multimodal large language models (MLLMs) recently showed strong capacity in
integrating data among multiple modalities, empowered by a generalizable
attention architecture. Advanced methods predominantly focus on
language-centric tuning while less exploring multimodal tokens mixed through
attention, posing challenges in high-level tasks that require fine-grained
cognition and emotion understanding. In this work, we identify the attention
deficit disorder problem in multimodal learning, caused by inconsistent
cross-modal attention and layer-by-layer decayed attention activation. To
address this, we propose a novel attention mechanism, termed MOdular Duplex
Attention (MODA), simultaneously conducting the inner-modal refinement and
inter-modal interaction. MODA employs a correct-after-align strategy to
effectively decouple modality alignment from cross-layer token mixing. In the
alignment phase, tokens are mapped to duplex modality spaces based on the basis
vectors, enabling the interaction between visual and language modality.
Further, the correctness of attention scores is ensured through adaptive masked
attention, which enhances the model's flexibility by allowing customizable
masking patterns for different modalities. Extensive experiments on 21
benchmark datasets verify the effectiveness of MODA in perception, cognition,
and emotion tasks. Source code and demo are available in
https://zzcheng.top/MODA.

</details>


### [172] [UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object Re-Identification](https://arxiv.org/abs/2507.04638)
*Xixi Wan,Aihua Zheng,Bo Jiang,Beibei Wang,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: UGG-ReID is a robust multi-modal object ReID method that addresses uncertainty from noise and inter-modal conflicts using uncertainty-guided graphs and expert routing.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook uncertainty from noise and conflicts, especially in fine-grained occlusion and frame loss, hindering multi-modal learning.

Method: Proposes Gaussian patch-graph representation and uncertainty-guided expert routing to enhance robustness and multi-modal fusion.

Result: Achieves superior performance on five datasets, excelling in noise immunity compared to current methods.

Conclusion: UGG-ReID effectively mitigates uncertainty and noise, improving multi-modal object ReID performance.

Abstract: Multi-modal object Re-IDentification (ReID) has gained considerable attention
with the goal of retrieving specific targets across cameras using heterogeneous
visual data sources. Existing methods primarily aim to improve identification
performance, but often overlook the uncertainty arising from inherent defects,
such as intra-modal noise and inter-modal conflicts. This uncertainty is
particularly significant in the case of fine-grained local occlusion and frame
loss, which becomes a challenge in multi-modal learning. To address the above
challenge, we propose a robust approach named Uncertainty-Guided Graph model
for multi-modal object ReID (UGG-ReID). UGG-ReID is designed to mitigate noise
interference and facilitate effective multi-modal fusion by estimating both
local and sample-level aleatoric uncertainty and explicitly modeling their
dependencies. Specifically, we first propose the Gaussian patch-graph
representation model that leverages uncertainty to quantify fine-grained local
cues and capture their structural relationships. This process boosts the
expressiveness of modal-specific information, ensuring that the generated
embeddings are both more informative and robust. Subsequently, we design an
uncertainty-guided mixture of experts strategy that dynamically routes samples
to experts exhibiting low uncertainty. This strategy effectively suppresses
noise-induced instability, leading to enhanced robustness. Meanwhile, we design
an uncertainty-guided routing to strengthen the multi-modal interaction,
improving the performance. UGG-ReID is comprehensively evaluated on five
representative multi-modal object ReID datasets, encompassing diverse spectral
modalities. Experimental results show that the proposed method achieves
excellent performance on all datasets and is significantly better than current
methods in terms of noise immunity. Our code will be made public upon
acceptance.

</details>


### [173] [VectorLLM: Human-like Extraction of Structured Building Contours vis Multimodal LLMs](https://arxiv.org/abs/2507.04664)
*Tao Zhang,Shiqing Wei,Shihao Chen,Wenling Yu,Muying Luo,Shunping Ji*

Main category: cs.CV

TL;DR: VectorLLM, a Multi-modal Large Language Model (MLLM), directly extracts building contours from remote sensing images, outperforming existing methods and showing strong generalization.


<details>
  <summary>Details</summary>
Motivation: Current methods for building contour extraction are complex and lack scalability. VectorLLM leverages LLMs' reasoning to simplify and improve the process.

Method: VectorLLM uses a vision backbone, MLP connector, and LLM with learnable position embeddings for corner-point regression, mimicking human annotation.

Result: VectorLLM outperforms SOTA methods by 5.6-13.6 AP across datasets and shows strong zero-shot performance on unseen objects.

Conclusion: VectorLLM introduces a new paradigm for vector extraction, combining high accuracy and generalization, with open-source release for community development.

Abstract: Automatically extracting vectorized building contours from remote sensing
imagery is crucial for urban planning, population estimation, and disaster
assessment. Current state-of-the-art methods rely on complex multi-stage
pipelines involving pixel segmentation, vectorization, and polygon refinement,
which limits their scalability and real-world applicability. Inspired by the
remarkable reasoning capabilities of Large Language Models (LLMs), we introduce
VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for
regular building contour extraction from remote sensing images. Unlike existing
approaches, VectorLLM performs corner-point by corner-point regression of
building contours directly, mimicking human annotators' labeling process. Our
architecture consists of a vision foundation backbone, an MLP connector, and an
LLM, enhanced with learnable position embeddings to improve spatial
understanding capability. Through comprehensive exploration of training
strategies including pretraining, supervised fine-tuning, and preference
optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM significantly
outperformed the previous SOTA methods by 5.6 AP, 7.1 AP, 13.6 AP, respectively
in the three datasets. Remarkably, VectorLLM exhibits strong zero-shot
performance on unseen objects including aircraft, water bodies, and oil tanks,
highlighting its potential for unified modeling of diverse remote sensing
object contour extraction tasks. Overall, this work establishes a new paradigm
for vector extraction in remote sensing, leveraging the topological reasoning
capabilities of LLMs to achieve both high accuracy and exceptional
generalization. All the codes and weights will be published for promoting
community development.

</details>


### [174] [What's Making That Sound Right Now? Video-centric Audio-Visual Localization](https://arxiv.org/abs/2507.04667)
*Hahyeon Choi,Junhoo Lee,Nojun Kwak*

Main category: cs.CV

TL;DR: AVATAR introduces a video-centric benchmark for Audio-Visual Localization (AVL) with temporal dynamics, and TAVLO, a model leveraging high-resolution temporal modeling, outperforms conventional methods.


<details>
  <summary>Details</summary>
Motivation: Existing AVL studies lack temporal dynamics and assume overly simplified scenarios (visible, single sound sources).

Method: AVATAR benchmark includes four scenarios (Single-sound, Mixed-sound, Multi-entity, Off-screen). TAVLO integrates temporal information explicitly.

Result: Conventional methods fail with temporal variations; TAVLO achieves robust alignment.

Conclusion: Temporal dynamics are crucial for AVL; AVATAR and TAVLO set a new standard for video-centric AVL.

Abstract: Audio-Visual Localization (AVL) aims to identify sound-emitting sources
within a visual scene. However, existing studies focus on image-level
audio-visual associations, failing to capture temporal dynamics. Moreover, they
assume simplified scenarios where sound sources are always visible and involve
only a single object. To address these limitations, we propose AVATAR, a
video-centric AVL benchmark that incorporates high-resolution temporal
information. AVATAR introduces four distinct scenarios -- Single-sound,
Mixed-sound, Multi-entity, and Off-screen -- enabling a more comprehensive
evaluation of AVL models. Additionally, we present TAVLO, a novel video-centric
AVL model that explicitly integrates temporal information. Experimental results
show that conventional methods struggle to track temporal variations due to
their reliance on global audio features and frame-level mappings. In contrast,
TAVLO achieves robust and precise audio-visual alignment by leveraging
high-resolution temporal modeling. Our work empirically demonstrates the
importance of temporal dynamics in AVL and establishes a new standard for
video-centric audio-visual localization.

</details>


### [175] [ChangeBridge: Spatiotemporal Image Generation with Multimodal Controls for Remote Sensing](https://arxiv.org/abs/2507.04678)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Zhuo Zheng*

Main category: cs.CV

TL;DR: ChangeBridge is a spatiotemporal diffusion model for remote sensing image synthesis, enabling future scenario simulation using multimodal controls.


<details>
  <summary>Details</summary>
Motivation: Existing generative methods lack the ability to simulate future scenarios from given images, which is crucial for urban planning and land management.

Method: ChangeBridge uses a conditional spatiotemporal diffusion model with multimodal controls (text prompts, layouts, semantic maps) to synthesize post-event images from pre-event ones, modeling the evolution as a noise-to-image diffusion bridge.

Result: ChangeBridge successfully generates high-fidelity future scenarios aligned with given conditions, including event-driven variations.

Conclusion: ChangeBridge is the first spatiotemporal generative model for remote sensing with multimodal controls, demonstrating effective future scenario simulation.

Abstract: Recent advancements in generative methods, especially diffusion models, have
made great progress in remote sensing image synthesis. Despite these
advancements, existing methods have not explored the simulation of future
scenarios based on given scenario images. This simulation capability has wide
applications for urban planning, land managementChangeBridge: Spatiotemporal
Image Generation with Multimodal Controls, and beyond. In this work, we propose
ChangeBridge, a conditional spatiotemporal diffusion model. Given pre-event
images and conditioned on multimodal spatial controls (e.g., text prompts,
instance layouts, and semantic maps), ChangeBridge can synthesize post-event
images. The core idea behind ChangeBridge is to modeling the noise-to-image
diffusion model, as a pre-to-post diffusion bridge. Conditioned on multimodal
controls, ChangeBridge leverages a stochastic Brownian-bridge diffusion,
directly modeling the spatiotemporal evolution between pre-event and post-event
states. To the best of our knowledge, ChangeBridge is the first spatiotemporal
generative model with multimodal controls for remote sensing. Experimental
results demonstrate that ChangeBridge can simulate high-fidelity future
scenarios aligned with given conditions, including event and event-driven
background variations. Code will be available.

</details>


### [176] [Colorectal Cancer Tumor Grade Segmentation in Digital Histopathology Images: From Giga to Mini Challenge](https://arxiv.org/abs/2507.04681)
*Alper Bahcekapili,Duygu Arslan,Umut Ozdemir,Berkay Ozkirli,Emre Akbas,Ahmet Acar,Gozde B. Akar,Bingdou He,Shuoyu Xu,Umit Mert Caglar,Alptekin Temizel,Guillaume Picaud,Marc Chaumont,Grard Subsol,Luc Tot,Fahad Alsharekh,Shahad Alghannam,Hexiang Mao,Wenhua Zhang*

Main category: cs.CV

TL;DR: The paper discusses a challenge aimed at automating colorectal cancer tumor grading and segmentation using a public dataset, with top-performing methods surpassing a baseline model.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer grading is subjective and suffers from variability and a shortage of pathologists, necessitating automated solutions.

Method: The ICIP Grand Challenge used the METU CCTGS dataset (103 whole-slide images) and evaluated submissions via metrics like macro F-score and mIoU.

Result: Six of 39 teams outperformed the Swin Transformer baseline (62.92 F-score).

Conclusion: The challenge highlights progress in automated CRC grading and segmentation, showcasing effective methods.

Abstract: Colorectal cancer (CRC) is the third most diagnosed cancer and the second
leading cause of cancer-related death worldwide. Accurate histopathological
grading of CRC is essential for prognosis and treatment planning but remains a
subjective process prone to observer variability and limited by global
shortages of trained pathologists. To promote automated and standardized
solutions, we organized the ICIP Grand Challenge on Colorectal Cancer Tumor
Grading and Segmentation using the publicly available METU CCTGS dataset. The
dataset comprises 103 whole-slide images with expert pixel-level annotations
for five tissue classes. Participants submitted segmentation masks via Codalab,
evaluated using metrics such as macro F-score and mIoU. Among 39 participating
teams, six outperformed the Swin Transformer baseline (62.92 F-score). This
paper presents an overview of the challenge, dataset, and the top-performing
methods

</details>


### [177] [TeethGenerator: A two-stage framework for paired pre- and post-orthodontic 3D dental data generation](https://arxiv.org/abs/2507.04685)
*Changsong Lei,Yaqian Liang,Shaofeng Wang,Jiajia Dai,Yong-Jin Liu*

Main category: cs.CV

TL;DR: TeethGenerator is a two-stage framework for generating paired 3D orthodontic teeth models to aid in training tooth arrangement networks, using diffusion models and style conditioning.


<details>
  <summary>Details</summary>
Motivation: The labor-intensive data collection for paired 3D teeth models hinders neural network development in digital orthodontics, necessitating a synthetic solution.

Method: A two-stage framework: (1) teeth shape generation via diffusion models for post-orthodontic models, and (2) teeth style generation for pre-orthodontic models using style conditioning.

Result: Synthetic data closely matches real orthodontic data distribution and enhances tooth alignment performance when combined with real data.

Conclusion: TeethGenerator effectively addresses the data bottleneck in orthodontic research, improving downstream network training.

Abstract: Digital orthodontics represents a prominent and critical application of
computer vision technology in the medical field. So far, the labor-intensive
process of collecting clinical data, particularly in acquiring paired 3D
orthodontic teeth models, constitutes a crucial bottleneck for developing tooth
arrangement neural networks. Although numerous general 3D shape generation
methods have been proposed, most of them focus on single-object generation and
are insufficient for generating anatomically structured teeth models, each
comprising 24-32 segmented teeth. In this paper, we propose TeethGenerator, a
novel two-stage framework designed to synthesize paired 3D teeth models pre-
and post-orthodontic, aiming to facilitate the training of downstream tooth
arrangement networks. Specifically, our approach consists of two key modules:
(1) a teeth shape generation module that leverages a diffusion model to learn
the distribution of morphological characteristics of teeth, enabling the
generation of diverse post-orthodontic teeth models; and (2) a teeth style
generation module that synthesizes corresponding pre-orthodontic teeth models
by incorporating desired styles as conditional inputs. Extensive qualitative
and quantitative experiments demonstrate that our synthetic dataset aligns
closely with the distribution of real orthodontic data, and promotes tooth
alignment performance significantly when combined with real data for training.
The code and dataset are available at
https://github.com/lcshhh/teeth_generator.

</details>


### [178] [Structure-Guided Diffusion Models for High-Fidelity Portrait Shadow Removal](https://arxiv.org/abs/2507.04692)
*Wanchang Yu,Qing Zhang,Rongjia Zheng,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: A diffusion-based method for portrait shadow removal using structure-guided inpainting and detail restoration, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: To address common issues in shadow removal like facial identity tampering, shadow residuals, and loss of details by leveraging diffusion models.

Method: Trains a shadow-independent structure extraction network, uses it to guide a diffusion inpainting model, and refines results with a detail restoration diffusion model.

Result: Outperforms existing methods, avoiding issues like color distortion and structure blurring while preserving fine details.

Conclusion: The approach effectively removes shadows in portraits while maintaining high fidelity and detail, with code publicly available.

Abstract: We present a diffusion-based portrait shadow removal approach that can
robustly produce high-fidelity results. Unlike previous methods, we cast shadow
removal as diffusion-based inpainting. To this end, we first train a
shadow-independent structure extraction network on a real-world portrait
dataset with various synthetic lighting conditions, which allows to generate a
shadow-independent structure map including facial details while excluding the
unwanted shadow boundaries. The structure map is then used as condition to
train a structure-guided inpainting diffusion model for removing shadows in a
generative manner. Finally, to restore the fine-scale details (e.g., eyelashes,
moles and spots) that may not be captured by the structure map, we take the
gradients inside the shadow regions as guidance and train a detail restoration
diffusion model to refine the shadow removal result. Extensive experiments on
the benchmark datasets show that our method clearly outperforms existing
methods, and is effective to avoid previously common issues such as facial
identity tampering, shadow residual, color distortion, structure blurring, and
loss of details. Our code is available at
https://github.com/wanchang-yu/Structure-Guided-Diffusion-for-Portrait-Shadow-Removal.

</details>


### [179] [A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets](https://arxiv.org/abs/2507.04699)
*Zexi Jia,Chuanwei Huang,Hongyan Fei,Yeshuang Zhu,Zhiqiang Yuan,Ying Deng,Jiapei Zhang,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: A block-based diffusion method generates counterfactual datasets for VLMs, improving compositional reasoning with less training data.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with compositional reasoning due to lack of high-quality image-text data.

Method: Uses LLMs to identify entities and relationships, generates image blocks as puzzle pieces, and introduces a specialized loss function.

Result: Significantly improves VLM performance on visual reasoning tasks, achieving SOTA with less data.

Conclusion: The approach effectively enhances VLM training efficiency and reasoning capabilities.

Abstract: Vision-language models (VLMs) often struggle with compositional reasoning due
to insufficient high-quality image-text data. To tackle this challenge, we
propose a novel block-based diffusion approach that automatically generates
counterfactual datasets without manual annotation. Our method utilizes large
language models to identify entities and their spatial relationships. It then
independently generates image blocks as "puzzle pieces" coherently arranged
according to specified compositional rules. This process creates diverse,
high-fidelity counterfactual image-text pairs with precisely controlled
variations. In addition, we introduce a specialized loss function that
differentiates inter-set from intra-set samples, enhancing training efficiency
and reducing the need for negative samples. Experiments demonstrate that
fine-tuning VLMs with our counterfactual datasets significantly improves visual
reasoning performance. Our approach achieves state-of-the-art results across
multiple benchmarks while using substantially less training data than existing
methods.

</details>


### [180] [Tempo-R0: A Video-MLLM for Temporal Video Grounding through Efficient Temporal Sensing Reinforcement Learning](https://arxiv.org/abs/2507.04702)
*Feng Yue,Zhaoxing Zhang,Junming Jiao,Zhengyu Liang,Shiwen Cao,Feifei Zhang,Rong Shen*

Main category: cs.CV

TL;DR: Tempo-R0, a Video-MLLM, improves Temporal Video Grounding (TVG) via multimodal temporal sensing reinforcement, achieving a 3.5% advantage over SOTA methods.


<details>
  <summary>Details</summary>
Motivation: TVG is challenging due to video information volume and redundancy. Models need comprehensive understanding to retrieve query-relevant clips accurately.

Method: Uses Self-adaptive Attention Allocation (SAA) for efficient attention, Explicit Timestamp-modal Aligned (ETA) for boundary perception, and PIR-GRPO for temporal reasoning.

Result: Achieves a 3.5% improvement over SOTA on QVHighlights benchmarks.

Conclusion: Tempo-R0 effectively addresses TVG challenges with innovative methods, outperforming existing solutions.

Abstract: Temporal Video Grounding (TVG), which requires pinpointing relevant temporal
segments from video based on language query, has always been a highly
challenging task in the field of video understanding. Videos often have a
larger volume of information and redundancy than texts or images. Models should
present comprehensive understanding of the whole video to accurately retrieve
query-relevant clips. We thus propose Tempo-R0: a Video Multimodal Large
Language Model (Video-MLLM) for the temporal video grounding task via
multimodal temporal sensing reinforcement. Specifically, during the
preprocessing stage of our pipeline, we employ Self-adaptive Attention
Allocation (SAA) method based on frame content variation to efficiently use the
MLLM's limited attention. The Explicit Timestamp-modal Aligned (ETA) method is
also utilized to strengthen our model's capability to perceive the boundaries
of events in the video. In the fine-tuning part of our pipeline, we creatively
apply Partial Irrelevance Refusing-based Group Relative Policy Optimization
(PIR-GRPO) in TVG area to foster model's temporal reasoning from not only
accepting relevant video-query pairs but also refusing irrelevant ones.
Experiments demonstrate that our method accomplishes a notable advantage over
SOTA solutions by around 3.5% on both the original QVHighlights testbench and
its corrected version with more reasonable ground truth annotations.

</details>


### [181] [Identity-Preserving Text-to-Video Generation Guided by Simple yet Effective Spatial-Temporal Decoupled Representations](https://arxiv.org/abs/2507.04705)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Han Feng,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: A spatial-temporal decoupled framework for identity-preserving text-to-video generation, balancing spatial coherence and temporal smoothness.


<details>
  <summary>Details</summary>
Motivation: Current end-to-end frameworks struggle with spatial-temporal trade-offs, compromising either identity preservation or motion consistency.

Method: Proposes semantic prompt optimization and stage-wise decoupled generation, separating spatial (layouts) and temporal (motion) features.

Result: Achieves excellent spatiotemporal consistency, excelling in identity preservation, text relevance, and video quality.

Conclusion: The framework's simplicity and effectiveness secure it the runner-up position in the 2025 ACM MultiMedia Challenge.

Abstract: Identity-preserving text-to-video (IPT2V) generation, which aims to create
high-fidelity videos with consistent human identity, has become crucial for
downstream applications. However, current end-to-end frameworks suffer a
critical spatial-temporal trade-off: optimizing for spatially coherent layouts
of key elements (e.g., character identity preservation) often compromises
instruction-compliant temporal smoothness, while prioritizing dynamic realism
risks disrupting the spatial coherence of visual structures. To tackle this
issue, we propose a simple yet effective spatial-temporal decoupled framework
that decomposes representations into spatial features for layouts and temporal
features for motion dynamics. Specifically, our paper proposes a semantic
prompt optimization mechanism and stage-wise decoupled generation paradigm. The
former module decouples the prompt into spatial and temporal components.
Aligned with the subsequent stage-wise decoupled approach, the spatial prompts
guide the text-to-image (T2I) stage to generate coherent spatial features,
while the temporal prompts direct the sequential image-to-video (I2V) stage to
ensure motion consistency. Experimental results validate that our approach
achieves excellent spatiotemporal consistency, demonstrating outstanding
performance in identity preservation, text relevance, and video quality. By
leveraging this simple yet robust mechanism, our algorithm secures the
runner-up position in 2025 ACM MultiMedia Challenge.

</details>


### [182] [Geometric-Guided Few-Shot Dental Landmark Detection with Human-Centric Foundation Model](https://arxiv.org/abs/2507.04710)
*Anbang Wang,Marawan Elbatel,Keyuan Liu,Lizhuo Lin,Meng Lan,Yanqi Yang,Xiaomeng Li*

Main category: cs.CV

TL;DR: GeoSapiens is a few-shot learning framework for dental landmark detection on CBCT scans, outperforming existing methods with limited training data.


<details>
  <summary>Details</summary>
Motivation: Manual landmark annotation is time-consuming and inconsistent; deep learning solutions are hindered by data scarcity and high annotation costs.

Method: GeoSapiens combines a robust baseline model (Sapiens) with a novel geometric loss function to improve landmark detection accuracy.

Result: GeoSapiens achieved an 8.18% higher success rate at a strict 0.5 mm threshold compared to leading methods.

Conclusion: GeoSapiens offers an efficient, accurate solution for dental landmark detection, addressing data scarcity and improving clinical workflows.

Abstract: Accurate detection of anatomic landmarks is essential for assessing alveolar
bone and root conditions, thereby optimizing clinical outcomes in orthodontics,
periodontics, and implant dentistry. Manual annotation of landmarks on
cone-beam computed tomography (CBCT) by dentists is time-consuming,
labor-intensive, and subject to inter-observer variability. Deep learning-based
automated methods present a promising approach to streamline this process
efficiently. However, the scarcity of training data and the high cost of expert
annotations hinder the adoption of conventional deep learning techniques. To
overcome these challenges, we introduce GeoSapiens, a novel few-shot learning
framework designed for robust dental landmark detection using limited annotated
CBCT of anterior teeth. Our GeoSapiens framework comprises two key components:
(1) a robust baseline adapted from Sapiens, a foundational model that has
achieved state-of-the-art performance in human-centric vision tasks, and (2) a
novel geometric loss function that improves the model's capacity to capture
critical geometric relationships among anatomical structures. Experiments
conducted on our collected dataset of anterior teeth landmarks revealed that
GeoSapiens surpassed existing landmark detection methods, outperforming the
leading approach by an 8.18% higher success detection rate at a strict 0.5 mm
threshold-a standard widely recognized in dental diagnostics. Code is available
at: https://github.com/xmed-lab/GeoSapiens.

</details>


### [183] [Unleashing the Power of Neural Collapse: Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery](https://arxiv.org/abs/2507.04725)
*Jizhou Han,Shaokun Wang,Yuhang He,Chenhao Ding,Qiang Wang,Xinyuan Gao,SongLin Dong,Yihong Gong*

Main category: cs.CV

TL;DR: NC-GCD improves GCD by fixing ETF prototypes for better geometric structure and consistency, using a unified loss and stable clustering.


<details>
  <summary>Details</summary>
Motivation: Address inconsistent optimization and category confusion in GCD, which hampers novel category performance.

Method: Pre-assigns ETF prototypes, introduces Consistent ETF Alignment Loss, and uses Semantic Consistency Matcher (SCM) for stable clustering.

Result: Achieves strong performance on GCD benchmarks, notably improving novel category accuracy.

Conclusion: NC-GCD effectively addresses GCD challenges, enhancing performance and consistency.

Abstract: Generalized Category Discovery (GCD) focuses on classifying known categories
while simultaneously discovering novel categories from unlabeled data. However,
previous GCD methods face challenges due to inconsistent optimization
objectives and category confusion. This leads to feature overlap and ultimately
hinders performance on novel categories. To address these issues, we propose
the Neural Collapse-inspired Generalized Category Discovery (NC-GCD) framework.
By pre-assigning and fixing Equiangular Tight Frame (ETF) prototypes, our
method ensures an optimal geometric structure and a consistent optimization
objective for both known and novel categories. We introduce a Consistent ETF
Alignment Loss that unifies supervised and unsupervised ETF alignment and
enhances category separability. Additionally, a Semantic Consistency Matcher
(SCM) is designed to maintain stable and consistent label assignments across
clustering iterations. Our method achieves strong performance on multiple GCD
benchmarks, significantly enhancing novel category accuracy and demonstrating
its effectiveness.

</details>


### [184] [Losing Control: Data Poisoning Attack on Guided Diffusion via ControlNet](https://arxiv.org/abs/2507.04726)
*Raz Lapid,Almog Dubin*

Main category: cs.CV

TL;DR: A novel data poisoning method targets ControlNets, enabling NSFW image generation without text triggers, revealing vulnerabilities in open-source pipelines.


<details>
  <summary>Details</summary>
Motivation: To expose the vulnerability of ControlNets to stealthy data poisoning attacks, especially when fine-tuned on community-shared data.

Method: Inject poisoned samples pairing subtly triggered inputs with NSFW targets, ensuring clean-prompt fidelity while reliably producing NSFW outputs.

Result: High attack success rate on large-scale datasets, with imperceptible triggers in raw inputs.

Conclusion: Highlights critical vulnerabilities in ControlNets and emphasizes the need for robust data sanitization and defenses.

Abstract: Text-to-image diffusion models have achieved remarkable success in
translating textual prompts into high-fidelity images. ControlNets further
extend these models by allowing precise, image-based conditioning (e.g., edge
maps, depth, pose), enabling fine-grained control over structure and style.
However, their dependence on large, publicly scraped datasets -- and the
increasing use of community-shared data for fine-tuning -- exposes them to
stealthy data poisoning attacks. In this work, we introduce a novel data
poisoning method that manipulates ControlNets to generate images containing
specific content without any text triggers. By injecting poisoned samples --
each pairing a subtly triggered input with an NSFW target -- the model retains
clean-prompt fidelity yet reliably produces NSFW outputs when the trigger is
present. On large-scale, high-quality datasets, our backdoor achieves high
attack success rate while remaining imperceptible in raw inputs. These results
reveal a critical vulnerability in open-source ControlNets pipelines and
underscore the need for robust data sanitization and defense mechanisms.

</details>


### [185] [An analysis of vision-language models for fabric retrieval](https://arxiv.org/abs/2507.04735)
*Francesco Giuliari,Asif Khan Pattan,Mohamed Lamine Mekhalfi,Fabio Poiesi*

Main category: cs.CV

TL;DR: The paper explores zero-shot text-to-image retrieval for fabric samples using VLMs, introduces an automated annotation pipeline with MLLMs, and evaluates three models, finding structured descriptions improve accuracy, with the Perception Encoder performing best.


<details>
  <summary>Details</summary>
Motivation: Cross-modal retrieval is crucial for specialized domains like manufacturing, where product info combines visuals and text, but lacks public datasets.

Method: Uses MLLMs to generate freeform and structured textual descriptions, evaluates retrieval performance with CLIP, LAION-CLIP, and Perception Encoder.

Result: Structured descriptions boost retrieval accuracy, especially for complex fabrics; Perception Encoder excels due to feature alignment.

Conclusion: Combining technical descriptions with advanced VLMs optimizes cross-modal retrieval in industrial settings, though domain adaptation remains needed.

Abstract: Effective cross-modal retrieval is essential for applications like
information retrieval and recommendation systems, particularly in specialized
domains such as manufacturing, where product information often consists of
visual samples paired with a textual description. This paper investigates the
use of Vision Language Models(VLMs) for zero-shot text-to-image retrieval on
fabric samples. We address the lack of publicly available datasets by
introducing an automated annotation pipeline that uses Multimodal Large
Language Models (MLLMs) to generate two types of textual descriptions: freeform
natural language and structured attribute-based descriptions. We produce these
descriptions to evaluate retrieval performance across three Vision-Language
Models: CLIP, LAION-CLIP, and Meta's Perception Encoder. Our experiments
demonstrate that structured, attribute-rich descriptions significantly enhance
retrieval accuracy, particularly for visually complex fabric classes, with the
Perception Encoder outperforming other models due to its robust feature
alignment capabilities. However, zero-shot retrieval remains challenging in
this fine-grained domain, underscoring the need for domain-adapted approaches.
Our findings highlight the importance of combining technical textual
descriptions with advanced VLMs to optimize cross-modal retrieval in industrial
applications.

</details>


### [186] [Vision-Language Models Can't See the Obvious](https://arxiv.org/abs/2507.04741)
*Yasser Dahou,Ngoc Dung Huynh,Phuc H. Le-Khac,Wamiq Reyaz Para,Ankit Singh,Sanath Narayan*

Main category: cs.CV

TL;DR: SalBench is a benchmark to evaluate LVLM's ability to detect visually salient features, revealing limitations in identifying obvious anomalies.


<details>
  <summary>Details</summary>
Motivation: To assess LVLM's alignment with human visual processing of low-level features like color and orientation.

Method: SalBench includes three tasks (Odd-One-Out Detection, Referring Odd-One-Out, Visual Referring Odd-One-Out) using images with rare or unexpected elements.

Result: LVLM, including GPT-4o, perform poorly (47.6% accuracy) on detecting obvious visual anomalies.

Conclusion: SalBench highlights LVLM's limitations in human-like visual attention and serves as a tool for future model evaluation.

Abstract: We present Saliency Benchmark (SalBench), a novel benchmark designed to
assess the capability of Large Vision-Language Models (LVLM) in detecting
visually salient features that are readily apparent to humans, such as a large
circle amidst a grid of smaller ones. This benchmark focuses on low-level
features including color, intensity, and orientation, which are fundamental to
human visual processing. Our SalBench consists of images that highlight rare,
unusual, or unexpected elements within scenes, and naturally draw human
attention. It comprises three novel tasks for evaluating the perceptual
capabilities of LVLM: Odd-One-Out Detection, Referring Odd-One-Out, and Visual
Referring Odd-One-Out. We perform a comprehensive evaluation of
state-of-the-art LVLM using SalBench and our findings reveal a surprising
limitation: LVLM struggle to identify seemingly obvious visual anomalies, with
even the advanced GPT-4o achieving only 47.6\% accuracy on such a simple task.
SalBench will be an important step in measuring the capabilities of LVLM that
align with the subtle definition of human attention.

</details>


### [187] [MatDecompSDF: High-Fidelity 3D Shape and PBR Material Decomposition from Multi-View Images](https://arxiv.org/abs/2507.04749)
*Chengyu Wang,Isabella Bennett,Henry Scott,Liang Zhang,Mei Chen,Hao Li,Rui Zhao*

Main category: cs.CV

TL;DR: MatDecompSDF is a framework for recovering 3D shapes and material properties from multi-view images using neural SDF, material fields, and lighting models, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of disentangling geometry, materials, and illumination from 2D observations in inverse rendering is addressed.

Method: Joint optimization of neural SDF, material fields, and lighting models with a differentiable rendering layer and physical priors.

Result: Outperforms existing methods in accuracy, material fidelity, and view synthesis, producing editable and relightable assets.

Conclusion: MatDecompSDF is practical for digital content creation, offering high-fidelity decompositions and integration into graphics pipelines.

Abstract: We present MatDecompSDF, a novel framework for recovering high-fidelity 3D
shapes and decomposing their physically-based material properties from
multi-view images. The core challenge of inverse rendering lies in the
ill-posed disentanglement of geometry, materials, and illumination from 2D
observations. Our method addresses this by jointly optimizing three neural
components: a neural Signed Distance Function (SDF) to represent complex
geometry, a spatially-varying neural field for predicting PBR material
parameters (albedo, roughness, metallic), and an MLP-based model for capturing
unknown environmental lighting. The key to our approach is a physically-based
differentiable rendering layer that connects these 3D properties to the input
images, allowing for end-to-end optimization. We introduce a set of carefully
designed physical priors and geometric regularizations, including a material
smoothness loss and an Eikonal loss, to effectively constrain the problem and
achieve robust decomposition. Extensive experiments on both synthetic and
real-world datasets (e.g., DTU) demonstrate that MatDecompSDF surpasses
state-of-the-art methods in geometric accuracy, material fidelity, and novel
view synthesis. Crucially, our method produces editable and relightable assets
that can be seamlessly integrated into standard graphics pipelines, validating
its practical utility for digital content creation.

</details>


### [188] [MCFormer: A Multi-Cost-Volume Network and Comprehensive Benchmark for Particle Image Velocimetry](https://arxiv.org/abs/2507.04750)
*Zicheng Lin,Xiaoqiang Li,Yichao Wang,Chuan Zhu*

Main category: cs.CV

TL;DR: The paper introduces a synthetic PIV benchmark dataset and a new deep learning model (MCFormer) to evaluate optical flow methods for PIV, showing MCFormer's superior performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of standardized evaluation for optical flow models on PIV data due to limited datasets and benchmarks.

Method: Created a synthetic PIV dataset from CFD simulations and proposed MCFormer, a deep network using multi-frame temporal info and cost volumes.

Result: MCFormer outperforms existing methods with the lowest normalized endpoint error (NEPE).

Conclusion: Provides a benchmark for future PIV research and a state-of-the-art method, with open-sourced dataset and code.

Abstract: Particle Image Velocimetry (PIV) is fundamental to fluid dynamics, yet deep
learning applications face significant hurdles. A critical gap exists: the lack
of comprehensive evaluation of how diverse optical flow models perform
specifically on PIV data, largely due to limitations in available datasets and
the absence of a standardized benchmark. This prevents fair comparison and
hinders progress. To address this, our primary contribution is a novel,
large-scale synthetic PIV benchmark dataset generated from diverse CFD
simulations (JHTDB and Blasius). It features unprecedented variety in particle
densities, flow velocities, and continuous motion, enabling, for the first
time, a standardized and rigorous evaluation of various optical flow and PIV
algorithms. Complementing this, we propose Multi Cost Volume PIV (MCFormer), a
new deep network architecture leveraging multi-frame temporal information and
multiple cost volumes, specifically designed for PIV's sparse nature. Our
comprehensive benchmark evaluation, the first of its kind, reveals significant
performance variations among adapted optical flow models and demonstrates that
MCFormer significantly outperforms existing methods, achieving the lowest
overall normalized endpoint error (NEPE). This work provides both a
foundational benchmark resource essential for future PIV research and a
state-of-the-art method tailored for PIV challenges. We make our benchmark
dataset and code publicly available to foster future research in this area.

</details>


### [189] [Robustifying 3D Perception through Least-Squares Multi-Agent Graphs Object Tracking](https://arxiv.org/abs/2507.04762)
*Maria Damanaki,Ioulia Kapsali,Nikos Piperigkos,Alexandros Gkillas,Aris S. Lalos*

Main category: cs.CV

TL;DR: A novel multi-agent framework using least-squares graph on 3D LiDAR data to mitigate adversarial noise, improving tracking accuracy by 23.3% over existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing resilience of EdgeAI systems like autonomous vehicles against adversarial threats by improving multi-agent cooperation for better situational awareness.

Method: Uses least-squares graph to refine object tracking by reducing positional errors via overlapped bounding boxes and differential coordinates, followed by two-stage tracking.

Result: Outperforms state-of-the-art single and multi-agent tracking by up to 23.3% on the V2V4Real dataset under adversarial conditions.

Conclusion: The proposed framework offers a resilient solution for adversarial noise mitigation without needing extra defenses, advancing multi-agent tracking robustness.

Abstract: The critical perception capabilities of EdgeAI systems, such as autonomous
vehicles, are required to be resilient against adversarial threats, by enabling
accurate identification and localization of multiple objects in the scene over
time, mitigating their impact. Single-agent tracking offers resilience to
adversarial attacks but lacks situational awareness, underscoring the need for
multi-agent cooperation to enhance context understanding and robustness. This
paper proposes a novel mitigation framework on 3D LiDAR scene against
adversarial noise by tracking objects based on least-squares graph on
multi-agent adversarial bounding boxes. Specifically, we employ the
least-squares graph tool to reduce the induced positional error of each
detection's centroid utilizing overlapped bounding boxes on a fully connected
graph via differential coordinates and anchor points. Hence, the multi-vehicle
detections are fused and refined mitigating the adversarial impact, and
associated with existing tracks in two stages performing tracking to further
suppress the adversarial threat. An extensive evaluation study on the
real-world V2V4Real dataset demonstrates that the proposed method significantly
outperforms both state-of-the-art single and multi-agent tracking frameworks by
up to 23.3% under challenging adversarial conditions, operating as a resilient
approach without relying on additional defense mechanisms.

</details>


### [190] [GraphBrep: Learning B-Rep in Graph Structure for Efficient CAD Generation](https://arxiv.org/abs/2507.04765)
*Weilin Lai,Tie Xu,Hu Wang*

Main category: cs.CV

TL;DR: GraphBrep is a B-Rep generation model that reduces computational redundancy by explicitly representing topology with a graph diffusion model, achieving faster training and inference while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods embed topology into geometric features, causing redundancy and high computational costs. GraphBrep aims to address this by explicitly representing topology.

Method: GraphBrep uses an undirected weighted graph to represent surface topology and a graph diffusion model to learn topology conditioned on surface features.

Result: The method reduces training and inference times by up to 31.3% and 56.3%, respectively, while maintaining high-quality CAD generation.

Conclusion: GraphBrep offers a compact and efficient solution for B-Rep generation, outperforming existing methods in computational efficiency without sacrificing quality.

Abstract: Direct B-Rep generation is increasingly important in CAD workflows,
eliminating costly modeling sequence data and supporting complex features. A
key challenge is modeling joint distribution of the misaligned geometry and
topology. Existing methods tend to implicitly embed topology into the geometric
features of edges. Although this integration ensures feature alignment, it also
causes edge geometry to carry more redundant structural information compared to
the original B-Rep, leading to significantly higher computational cost. To
reduce redundancy, we propose GraphBrep, a B-Rep generation model that
explicitly represents and learns compact topology. Following the original
structure of B-Rep, we construct an undirected weighted graph to represent
surface topology. A graph diffusion model is employed to learn topology
conditioned on surface features, serving as the basis for determining
connectivity between primitive surfaces. The explicit representation ensures a
compact data structure, effectively reducing computational cost during both
training and inference. Experiments on two large-scale unconditional datasets
and one category-conditional dataset demonstrate the proposed method
significantly reduces training and inference times (up to 31.3% and 56.3% for
given datasets, respectively) while maintaining high-quality CAD generation
compared with SOTA.

</details>


### [191] [From Imitation to Innovation: The Emergence of AI Unique Artistic Styles and the Challenge of Copyright Protection](https://arxiv.org/abs/2507.04769)
*Zexi Jia,Chuanwei Huang,Yeshuang Zhu,Hongyan Fei,Ying Deng,Zhiqiang Yuan,Jiapei Zhang,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: The paper addresses the lack of systematic legal standards for AI art copyrights by proposing ArtBulb, a framework combining style-based clustering and MLLMs, and introduces the AICD benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: To establish legal and technological standards for AI art copyrights due to the absence of reliable evaluation methods.

Method: Proposes ArtBulb, a framework using style description-based multimodal clustering and MLLMs, and introduces the AICD dataset.

Result: ArtBulb outperforms existing models in evaluations, demonstrating effectiveness in AI art copyright judgment.

Conclusion: The work bridges legal and technological gaps, raising awareness about AI art copyright issues.

Abstract: Current legal frameworks consider AI-generated works eligible for copyright
protection when they meet originality requirements and involve substantial
human intellectual input. However, systematic legal standards and reliable
evaluation methods for AI art copyrights are lacking. Through comprehensive
analysis of legal precedents, we establish three essential criteria for
determining distinctive artistic style: stylistic consistency, creative
uniqueness, and expressive accuracy. To address these challenges, we introduce
ArtBulb, an interpretable and quantifiable framework for AI art copyright
judgment that combines a novel style description-based multimodal clustering
method with multimodal large language models (MLLMs). We also present AICD, the
first benchmark dataset for AI art copyright annotated by artists and legal
experts. Experimental results demonstrate that ArtBulb outperforms existing
models in both quantitative and qualitative evaluations. Our work aims to
bridge the gap between the legal and technological communities and bring
greater attention to the societal issue of AI art copyrights.

</details>


### [192] [Model Compression using Progressive Channel Pruning](https://arxiv.org/abs/2507.04792)
*Jinyang Guo,Weichen Zhang,Wanli Ouyang,Dong Xu*

Main category: cs.CV

TL;DR: Progressive Channel Pruning (PCP) is a framework for iteratively pruning channels in CNNs, outperforming existing methods by using a three-step pipeline and extending to transfer learning.


<details>
  <summary>Details</summary>
Motivation: Existing channel pruning methods prune channels once per layer, potentially leading to suboptimal accuracy. PCP aims to improve this by iteratively pruning small numbers of channels from selected layers.

Method: PCP uses a three-step pipeline: (1) attempting to prune channels and estimate accuracy drop, (2) selecting layers with minimal accuracy impact, and (3) pruning selected layers. It also extends to transfer learning by using source and target domain data.

Result: PCP outperforms existing channel pruning methods in both supervised and transfer learning settings, as shown in experiments on benchmark datasets.

Conclusion: PCP provides a more effective and flexible approach to channel pruning, enhancing CNN acceleration while maintaining accuracy.

Abstract: In this work, we propose a simple but effective channel pruning framework
called Progressive Channel Pruning (PCP) to accelerate Convolutional Neural
Networks (CNNs). In contrast to the existing channel pruning methods that prune
channels only once per layer in a layer-by-layer fashion, our new progressive
framework iteratively prunes a small number of channels from several selected
layers, which consists of a three-step attempting-selecting-pruning pipeline in
each iteration. In the attempting step, we attempt to prune a pre-defined
number of channels from one layer by using any existing channel pruning methods
and estimate the accuracy drop for this layer based on the labelled samples in
the validation set. In the selecting step, based on the estimated accuracy
drops for all layers, we propose a greedy strategy to automatically select a
set of layers that will lead to less overall accuracy drop after pruning these
layers. In the pruning step, we prune a small number of channels from these
selected layers. We further extend our PCP framework to prune channels for the
deep transfer learning methods like Domain Adversarial Neural Network (DANN),
in which we effectively reduce the data distribution mismatch in the channel
pruning process by using both labelled samples from the source domain and
pseudo-labelled samples from the target domain. Our comprehensive experiments
on two benchmark datasets demonstrate that our PCP framework outperforms the
existing channel pruning approaches under both supervised learning and transfer
learning settings.

</details>


### [193] [PointGAC: Geometric-Aware Codebook for Masked Point Cloud Modeling](https://arxiv.org/abs/2507.04801)
*Abiao Li,Chenlei Lv,Yuming Fang,Yifan Zuo,Jian Zhang,Guofeng Mei*

Main category: cs.CV

TL;DR: PointGAC introduces a clustering-based masked point cloud modeling method to improve generalized feature learning by aligning feature distributions of masked regions using an online codebook-guided teacher-student framework.


<details>
  <summary>Details</summary>
Motivation: Existing masked point cloud modeling methods over-constrain models to learn masked region details, failing to capture generalized features.

Method: PointGAC uses a geometry-aware partitioning strategy, an online k-means codebook updated by a teacher model, and a student model aligning masked feature assignments to cluster centers.

Result: The method effectively learns generalized features and performs well on downstream tasks.

Conclusion: PointGAC addresses limitations of regression-based MPM methods by focusing on feature distribution alignment, validated by experiments.

Abstract: Most masked point cloud modeling (MPM) methods follow a regression paradigm
to reconstruct the coordinate or feature of masked regions. However, they tend
to over-constrain the model to learn the details of the masked region,
resulting in failure to capture generalized features. To address this
limitation, we propose \textbf{\textit{PointGAC}}, a novel clustering-based MPM
method that aims to align the feature distribution of masked regions.
Specially, it features an online codebook-guided teacher-student framework.
Firstly, it presents a geometry-aware partitioning strategy to extract initial
patches. Then, the teacher model updates a codebook via online k-means based on
features extracted from the complete patches. This procedure facilitates
codebook vectors to become cluster centers. Afterward, we assigns the unmasked
features to their corresponding cluster centers, and the student model aligns
the assignment for the reconstructed masked features. This strategy focuses on
identifying the cluster centers to which the masked features belong, enabling
the model to learn more generalized feature representations. Benefiting from a
proposed codebook maintenance mechanism, codebook vectors are actively updated,
which further increases the efficiency of semantic feature learning.
Experiments validate the effectiveness of the proposed method on various
downstream tasks. Code is available at https://github.com/LAB123-tech/PointGAC

</details>


### [194] [UDF-GMA: Uncertainty Disentanglement and Fusion for General Movement Assessment](https://arxiv.org/abs/2507.04814)
*Zeqi Luo,Ali Gooya,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: UDF-GMA introduces a method to model epistemic and aleatoric uncertainties in pose-based automated GMA, improving reliability and class separation.


<details>
  <summary>Details</summary>
Motivation: Automated GMA methods face uncertainty issues due to limited data and noisy pose estimation, reducing clinical reliability.

Method: UDF-GMA models aleatoric uncertainty directly and estimates epistemic uncertainty via Bayesian approximation, fusing these with motion representations.

Result: Experiments on the Pmi-GMA dataset show UDF-GMA effectively predicts poor repertoire and generalizes well.

Conclusion: UDF-GMA enhances automated GMA by addressing uncertainties, improving clinical reliability and class separation.

Abstract: General movement assessment (GMA) is a non-invasive tool for the early
detection of brain dysfunction through the qualitative assessment of general
movements, and the development of automated methods can broaden its
application. However, mainstream pose-based automated GMA methods are prone to
uncertainty due to limited high-quality data and noisy pose estimation,
hindering clinical reliability without reliable uncertainty measures. In this
work, we introduce UDF-GMA which explicitly models epistemic uncertainty in
model parameters and aleatoric uncertainty from data noise for pose-based
automated GMA. UDF-GMA effectively disentangles uncertainties by directly
modelling aleatoric uncertainty and estimating epistemic uncertainty through
Bayesian approximation. We further propose fusing these uncertainties with the
embedded motion representation to enhance class separation. Extensive
experiments on the Pmi-GMA benchmark dataset demonstrate the effectiveness and
generalisability of the proposed approach in predicting poor repertoire.

</details>


### [195] [From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach](https://arxiv.org/abs/2507.04815)
*Mihai Masala,Marius Leordeanu*

Main category: cs.CV

TL;DR: The paper addresses the scarcity of long-form video descriptions by proposing a shared vision-language representation using event graphs for explainable and automated caption generation.


<details>
  <summary>Details</summary>
Motivation: Current datasets lack long-form video descriptions due to costly manual annotation and the complexity of linking visual events to language. Existing methods excel in short captions but fail to explain vision-language relationships.

Method: The authors introduce a shared representation between vision and language using event graphs in space and time. This method integrates multiple vision tasks and serves as an automatic teacher for training neural pathways.

Result: The approach generates coherent, rich, and relevant textual descriptions, validated by standard metrics, human annotations, and state-of-the-art VLMs.

Conclusion: The proposed neuro-analytical method effectively bridges vision and language for long-form video descriptions, offering explainability and automation.

Abstract: The task of describing video content in natural language is commonly referred
to as video captioning. Unlike conventional video captions, which are typically
brief and widely available, long-form paragraph descriptions in natural
language are scarce. This limitation of current datasets is due to the
expensive human manual annotation required and to the highly challenging task
of explaining the language formation process from the perspective of the
underlying story, as a complex system of interconnected events in space and
time. Through a thorough analysis of recently published methods and available
datasets, we identify a general lack of published resources dedicated to the
problem of describing videos in complex language, beyond the level of
descriptions in the form of enumerations of simple captions. Furthermore, while
state-of-the-art methods produce impressive results on the task of generating
shorter captions from videos by direct end-to-end learning between the videos
and text, the problem of explaining the relationship between vision and
language is still beyond our reach. In this work, we propose a shared
representation between vision and language, based on graphs of events in space
and time, which can be obtained in an explainable and analytical way, to
integrate and connect multiple vision tasks to produce the final natural
language description. Moreover, we also demonstrate how our automated and
explainable video description generation process can function as a fully
automatic teacher to effectively train direct, end-to-end neural student
pathways, within a self-supervised neuro-analytical system. We validate that
our explainable neuro-analytical approach generates coherent, rich and relevant
textual descriptions on videos collected from multiple varied datasets, using
both standard evaluation metrics, human annotations and consensus from
ensembles of state-of-the-art VLMs.

</details>


### [196] [SeqGrowGraph: Learning Lane Topology as a Chain of Graph Expansions](https://arxiv.org/abs/2507.04822)
*Mengwei Xie,Shuang Zeng,Xinyuan Chang,Xinran Liu,Zheng Pan,Mu Xu,Xing Wei*

Main category: cs.CV

TL;DR: SeqGrowGraph is a novel framework for learning lane topology as a chain of graph expansions, achieving state-of-the-art performance on nuScenes and Argoverse 2 datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex, non-linear lane structures like loops and bidirectional lanes, which are common in real-world road networks.

Method: SeqGrowGraph incrementally constructs a directed graph of intersections and centerlines, expanding the adjacency and geometric matrices at each step. A transformer model predicts expansions autoregressively using depth-first search ordering.

Result: The framework achieves state-of-the-art performance on nuScenes and Argoverse 2 datasets.

Conclusion: SeqGrowGraph effectively models complex lane topologies, advancing autonomous driving capabilities.

Abstract: Accurate lane topology is essential for autonomous driving, yet traditional
methods struggle to model the complex, non-linear structures-such as loops and
bidirectional lanes-prevalent in real-world road structure. We present
SeqGrowGraph, a novel framework that learns lane topology as a chain of graph
expansions, inspired by human map-drawing processes. Representing the lane
graph as a directed graph $G=(V,E)$, with intersections ($V$) and centerlines
($E$), SeqGrowGraph incrementally constructs this graph by introducing one
vertex at a time. At each step, an adjacency matrix ($A$) expands from $n
\times n$ to $(n+1) \times (n+1)$ to encode connectivity, while a geometric
matrix ($M$) captures centerline shapes as quadratic B\'ezier curves. The graph
is serialized into sequences, enabling a transformer model to autoregressively
predict the chain of expansions, guided by a depth-first search ordering.
Evaluated on nuScenes and Argoverse 2 datasets, SeqGrowGraph achieves
state-of-the-art performance.

</details>


### [197] [RIPE: Reinforcement Learning on Unlabeled Image Pairs for Robust Keypoint Extraction](https://arxiv.org/abs/2507.04839)
*Johannes Knzel,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: RIPE is a reinforcement learning framework for weakly-supervised keypoint extraction, requiring minimal supervision (binary labels) and achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of conventional training methods that rely on artificial transformations or 3D data, RIPE aims to simplify data preparation and improve generalization.

Method: RIPE uses a hyper-column approach for keypoint description and introduces an auxiliary loss to enhance descriptor discriminability.

Result: RIPE achieves competitive performance on benchmarks, simplifying data preparation without sacrificing accuracy.

Conclusion: RIPE advances robust keypoint extraction and description, with publicly available code for further research.

Abstract: We introduce RIPE, an innovative reinforcement learning-based framework for
weakly-supervised training of a keypoint extractor that excels in both
detection and description tasks. In contrast to conventional training regimes
that depend heavily on artificial transformations, pre-generated models, or 3D
data, RIPE requires only a binary label indicating whether paired images
represent the same scene. This minimal supervision significantly expands the
pool of training data, enabling the creation of a highly generalized and robust
keypoint extractor.
  RIPE utilizes the encoder's intermediate layers for the description of the
keypoints with a hyper-column approach to integrate information from different
scales. Additionally, we propose an auxiliary loss to enhance the
discriminative capability of the learned descriptors.
  Comprehensive evaluations on standard benchmarks demonstrate that RIPE
simplifies data preparation while achieving competitive performance compared to
state-of-the-art techniques, marking a significant advancement in robust
keypoint extraction and description. To support further research, we have made
our code publicly available at https://github.com/fraunhoferhhi/RIPE.

</details>


### [198] [CMET: Clustering guided METric for quantifying embedding quality](https://arxiv.org/abs/2507.04840)
*Sourav Ghosh,Chayan Maitra,Rajat K. De*

Main category: cs.CV

TL;DR: The paper introduces CMET, a novel metric for evaluating embedding quality in dimensionality transformations, addressing the inefficiency of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for comparing embeddings with original data are computationally expensive, necessitating a more efficient solution.

Method: The study proposes CMET, a clustering-guided metric with two scores (CMET_L and CMET_G) to measure local and global shape preservation.

Result: CMET outperforms state-of-the-art methods across synthetic, biological, and image datasets, demonstrating efficiency and stability.

Conclusion: CMET is a reliable, low-complexity metric suitable for diverse datasets and hyper-parameter choices.

Abstract: Due to rapid advancements in technology, datasets are available from various
domains. In order to carry out more relevant and appropriate analysis, it is
often necessary to project the dataset into a higher or lower dimensional space
based on requirement. Projecting the data in a higher-dimensional space helps
in unfolding intricate patterns, enhancing the performance of the underlying
models. On the other hand, dimensionality reduction is helpful in denoising
data while capturing maximal information, as well as reducing execution time
and memory.In this context, it is not always statistically evident whether the
transformed embedding retains the local and global structure of the original
data. Most of the existing metrics that are used for comparing the local and
global shape of the embedding against the original one are highly expensive in
terms of time and space complexity. In order to address this issue, the
objective of this study is to formulate a novel metric, called Clustering
guided METric (CMET), for quantifying embedding quality. It is effective to
serve the purpose of quantitative comparison between an embedding and the
original data. CMET consists of two scores, viz., CMET_L and CMET_G, that
measure the degree of local and global shape preservation capability,
respectively. The efficacy of CMET has been demonstrated on a wide variety of
datasets, including four synthetic, two biological, and two image datasets.
Results reflect the favorable performance of CMET against the state-of-the-art
methods. Capability to handle both small and large data, low algorithmic
complexity, better and stable performance across all kinds of data, and
different choices of hyper-parameters feature CMET as a reliable metric.

</details>


### [199] [Efficient SAR Vessel Detection for FPGA-Based On-Satellite Sensing](https://arxiv.org/abs/2507.04842)
*Colin Laganier,Liam Fletcher,Elim Kwan,Richard Walters,Victoria Nockles*

Main category: cs.CV

TL;DR: A lightweight YOLOv8 model optimized for FPGA deployment achieves near-state-of-the-art SAR vessel detection performance under satellite power constraints.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of satellite data and latency issues in transmission necessitate efficient on-satellite ML solutions for time-sensitive tasks like SAR vessel detection.

Method: Developed a customized YOLOv8 architecture optimized for FPGA-based processing (<10W), trained and evaluated on the xView3-SAR dataset, and deployed on a Kria KV260 MPSoC.

Result: The FPGA-based model performs only ~2-3% worse than GPU-based models while being significantly smaller in size.

Conclusion: This work enables efficient, autonomous SAR analysis for Earth observation, addressing power and size constraints for satellite deployment.

Abstract: Rapid analysis of satellite data is vital for many remote sensing
applications, from disaster response to environmental monitoring, but is
becoming harder to achieve with the increasing volumes of data generated by
modern satellites. On-satellite machine learning (ML) offers a potential
solution, by reducing latency associated with transmission of these large data
volumes to ground stations, but state-of-the-art models are often too large or
power-hungry for satellite deployment. Vessel detection using Synthetic
Aperture Radar (SAR) is a critical time-sensitive task for maritime security
that exemplifies this challenge. SAR vessel detection has previously been
demonstrated only by ML models that either are too large for satellite
deployment, have not been developed for sufficiently low-power hardware, or
have only been developed and tested on small SAR datasets that do not
sufficiently represent the real-world task. Here we address this issue by
developing and deploying a new efficient and highly performant SAR vessel
detection model, using a customised YOLOv8 architecture specifically optimized
for FPGA-based processing within common satellite power constraints (<10W). We
train and evaluate our model on the largest and most diverse open SAR vessel
dataset, xView3-SAR, and deploy it on a Kria KV260 MPSoC. We show that our
FPGA-based model has detection and classification performance only ~2% and 3%
lower than values from state-of-the-art GPU-based models, despite being two to
three orders of magnitude smaller in size. This work demonstrates small yet
highly performant ML models for time-critical SAR analysis, paving the way for
more autonomous, responsive, and scalable Earth observation systems.

</details>


### [200] [Semantically Consistent Discrete Diffusion for 3D Biological Graph Modeling](https://arxiv.org/abs/2507.04856)
*Chinmay Prabhakar,Suprosanna Shit,Tamaz Amiranashvili,Hongwei Bran Li,Bjoern Menze*

Main category: cs.CV

TL;DR: A novel 3D biological graph generation method ensures anatomical validity by using a projection operator and edge-deletion-based noising, outperforming existing methods on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods struggle to generate anatomically valid 3D biological graphs, limiting their utility in biological and clinical research.

Method: Proposes a projection operator for sampling to fix inconsistencies and an edge-deletion-based noising procedure tailored for sparse biological graphs.

Result: Outperforms previous methods on human circle of Willis and lung airways datasets, improving downstream graph labeling and serving as a link predictor.

Conclusion: The method advances 3D biological graph generation by ensuring structural and semantic plausibility, enhancing practical applications.

Abstract: 3D spatial graphs play a crucial role in biological and clinical research by
modeling anatomical networks such as blood vessels,neurons, and airways.
However, generating 3D biological graphs while maintaining anatomical validity
remains challenging, a key limitation of existing diffusion-based methods. In
this work, we propose a novel 3D biological graph generation method that
adheres to structural and semantic plausibility conditions. We achieve this by
using a novel projection operator during sampling that stochastically fixes
inconsistencies. Further, we adopt a superior edge-deletion-based noising
procedure suitable for sparse biological graphs. Our method demonstrates
superior performance on two real-world datasets, human circle of Willis and
lung airways, compared to previous approaches. Importantly, we demonstrate that
the generated samples significantly enhance downstream graph labeling
performance. Furthermore, we show that our generative model is a reasonable
out-of-the-box link predictior.

</details>


### [201] [Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite](https://arxiv.org/abs/2507.04878)
*Yanco Amor Torterolo-Orta,Jaione Macicior-Mitxelena,Marina Miguez-Lamanuzzi,Ana Garca-Serrano*

Main category: cs.CV

TL;DR: The GRESEL team participated in the IberLEF 2025 PastReader task, testing web-based OCR, traditional OCR, and a compact multimodal model on consumer hardware, yielding satisfactory but improvable results.


<details>
  <summary>Details</summary>
Motivation: To participate in the shared task and compare different transcription approaches for historical texts.

Method: Three experiments: web-based OCR, traditional OCR, and a compact multimodal model, all run on consumer-grade hardware.

Result: Satisfactory results with potential for improvement.

Conclusion: Future work will explore new techniques using the shared task's Spanish dataset, collaborating with BNE.

Abstract: This article presents the experiments and results obtained by the GRESEL team
in the IberLEF 2025 shared task PastReader: Transcribing Texts from the Past.
Three types of experiments were conducted with the dual aim of participating in
the task and enabling comparisons across different approaches. These included
the use of a web-based OCR service, a traditional OCR engine, and a compact
multimodal model. All experiments were run on consumer-grade hardware, which,
despite lacking high-performance computing capacity, provided sufficient
storage and stability. The results, while satisfactory, leave room for further
improvement. Future work will focus on exploring new techniques and ideas using
the Spanish-language dataset provided by the shared task, in collaboration with
Biblioteca Nacional de Espa\~na (BNE).

</details>


### [202] [HGNet: High-Order Spatial Awareness Hypergraph and Multi-Scale Context Attention Network for Colorectal Polyp Detection](https://arxiv.org/abs/2507.04880)
*Xiaofang Liu,Lingling Sun,Xuqing Zhang,Yuannong Ye,Bin zhao*

Main category: cs.CV

TL;DR: HGNet improves colorectal cancer detection by integrating high-order spatial awareness and multi-scale context attention, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Early detection of colorectal cancer is critical, but current models fail in detecting small lesions, localizing boundaries, and providing interpretable decisions.

Method: HGNet combines Efficient Multi-Scale Context Attention (EMCA), spatial hypergraph convolution, transfer learning, and Eigen-CAM for visualization.

Result: HGNet achieves 94% accuracy, 90.6% recall, and 90% mAP@0.5, excelling in small lesion detection and interpretability.

Conclusion: HGNet significantly advances colorectal cancer detection with improved performance and clinical interpretability.

Abstract: Colorectal cancer (CRC) is closely linked to the malignant transformation of
colorectal polyps, making early detection essential. However, current models
struggle with detecting small lesions, accurately localizing boundaries, and
providing interpretable decisions. To address these issues, we propose HGNet,
which integrates High-Order Spatial Awareness Hypergraph and Multi-Scale
Context Attention. Key innovations include: (1) an Efficient Multi-Scale
Context Attention (EMCA) module to enhance lesion feature representation and
boundary modeling; (2) the deployment of a spatial hypergraph convolution
module before the detection head to capture higher-order spatial relationships
between nodes; (3) the application of transfer learning to address the scarcity
of medical image data; and (4) Eigen Class Activation Map (Eigen-CAM) for
decision visualization. Experimental results show that HGNet achieves 94%
accuracy, 90.6% recall, and 90% mAP@0.5, significantly improving small lesion
differentiation and clinical interpretability. The source code will be made
publicly available upon publication of this paper.

</details>


### [203] [HV-MMBench: Benchmarking MLLMs for Human-Centric Video Understanding](https://arxiv.org/abs/2507.04909)
*Yuxuan Cai,Jiangning Zhang,Zhenye Gan,Qingdong He,Xiaobin Hu,Junwei Zhu,Yabiao Wang,Chengjie Wang,Zhucun Xue,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: HV-MMBench is a new benchmark for evaluating Multimodal Large Language Models (MLLMs) in human-centric video understanding, addressing gaps in existing benchmarks with diverse tasks, data types, and temporal coverage.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for human-centric video understanding lack comprehensive evaluation of perceptual and cognitive abilities, focusing narrowly on generation quality and action recognition.

Method: Proposes HV-MMBench, featuring 15 diverse tasks, multiple question formats, varied evaluation metrics, and multi-domain video coverage with temporal diversity.

Result: HV-MMBench enables a holistic assessment of MLLMs, covering basic to advanced human-centric video understanding tasks.

Conclusion: HV-MMBench fills a critical gap in evaluating MLLMs for human-centric video understanding, offering a robust and comprehensive benchmark.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances in visual understanding tasks involving both images and videos.
However, their capacity to comprehend human-centric video data remains
underexplored, primarily due to the absence of comprehensive and high-quality
evaluation benchmarks. Existing human-centric benchmarks predominantly
emphasize video generation quality and action recognition, while overlooking
essential perceptual and cognitive abilities required in human-centered
scenarios. Furthermore, they are often limited by single-question paradigms and
overly simplistic evaluation metrics. To address above limitations, we propose
a modern HV-MMBench, a rigorously curated benchmark designed to provide a more
holistic evaluation of MLLMs in human-centric video understanding. Compared to
existing human-centric video benchmarks, our work offers the following key
features: (1) Diverse evaluation dimensions: HV-MMBench encompasses 15 tasks,
ranging from basic attribute perception (e.g., age estimation, emotion
recognition) to advanced cognitive reasoning (e.g., social relationship
prediction, intention prediction), enabling comprehensive assessment of model
capabilities; (2) Varied data types: The benchmark includes multiple-choice,
fill-in-blank, true/false, and open-ended question formats, combined with
diverse evaluation metrics, to more accurately and robustly reflect model
performance; (3) Multi-domain video coverage: The benchmark spans 50 distinct
visual scenarios, enabling comprehensive evaluation across fine-grained scene
variations; (4) Temporal coverage: The benchmark covers videos from short-term
(10 seconds) to long-term (up to 30min) durations, supporting systematic
analysis of models temporal reasoning abilities across diverse contextual
lengths.

</details>


### [204] [Leveraging Self-Supervised Features for Efficient Flooded Region Identification in UAV Aerial Images](https://arxiv.org/abs/2507.04915)
*Dibyabha Deb,Ujjwal Verma*

Main category: cs.CV

TL;DR: The paper proposes self-supervised segmentation methods using DINOv2 features to identify flooded regions in UAV images, reducing reliance on manual annotations.


<details>
  <summary>Details</summary>
Motivation: Traditional damage assessment methods are manual and time-consuming; UAV image analysis offers a more objective approach, but supervised learning requires extensive annotations.

Method: Two encoder-decoder segmentation approaches integrate DINOv2's self-supervised features with traditional backbones, tested for generalization on UAV images.

Result: DINOv2's features, pretrained on natural images, transfer well to aerial segmentation, enabling high accuracy with limited labeled data.

Conclusion: Self-supervised features like DINOv2 streamline aerial segmentation workflows, reducing dependency on manual annotations while maintaining accuracy.

Abstract: Identifying regions affected by disasters is a vital step in effectively
managing and planning relief and rescue efforts. Unlike the traditional
approaches of manually assessing post-disaster damage, analyzing images of
Unmanned Aerial Vehicles (UAVs) offers an objective and reliable way to assess
the damage. In the past, segmentation techniques have been adopted to identify
post-flood damage in UAV aerial images. However, most of these supervised
learning approaches rely on manually annotated datasets. Indeed, annotating
images is a time-consuming and error-prone task that requires domain expertise.
This work focuses on leveraging self-supervised features to accurately identify
flooded regions in UAV aerial images. This work proposes two
encoder-decoder-based segmentation approaches, which integrate the visual
features learned from DINOv2 with the traditional encoder backbone. This study
investigates the generalization of self-supervised features for UAV aerial
images. Specifically, we evaluate the effectiveness of features from the DINOv2
model, trained on non-aerial images, for segmenting aerial images, noting the
distinct perspectives between the two image types. Our results demonstrate that
DINOv2's self-supervised pretraining on natural images generates transferable,
general-purpose visual features that streamline the development of aerial
segmentation workflows. By leveraging these features as a foundation, we
significantly reduce reliance on labor-intensive manual annotation processes,
enabling high-accuracy segmentation with limited labeled aerial data.

</details>


### [205] [RainShift: A Benchmark for Precipitation Downscaling Across Geographies](https://arxiv.org/abs/2507.04930)
*Paula Harder,Luca Schmidt,Francis Pelletier,Nicole Ludwig,Matthew Chantry,Christian Lessig,Alex Hernandez-Garcia,David Rolnick*

Main category: cs.CV

TL;DR: The paper introduces RainShift, a dataset and benchmark to evaluate deep learning-based downscaling methods for Earth System Models (ESMs) under geographic distribution shifts, revealing performance drops in out-of-distribution regions and suggesting improvements like data alignment.


<details>
  <summary>Details</summary>
Motivation: The need to assess the generalization of deep learning-based super-resolution models across geographic regions due to uneven high-resolution data availability and regional climatic variations.

Method: Evaluation of state-of-the-art downscaling approaches (GANs, diffusion models) using the RainShift dataset, focusing on generalization across data gaps between the Global North and South.

Result: Significant performance drops in out-of-distribution regions; expanding training domains improves generalization but is insufficient for geographically distinct shifts. Data alignment can enhance spatial generalization.

Conclusion: The work advances global applicability of downscaling methods and aims to reduce inequities in access to high-resolution climate information.

Abstract: Earth System Models (ESM) are our main tool for projecting the impacts of
climate change. However, running these models at sufficient resolution for
local-scale risk-assessments is not computationally feasible. Deep
learning-based super-resolution models offer a promising solution to downscale
ESM outputs to higher resolutions by learning from data. Yet, due to regional
variations in climatic processes, these models typically require retraining for
each geographical area-demanding high-resolution observational data, which is
unevenly available across the globe. This highlights the need to assess how
well these models generalize across geographic regions. To address this, we
introduce RainShift, a dataset and benchmark for evaluating downscaling under
geographic distribution shifts. We evaluate state-of-the-art downscaling
approaches including GANs and diffusion models in generalizing across data gaps
between the Global North and Global South. Our findings reveal substantial
performance drops in out-of-distribution regions, depending on model and
geographic area. While expanding the training domain generally improves
generalization, it is insufficient to overcome shifts between geographically
distinct regions. We show that addressing these shifts through, for example,
data alignment can improve spatial generalization. Our work advances the global
applicability of downscaling methods and represents a step toward reducing
inequities in access to high-resolution climate information.

</details>


### [206] [ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding](https://arxiv.org/abs/2507.04943)
*Jianjiang Yang,Ziyan Huang,Yanshu Li*

Main category: cs.CV

TL;DR: ReLoop is a closed-loop training framework for MLLMs to reduce hallucinations by enforcing multimodal consistency through semantic, visual, and attention feedback mechanisms.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from hallucinations (incorrect outputs), lacking internal validation during training. Existing methods rely on external fixes, not direct training mechanisms.

Method: ReLoop uses a ring-shaped structure with three consistency feedback mechanisms: semantic reconstruction, visual description, and attention supervision.

Result: ReLoop reduces hallucination rates in MLLMs across benchmarks, proving effective for hallucination mitigation.

Conclusion: ReLoop provides a robust solution for hallucination reduction in MLLMs, with plans to release code and data.

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable
progress in open-ended visual question answering, they remain vulnerable to
hallucinations. These are outputs that contradict or misrepresent input
semantics, posing a critical challenge to the reliability and factual
consistency. Existing methods often rely on external verification or post-hoc
correction, lacking an internal mechanism to validate outputs directly during
training. To bridge this gap, we propose ReLoop, a unified closed-loop training
framework that encourages multimodal consistency for cross-modal understanding
in MLLMs. ReLoop adopts a ring-shaped structure that integrates three
complementary consistency feedback mechanisms, obliging MLLMs to "seeing twice
and thinking backwards". Specifically, ReLoop employs the frozen Consistency
Feedback Plugin (CFP), comprising semantic reconstruction, visual description,
and an attention supervision module for attention alignment. These components
collectively enforce semantic reversibility, visual consistency, and
interpretable attention, enabling the model to correct its outputs during
training. Extensive evaluations and analyses demonstrate the effectiveness of
ReLoop in reducing hallucination rates across multiple benchmarks, establishing
a robust method for hallucination mitigation in MLLMs. We will release our
source code and data in the camera-ready version.

</details>


### [207] [Taming the Tri-Space Tension: ARC-Guided Hallucination Modeling and Control for Text-to-Image Generation](https://arxiv.org/abs/2507.04946)
*Jianjiang Yang,Ziyan Huang*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in text-to-image diffusion models by proposing a cognitive framework (Hallucination Tri-Space) and a dynamic controller (TM-ARC) to reduce misalignment without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: Persistent hallucinations in text-to-image models reveal structured misalignments, prompting a need for deeper understanding and mitigation.

Method: Introduces the Hallucination Tri-Space (semantic coherence, structural alignment, knowledge grounding) and Alignment Risk Code (ARC) to quantify misalignment. Develops TM-ARC, a latent-space controller for targeted interventions.

Result: TM-ARC significantly reduces hallucinations while maintaining image quality and diversity.

Conclusion: The framework provides a unified, interpretable solution for mitigating generative failures in diffusion models.

Abstract: Despite remarkable progress in image quality and prompt fidelity,
text-to-image (T2I) diffusion models continue to exhibit persistent
"hallucinations", where generated content subtly or significantly diverges from
the intended prompt semantics. While often regarded as unpredictable artifacts,
we argue that these failures reflect deeper, structured misalignments within
the generative process. In this work, we propose a cognitively inspired
perspective that reinterprets hallucinations as trajectory drift within a
latent alignment space. Empirical observations reveal that generation unfolds
within a multiaxial cognitive tension field, where the model must continuously
negotiate competing demands across three key critical axes: semantic coherence,
structural alignment, and knowledge grounding. We then formalize this
three-axis space as the \textbf{Hallucination Tri-Space} and introduce the
Alignment Risk Code (ARC): a dynamic vector representation that quantifies
real-time alignment tension during generation. The magnitude of ARC captures
overall misalignment, its direction identifies the dominant failure axis, and
its imbalance reflects tension asymmetry. Based on this formulation, we develop
the TensionModulator (TM-ARC): a lightweight controller that operates entirely
in latent space. TM-ARC monitors ARC signals and applies targeted,
axis-specific interventions during the sampling process. Extensive experiments
on standard T2I benchmarks demonstrate that our approach significantly reduces
hallucination without compromising image quality or diversity. This framework
offers a unified and interpretable approach for understanding and mitigating
generative failures in diffusion-based T2I systems.

</details>


### [208] [DC-AR: Efficient Masked Autoregressive Image Generation with Deep Compression Hybrid Tokenizer](https://arxiv.org/abs/2507.04947)
*Yecheng Wu,Junyu Chen,Zhuoyang Zhang,Enze Xie,Jincheng Yu,Junsong Chen,Jinyi Hu,Yao Lu,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AR is a masked autoregressive framework for text-to-image generation, combining a deep compression hybrid tokenizer (DC-HT) with a hybrid masked autoregressive approach to achieve high-quality, efficient image generation.


<details>
  <summary>Details</summary>
Motivation: Prior masked AR models were limited by tokenizers, lagging behind diffusion models in quality or efficiency. DC-AR aims to overcome this by introducing DC-HT and a hybrid framework.

Method: DC-AR uses DC-HT for 32x spatial compression and high fidelity, then extends MaskGIT with a hybrid approach: discrete tokens for structure and residual tokens for refinements.

Result: State-of-the-art results: gFID of 5.49 on MJHQ-30K, GenEval score of 0.69, with 1.5-7.9x higher throughput and 2.0-3.5x lower latency than competitors.

Conclusion: DC-AR sets a new benchmark for text-to-image generation, balancing quality and efficiency through innovative tokenization and hybrid modeling.

Abstract: We introduce DC-AR, a novel masked autoregressive (AR) text-to-image
generation framework that delivers superior image generation quality with
exceptional computational efficiency. Due to the tokenizers' limitations, prior
masked AR models have lagged behind diffusion models in terms of quality or
efficiency. We overcome this limitation by introducing DC-HT - a deep
compression hybrid tokenizer for AR models that achieves a 32x spatial
compression ratio while maintaining high reconstruction fidelity and
cross-resolution generalization ability. Building upon DC-HT, we extend MaskGIT
and create a new hybrid masked autoregressive image generation framework that
first produces the structural elements through discrete tokens and then applies
refinements via residual tokens. DC-AR achieves state-of-the-art results with a
gFID of 5.49 on MJHQ-30K and an overall score of 0.69 on GenEval, while
offering 1.5-7.9x higher throughput and 2.0-3.5x lower latency compared to
prior leading diffusion and autoregressive models.

</details>


### [209] [Boosting Temporal Sentence Grounding via Causal Inference](https://arxiv.org/abs/2507.04958)
*Kefan Tang,Lihuo He,Jisheng Dang,Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes a causal inference-based framework (CICR) for Temporal Sentence Grounding (TSG) to address spurious correlations in video-text data, improving robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing TSG methods overlook spurious correlations caused by textual biases and video overfitting, leading to unreliable predictions.

Method: The framework uses causal intervention (do-calculus) for textual biases and counterfactual reasoning for video features to eliminate spurious correlations.

Result: Experiments show the method outperforms existing approaches on public datasets.

Conclusion: The proposed CICR framework effectively mitigates spurious correlations, enhancing TSG performance and generalization.

Abstract: Temporal Sentence Grounding (TSG) aims to identify relevant moments in an
untrimmed video that semantically correspond to a given textual query. Despite
existing studies having made substantial progress, they often overlook the
issue of spurious correlations between video and textual queries. These
spurious correlations arise from two primary factors: (1) inherent biases in
the textual data, such as frequent co-occurrences of specific verbs or phrases,
and (2) the model's tendency to overfit to salient or repetitive patterns in
video content. Such biases mislead the model into associating textual cues with
incorrect visual moments, resulting in unreliable predictions and poor
generalization to out-of-distribution examples. To overcome these limitations,
we propose a novel TSG framework, causal intervention and counterfactual
reasoning that utilizes causal inference to eliminate spurious correlations and
enhance the model's robustness. Specifically, we first formulate the TSG task
from a causal perspective with a structural causal model. Then, to address
unobserved confounders reflecting textual biases toward specific verbs or
phrases, a textual causal intervention is proposed, utilizing do-calculus to
estimate the causal effects. Furthermore, visual counterfactual reasoning is
performed by constructing a counterfactual scenario that focuses solely on
video features, excluding the query and fused multi-modal features. This allows
us to debias the model by isolating and removing the influence of the video
from the overall effect. Experiments on public datasets demonstrate the
superiority of the proposed method. The code is available at
https://github.com/Tangkfan/CICR.

</details>


### [210] [Hear-Your-Click: Interactive Video-to-Audio Generation via Object-aware Contrastive Audio-Visual Fine-tuning](https://arxiv.org/abs/2507.04959)
*Yingshan Liang,Keyu Fan,Zhicheng Du,Yiran Wang,Qingyang Shi,Xinyu Zhang,Jiasheng Lu,Peiwu Qin*

Main category: cs.CV

TL;DR: Hear-Your-Click is an interactive V2A framework for generating sounds for specific video objects by clicking, using object-aware techniques and tailored data augmentation.


<details>
  <summary>Details</summary>
Motivation: Current V2A methods lack precision in complex scenes and fail to target specific objects.

Method: Proposes Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with Mask-guided Visual Encoder (MVE) and data augmentation (RVS, MLM).

Result: Improved precision and performance in audio generation, validated by a new CAV score metric.

Conclusion: The framework enhances control and quality in V2A generation for specific objects.

Abstract: Video-to-audio (V2A) generation shows great potential in fields such as film
production. Despite significant advances, current V2A methods, which rely on
global video information, struggle with complex scenes and often fail to
generate audio tailored to specific objects or regions in the videos. To
address these limitations, we introduce Hear-Your-Click, an interactive V2A
framework that enables users to generate sounds for specific objects in the
videos by simply clicking on the frame. To achieve this, we propose
Object-aware Contrastive Audio-Visual Fine-tuning (OCAV) with a Mask-guided
Visual Encoder (MVE) to obtain object-level visual features aligned with
corresponding audio segments. Furthermore, we tailor two data augmentation
strategies: Random Video Stitching (RVS) and Mask-guided Loudness Modulation
(MLM), aimed at enhancing the model's sensitivity to the segmented objects. To
effectively measure the audio-visual correspondence, we design a new evaluation
metric, the CAV score, for evaluation. Extensive experiments demonstrate that
our framework offers more precise control and improved generation performance
across various metrics. Project Page:
https://github.com/SynapGrid/Hear-Your-Click

</details>


### [211] [InterGSEdit: Interactive 3D Gaussian Splatting Editing with 3D Geometry-Consistent Attention Prior](https://arxiv.org/abs/2507.04961)
*Minghao Wen,Shengjie Wu,Kangkan Wang,Dong Liang*

Main category: cs.CV

TL;DR: InterGSEdit introduces an interactive framework for 3D Gaussian Splatting editing, addressing inconsistency and lack of user control by leveraging CLIP-based semantic consistency and adaptive attention fusion.


<details>
  <summary>Details</summary>
Motivation: Existing 3D editing methods suffer from local inconsistency and rigid text-prompt reliance, limiting flexibility and quality.

Method: Uses CLIP-based Semantic Consistency Selection (CSCS) to select reference views, constructs 3D Geometry-Consistent Attention Prior (GAP3D), and fuses attention via an Attention Fusion Network (AFN).

Result: Achieves state-of-the-art performance with consistent, high-fidelity 3D editing and improved user control.

Conclusion: InterGSEdit effectively balances geometric consistency and fine-grained features, enhancing 3DGS editing quality and usability.

Abstract: 3D Gaussian Splatting based 3D editing has demonstrated impressive
performance in recent years. However, the multi-view editing often exhibits
significant local inconsistency, especially in areas of non-rigid deformation,
which lead to local artifacts, texture blurring, or semantic variations in
edited 3D scenes. We also found that the existing editing methods, which rely
entirely on text prompts make the editing process a "one-shot deal", making it
difficult for users to control the editing degree flexibly. In response to
these challenges, we present InterGSEdit, a novel framework for high-quality
3DGS editing via interactively selecting key views with users' preferences. We
propose a CLIP-based Semantic Consistency Selection (CSCS) strategy to
adaptively screen a group of semantically consistent reference views for each
user-selected key view. Then, the cross-attention maps derived from the
reference views are used in a weighted Gaussian Splatting unprojection to
construct the 3D Geometry-Consistent Attention Prior ($GAP^{3D}$). We project
$GAP^{3D}$ to obtain 3D-constrained attention, which are fused with 2D
cross-attention via Attention Fusion Network (AFN). AFN employs an adaptive
attention strategy that prioritizes 3D-constrained attention for geometric
consistency during early inference, and gradually prioritizes 2D
cross-attention maps in diffusion for fine-grained features during the later
inference. Extensive experiments demonstrate that InterGSEdit achieves
state-of-the-art performance, delivering consistent, high-fidelity 3DGS editing
with improved user experience.

</details>


### [212] [Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models](https://arxiv.org/abs/2507.04976)
*Eunseop Yoon,Hee Suk Yoon,Mark A. Hasegawa-Johnson,Chang D. Yoo*

Main category: cs.CV

TL;DR: The paper addresses the limitation of Video-LLMs in rejecting irrelevant questions by proposing a framework for alignment for answerability, along with an evaluation framework and dataset pipeline.


<details>
  <summary>Details</summary>
Motivation: Current Video-LLMs struggle with questions beyond video content, lacking training to assess question relevance.

Method: Proposes alignment for answerability, an evaluation framework, and a dataset pipeline leveraging existing video-description pairs.

Result: Demonstrates that even top Video-LLMs fail to reject unfit questions, highlighting the need for the proposed framework.

Conclusion: The alignment for answerability framework enhances Video-LLMs' ability to evaluate and decline irrelevant questions.

Abstract: In the broader context of deep learning, Multimodal Large Language Models
have achieved significant breakthroughs by leveraging powerful Large Language
Models as a backbone to align different modalities into the language space. A
prime exemplification is the development of Video Large Language Models
(Video-LLMs). While numerous advancements have been proposed to enhance the
video understanding capabilities of these models, they are predominantly
trained on questions generated directly from video content. However, in
real-world scenarios, users often pose questions that extend beyond the
informational scope of the video, highlighting the need for Video-LLMs to
assess the relevance of the question. We demonstrate that even the
best-performing Video-LLMs fail to reject unfit questions-not necessarily due
to a lack of video understanding, but because they have not been trained to
identify and refuse such questions. To address this limitation, we propose
alignment for answerability, a framework that equips Video-LLMs with the
ability to evaluate the relevance of a question based on the input video and
appropriately decline to answer when the question exceeds the scope of the
video, as well as an evaluation framework with a comprehensive set of metrics
designed to measure model behavior before and after alignment. Furthermore, we
present a pipeline for creating a dataset specifically tailored for alignment
for answerability, leveraging existing video-description paired datasets.

</details>


### [213] [Parameterized Diffusion Optimization enabled Autoregressive Ordinal Regression for Diabetic Retinopathy Grading](https://arxiv.org/abs/2507.04978)
*Qinkai Yu,Wei Zhou,Hantao Liu,Yanyu Xu,Meng Wang,Yitian Zhao,Huazhu Fu,Xujiong Ye,Yalin Zheng,Yanda Meng*

Main category: cs.CV

TL;DR: A novel autoregressive ordinal regression method (AOR-DR) is proposed to improve diabetic retinopathy (DR) severity grading by addressing uneven distribution and category ambiguity, leveraging clinical knowledge and diffusion processes.


<details>
  <summary>Details</summary>
Motivation: Accurate DR severity evaluation is crucial for timely care, but challenges like uneven severity distribution and ambiguous category boundaries hinder performance.

Method: AOR-DR decomposes DR grading into ordered steps, fusing previous predictions with image features, and uses diffusion for conditional probability modeling.

Result: Outperforms six state-of-the-art methods on four large-scale datasets.

Conclusion: AOR-DR effectively addresses DR grading challenges, offering superior performance and leveraging pre-trained models.

Abstract: As a long-term complication of diabetes, diabetic retinopathy (DR) progresses
slowly, potentially taking years to threaten vision. An accurate and robust
evaluation of its severity is vital to ensure prompt management and care.
Ordinal regression leverages the underlying inherent order between categories
to achieve superior performance beyond traditional classification. However,
there exist challenges leading to lower DR classification performance: 1) The
uneven distribution of DR severity levels, characterized by a long-tailed
pattern, adds complexity to the grading process. 2)The ambiguity in defining
category boundaries introduces additional challenges, making the classification
process more complex and prone to inconsistencies. This work proposes a novel
autoregressive ordinal regression method called AOR-DR to address the above
challenges by leveraging the clinical knowledge of inherent ordinal information
in DR grading dataset settings. Specifically, we decompose the DR grading task
into a series of ordered steps by fusing the prediction of the previous steps
with extracted image features as conditions for the current prediction step.
Additionally, we exploit the diffusion process to facilitate conditional
probability modeling, enabling the direct use of continuous global image
features for autoregression without relearning contextual information from
patch-level features. This ensures the effectiveness of the autoregressive
process and leverages the capabilities of pre-trained large-scale foundation
models. Extensive experiments were conducted on four large-scale publicly
available color fundus datasets, demonstrating our model's effectiveness and
superior performance over six recent state-of-the-art ordinal regression
methods. The implementation code is available at
https://github.com/Qinkaiyu/AOR-DR.

</details>


### [214] [TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation](https://arxiv.org/abs/2507.04984)
*Zonglin Lyu,Chen Chen*

Main category: cs.CV

TL;DR: TLB-VFI proposes an efficient video-based diffusion model for Video Frame Interpolation, improving performance and reducing parameters and training data needs.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in image-based and video-based diffusion models for VFI, balancing temporal information extraction and computational efficiency.

Method: Uses 3D-wavelet gating and temporal-aware autoencoder for temporal information, combined with optical flow guidance.

Result: Achieves 20% FID improvement, 3x fewer parameters, 2.3x speedup, and 9000x less training data than SOTA.

Conclusion: TLB-VFI efficiently bridges the gap between performance and computational cost in VFI.

Abstract: Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$
(we use n to denote time in videos to avoid notation overload with the timestep
$t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and
$I_1$. Recent approaches apply diffusion models (both image-based and
video-based) in this task and achieve strong performance. However, image-based
diffusion models are unable to extract temporal information and are relatively
inefficient compared to non-diffusion methods. Video-based diffusion models can
extract temporal information, but they are too large in terms of training
scale, model size, and inference time. To mitigate the above issues, we propose
Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation
(TLB-VFI), an efficient video-based diffusion model. By extracting rich
temporal information from video inputs through our proposed 3D-wavelet gating
and temporal-aware autoencoder, our method achieves 20% improvement in FID on
the most challenging datasets over recent SOTA of image-based diffusion models.
Meanwhile, due to the existence of rich temporal information, our method
achieves strong performance while having 3times fewer parameters. Such a
parameter reduction results in 2.3x speed up. By incorporating optical flow
guidance, our method requires 9000x less training data and achieves over 20x
fewer parameters than video-based diffusion models. Codes and results are
available at our project page: https://zonglinl.github.io/tlbvfi_page.

</details>


### [215] [AI for the Routine, Humans for the Complex: Accuracy-Driven Data Labelling with Mixed Integer Linear Programming](https://arxiv.org/abs/2507.04990)
*Mohammad Hossein Amini,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.CV

TL;DR: OPAL is a human-assisted labeling method using MILP to minimize manual effort while achieving near-perfect label accuracy, outperforming baselines in testing vision systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of label scarcity in deep learning, especially the need for high-accuracy labels in testing, despite tolerance for noise in training.

Method: OPAL employs a mixed-integer linear programming (MILP) formulation to minimize labeling effort for a specified accuracy target, evaluated on vision tasks.

Result: OPAL achieves 98.8% accuracy, reduces manual labeling by over 50%, and outperforms baselines in labeling and validation tasks.

Conclusion: OPAL effectively balances accuracy and manual effort, with further improvements possible through active learning.

Abstract: The scarcity of accurately labelled data remains a major challenge in deep
learning (DL). Many DL approaches rely on semi-supervised methods, which focus
on constructing large datasets that require only a minimal amount of
human-labelled data. Since DL training algorithms can tolerate moderate label
noise, it has generally been acceptable for the accuracy of labels in large
training datasets to fall well short of a perfect 100%. However, when it comes
to testing DL models, achieving high label accuracy-as close to 100% as
possible-is paramount for reliable verification. In this article, we introduce
OPAL, a human-assisted labelling method that can be configured to target a
desired accuracy level while minimizing the manual effort required for
labelling. The main contribution of OPAL is a mixed-integer linear programming
(MILP) formulation that minimizes labelling effort subject to a specified
accuracy target. We evaluate OPAL for two tasks in the context of testing
vision systems: automatic labelling of test data and automated validation of
test data. Our evaluation, based on more than 2500 experiments performed on
seven datasets, comparing OPAL with eight baseline methods, shows that OPAL,
relying on its MILP formulation, achieves an average accuracy of 98.8%, just
1.2% below perfect accuracy, while cutting manual labelling by more than half.
Further, OPAL significantly outperforms automated labelling baselines in
labelling accuracy across all seven datasets, with large effect sizes, when all
methods are provided with the same manual-labelling budget. For automated
test-input validation, on average, OPAL reduces manual effort by 28.8% while
achieving 4.5% higher accuracy than the SOTA validation baselines. Finally, we
show that augmenting OPAL with an active learning loop leads to an additional
4.5% reduction in required manual labelling, without compromising accuracy.

</details>


### [216] [Robust Incomplete-Modality Alignment for Ophthalmic Disease Grading and Diagnosis via Labeled Optimal Transport](https://arxiv.org/abs/2507.04999)
*Qinkai Yu,Jianyang Xie,Yitian Zhao,Cheng Chen,Lijun Zhang,Liming Chen,Jun Cheng,Lu Liu,Yalin Zheng,Yanda Meng*

Main category: cs.CV

TL;DR: A novel multimodal alignment and fusion framework improves ophthalmic diagnostics by robustly handling missing OCT or fundus image data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Uneven healthcare resource distribution leads to incomplete multimodal data, reducing diagnostic accuracy. Current methods (imputation, distillation) fail to address key challenges like lesion feature reconstruction and reliance on fully paired data.

Method: Proposes a framework using Optimal Transport for multi-scale feature alignment (class-wise and feature-wise) and asymmetric fusion to leverage distinct OCT and fundus image characteristics.

Result: Achieves state-of-the-art performance on three datasets, even with incomplete modalities.

Conclusion: The framework effectively addresses modality incompleteness, enhancing diagnostic robustness in real-world clinical settings.

Abstract: Multimodal ophthalmic imaging-based diagnosis integrates color fundus image
with optical coherence tomography (OCT) to provide a comprehensive view of
ocular pathologies. However, the uneven global distribution of healthcare
resources often results in real-world clinical scenarios encountering
incomplete multimodal data, which significantly compromises diagnostic
accuracy. Existing commonly used pipelines, such as modality imputation and
distillation methods, face notable limitations: 1)Imputation methods struggle
with accurately reconstructing key lesion features, since OCT lesions are
localized, while fundus images vary in style. 2)distillation methods rely
heavily on fully paired multimodal training data. To address these challenges,
we propose a novel multimodal alignment and fusion framework capable of
robustly handling missing modalities in the task of ophthalmic diagnostics. By
considering the distinctive feature characteristics of OCT and fundus images,
we emphasize the alignment of semantic features within the same category and
explicitly learn soft matching between modalities, allowing the missing
modality to utilize existing modality information, achieving robust cross-modal
feature alignment under the missing modality. Specifically, we leverage the
Optimal Transport for multi-scale modality feature alignment: class-wise
alignment through predicted class prototypes and feature-wise alignment via
cross-modal shared feature transport. Furthermore, we propose an asymmetric
fusion strategy that effectively exploits the distinct characteristics of OCT
and fundus modalities. Extensive evaluations on three large ophthalmic
multimodal datasets demonstrate our model's superior performance under various
modality-incomplete scenarios, achieving Sota performance in both complete
modality and inter-modality incompleteness conditions. Code is available at
https://github.com/Qinkaiyu/RIMA

</details>


### [217] [Multi-modal Representations for Fine-grained Multi-label Critical View of Safety Recognition](https://arxiv.org/abs/2507.05007)
*Britty Baby,Vinkle Srivastav,Pooja P. Jain,Kun Yuan,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: CVS-AdaptNet improves CVS recognition in laparoscopic cholecystectomy by using a multi-label, multi-modal framework with text prompts, outperforming image-only methods.


<details>
  <summary>Details</summary>
Motivation: Assessing Critical View of Safety (CVS) is complex, and traditional vision-only models rely on costly annotations. This study explores text as a tool for training and inference in multi-modal models.

Method: Proposes CVS-AdaptNet, a multi-label adaptation strategy aligning image embeddings with textual descriptions of CVS criteria using prompts. Evaluated on Endoscapes-CVS201 dataset.

Result: CVS-AdaptNet achieves 57.6 mAP, a 6-point improvement over ResNet50 baseline (51.5 mAP).

Conclusion: Text-enhanced multi-modal frameworks show promise for specialized surgical tasks, though further work is needed to match spatial annotation-based methods.

Abstract: The Critical View of Safety (CVS) is crucial for safe laparoscopic
cholecystectomy, yet assessing CVS criteria remains a complex and challenging
task, even for experts. Traditional models for CVS recognition depend on
vision-only models learning with costly, labor-intensive spatial annotations.
This study investigates how text can be harnessed as a powerful tool for both
training and inference in multi-modal surgical foundation models to automate
CVS recognition. Unlike many existing multi-modal models, which are primarily
adapted for multi-class classification, CVS recognition requires a multi-label
framework. Zero-shot evaluation of existing multi-modal surgical models shows a
significant performance gap for this task. To address this, we propose
CVS-AdaptNet, a multi-label adaptation strategy that enhances fine-grained,
binary classification across multiple labels by aligning image embeddings with
textual descriptions of each CVS criterion using positive and negative prompts.
By adapting PeskaVLP, a state-of-the-art surgical foundation model, on the
Endoscapes-CVS201 dataset, CVS-AdaptNet achieves 57.6 mAP, improving over the
ResNet50 image-only baseline (51.5 mAP) by 6 points. Our results show that
CVS-AdaptNet's multi-label, multi-modal framework, enhanced by textual prompts,
boosts CVS recognition over image-only methods. We also propose text-specific
inference methods, that helps in analysing the image-text alignment. While
further work is needed to match state-of-the-art spatial annotation-based
methods, this approach highlights the potential of adapting generalist models
to specialized surgical tasks. Code:
https://github.com/CAMMA-public/CVS-AdaptNet

</details>


### [218] [Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision](https://arxiv.org/abs/2507.05020)
*Soham Walimbe,Britty Baby,Vinkle Srivastav,Nicolas Padoy*

Main category: cs.CV

TL;DR: MML-SurgAdapt is a unified multi-task framework using Vision-Language Models (VLMs) and Single Positive Multi-Label (SPML) learning to handle diverse surgical tasks with partial annotations, reducing labeling burden by 23%.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical AI models lack flexibility, requiring separate models for each task. MML-SurgAdapt addresses this by unifying tasks and handling noisy annotations.

Method: The framework integrates CLIP-based VLMs and SPML learning to train on multiple surgical tasks with incomplete annotations, using custom prompts.

Result: MML-SurgAdapt matches task-specific benchmarks and outperforms existing SPML frameworks, reducing annotation burden.

Conclusion: The approach is scalable, efficient, and generalizable, offering a novel solution for multi-task learning in surgical computer vision.

Abstract: Surgical AI often involves multiple tasks within a single procedure, like
phase recognition or assessing the Critical View of Safety in laparoscopic
cholecystectomy. Traditional models, built for one task at a time, lack
flexibility, requiring a separate model for each. To address this, we introduce
MML-SurgAdapt, a unified multi-task framework with Vision-Language Models
(VLMs), specifically CLIP, to handle diverse surgical tasks through natural
language supervision. A key challenge in multi-task learning is the presence of
partial annotations when integrating different tasks. To overcome this, we
employ Single Positive Multi-Label (SPML) learning, which traditionally reduces
annotation burden by training models with only one positive label per instance.
Our framework extends this approach to integrate data from multiple surgical
tasks within a single procedure, enabling effective learning despite incomplete
or noisy annotations. We demonstrate the effectiveness of our model on a
combined dataset consisting of Cholec80, Endoscapes2023, and CholecT50,
utilizing custom prompts. Extensive evaluation shows that MML-SurgAdapt
performs comparably to task-specific benchmarks, with the added advantage of
handling noisy annotations. It also outperforms the existing SPML frameworks
for the task. By reducing the required labels by 23%, our approach proposes a
more scalable and efficient labeling process, significantly easing the
annotation burden on clinicians. To our knowledge, this is the first
application of SPML to integrate data from multiple surgical tasks, presenting
a novel and generalizable solution for multi-task learning in surgical computer
vision. Implementation is available at:
https://github.com/CAMMA-public/MML-SurgAdapt

</details>


### [219] [Estimating Object Physical Properties from RGB-D Vision and Depth Robot Sensors Using Deep Learning](https://arxiv.org/abs/2507.05029)
*Ricardo Cardoso,Plinio Moreno*

Main category: cs.CV

TL;DR: A novel method combining sparse point-cloud data and RGB images to estimate object mass, outperforming benchmarks by using synthetic data for training.


<details>
  <summary>Details</summary>
Motivation: Accurate mass estimation from vision sensors is underexplored but crucial for robotic tasks like grasping and manipulation.

Method: Uses sparse point-cloud data and RGB images, trains on synthetic data (ShapeNetSem models), and employs a depth estimator for dense depth maps.

Result: Significantly outperforms existing benchmarks in mass estimation.

Conclusion: The approach enhances robotic performance by improving mass estimation using vision sensors, with tools and datasets publicly available.

Abstract: Inertial mass plays a crucial role in robotic applications such as object
grasping, manipulation, and simulation, providing a strong prior for planning
and control. Accurately estimating an object's mass before interaction can
significantly enhance the performance of various robotic tasks. However, mass
estimation using only vision sensors is a relatively underexplored area. This
paper proposes a novel approach combining sparse point-cloud data from depth
images with RGB images to estimate the mass of objects. We evaluate a range of
point-cloud processing architectures, alongside RGB-only methods. To overcome
the limited availability of training data, we create a synthetic dataset using
ShapeNetSem 3D models, simulating RGBD images via a Kinect camera. This
synthetic data is used to train an image generation model for estimating dense
depth maps, which we then use to augment an existing dataset of images paired
with mass values. Our approach significantly outperforms existing benchmarks
across all evaluated metrics. The data generation
(https://github.com/RavineWindteer/ShapenetSem-to-RGBD) as well as the training
of the depth estimator (https://github.com/RavineWindteer/GLPDepth-Edited) and
the mass estimator (https://github.com/RavineWindteer/Depth-mass-estimator) are
available online.

</details>


### [220] [INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling](https://arxiv.org/abs/2507.05056)
*Xin Dong,Shichao Dong,Jin Wang,Jing Huang,Li Zhou,Zenghui Sun,Lihua Jing,Jingsong Lan,Xiaoyong Zhu,Bo Zheng*

Main category: cs.CV

TL;DR: The paper addresses hallucinations in large vision-language models (LVLMs) by proposing INTER, a training-free algorithm that leverages multimodal interaction information to reduce inconsistencies in responses.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LVLMs create plausible but inconsistent responses, unlike human cognition which effectively uses multimodal interactions. The study aims to bridge this gap.

Method: The authors analyze LVLMs' cognitive behavior and propose INTER, a novel algorithm that guides LVLMs to reuse multimodal interaction information during response generation.

Result: INTER improves performance by up to 3.4% on six benchmarks, including VQA and image captioning, compared to state-of-the-art methods.

Conclusion: INTER effectively reduces hallucinations in LVLMs without additional training data, demonstrating its potential for real-world applications.

Abstract: Hallucinations in large vision-language models (LVLMs) pose significant
challenges for real-world applications, as LVLMs may generate responses that
appear plausible yet remain inconsistent with the associated visual content.
This issue rarely occurs in human cognition. We argue that this discrepancy
arises from humans' ability to effectively leverage multimodal interaction
information in data samples. Specifically, humans typically first gather
multimodal information, analyze the interactions across modalities for
understanding, and then express their understanding through language. Motivated
by this observation, we conduct extensive experiments on popular LVLMs and
obtained insights that surprisingly reveal human-like, though less pronounced,
cognitive behavior of LVLMs on multimodal samples. Building on these findings,
we further propose \textbf{INTER}: \textbf{Inter}action Guidance Sampling, a
novel training-free algorithm that mitigate hallucinations without requiring
additional data. Specifically, INTER explicitly guides LVLMs to effectively
reapply their understanding of multimodal interaction information when
generating responses, thereby reducing potential hallucinations. On six
benchmarks including VQA and image captioning tasks, INTER achieves an average
improvement of up to 3.4\% on five LVLMs compared to the state-of-the-art
decoding strategy. The code will be released when the paper is accepted.

</details>


### [221] [AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics](https://arxiv.org/abs/2507.05063)
*Jan Carreras Boada,Rao Muhammad Umer,Carsten Marr*

Main category: cs.CV

TL;DR: Synthetic images generated with a fine-tuned stable diffusion model improve classifier performance for imbalanced biomedical datasets, enhancing accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Biomedical datasets often suffer from sample imbalance and privacy constraints, limiting machine learning model accuracy. Synthetic images can address these issues.

Method: Used a fine-tuned stable diffusion model with LoRA weights, guided by few-shot real samples, to generate synthetic white blood cell images. Tested with ResNet and CLIP-based classifiers.

Result: Accuracy improved from 27.3% to 78.4% for ResNet and from 61.8% to 76.8% for CLIP when adding synthetic images.

Conclusion: Synthetic images are a valuable tool for overcoming dataset limitations, improving model generalization, and aiding medical diagnosis.

Abstract: Biomedical datasets often contain a large sample imbalance and are subject to
strict privacy constraints, which together hinder the development of accurate
machine learning models. One potential solution is to generate synthetic
images, as this can improve data availability while preserving patient privacy.
However, it remains difficult to generate synthetic images of sufficient
quality for training robust classifiers. In this work, we focus on the
classification of single white blood cells, a key component in the diagnosis of
hematological diseases such as acute myeloid leukemia (AML), a severe blood
cancer. We demonstrate how synthetic images generated with a fine-tuned stable
diffusion model using LoRA weights when guided by real few-shot samples of the
target white blood cell classes, can enhance classifier performance for limited
data. When training a ResNet classifier, accuracy increased from 27.3\% to
78.4\% (+51.1\%) by adding 5000 synthetic images per class to a small and
highly imbalanced real dataset. For a CLIP-based classifier, the accuracy
improved from 61.8\% to 76.8\% (+15.0\%). The synthetic images are highly
similar to real images, and they can help overcome dataset limitations,
enhancing model generalization. Our results establish synthetic images as a
tool in biomedical research, improving machine learning models, and
facilitating medical diagnosis and research.

</details>


### [222] [ICAS: Detecting Training Data from Autoregressive Image Generative Models](https://arxiv.org/abs/2507.05068)
*Hongyao Yu,Yixiang Qiu,Yiheng Yang,Hao Fang,Tianqu Zhuang,Jiaxin Hong,Bin Chen,Hao Wu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: The paper introduces a membership inference method for autoregressive image generation models to detect unauthorized training data usage, demonstrating superior performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Address concerns about data privacy and copyright in autoregressive image generation by detecting unauthorized training data usage.

Method: Uses implicit token-wise classification scores and an adaptive score aggregation strategy to identify training data.

Result: The method outperforms existing algorithms, shows robustness, and reveals vulnerabilities in large foundation models.

Conclusion: The approach effectively detects training data in autoregressive models, highlighting risks in large-scale models and specific autoregressive paradigms.

Abstract: Autoregressive image generation has witnessed rapid advancements, with
prominent models such as scale-wise visual auto-regression pushing the
boundaries of visual synthesis. However, these developments also raise
significant concerns regarding data privacy and copyright. In response,
training data detection has emerged as a critical task for identifying
unauthorized data usage in model training. To better understand the
vulnerability of autoregressive image generative models to such detection, we
conduct the first study applying membership inference to this domain. Our
approach comprises two key components: implicit classification and an adaptive
score aggregation strategy. First, we compute the implicit token-wise
classification score within the query image. Then we propose an adaptive score
aggregation strategy to acquire a final score, which places greater emphasis on
the tokens with lower scores. A higher final score indicates that the sample is
more likely to be involved in the training set. To validate the effectiveness
of our method, we adapt existing detection algorithms originally designed for
LLMs to visual autoregressive models. Extensive experiments demonstrate the
superiority of our method in both class-conditional and text-to-image
scenarios. Moreover, our approach exhibits strong robustness and generalization
under various data transformations. Furthermore, sufficient experiments suggest
two novel key findings: (1) A linear scaling law on membership inference,
exposing the vulnerability of large foundation models. (2) Training data from
scale-wise visual autoregressive models is easier to detect than other
autoregressive paradigms.Our code is available at
https://github.com/Chrisqcwx/ImageAR-MIA.

</details>


### [223] [MoDiT: Learning Highly Consistent 3D Motion Coefficients with Diffusion Transformer for Talking Head Generation](https://arxiv.org/abs/2507.05092)
*Yucheng Wang,Dan Xu*

Main category: cs.CV

TL;DR: MoDiT combines 3DMM with a Diffusion-based Transformer to improve audio-driven talking head generation by addressing temporal jittering, identity drift, and unnatural blinking.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing methods (GANs/UNet-based diffusion models) for generating consistent and realistic facial animations in applications like virtual assistants and films.

Method: Proposes MoDiT: hierarchical denoising with revised temporal attention, 3DMM integration for spatial constraints, and refined blinking strategy.

Result: Mitigates temporal jittering, improves identity preservation, and enhances natural blinking behavior.

Conclusion: MoDiT offers a robust solution for realistic and consistent talking head generation.

Abstract: Audio-driven talking head generation is critical for applications such as
virtual assistants, video games, and films, where natural lip movements are
essential. Despite progress in this field, challenges remain in producing both
consistent and realistic facial animations. Existing methods, often based on
GANs or UNet-based diffusion models, face three major limitations: (i) temporal
jittering caused by weak temporal constraints, resulting in frame
inconsistencies; (ii) identity drift due to insufficient 3D information
extraction, leading to poor preservation of facial identity; and (iii)
unnatural blinking behavior due to inadequate modeling of realistic blink
dynamics. To address these issues, we propose MoDiT, a novel framework that
combines the 3D Morphable Model (3DMM) with a Diffusion-based Transformer. Our
contributions include: (i) A hierarchical denoising strategy with revised
temporal attention and biased self/cross-attention mechanisms, enabling the
model to refine lip synchronization and progressively enhance full-face
coherence, effectively mitigating temporal jittering. (ii) The integration of
3DMM coefficients to provide explicit spatial constraints, ensuring accurate
3D-informed optical flow prediction and improved lip synchronization using
Wav2Lip results, thereby preserving identity consistency. (iii) A refined
blinking strategy to model natural eye movements, with smoother and more
realistic blinking behaviors.

</details>


### [224] [Reviving Cultural Heritage: A Novel Approach for Comprehensive Historical Document Restoration](https://arxiv.org/abs/2507.05108)
*Yuyi Zhang,Peirong Zhang,Zhenhua Yang,Pengyu Yan,Yongxin Shi,Pengwei Liu,Fengjun Guo,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper introduces a full-page historical document restoration (HDR) dataset (FPHDR) and an automated solution (AutoHDR) to address limitations of existing methods. AutoHDR uses a three-stage workflow and achieves significant OCR accuracy improvements, especially with human-machine collaboration.


<details>
  <summary>Details</summary>
Motivation: Existing HDR methods are limited to single modalities or small-scale restoration, failing practical needs. The paper aims to bridge this gap for better cultural heritage preservation.

Method: AutoHDR employs a three-stage approach: OCR-assisted damage localization, vision-language context text prediction, and patch autoregressive appearance restoration. It supports human-machine collaboration.

Result: AutoHDR improves OCR accuracy from 46.83% to 84.05% for severely damaged documents, reaching 94.25% with human collaboration.

Conclusion: AutoHDR advances automated historical document restoration, offering a scalable and collaborative solution for cultural heritage preservation.

Abstract: Historical documents represent an invaluable cultural heritage, yet have
undergone significant degradation over time through tears, water erosion, and
oxidation. Existing Historical Document Restoration (HDR) methods primarily
focus on single modality or limited-size restoration, failing to meet practical
needs. To fill this gap, we present a full-page HDR dataset (FPHDR) and a novel
automated HDR solution (AutoHDR). Specifically, FPHDR comprises 1,633 real and
6,543 synthetic images with character-level and line-level locations, as well
as character annotations in different damage grades. AutoHDR mimics historians'
restoration workflows through a three-stage approach: OCR-assisted damage
localization, vision-language context text prediction, and patch autoregressive
appearance restoration. The modular architecture of AutoHDR enables seamless
human-machine collaboration, allowing for flexible intervention and
optimization at each restoration stage. Experiments demonstrate AutoHDR's
remarkable performance in HDR. When processing severely damaged documents, our
method improves OCR accuracy from 46.83\% to 84.05\%, with further enhancement
to 94.25\% through human-machine collaboration. We believe this work represents
a significant advancement in automated historical document restoration and
contributes substantially to cultural heritage preservation. The model and
dataset are available at https://github.com/SCUT-DLVCLab/AutoHDR.

</details>


### [225] [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116)
*Juyi Lin,Amir Taherin,Arash Akbari,Arman Akbari,Lei Lu,Guangyu Chen,Taskin Padir,Xiaomeng Yang,Weiwei Chen,Yiqian Li,Xue Lin,David Kaeli,Pu Zhao,Yanzhi Wang*

Main category: cs.CV

TL;DR: VOTE is an efficient framework for Vision Language Action (VLA) models, improving generalization and speed without additional visual components.


<details>
  <summary>Details</summary>
Motivation: Existing VLA models struggle with novel objects/environments and rely on costly additional components, prompting a need for efficient solutions.

Method: Proposes tokenizer-free fine-tuning for parallel action prediction and an ensemble voting strategy for action sampling.

Result: Achieves state-of-the-art performance with 35x faster inference and 145 Hz throughput.

Conclusion: VOTE offers a scalable, efficient solution for VLA models, with open-sourced details and code.

Abstract: Recent large-scale Vision Language Action (VLA) models have shown superior
performance in robotic manipulation tasks guided by natural language. However,
their generalization remains limited when applied to novel objects or
unfamiliar environments that lie outside the training distribution. To address
this, many existing approaches integrate additional components such as depth
estimation, segmentation, or even diffusion to improve generalization, at the
cost of adding significant computation overhead, resulting in low efficiency.
This motivates the exploration of efficient action prediction methods, which
are independent of additional high-level visual representations or diffusion
techniques. In this work, we propose VOTE, an efficient and general framework
for the optimization and acceleration of VLA models. In details, we propose a
novel tokenizer-free fine-tuning approach for parallel accurate action
prediction, which reduces computational overhead and accelerates inference
speed. Additionally, we adopt an ensemble voting strategy for the action
sampling, which significantly improves model performance and enhances
generalization. Experimental results show that our method achieves
state-of-the-art performance with 35$\times$ faster inference and 145 Hz
throughput. All the details and codes will be open-sourced.

</details>


### [226] [VERITAS: Verification and Explanation of Realness in Images for Transparency in AI Systems](https://arxiv.org/abs/2507.05146)
*Aadi Srivastava,Vignesh Natarajkumar,Utkarsh Bheemanaboyna,Devisree Akashapu,Nagraj Gaonkar,Archit Joshi*

Main category: cs.CV

TL;DR: VERITAS is a framework for detecting and explaining AI-generated small images (32x32) through artifact localization and semantic reasoning, offering human-readable explanations.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated content blurs authenticity, and existing detection methods lack transparency in decision-making.

Method: VERITAS uses artifact localization and semantic reasoning to detect and explain AI-generated images.

Result: The framework accurately detects synthetic images and provides clear, human-readable explanations.

Conclusion: VERITAS enhances transparency in synthetic image detection, addressing concerns about content authenticity.

Abstract: The widespread and rapid adoption of AI-generated content, created by models
such as Generative Adversarial Networks (GANs) and Diffusion Models, has
revolutionized the digital media landscape by allowing efficient and creative
content generation. However, these models also blur the difference between real
images and AI-generated synthetic images, raising concerns regarding content
authenticity and integrity. While many existing solutions to detect fake images
focus solely on classification and higher-resolution images, they often lack
transparency in their decision-making, making it difficult for users to
understand why an image is classified as fake. In this paper, we present
VERITAS, a comprehensive framework that not only accurately detects whether a
small (32x32) image is AI-generated but also explains why it was classified
that way through artifact localization and semantic reasoning. VERITAS produces
human-readable explanations that describe key artifacts in synthetic images. We
show that this architecture offers clear explanations of the basis of zero-shot
synthetic image detection tasks. Code and relevant prompts can be found at
https://github.com/V-i-g-n-e-s-h-N/VERITAS .

</details>


### [227] [LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains](https://arxiv.org/abs/2507.05162)
*Nicholas Chivaran,Jianbing Ni*

Main category: cs.CV

TL;DR: LAID is a framework evaluating lightweight neural networks for AI-generated image detection, showing competitive accuracy with lower computational costs.


<details>
  <summary>Details</summary>
Motivation: Address concerns about AI-generated image misuse on social media by reducing reliance on computationally intensive detection methods.

Method: Benchmarks and evaluates off-the-shelf lightweight neural networks on the GenImage dataset across spatial, spectral, and fusion domains.

Result: Lightweight models achieve competitive accuracy under adversarial conditions with lower memory and computation costs.

Conclusion: LAID provides insights into efficiency-performance trade-offs, enabling practical and scalable AI-generated image detection.

Abstract: The recent proliferation of photorealistic AI-generated images (AIGI) has
raised urgent concerns about their potential misuse, particularly on social
media platforms. Current state-of-the-art AIGI detection methods typically rely
on large, deep neural architectures, creating significant computational
barriers to real-time, large-scale deployment on platforms like social media.
To challenge this reliance on computationally intensive models, we introduce
LAID, the first framework -- to our knowledge -- that benchmarks and evaluates
the detection performance and efficiency of off-the-shelf lightweight neural
networks. In this framework, we comprehensively train and evaluate selected
models on a representative subset of the GenImage dataset across spatial,
spectral, and fusion image domains. Our results demonstrate that lightweight
models can achieve competitive accuracy, even under adversarial conditions,
while incurring substantially lower memory and computation costs compared to
current state-of-the-art methods. This study offers valuable insight into the
trade-off between efficiency and performance in AIGI detection and lays a
foundation for the development of practical, scalable, and trustworthy
detection systems. The source code of LAID can be found at:
https://github.com/nchivar/LAID.

</details>


### [228] [4DSloMo: 4D Reconstruction for High Speed Scene with Asynchronous Capture](https://arxiv.org/abs/2507.05163)
*Yutian Chen,Shi Guo,Tianshuo Yang,Lihe Ding,Xiuyuan Yu,Jinwei Gu,Tianfan Xue*

Main category: cs.CV

TL;DR: A method for high-speed 4D reconstruction using low FPS cameras via asynchronous capture and a generative model to fix artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing 4D capture systems are limited to low frame rates, causing poor results for high-speed motion.

Method: Asynchronous capture increases effective frame rate; a video-diffusion-based model fixes sparse-view artifacts.

Result: Achieves 100-200 FPS equivalent without high-speed cameras, improving reconstruction quality.

Conclusion: The proposed system enhances high-speed 4D reconstruction using low FPS cameras.

Abstract: Reconstructing fast-dynamic scenes from multi-view videos is crucial for
high-speed motion analysis and realistic 4D reconstruction. However, the
majority of 4D capture systems are limited to frame rates below 30 FPS (frames
per second), and a direct 4D reconstruction of high-speed motion from low FPS
input may lead to undesirable results. In this work, we propose a high-speed 4D
capturing system only using low FPS cameras, through novel capturing and
processing modules. On the capturing side, we propose an asynchronous capture
scheme that increases the effective frame rate by staggering the start times of
cameras. By grouping cameras and leveraging a base frame rate of 25 FPS, our
method achieves an equivalent frame rate of 100-200 FPS without requiring
specialized high-speed cameras. On processing side, we also propose a novel
generative model to fix artifacts caused by 4D sparse-view reconstruction, as
asynchrony reduces the number of viewpoints at each timestamp. Specifically, we
propose to train a video-diffusion-based artifact-fix model for sparse 4D
reconstruction, which refines missing details, maintains temporal consistency,
and improves overall reconstruction quality. Experimental results demonstrate
that our method significantly enhances high-speed 4D reconstruction compared to
synchronous capture.

</details>


### [229] [Differential Attention for Multimodal Crisis Event Analysis](https://arxiv.org/abs/2507.05165)
*Nusrat Munia,Junfeng Zhu,Olfa Nasraoui,Abdullah-Al-Zubaer Imran*

Main category: cs.CV

TL;DR: The paper explores vision-language models (VLMs) and advanced fusion strategies to improve crisis data classification, outperforming traditional methods with pretrained models and adaptive fusion techniques.


<details>
  <summary>Details</summary>
Motivation: Social networks provide valuable but noisy multimodal crisis data, posing challenges for effective information extraction and integration.

Method: Uses LLaVA-generated text for text-image alignment, CLIP-based embeddings, and Guided Cross Attention (Guided CA) with Differential Attention for multimodal fusion.

Result: Combination of pretrained VLMs, enriched text, and adaptive fusion outperforms state-of-the-art models on the CrisisMMD dataset.

Conclusion: The approach enhances classification accuracy and reliability for disaster response tasks, offering more interpretable models.

Abstract: Social networks can be a valuable source of information during crisis events.
In particular, users can post a stream of multimodal data that can be critical
for real-time humanitarian response. However, effectively extracting meaningful
information from this large and noisy data stream and effectively integrating
heterogeneous data remains a formidable challenge. In this work, we explore
vision language models (VLMs) and advanced fusion strategies to enhance the
classification of crisis data in three different tasks. We incorporate
LLaVA-generated text to improve text-image alignment. Additionally, we leverage
Contrastive Language-Image Pretraining (CLIP)-based vision and text embeddings,
which, without task-specific fine-tuning, outperform traditional models. To
further refine multimodal fusion, we employ Guided Cross Attention (Guided CA)
and combine it with the Differential Attention mechanism to enhance feature
alignment by emphasizing critical information while filtering out irrelevant
content. Our results show that while Differential Attention improves
classification performance, Guided CA remains highly effective in aligning
multimodal features. Extensive experiments on the CrisisMMD benchmark data set
demonstrate that the combination of pretrained VLMs, enriched textual
descriptions, and adaptive fusion strategies consistently outperforms
state-of-the-art models in classification accuracy, contributing to more
reliable and interpretable models for three different tasks that are crucial
for disaster response. Our code is available at
https://github.com/Munia03/Multimodal_Crisis_Event.

</details>


### [230] [Semantic Frame Interpolation](https://arxiv.org/abs/2507.05173)
*Yijia Hong,Jiangning Zhang,Ran Yi,Yuji Wang,Weijian Cao,Xiaobin Hu,Zhucun Xue,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: The paper introduces a new Semantic Frame Interpolation (SFI) task, proposes the SemFi model for high-consistency content generation, and presents the SFI-300K dataset for benchmarking.


<details>
  <summary>Details</summary>
Motivation: Traditional frame interpolation lacks text control and handles minimal frame differences, while existing models generate fixed frames with inconsistent results.

Method: Proposes SemFi model with Mixture-of-LoRA module for multi-frame-rate inference and introduces SFI-300K dataset with tailored metrics.

Result: SemFi excels in SFI tasks, validated by experiments on SFI-300K, showing high consistency and alignment with control conditions.

Conclusion: The work defines SFI, provides a robust model (SemFi) and dataset (SFI-300K), advancing frame interpolation with text control and multi-frame-rate support.

Abstract: Generating intermediate video content of varying lengths based on given first
and last frames, along with text prompt information, offers significant
research and application potential. However, traditional frame interpolation
tasks primarily focus on scenarios with a small number of frames, no text
control, and minimal differences between the first and last frames. Recent
community developers have utilized large video models represented by Wan to
endow frame-to-frame capabilities. However, these models can only generate a
fixed number of frames and often fail to produce satisfactory results for
certain frame lengths, while this setting lacks a clear official definition and
a well-established benchmark. In this paper, we first propose a new practical
Semantic Frame Interpolation (SFI) task from the perspective of academic
definition, which covers the above two settings and supports inference at
multiple frame rates. To achieve this goal, we propose a novel SemFi model
building upon Wan2.1, which incorporates a Mixture-of-LoRA module to ensure the
generation of high-consistency content that aligns with control conditions
across various frame length limitations. Furthermore, we propose SFI-300K, the
first general-purpose dataset and benchmark specifically designed for SFI. To
support this, we collect and process data from the perspective of SFI,
carefully designing evaluation metrics and methods to assess the model's
performance across multiple dimensions, encompassing image and video, and
various aspects, including consistency and diversity. Through extensive
experiments on SFI-300K, we demonstrate that our method is particularly
well-suited to meet the requirements of the SFI task.

</details>


### [231] [$\varphi$-Adapt: A Physics-Informed Adaptation Learning Approach to 2D Quantum Material Discovery](https://arxiv.org/abs/2507.05184)
*Hoang-Quan Nguyen,Xuan Bac Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: The paper introduces a Physics-informed Adaptation Learning approach to improve quantum flake thickness estimation, addressing data scarcity and generalization challenges with synthetic data generation and a physics-informed adaptation method.


<details>
  <summary>Details</summary>
Motivation: Accurate quantum flake characterization is crucial for qubit performance, but existing computer vision methods struggle with thickness estimation due to limited data, poor generalization, and lack of interpretability.

Method: Proposes a synthetic data generation framework for diverse quantum flake samples and a physics-informed adaptation method ($\varphi$-Adapt) to bridge the gap between synthetic and real-world data.

Result: Achieves state-of-the-art performance on benchmarks, outperforming existing methods.

Conclusion: The approach advances physics-based modeling and domain adaptation, offering valuable tools for deep learning and materials science.

Abstract: Characterizing quantum flakes is a critical step in quantum hardware
engineering because the quality of these flakes directly influences qubit
performance. Although computer vision methods for identifying two-dimensional
quantum flakes have emerged, they still face significant challenges in
estimating flake thickness. These challenges include limited data, poor
generalization, sensitivity to domain shifts, and a lack of physical
interpretability. In this paper, we introduce one of the first Physics-informed
Adaptation Learning approaches to overcome these obstacles. We focus on two
main issues, i.e., data scarcity and generalization. First, we propose a new
synthetic data generation framework that produces diverse quantum flake samples
across various materials and configurations, reducing the need for
time-consuming manual collection. Second, we present $\varphi$-Adapt, a
physics-informed adaptation method that bridges the performance gap between
models trained on synthetic data and those deployed in real-world settings.
Experimental results show that our approach achieves state-of-the-art
performance on multiple benchmarks, outperforming existing methods. Our
proposed approach advances the integration of physics-based modeling and domain
adaptation. It also addresses a critical gap in leveraging synthesized data for
real-world 2D material analysis, offering impactful tools for deep learning and
materials science communities.

</details>


### [232] [Satellite-based Rabi rice paddy field mapping in India: a case study on Telangana state](https://arxiv.org/abs/2507.05189)
*Prashanth Reddy Putta,Fabio Dell'Acqua*

Main category: cs.CV

TL;DR: A phenology-driven classification framework improves rice area monitoring in fragmented landscapes, achieving 93.3% accuracy by adapting to local agro-ecological variations.


<details>
  <summary>Details</summary>
Motivation: Accurate rice area monitoring is crucial for food security and policy in smallholder farming regions, but conventional methods struggle with spatiotemporal heterogeneity.

Method: Developed a phenology-driven classification framework, calibrated district-specifically for 32 districts in Telangana, India, during the 2018-19 Rabi rice season.

Result: Achieved 93.3% overall accuracy (8.0% improvement over conventional methods), mapped 732,345 hectares, and validated with strong agreement (R^2 = 0.981) against government data.

Conclusion: Remote sensing must adapt to landscape complexity for accurate monitoring, advancing region-specific approaches for policy and food security.

Abstract: Accurate rice area monitoring is critical for food security and agricultural
policy in smallholder farming regions, yet conventional remote sensing
approaches struggle with the spatiotemporal heterogeneity characteristic of
fragmented agricultural landscapes. This study developed a phenology-driven
classification framework that systematically adapts to local agro-ecological
variations across 32 districts in Telangana, India during the 2018-19 Rabi rice
season. The research reveals significant spatiotemporal diversity, with
phenological timing varying by up to 50 days between districts and field sizes
ranging from 0.01 to 2.94 hectares. Our district-specific calibration approach
achieved 93.3% overall accuracy, an 8.0 percentage point improvement over
conventional regional clustering methods, with strong validation against
official government statistics (R^2 = 0.981) demonstrating excellent agreement
between remotely sensed and ground truth data. The framework successfully
mapped 732,345 hectares by adapting to agro-climatic variations, with Northern
districts requiring extended land preparation phases (up to 55 days) while
Southern districts showed compressed cultivation cycles. Field size analysis
revealed accuracy declining 6.8 percentage points from medium to tiny fields,
providing insights for operational monitoring in fragmented landscapes. These
findings demonstrate that remote sensing frameworks must embrace rather than
simplify landscape complexity, advancing region-specific agricultural
monitoring approaches that maintain scientific rigor while serving practical
policy and food security applications.

</details>


### [233] [All in One: Visual-Description-Guided Unified Point Cloud Segmentation](https://arxiv.org/abs/2507.05211)
*Zongyan Han,Mohamed El Amine Boudjoghra,Jiahua Dong,Jinhong Wang,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: VDG-Uni3DSeg integrates vision-language models and LLMs to improve 3D point cloud segmentation by leveraging multimodal cues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with sparse data, limited annotations, and fine-grained class differentiation in 3D point clouds.

Method: Proposes VDG-Uni3DSeg, using CLIP and LLMs for multimodal cues, Semantic-Visual Contrastive Loss, and Spatial Enhanced Module.

Result: Achieves top performance in semantic, instance, and panoptic segmentation.

Conclusion: VDG-Uni3DSeg offers a scalable, practical solution for 3D scene understanding.

Abstract: Unified segmentation of 3D point clouds is crucial for scene understanding,
but is hindered by its sparse structure, limited annotations, and the challenge
of distinguishing fine-grained object classes in complex environments. Existing
methods often struggle to capture rich semantic and contextual information due
to limited supervision and a lack of diverse multimodal cues, leading to
suboptimal differentiation of classes and instances. To address these
challenges, we propose VDG-Uni3DSeg, a novel framework that integrates
pre-trained vision-language models (e.g., CLIP) and large language models
(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual
descriptions and reference images from the internet, our method incorporates
rich multimodal cues, facilitating fine-grained class and instance separation.
We further design a Semantic-Visual Contrastive Loss to align point features
with multimodal queries and a Spatial Enhanced Module to model scene-wide
relationships efficiently. Operating within a closed-set paradigm that utilizes
multimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art
results in semantic, instance, and panoptic segmentation, offering a scalable
and practical solution for 3D understanding. Our code is available at
https://github.com/Hanzy1996/VDG-Uni3DSeg.

</details>


### [234] [CTA: Cross-Task Alignment for Better Test Time Training](https://arxiv.org/abs/2507.05221)
*Samuel Barbeau,Pedram Fekri,David Osowiechi,Ali Bahri,Moslem YazdanpanahMasih Aminbeidokhti,Christian Desrosiers*

Main category: cs.CV

TL;DR: CTA improves Test-Time Training by aligning supervised and self-supervised encoders, enhancing robustness without specialized architectures.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle with distribution shifts; TTT needs improvement for better robustness.

Method: CTA aligns supervised and self-supervised encoders using multi-modal contrastive learning.

Result: CTA achieves better robustness and generalization than state-of-the-art methods on benchmarks.

Conclusion: CTA offers a scalable and effective way to enhance TTT without architectural constraints.

Abstract: Deep learning models have demonstrated exceptional performance across a wide
range of computer vision tasks. However, their performance often degrades
significantly when faced with distribution shifts, such as domain or dataset
changes. Test-Time Training (TTT) has emerged as an effective method to enhance
model robustness by incorporating an auxiliary unsupervised task during
training and leveraging it for model updates at test time. In this work, we
introduce CTA (Cross-Task Alignment), a novel approach for improving TTT.
Unlike existing TTT methods, CTA does not require a specialized model
architecture and instead takes inspiration from the success of multi-modal
contrastive learning to align a supervised encoder with a self-supervised one.
This process enforces alignment between the learned representations of both
models, thereby mitigating the risk of gradient interference, preserving the
intrinsic robustness of self-supervised learning and enabling more semantically
meaningful updates at test-time. Experimental results demonstrate substantial
improvements in robustness and generalization over the state-of-the-art on
several benchmark datasets.

</details>


### [235] [Self-Supervised Real-Time Tracking of Military Vehicles in Low-FPS UAV Footage](https://arxiv.org/abs/2507.05229)
*Markiyan Kostiv,Anatolii Adamovskyi,Yevhen Cherniavskyi,Mykyta Varenyk,Ostap Viniavskyi,Igor Krashenyi,Oles Dobosevych*

Main category: cs.CV

TL;DR: The paper addresses multi-object tracking (MOT) in low-frame-rate UAV videos, proposing instance association learning from single-frame annotations to handle challenges like appearance changes and image degradation.


<details>
  <summary>Details</summary>
Motivation: Tracking objects in low-frame-rate UAV videos is complex due to rapid appearance changes, position shifts, and image degradation from streaming/compression.

Method: Uses instance association learning from single-frame annotations, leveraging global scene features for robust tracking.

Result: The approach maintains high association quality even with reduced image resolution and latent representation size for faster inference.

Conclusion: The method is effective for MOT in challenging UAV scenarios, and a benchmark dataset of military vehicles is introduced.

Abstract: Multi-object tracking (MOT) aims to maintain consistent identities of objects
across video frames. Associating objects in low-frame-rate videos captured by
moving unmanned aerial vehicles (UAVs) in actual combat scenarios is complex
due to rapid changes in object appearance and position within the frame. The
task becomes even more challenging due to image degradation caused by cloud
video streaming and compression algorithms. We present how instance association
learning from single-frame annotations can overcome these challenges. We show
that global features of the scene provide crucial context for low-FPS instance
association, allowing our solution to be robust to distractors and gaps in
detections. We also demonstrate that such a tracking approach maintains high
association quality even when reducing the input image resolution and latent
representation size for faster inference. Finally, we present a benchmark
dataset of annotated military vehicles collected from publicly available data
sources. This paper was initially presented at the NATO Science and Technology
Organization Symposium (ICMCIS) organized by the Information Systems Technology
(IST)Scientific and Technical Committee, IST-209-RSY - the ICMCIS, held in
Oeiras, Portugal, 13-14 May 2025.

</details>


### [236] [Physics-Guided Dual Implicit Neural Representations for Source Separation](https://arxiv.org/abs/2507.05249)
*Yuan Ni,Zhantao Chen,Alexander N. Petsch,Edmund Xu,Cheng Peng,Alexander I. Kolesnikov,Sugata Chowdhury,Arun Bansil,Jana B. Thayer,Joshua J. Turner*

Main category: cs.CV

TL;DR: A self-supervised machine-learning approach using dual implicit neural networks separates physical signals from distortions and background in raw data without labeled examples.


<details>
  <summary>Details</summary>
Motivation: Advanced experimental techniques often produce data with unwanted distortions and background, obscuring relevant physical information.

Method: Joint training of two neural networks: one for signal distortions and another for background, minimizing a reconstruction loss without labeled data.

Result: Successfully separates meaningful signals from complex backgrounds in 4D parameter space, even with varying signal characteristics.

Conclusion: The method provides a versatile solution for source separation in diverse fields, from astronomy to biomedical imaging.

Abstract: Significant challenges exist in efficient data analysis of most advanced
experimental and observational techniques because the collected signals often
include unwanted contributions--such as background and signal distortions--that
can obscure the physically relevant information of interest. To address this,
we have developed a self-supervised machine-learning approach for source
separation using a dual implicit neural representation framework that jointly
trains two neural networks: one for approximating distortions of the physical
signal of interest and the other for learning the effective background
contribution. Our method learns directly from the raw data by minimizing a
reconstruction-based loss function without requiring labeled data or
pre-defined dictionaries. We demonstrate the effectiveness of our framework by
considering a challenging case study involving large-scale simulated as well as
experimental momentum-energy-dependent inelastic neutron scattering data in a
four-dimensional parameter space, characterized by heterogeneous background
contributions and unknown distortions to the target signal. The method is found
to successfully separate physically meaningful signals from a complex or
structured background even when the signal characteristics vary across all four
dimensions of the parameter space. An analytical approach that informs the
choice of the regularization parameter is presented. Our method offers a
versatile framework for addressing source separation problems across diverse
domains, ranging from superimposed signals in astronomical measurements to
structural features in biomedical image reconstructions.

</details>


### [237] [From Marginal to Joint Predictions: Evaluating Scene-Consistent Trajectory Prediction Approaches for Automated Driving](https://arxiv.org/abs/2507.05254)
*Fabian Konstantinidis,Ariel Dallari Guerreiro,Raphael Trumpp,Moritz Sackmann,Ulrich Hofmann,Marco Caccamo,Christoph Stiller*

Main category: cs.CV

TL;DR: The paper compares joint motion prediction models for automated vehicles, evaluating accuracy, multi-modality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Improving motion prediction of traffic participants to enhance safety and efficiency in automated driving by addressing limitations of marginal prediction models.

Method: Systematic investigation of joint prediction approaches: post-processing marginal predictions, explicit joint training, and generative task framing.

Result: Comprehensive analysis of each approach's strengths and limitations in terms of accuracy, multi-modality, and inference efficiency.

Conclusion: Joint prediction models outperform marginal ones, but trade-offs exist; the study provides insights for future model development.

Abstract: Accurate motion prediction of surrounding traffic participants is crucial for
the safe and efficient operation of automated vehicles in dynamic environments.
Marginal prediction models commonly forecast each agent's future trajectories
independently, often leading to sub-optimal planning decisions for an automated
vehicle. In contrast, joint prediction models explicitly account for the
interactions between agents, yielding socially and physically consistent
predictions on a scene level. However, existing approaches differ not only in
their problem formulation but also in the model architectures and
implementation details used, making it difficult to compare them. In this work,
we systematically investigate different approaches to joint motion prediction,
including post-processing of the marginal predictions, explicitly training the
model for joint predictions, and framing the problem as a generative task. We
evaluate each approach in terms of prediction accuracy, multi-modality, and
inference efficiency, offering a comprehensive analysis of the strengths and
limitations of each approach. Several prediction examples are available at
https://frommarginaltojointpred.github.io/.

</details>


### [238] [Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning](https://arxiv.org/abs/2507.05255)
*Yana Wei,Liang Zhao,Jianjian Sun,Kangheng Lin,Jisheng Yin,Jingcheng Hu,Yinmin Zhang,En Yu,Haoran Lv,Zejia Weng,Jia Wang,Chunrui Han,Yuang Peng,Qi Han,Zheng Ge,Xiangyu Zhang,Daxin Jiang,Vishal M. Patel*

Main category: cs.CV

TL;DR: The paper introduces a two-stage paradigm for enhancing Multimodal LLMs (MLLMs) using linguistic cold-start fine-tuning and multimodal reinforcement learning, achieving state-of-the-art performance on reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To unlock advanced visual reasoning in MLLMs by transferring the reasoning capabilities of LLMs through verifiable rewards.

Method: A two-stage approach: 1) massive linguistic cold-start fine-tuning, 2) multimodal reinforcement learning (RL) with nearly 1,000 steps.

Result: The model, Open-Vision-Reasoner (OVR), achieves top performance on benchmarks (e.g., 95.3% on MATH500). Three key insights about behavior transfer and RL are revealed.

Conclusion: The work demonstrates the effectiveness of the proposed paradigm and releases resources to advance multimodal reasoning.

Abstract: The remarkable reasoning capability of large language models (LLMs) stems
from cognitive behaviors that emerge through reinforcement with verifiable
rewards. This work investigates how to transfer this principle to Multimodal
LLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage
paradigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,
followed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,
surpassing all previous open-source efforts in scale. This pioneering work
reveals three fundamental insights: 1) Behavior transfer emerges surprisingly
early in cold start due to linguistic mental imagery. 2) Cold start broadly
memorizes visual behaviors, while RL critically discerns and scales up
effective patterns. 3) Transfer strategically favors high-utility behaviors
such as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),
achieves state-of-the-art performance on a suite of reasoning benchmarks,
including 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We
release our model, data, and training dynamics to catalyze the development of
more capable, behavior-aligned multimodal reasoners.

</details>


### [239] [SegmentDreamer: Towards High-fidelity Text-to-3D Synthesis with Segmented Consistency Trajectory Distillation](https://arxiv.org/abs/2507.05256)
*Jiahao Zhu,Zixuan Chen,Guangcong Wang,Xiaohua Xie,Yi Zhou*

Main category: cs.CV

TL;DR: SegmentDreamer improves text-to-3D generation by addressing imbalance in consistency models with Segmented Consistency Trajectory Distillation (SCTD), achieving higher fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing CD-based methods suffer from improper conditional guidance due to imbalance between self- and cross-consistency, leading to sub-optimal results.

Method: Proposes SCTD to reformulate SDS, explicitly defining self- and cross-consistency relationships and partitioning PF-ODE trajectories for tighter error bounds. Also introduces a faster distillation pipeline.

Result: SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling high-fidelity 3D asset creation via 3D Gaussian Splatting.

Conclusion: SegmentDreamer effectively mitigates consistency imbalance, offering superior text-to-3D generation with improved visual quality and efficiency.

Abstract: Recent advancements in text-to-3D generation improve the visual quality of
Score Distillation Sampling (SDS) and its variants by directly connecting
Consistency Distillation (CD) to score distillation. However, due to the
imbalance between self-consistency and cross-consistency, these CD-based
methods inherently suffer from improper conditional guidance, leading to
sub-optimal generation results. To address this issue, we present
SegmentDreamer, a novel framework designed to fully unleash the potential of
consistency models for high-fidelity text-to-3D generation. Specifically, we
reformulate SDS through the proposed Segmented Consistency Trajectory
Distillation (SCTD), effectively mitigating the imbalance issues by explicitly
defining the relationship between self- and cross-consistency. Moreover, SCTD
partitions the Probability Flow Ordinary Differential Equation (PF-ODE)
trajectory into multiple sub-trajectories and ensures consistency within each
segment, which can theoretically provide a significantly tighter upper bound on
distillation error. Additionally, we propose a distillation pipeline for a more
swift and stable generation. Extensive experiments demonstrate that our
SegmentDreamer outperforms state-of-the-art methods in visual quality, enabling
high-fidelity 3D asset creation through 3D Gaussian Splatting (3DGS).

</details>


### [240] [Spatio-Temporal LLM: Reasoning about Environments and Actions](https://arxiv.org/abs/2507.05258)
*Haozhen Zheng,Beitong Tian,Mingyuan Wu,Zhenggang Tang,Klara Nahrstedt,Alex Schwing*

Main category: cs.CV

TL;DR: The paper addresses the challenge of holistic spatio-temporal understanding in Multimodal Large Language Models (MLLMs) and introduces a new model, ST-LLM, to improve performance on such tasks.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with prompts requiring simultaneous understanding of an entire environment and recent actions in a video, which is crucial for real-world agent operation.

Method: The authors develop a framework to collect a large-scale dataset (REA) and propose ST-LLM, a model with projectors for enhanced spatial and temporal understanding.

Result: ST-LLM significantly outperforms prior methods on the REA dataset.

Conclusion: The proposed ST-LLM effectively addresses the spatio-temporal understanding gap in MLLMs, with code and data made publicly available.

Abstract: Despite the significant recent progress of Multimodal Large Language Models
(MLLMs), MLLMs still struggle to correctly answer prompts that require a
holistic spatio-temporal understanding. Specifically, it is challenging to
address prompts that refer to 1) the entirety of an environment that an agent
equipped with an MLLM can operate in; and simultaneously also refer to 2)
recent actions that just happened and are encoded in a video clip. However,
such a holistic spatio-temporal understanding is important for agents operating
in the real world. To address this issue, we first develop a framework to
collect a large-scale dataset. Using the collected "Reasoning about
Environments and Actions" (REA) dataset, we show that recent methods indeed
struggle to correctly answer the prompts. To improve, we develop a
"spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve
both spatial understanding of an environment and temporal understanding of
recent observations. On the collected REA data, we show that the proposed
method significantly improves results compared to prior work. Code and data are
available at https://zoezheng126.github.io/STLLM-website/.

</details>


### [241] [Beyond Simple Edits: X-Planner for Complex Instruction-Based Image Editing](https://arxiv.org/abs/2507.05259)
*Chun-Hsiao Yeh,Yilin Wang,Nanxuan Zhao,Richard Zhang,Yuheng Li,Yi Ma,Krishna Kumar Singh*

Main category: cs.CV

TL;DR: X-Planner, an MLLM-based system, improves image editing by decomposing complex instructions into clear sub-tasks, automating masks, and preserving identity.


<details>
  <summary>Details</summary>
Motivation: Address challenges in diffusion-based image editing like poor identity preservation, unintended edits, and reliance on manual masks.

Method: Uses chain-of-thought reasoning to break down instructions, generates precise edit types and masks, and trains on large-scale data.

Result: Achieves state-of-the-art performance on existing and new complex editing benchmarks.

Conclusion: X-Planner effectively bridges user intent with editing capabilities, improving accuracy and automation.

Abstract: Recent diffusion-based image editing methods have significantly advanced
text-guided tasks but often struggle to interpret complex, indirect
instructions. Moreover, current models frequently suffer from poor identity
preservation, unintended edits, or rely heavily on manual masks. To address
these challenges, we introduce X-Planner, a Multimodal Large Language Model
(MLLM)-based planning system that effectively bridges user intent with editing
model capabilities. X-Planner employs chain-of-thought reasoning to
systematically decompose complex instructions into simpler, clear
sub-instructions. For each sub-instruction, X-Planner automatically generates
precise edit types and segmentation masks, eliminating manual intervention and
ensuring localized, identity-preserving edits. Additionally, we propose a novel
automated pipeline for generating large-scale data to train X-Planner which
achieves state-of-the-art results on both existing benchmarks and our newly
introduced complex editing benchmark.

</details>


### [242] [Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon Distillation for Better LiDAR Representations](https://arxiv.org/abs/2507.05260)
*Xiang Xu,Lingdong Kong,Song Wang,Chuanwei Zhou,Qingshan Liu*

Main category: cs.CV

TL;DR: LiMA is a novel framework for LiDAR representation learning that leverages long-term spatiotemporal cues through cross-view aggregation, long-term feature propagation, and cross-sequence memory alignment, improving performance in semantic segmentation and 3D object detection.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR representation strategies often ignore spatiotemporal cues in sequences, limiting effectiveness. LiMA addresses this gap by capturing longer-range temporal correlations.

Method: LiMA includes: 1) Cross-View Aggregation for unified memory, 2) Long-Term Feature Propagation for temporal coherence, and 3) Cross-Sequence Memory Alignment for generalization.

Result: LiMA significantly enhances LiDAR semantic segmentation and 3D object detection on benchmarks, with no extra computational cost in downstream tasks.

Conclusion: LiMA advances LiDAR representation learning and inspires better pretraining paradigms for autonomous driving. The code is publicly available.

Abstract: LiDAR representation learning aims to extract rich structural and semantic
information from large-scale, readily available datasets, reducing reliance on
costly human annotations. However, existing LiDAR representation strategies
often overlook the inherent spatiotemporal cues in LiDAR sequences, limiting
their effectiveness. In this work, we propose LiMA, a novel long-term
image-to-LiDAR Memory Aggregation framework that explicitly captures longer
range temporal correlations to enhance LiDAR representation learning. LiMA
comprises three key components: 1) a Cross-View Aggregation module that aligns
and fuses overlapping regions across neighboring camera views, constructing a
more unified and redundancy-free memory bank; 2) a Long-Term Feature
Propagation mechanism that efficiently aligns and integrates multi-frame image
features, reinforcing temporal coherence during LiDAR representation learning;
and 3) a Cross-Sequence Memory Alignment strategy that enforces consistency
across driving sequences, improving generalization to unseen environments. LiMA
maintains high pretraining efficiency and incurs no additional computational
overhead during downstream tasks. Extensive experiments on mainstream
LiDAR-based perception benchmarks demonstrate that LiMA significantly improves
both LiDAR semantic segmentation and 3D object detection. We hope this work
inspires more effective pretraining paradigms for autonomous driving. The code
has be made publicly accessible for future research.

</details>
