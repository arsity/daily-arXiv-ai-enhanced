<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: Real-time generative drawing system that integrates both formal (structural/compositional) and contextual (semantic/thematic) intent from sketches, enabling collaborative AI-human co-creation with low latency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of text-prompt-based generative systems that only capture high-level context, by also analyzing intuitive geometric features and enabling real-time collaborative creation regardless of artistic expertise.

Method: Multi-stage generation pipeline that jointly conditions dual intent signals (geometric features + semantic cues), combining contour-preserving structural control with style/content-aware image synthesis, implemented with touchscreen interface and distributed inference architecture.

Result: Achieves low-latency, two-stage transformation supporting multi-user collaboration on shared canvases, enabling synchronous co-authored visual creation.

Conclusion: Redefines human-AI interaction as a process of co-creation and mutual enhancement, making visual creation accessible to users regardless of artistic skill level.

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF is a training-free method that enhances Vision-Language-Action models by integrating temporal information from historical frames using selective fusion strategies, improving performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models process visual inputs frame-by-frame, discarding valuable temporal information and making them vulnerable to visual noise while ignoring coherence between consecutive frames in manipulation tasks.

Method: Temporal Token Fusion (TTF) employs dual-dimension detection combining grayscale pixel difference analysis with attention-based semantic relevance assessment, using hard fusion strategies and keyframe anchoring to prevent error accumulation.

Result: Consistent improvements across benchmarks: 4.0pp average on LIBERO (72.4% vs 68.4% baseline), 4.8% relative improvement on SimplerEnv, and 8.7% relative improvement on real robot tasks. Model-agnostic across OpenVLA and VLA-Cache architectures.

Conclusion: TTF demonstrates that selective temporal integration enhances VLA performance without training. The approach reveals that Query matrix reuse in attention mechanisms improves rather than compromises performance, suggesting promising directions for computational acceleration strategies.

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [3] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: Unsupervised slide-quality assessment pipeline combining expert-inspired visual metrics with CLIP-ViT embeddings using Isolation Forest anomaly scoring, achieving strong correlation with human ratings.


<details>
  <summary>Details</summary>
Motivation: To provide scalable, objective feedback on presentation slide quality by approximating audience perceptions through automated assessment.

Method: Combines seven visual-design metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring trained on 12k professional lecture slides.

Result: Achieved Pearson correlations up to 0.83 with human visual-quality ratings (1.79x to 3.23x stronger than leading vision-language models), demonstrated convergent validity with visual ratings and discriminant validity against speaker-delivery scores.

Conclusion: Augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling real-time scalable feedback.

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [4] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: Efficient adversarial defense framework for 2D range-view LiDAR segmentation that provides strong protection with minimal computational overhead, validated on real-world autonomous driving scenarios.


<details>
  <summary>Details</summary>
Motivation: LiDAR segmentation is critical for autonomous vehicle safety but vulnerable to adversarial attacks. Most defenses target raw 3D point clouds with heavy generative models, while efficient 2D range-view representations lack dedicated lightweight defenses despite widespread use.

Method: Proposes a model-based purification framework with direct attack formulation in range-view domain. Uses an explainable purification network based on mathematically justified optimization problem for efficient adversarial defense.

Result: Achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. Real-world deployment demonstrates accurate operation in practical autonomous driving scenarios.

Conclusion: The framework provides strong adversarial resilience with minimal computational overhead, making it suitable for practical deployment in autonomous vehicles using 2D range-view LiDAR segmentation pipelines.

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [5] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: This paper presents a comprehensive review of Large Vision-Language Models (LVLMs) for object detection, highlighting their revolutionary impact through enhanced adaptability, contextual reasoning, and generalization compared to traditional deep learning architectures.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and analyze the state-of-the-art in LVLMs for object detection, examining how the fusion of language and vision capabilities enhances object detection and localization beyond conventional methods.

Method: The review follows a three-step research process: 1) discussing how VLMs function for object detection using NLP and CV techniques, 2) explaining architectural innovations and training paradigms, and 3) examining visual-textual integration approaches with comprehensive visualizations and performance comparisons.

Result: LVLMs demonstrate advanced contextual understanding and effectiveness in diverse scenarios including localization and segmentation. The review shows they are expected to soon meet or surpass traditional methods' performance, though some current limitations are identified.

Conclusion: Recent advancements in LVLMs are making and will continue to make a transformative impact on object detection and robotic applications, with proposed solutions and a clear roadmap for future advancement in this field.

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [6] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: A two-level fine-tuned LVLM pipeline for generating production-grade sports captions from images, achieving significant improvements in F1 and BERT scores with fast execution.


<details>
  <summary>Details</summary>
Motivation: Existing LLM/LVLMs lack sufficient sports domain knowledge and jargon to create natural, human-like descriptions of gameplay, limiting their application in sports journalism.

Method: Proposes a two-level fine-tuned LVLM pipeline specifically designed for sports caption generation from images in a stylized format.

Result: Achieved >8-10% improvement in F1 score and >2-10% improvement in BERT score compared to alternative approaches, with small memory footprint and fast execution (6 images per 3-5 seconds).

Conclusion: The pipeline successfully demonstrated practical application in live professional sports journalism during Super Bowl LIX, generating highly accurate and stylized captions for over 1000 images during gameplay.

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [7] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: Empirical evaluation reveals demographic biases in Large Vision Language Models for face recognition tasks, with PaliGemma and LLaVA showing higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 performs more consistently.


<details>
  <summary>Details</summary>
Motivation: Demographic biases remain a critical concern in face recognition systems, as foundation models often fail to perform equitably across diverse demographic groups including ethnicity/race, gender, and age.

Method: Fine-tuned and evaluated three pre-trained LVLMs (LLaVA, BLIP-2, PaliGemma) on a demographic-balanced dataset using metrics like group-specific BERTScores and Fairness Discrepancy Rate to quantify performance disparities.

Result: PaliGemma and LLaVA exhibited higher demographic biases for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 demonstrated comparatively consistent performance across demographic groups.

Conclusion: The study uncovers significant demographic biases in LVLMs for face recognition tasks, highlighting the need for improved fairness and reliability across diverse demographic groups in vision-language models.

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [8] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec introduces a novel spatial representation learning method using signed distance fields to create unified, geometry-aware embeddings for all geo-entity types without decomposition, outperforming existing methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing spatial representation methods either handle single entity types or require decomposition with high computational cost, and they lack geometric alignment which blurs fine-grained features like edges and boundaries.

Method: Geo2Vec uses signed distance fields (SDF) to adaptively sample points and encode signed distances (positive outside, negative inside) without decomposition. A neural network approximates the SDF to produce compact representations, with rotation-invariant positional encoding for high-frequency spatial variations.

Result: Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieves greater efficiency in real-world GeoAI applications.

Conclusion: The proposed Geo2Vec method provides a unified, efficient, and geometry-aware approach for spatial representation learning that works across all geo-entity types and improves downstream GeoAI model performance.

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [9] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: This paper presents an automated deep learning system using CNNs and XAI techniques to classify rice grain varieties and diagnose rice leaf diseases with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Rice is a globally important staple food, but manual quality inspection is labor-intensive and error-prone. There's a need for automated solutions to ensure quality control and yield improvement in rice production.

Method: Used Convolutional Neural Networks (CNN) for rice grain classification on a dataset of 75,000 images. For disease diagnosis, combined explainable AI (XAI) with deep learning models including CNN, VGG16, ResNet50, and MobileNetV2, using SHAP and LIME for interpretability.

Result: Achieved high classification accuracy with minimal misclassifications for five rice varieties. Successfully diagnosed rice leaf diseases including Brown Spot, Blast, Bacterial Blight, and Tungro with enhanced model transparency.

Conclusion: Deep learning with XAI shows strong potential for agricultural applications, enabling robust and interpretable systems for automated crop quality inspection and disease diagnosis that benefit farmers, consumers, and the agricultural economy.

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [10] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: Federated learning system with OpenMax algorithm for open-set facial recognition that distinguishes known/unknown individuals while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns and identity management challenges in facial recognition when unknown individuals appear in operational contexts.

Method: Integrates OpenMax algorithm into federated learning framework, using exchange of mean activation vectors and local distance measures to distinguish known/unknown subjects.

Result: Experimental results validate effectiveness of the proposed solution for reliable identification of both known and unknown individuals.

Conclusion: The approach demonstrates potential for enhancing privacy-aware and robust facial recognition in distributed environments through federated learning.

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [11] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: Deep learning model using ground-level photos for habitat classification achieves mean F1-score of 0.61 across 18 habitat classes, with some classes reaching above 0.90 accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional habitat classification relies on satellite imagery with field validation, but ground-level photography offers improved validation and scalability through citizen-science applications.

Method: Developed DeepLabV3-ResNet101 classifier using pre-processed ground-level imagery with resizing, normalization, augmentation, and re-sampling for class balance. Five-fold cross-validation on 18 habitat classes defined by Living England framework.

Result: Model achieved mean F1-score of 0.61 across all classes, with visually distinct habitats like Bare Soil/Silt/Peat and Bare Sand scoring above 0.90, while mixed/ambiguous classes performed lower.

Conclusion: Ground-level imagery combined with deep learning shows strong potential for ecological monitoring at scale, with practical applications supported by a web interface for practitioners.

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [12] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Interactive autoregressive video generation framework with multimodal control and low-latency streaming for digital human synthesis


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with high latency, heavy computational cost, and limited controllability for interactive digital human video generation

Method: Autoregressive framework using modified LLM to accept multimodal inputs (audio, pose, text) and guide diffusion denoising, with deep compression autoencoder (64× reduction) and large-scale 20K-hour dialogue dataset

Result: Achieves low latency, high efficiency, and fine-grained multimodal controllability in duplex conversation, multilingual synthesis, and interactive world model scenarios

Conclusion: Proposed framework enables practical real-time interactive digital human video generation with multimodal control capabilities

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [13] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: Survey paper explores digital watermarking and steganography as post-capture protection for ICAO-compliant facial images to combat morphing and deepfakes, while maintaining compliance standards.


<details>
  <summary>Details</summary>
Motivation: ICAO-compliant facial images are vulnerable to manipulation like morphing and deepfakes for identity theft, and traditional real-time detection methods offer no post-capture protection.

Method: Comprehensive analysis of state-of-the-art digital watermarking and steganography techniques that embed tamper-evident signals directly into images without compromising ICAO compliance.

Result: Evaluation of potential and drawbacks of various approaches, highlighting key trade-offs for secure deployment in identity verification systems.

Conclusion: Digital watermarking and steganography provide promising complementary solutions for persistent verification of ICAO-compliant images beyond real-time capture scenarios.

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [14] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM is a self-supervised framework that integrates cardiac MRI imaging with EHR data using prompt-guided representation learning for improved MACE prediction, outperforming existing methods and uncovering novel imaging risk signatures.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of major adverse cardiac events (MACE) remains a central challenge in cardiovascular prognosis, requiring better integration of imaging and clinical data.

Method: PRISM uses motion-aware multi-view distillation to extract temporally synchronized imaging features from cardiac cine MRI and modulates them with medically informed textual prompts, integrating with structured EHR data for survival analysis.

Result: PRISM consistently surpassed classical survival models and SOTA deep learning baselines across four independent clinical cohorts. Uncovered three distinct imaging signatures associated with elevated MACE risk and identified hypertension, diabetes, and smoking as dominant EHR risk factors.

Conclusion: The combined imaging and EHR representations from PRISM provide valuable insights into cardiac risk across diverse cohorts and enable fine-grained risk prediction with superior performance over existing methods.

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [15] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: EffNetViTLoRA: A hybrid CNN-Vision Transformer model with LoRA adaptation for Alzheimer's disease diagnosis using full ADNI MRI dataset, achieving 92.52% accuracy across AD, MCI, and CN categories.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of Alzheimer's disease is crucial as it's irreversible. MCI diagnosis is challenging due to subtle differences between diagnostic categories, and previous studies used limited data subsets leading to biased models.

Method: Integration of CNN with Vision Transformer to capture both local and global MRI features. Uses full ADNI T1-weighted MRI dataset. Incorporates Low-Rank Adaptation (LoRA) for effective domain adaptation of pretrained ViT model to reduce overfitting.

Result: Achieved 92.52% classification accuracy and 92.76% F1-score across three diagnostic categories (AD, MCI, CN) using the complete ADNI dataset.

Conclusion: The proposed EffNetViTLoRA model provides a robust and clinically reliable approach for AD diagnosis by leveraging comprehensive data and effective domain adaptation techniques, outperforming previous limited-data approaches.

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [16] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: Study evaluates commercial AI player tracking software accuracy using World Cup broadcast footage, finding variable precision in position/speed measurements and recommending tactical feeds for optimal results.


<details>
  <summary>Details</summary>
Motivation: To assess whether commercially available computer-vision and AI player tracking software can accurately measure player position, speed, and distance using broadcast footage, and determine the impact of camera feed type and resolution on accuracy.

Method: Used data from one 2022 FIFA World Cup match with tactical, programme, and camera 1 feeds. Three commercial tracking providers analyzed player position (x,y coordinates) and speed, compared against TRACAB Gen 5 high-definition multi-camera system as ground truth. Calculated root mean square error (RMSE) and mean bias.

Result: Position RMSE ranged from 1.68 to 16.39 m, speed RMSE from 0.34 to 2.38 m/s. Total match distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across providers. Tactical feed provided best results for maximizing player detection.

Conclusion: Computer-vision and AI tracking software can track players with fair precision when players are detected. Providers should use tactical feeds for position and speed tracking, and both 720p and 1080p resolutions are suitable with appropriate AI models.

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [17] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: JVLGS is a novel vision-language framework that integrates visual and textual modalities to improve gas leak segmentation, addressing challenges of blurry gas clouds and reducing false positives through post-processing.


<details>
  <summary>Details</summary>
Motivation: Gas leaks pose serious health and environmental threats, but current detection methods are limited by the blurry, non-rigid nature of gas clouds in infrared videos and lack effective representation.

Method: Proposes Joint Vision-Language Gas leak Segmentation (JVLGS) framework that combines visual and textual information, includes post-processing to reduce false positives from noise and non-target objects.

Result: Extensive experiments show JVLGS significantly outperforms state-of-the-art methods across diverse scenarios, achieving strong performance in both supervised and few-shot learning settings.

Conclusion: The integration of vision-language modalities provides complementary strengths for gas leak representation and segmentation, offering a robust solution that works well in multiple learning scenarios where competing methods fail.

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [18] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM is a framework that transfers knowledge from diverse pre-trained models into a single student model using voting mechanisms at logit and feature levels, overcoming limitations of existing methods that require specific model types and architectures.


<details>
  <summary>Details</summary>
Motivation: Leverage the collective knowledge from numerous diverse pre-trained models available online, which provide unique interpretations of the real world but are currently underutilized due to heterogeneity challenges in knowledge integration.

Method: Proposes a voting mechanism to capture consensus at both logit level (for models predicting target classes) and feature level (using visual representations from arbitrary label spaces), enabling knowledge transfer without constraints on model architectures or training data distributions.

Result: Extensive experiments show UNIFORM significantly enhances unsupervised object recognition performance compared to strong baselines, with remarkable scalability that benefits from over 100 teachers while existing methods saturate at much smaller scales.

Conclusion: UNIFORM effectively harnesses collective knowledge from diverse pre-trained models through a flexible voting-based framework, demonstrating superior performance and scalability in knowledge transfer without the data/inductive biases of previous approaches.

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [19] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow is a satellite imagery-based diffusion framework that generates structurally coherent Origin-Destination flow matrices without needing costly auxiliary data, while being robust to spatial index permutations.


<details>
  <summary>Details</summary>
Motivation: Existing OD flow generation methods rely on expensive auxiliary features with limited coverage and are sensitive to spatial topology changes like regional reindexing, which disrupts structural coherence.

Method: Uses a multi-kernel encoder to capture regional interactions and permutation-aware diffusion process with joint contrastive training that bridges satellite features with OD patterns, enforcing structural consistency through equivariant diffusion.

Result: Outperforms physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations on real-world urban datasets.

Conclusion: Sat2Flow provides a globally scalable solution for OD flow generation in data-scarce environments, eliminating region-specific data dependencies while maintaining structural invariance for robust mobility modeling.

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [20] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: Semi-supervised framework improves weed detection by addressing shadow bias and annotation costs using pseudo-labeling with unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Automated weed management faces challenges from environmental conditions and high annotation costs, requiring robust computer vision systems that work in real-world field conditions.

Method: Diagnostic-driven semi-supervised framework using ResNet for classification and YOLO/RF-DETR for detection, leveraging ~975 labeled and 10,000 unlabeled images with pseudo-labeling to mitigate shadow bias.

Result: Achieved F1 scores up to 0.90 and mAP50 scores exceeding 0.82, with improved recall critical for minimizing weed escapes in automated spraying systems.

Conclusion: Provides a field-tested framework for developing robust computer vision systems in precision agriculture that addresses real-world challenges through diagnostic insights and semi-supervised learning.

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [21] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TMR++ Aligned Preference Optimization (TAPO) and MotionFLUX form a unified system that improves text-to-motion generation with better semantic alignment and real-time synthesis using deterministic rectified flow matching.


<details>
  <summary>Details</summary>
Motivation: Address limitations in text-driven motion generation methods that struggle with precise alignment between linguistic descriptions and motion semantics, and suffer from slow, multi-step inference inefficiencies.

Method: Introduces TAPO framework for aligning subtle motion variations with textual modifiers through iterative adjustments, and MotionFLUX - a high-speed generation framework based on deterministic rectified flow matching that constructs optimal transport paths between noise distributions and motion spaces.

Result: The unified system outperforms state-of-the-art approaches in both semantic consistency and motion quality while significantly accelerating generation speed, enabling real-time synthesis without sacrificing quality.

Conclusion: TAPO and MotionFLUX provide an effective solution for precise text-to-motion alignment and real-time generation, addressing key challenges in virtual character animation and embodied agent motion synthesis.

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [22] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench is the first comprehensive benchmark for evaluating cross-video relational reasoning in MLLMs, revealing significant performance gaps compared to human capabilities and identifying architectural limitations.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs show strong performance on single-video tasks but their ability to reason across multiple videos remains underexplored, despite being essential for real-world applications like multi-camera surveillance and cross-video procedural learning.

Method: Developed CVBench with 1,000 question-answer pairs across three hierarchical tiers: cross-video object association, cross-video event association, and cross-video complex reasoning. Built from five domain-diverse video clusters and evaluated 10+ leading MLLMs under zero-shot or chain-of-thought prompting.

Result: Significant performance gaps observed: top models like GPT-4o achieve only 60% accuracy on causal reasoning tasks compared to 91% human performance. Identified fundamental bottlenecks including deficient inter-video context retention and poor disambiguation of overlapping entities.

Conclusion: CVBench provides a rigorous framework for diagnosing and advancing multi-video reasoning, offering architectural insights for next-generation MLLMs and establishing a benchmark for cross-video relational reasoning capabilities.

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [23] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack is a browser-based gaze estimation framework that achieves state-of-the-art performance with minimal calibration, real-time inference, and addresses privacy concerns by running on-device.


<details>
  <summary>Details</summary>
Motivation: Existing AI gaze estimation methods perform well on benchmarks but fall short in real-world applications compared to commercial solutions, with issues around model size, inference time, privacy, and insufficient accuracy from webcam-based methods due to head movement.

Method: Integrates lightweight SOTA gaze estimation models directly in browser, incorporates model-based head pose estimation, and uses on-device few-shot learning with as few as 9 calibration samples (k < 9).

Result: Achieves SOTA performance with 2.32 cm error margin on GazeCapture dataset and real-time inference speeds of 2.4 milliseconds on iPhone 14.

Conclusion: WebEyeTrack successfully bridges the gap between academic benchmarks and practical deployment by providing accurate, fast, privacy-preserving gaze estimation that adapts to new users with minimal calibration.

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [24] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2 is an end-to-end model for 2.5D relief recovery from single images, improving on V1 by incorporating real data training and achieving state-of-the-art depth/normal prediction performance.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of synthetic-only training in MonoRelief V1 and improve robustness against complex material and illumination variations in real-world scenarios.

Method: Generated ~15K pseudo-real images using text-to-image models with fused depth/normal pseudo-labels, built 800-sample real dataset via multi-view reconstruction, and progressively trained on both datasets.

Result: Achieves state-of-the-art performance in both depth and normal predictions, demonstrating improved robustness, accuracy and efficiency over previous methods.

Conclusion: MonoRelief V2 shows strong potential for downstream applications and successfully addresses the challenge of real-world 2.5D relief recovery from single images.

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [25] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet is a high-speed NMS-free object detector that achieves state-of-the-art performance on intersection traffic monitoring with 63.2% reduced computation and 16.2% faster inference than RT-DETR.


<details>
  <summary>Details</summary>
Motivation: End-to-end object detectors are promising for real-time applications but face high computational costs, especially in complex scenarios like intersection traffic monitoring with severe occlusion and high object density.

Method: Proposes FlowDet with decoupled encoder optimization for DETR architecture, featuring Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and Scale-Aware Attention (SAA) module for handling extreme scale variations.

Result: Achieves new SOTA on Intersection-Flow-5k dataset: improves AP(test) by 1.5% and AP50(test) by 1.6% over RT-DETR, while reducing GFLOPs by 63.2% and increasing inference speed by 16.2%.

Conclusion: Demonstrates a path towards highly efficient and accurate detectors for demanding real-world perception systems, with the Intersection-Flow-5k dataset providing a challenging benchmark for future research.

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [26] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: A unified framework for medical anomaly detection combining trainable encoder, prototype-guided reconstruction, and diversity-aware alignment loss to prevent prototype collapse and improve localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing reconstruction methods rely on frozen pre-trained encoders that limit domain adaptation, and prototype-based learning suffers from prototype collapse where few prototypes dominate training, reducing diversity and generalization.

Method: Combines trainable encoder with momentum branch for stable domain-adaptive feature learning, lightweight Prototype Extractor to mine normal prototypes for decoder guidance via attention, and Diversity-Aware Alignment Loss with diversity constraints and per-prototype normalization to prevent collapse.

Result: Significant improvements in representation quality and anomaly localization across multiple medical imaging benchmarks, outperforming prior methods. Visualizations and prototype assignment analyses validate anti-collapse mechanism effectiveness.

Conclusion: The proposed framework effectively addresses prototype collapse and domain adaptation limitations, providing enhanced interpretability and superior performance in medical anomaly detection tasks.

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [27] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch is a novel semi-supervised segmentation framework that uses multimodal prototype-guided contrastive learning between image/text prototypes and pixel labels to improve semantic boundary modeling in pathological images.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised segmentation methods mainly rely on perturbation-based consistency within image modality, which struggles to capture high-level semantic priors in structurally complex pathology images with ambiguous boundaries and expensive pixel-level annotations.

Method: Proposes MPAMatch with dual contrastive learning: image prototypes-pixel labels and text prototypes-pixel labels. Replaces ViT backbone with pathology-pretrained Uni foundation model in TransUNet architecture for better feature extraction.

Result: Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI datasets show MPAMatch's superiority over state-of-the-art methods.

Conclusion: MPAMatch provides dual advantages in structural and semantic modeling, significantly improving semantic boundary modeling through its novel multimodal prototype-guided supervision paradigm.

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [28] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: Proposes Customized Human Object Interaction Image Generation (CHOI) task and Interact-Custom model for simultaneous identity preservation and interaction control between human and object entities.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus on appearance preservation but neglect fine-grained interaction control between target entities. The paper aims to enable models with interaction control capability for human-object interaction scenarios.

Method: Processes a large-scale dataset with human-object pairs in different interactive poses, then designs a two-stage Interact-Custom model that first generates foreground masks to model spatial configuration, then generates target human-object interactions while preserving identity features.

Result: Extensive experiments on tailored metrics for CHOI task demonstrate the effectiveness of the approach in achieving simultaneous identity preservation and interaction control.

Conclusion: The proposed Interact-Custom model successfully addresses the CHOI task by decomposing features and modeling spatial configurations, providing high content controllability for customized human-object interaction image generation.

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [29] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: Proposes SGDDM for high-fidelity full-color holographic display at high frame rates and HoloMamba architecture for efficient 260+ FPS holographic video generation.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations in computer-generated holography: learning-based models cause color crosstalk in high frame rate displays, and existing methods neglect spatial-temporal correlations between frames, leading to computational inefficiency.

Method: 1) Spectrum-Guided Depth Division Multiplexing (SGDDM) optimizes phase distributions via frequency modulation. 2) HoloMamba - lightweight asymmetric Mamba-Unet architecture that models spatial-temporal correlations across video sequences.

Result: SGDDM achieves high-fidelity full-color display without frame rate compromise. HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, 2.6x faster than prior state-of-the-art methods.

Conclusion: The proposed approach successfully addresses key limitations in holographic video generation, enabling both high frame rates and high color fidelity while improving computational efficiency through spatial-temporal modeling.

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [30] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: SBDC is a guidance technique that uses discriminator training with adversarial loss to correct noisy pre-trained conditional diffusion models, improving performance with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Large datasets used for diffusion models often contain labeling errors that compromise generative capabilities and controllability, but the impact of these errors is not well studied.

Method: Score-based Discriminator Correction (SBDC) uses discriminator training with adversarial loss and prior noise detection techniques to assess sample authenticity, limiting guidance to early generation phases.

Result: Experiments show SBDC outperforms previous state-of-the-art methods across different noise settings, with computational efficiency and minimal inference time increase.

Conclusion: SBDC effectively aligns noisy pre-trained diffusion models without retraining, demonstrating superior performance while maintaining efficiency.

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [31] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: This thesis addresses generalization challenges in monocular 3D object detection by proposing four novel approaches: differentiable NMS for occlusion robustness, depth equivariant backbones for dataset generalization, segmentation-based method for large object detection, and mathematical analysis for camera parameter extrapolation.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection is crucial for applications like autonomous driving and robotics, but existing models struggle with generalization across diverse scenarios including occlusions, different datasets, varying object sizes, and camera parameters.

Method: 1) GrooMeD-NMS: mathematically differentiable NMS for occlusion robustness; 2) DEVIANT: depth equivariant backbones for dataset generalization; 3) SeaBird: segmentation-based approach with dice loss for large object detection; 4) Mathematical analysis of camera parameter extrapolation.

Result: The proposed methods address key generalization challenges: improved occlusion handling, better adaptation to new datasets, enhanced detection of large objects (addressing noise sensitivity), and improved performance with unseen camera parameters.

Conclusion: This thesis provides comprehensive solutions for improving monocular 3D object detection generalization across multiple challenging scenarios, making the technology more robust and applicable to real-world diverse environments.

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [32] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: Study evaluates YOLO model robustness across quantization formats, finding degradation-aware calibration for INT8 PTQ doesn't consistently improve robustness over standard calibration, except for larger models under specific noise conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how post-training quantization affects YOLO object detection model robustness against real-world input degradations like noise, blur, and compression artifacts when deploying on resource-constrained devices.

Method: Comprehensive empirical study evaluating YOLO models (nano to extra-large) across FP32, FP16, Dynamic UINT8, and Static INT8 precision formats. Introduced degradation-aware calibration strategy where TensorRT calibration is exposed to mix of clean and synthetically degraded images. Benchmarking on COCO dataset under seven degradation conditions and mixed-degradation scenario.

Result: Static INT8 TensorRT engines provide ~1.5-3.3x speedups with ~3-7% mAP50-95 accuracy drop on clean data. Degradation-aware calibration did not yield consistent broad robustness improvements over standard clean-data calibration across most models and degradations. Notable exception: larger model scales showed improvement under specific noise conditions.

Conclusion: Challenges exist in enhancing PTQ robustness through calibration strategies. Model capacity may influence calibration efficacy. Findings provide insights for deploying quantized detectors in uncontrolled environments, with degradation-aware calibration showing limited general effectiveness.

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [33] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: Proposes IELDM and IELFormer framework that integrates inverse evolution layers into diffusion models and segmentation networks to filter flawed synthetic data and improve cross-domain generalization in semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Address structural/semantic defects in diffusion-generated synthetic data that cause performance degradation in domain generalized semantic segmentation.

Method: Integrate inverse evolution layers (IELs) with Laplacian-based priors to detect spatial/semantic inconsistencies. Propose IELDM for better data augmentation and IELFormer segmentation model with multi-scale frequency fusion.

Result: Extensive experiments show superior generalization performance compared to existing methods on benchmark datasets.

Conclusion: IEL framework effectively suppresses generative defects and improves cross-domain generalization in semantic segmentation through enhanced data quality and artifact suppression.

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [34] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR is a novel skin image synthesis model that uses quantified lesion measurement scores and type labels to generate high-fidelity, clinically relevant skin images with controllable lesion characteristics through language prompts.


<details>
  <summary>Details</summary>
Motivation: Real-world clinical skin images are often limited, creating data shortages for deep learning models. Existing synthesis methods produce low-quality images and lack control over lesion location and type.

Method: Uses a multiscale lesion-focused VQVAE to encode images into discrete latent representations, then trains a Visual AutoRegressive Transformer on tokenized representations. Integrates lesion measurement scores and type labels as conditional embeddings to guide synthesis.

Result: Achieves best overall FID score (average 0.74) across seven lesion types, improving upon previous state-of-the-art by 6.3%. Enables controllable synthesis of high-fidelity skin images with specific lesion characteristics.

Conclusion: LF-VAR effectively generates clinically relevant synthetic skin images with controllable lesion features, addressing data limitations in dermatological deep learning applications.

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [35] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute is a framework for long-tailed recognition that combines difficulty-aware optimization with dynamic expert collaboration, improving performance on rare and difficult classes.


<details>
  <summary>Details</summary>
Motivation: Long-tailed recognition is challenging due to class imbalance and varying classification difficulty across categories. Simple class frequency reweighting often overlooks intrinsically hard-to-learn classes.

Method: DQRoute estimates class-wise difficulty using prediction uncertainty and historical performance for adaptive loss weighting. It employs a mixture-of-experts design where each expert specializes in different class distribution regions. Expert predictions are weighted by confidence scores from expert-specific OOD detectors at inference.

Result: Experiments on standard long-tailed benchmarks show DQRoute significantly improves performance, particularly on rare and difficult classes.

Conclusion: The integration of difficulty modeling with decentralized expert routing provides substantial benefits for long-tailed visual recognition tasks.

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [36] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT introduces point-level tokens for collaborative perception, addressing limitations of BEV representations by preserving 3D structural information through semantic-aware reordering, frequency-enhanced modeling, and multi-agent alignment.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception methods use 2D BEV representations that discard critical 3D structural information needed for accurate object recognition and localization. Point-cloud data offers richer structural cues but presents challenges due to its unordered, massive, and position-sensitive nature.

Method: CoPLOT framework uses point-level optimized tokens with: 1) Semantic-aware token reordering module for adaptive 1D reordering using scene-level and token-level semantic information, 2) Frequency-enhanced state space model to capture long-range dependencies across spatial and spectral domains, 3) Neighbor-to-ego alignment module with global agent-level correction and local token-level refinement to handle localization noise.

Result: Extensive experiments on simulated and real-world datasets show CoPLOT outperforms state-of-the-art models while achieving lower communication and computation overhead.

Conclusion: Point-level tokens with optimized processing pipeline effectively preserve detailed 3D structural information for collaborative perception, demonstrating superior performance with reduced computational costs compared to BEV-based approaches.

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [37] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: Lightweight unsupervised skeleton-based action localization using spatio-temporal graph neural networks that achieves state-of-the-art performance without manual labeling.


<details>
  <summary>Details</summary>
Motivation: Existing supervised and weakly supervised methods for fine-grained action localization require extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios.

Method: Pre-trains an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on pose-sequence denoising with blockwise partitions, then uses a novel Action Dynamics Metric (ADM) computed from low-dimensional embeddings to detect motion boundaries by identifying inflection points in curvature profiles.

Result: Achieves 82.66% mAP and 29.09 ms average localization latency on DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency.

Conclusion: The method generalizes robustly to unseen diving footage without retraining, demonstrating practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [38] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: A compact image denoising method using dynamically generated kernels that generalizes well to unseen noise types and levels despite being trained only on single-level Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: Deep learning denoising methods struggle with generalization to unseen noise distributions and suffer from overfitting despite requiring extensive training data and computational resources.

Method: Uses Feature Extraction Module for noise-invariant features, Global Statistics and Local Correlation Modules to capture noise characteristics, and Kernel Prediction Module to generate pixel-wise varying kernels applied iteratively for denoising.

Result: The compact model (~0.04M parameters) excels across diverse noise types and levels despite being trained only on single-level Gaussian noise, showing superior restoration quality and efficiency.

Conclusion: Iterative dynamic filtering shows promise for practical image denoising by preventing overfitting and improving resilience to unseen noise through adaptive kernel generation.

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [39] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge is a benchmark that systematically evaluates positional bias in large video language models (LVLMs) across different context positions, revealing significant biases in open-source models while commercial models show more consistent performance.


<details>
  <summary>Details</summary>
Motivation: Existing video understanding benchmarks assess overall performance but overlook nuanced behaviors like contextual positional bias, which is critical for understanding LVLM performance in real-world scenarios.

Method: Created a benchmark with 438 curated videos, 1,177 multiple-choice questions, and 120 open-ended questions. Used standardized probes with flexible control over context length, position, and types. Employed statistical measures and morphological pattern recognition for bias analysis.

Result: Evaluation of 27 state-of-the-art LVLMs revealed significant positional biases in many leading open-source models (typically head or neighbor-content preferences), while commercial models like Gemini2.5-Pro showed consistent performance across entire sequences.

Conclusion: The benchmark provides actionable insights for mitigating bias and guiding model enhancement, highlighting the need for addressing positional bias in LVLM development and evaluation.

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [40] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: Proposes ODAL framework using distributed vision foundation models for car interior object detection, achieving 89% score with fine-tuned LLaVA model that outperforms GPT-4o by 20%


<details>
  <summary>Details</summary>
Motivation: AI tasks like object detection in car interiors are crucial for personal assistants but face computational constraints in on-board vehicle systems

Method: Distributed architecture splitting tasks between on-board and cloud, leveraging vision foundation models with fine-tuning of lightweight models like LLaVA 1.5 7B

Result: Fine-tuned ODAL-LLaVA achieves 89% ODAL score (71% improvement over baseline), outperforms GPT-4o by nearly 20%, and reduces hallucinations with 3x higher SNR than GPT-4o

Conclusion: The framework demonstrates potential to set new standards for interior scene understanding in vehicles by overcoming computational constraints through distributed architecture and model optimization

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [41] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1 is a self-rewarding reinforcement learning method that improves visual reasoning in VLMs by decomposing reasoning into visual perception and language stages, using the model's own outputs for supervision without external annotations.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from visual hallucinations and language shortcuts due to sparse visual supervision and over-reliance on text priors. Existing methods using human annotations or external models are costly and cause distributional shifts.

Method: Decomposes VLM reasoning into visual perception and language reasoning stages. The model generates self-contained visual perceptions, then re-prompted to perform reasoning using only these perceptions to compute self-reward. Combines this with final output supervision.

Result: Improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.

Conclusion: Vision-SR1 provides an effective self-supervised approach that strengthens both visual perception and language reasoning without external visual supervision, addressing core limitations of current VLM training methods.

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [42] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: SNNs show 50-60% energy advantage over CNNs in hardware-agnostic analysis, but significant savings only occur on neuromorphic hardware with high input sparsity. Hardware assumptions and data characteristics critically impact energy efficiency comparisons.


<details>
  <summary>Details</summary>
Motivation: Recent studies question SNNs' reputation for energy efficiency compared to ANNs, especially in digital implementations. This work investigates SNNs for multi-output regression (3D satellite position estimation) to provide transparent energy efficiency evaluation.

Method: Proposed SNN trained using membrane potential of LIF neuron in final layer for 3D satellite position estimation from monocular images. Compared hardware-aware and hardware-agnostic energy estimation methods on photorealistic satellite dataset.

Result: SNN achieves comparable MSE to reference CNN. Hardware-agnostic methods show 50-60% energy advantage for SNNs, but hardware-aware analysis reveals significant savings only on neuromorphic hardware with high input sparsity. Dark pixel ratio significantly influences energy consumption.

Conclusion: Findings emphasize the need for transparent evaluation methods and explicit disclosure of underlying hardware assumptions to ensure fair neural network energy efficiency comparisons. Energy savings depend heavily on hardware platform and data characteristics.

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [43] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: A novel frequency-aware self-supervised learning method for Ultra-Wide-Field retinal image enhancement that addresses blurring and uneven illumination while preserving pathological details.


<details>
  <summary>Details</summary>
Motivation: UWF retinal imaging suffers from quality-degrading factors like blurring and uneven illumination that obscure fine details and mask pathological information. Existing methods fail to address UWF's unique requirements for preserving pathological details.

Method: Frequency-aware self-supervised learning with frequency-decoupled image deblurring and Retinex-guided illumination compensation modules. Includes asymmetric channel integration for combining global/local views and color preservation unit for multi-scale spatial/frequency information.

Result: The method enhances visualization quality and improves disease diagnosis performance by restoring fine local details and correcting uneven intensity. First attempt for UWF image enhancement.

Conclusion: Provides a robust and clinically valuable tool for improving retinal disease management through effective UWF image enhancement while preserving critical pathological details.

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [44] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT is a two-process 3D human reconstruction framework that effectively integrates multiple geometric priors (SMPL model, normal maps) to create high-quality textured 3D avatars from single RGB images, addressing geometric ambiguity and data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D human reconstruction faces challenges due to geometric ambiguity from single 2D images and scarcity of 3D training data. Current methods struggle to effectively integrate different geometric modalities, leading to view inconsistencies like facial distortions.

Method: Proposes SAT framework with two processes: 1) Supervisor Feature Regularization module using multi-view networks to provide intermediate features for better fusion of geometric priors, and 2) Online Animation Augmentation module that generates massive training samples from original 3D data through a one-feed-forward animation network.

Result: Extensive experiments on two benchmarks demonstrate superior performance compared to state-of-the-art methods, showing improved reconstruction quality and better handling of geometric ambiguities.

Conclusion: SAT effectively addresses the key challenges in monocular 3D human reconstruction by providing a unified framework for learning various geometric priors and augmenting training data, resulting in high-quality textured 3D avatars without view inconsistencies.

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [45] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: Physics-inspired unsupervised detector using graph theory and statistical physics to distinguish real from synthetic images with 94% accuracy, without needing labeled synthetic data.


<details>
  <summary>Details</summary>
Motivation: Deep generative models create highly realistic synthetic images that undermine media forensics and biometric security, while existing detectors fail on unseen generators or adversarial processing.

Method: Treats synthetic-image detection as community detection on sparse weighted graphs. Uses pretrained CNN features, transforms them into Multi-Edge Type QC-LDPC graph nodes, calibrates edge couplings at Nishimori temperature to create Random Bond Ising Model, and analyzes Bethe-Hessian spectrum gaps.

Result: Achieves over 94% accuracy on binary classification tasks (cat vs dog, male vs female) using real photos from FFHQ/CelebA and synthetic images from GANs/diffusion models, without labeled synthetic data or retraining.

Conclusion: Provides robust unsupervised detection method that works across different generative architectures through spectral analysis showing separated gaps for real images and collapsed spectrum for synthetic ones.

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [46] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS enhances 3D Gaussian Splatting with object segmentation capabilities by introducing cross-view consistent semantic masks, occlusion analysis, and label projection techniques, achieving state-of-the-art performance with 22x faster training.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting lacks 3D segmentation ability, limiting its applicability in scene understanding tasks that require identifying and isolating specific object components.

Method: Introduces cross-view consistent semantic masks, Occlusion Analysis Model to prevent occlusion overfitting, Main Gaussian Labeling model to lift 2D semantic prior to 3D, and Gaussian Projection Filter to avoid label conflicts. Uses random region sampling for efficient optimization.

Result: Outperforms previous state-of-the-art methods including Feature-3DGS in 3D scene segmentation, achieving 22x speedup in training at 1440X1080 resolution.

Conclusion: LabelGS successfully augments 3DGS with segmentation capabilities while maintaining high efficiency, making it suitable for scene understanding tasks requiring object isolation and identification.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [47] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FreeVPS introduces a training-free approach that combines image polyp segmentation (IPS) with SAM2's temporal modeling, using two modules to eliminate spatial errors and prevent error propagation for stable video polyp segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing video polyp segmentation methods struggle to balance spatiotemporal modeling and domain generalization, limiting real clinical applicability. SAM2 suffers from error accumulation during long-term polyp tracking.

Method: Recasts VPS as track-by-detect paradigm using IPS for spatial context and SAM2 for temporal modeling. Adds two training-free modules: intra-association filtering to eliminate spatial inaccuracies, and inter-association refinement to update memory bank and prevent error propagation.

Result: Achieves cutting-edge performance in both in-domain and out-of-domain scenarios. Demonstrates robust tracking capabilities in long-untrimmed colonoscopy videos.

Conclusion: FreeVPS provides a reliable solution for clinical video polyp segmentation by stabilizing SAM2 through synergistic modules that enhance both spatial accuracy and temporal coherence without requiring training.

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [48] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: Robust deepfake detection framework using face foundation models with triplet loss and attribution supervision for better generalization across diverse manipulation types.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models struggle to generalize beyond training distributions, especially for real-world media content, due to increasing realism and accessibility of deepfakes.

Method: Built on FSFM self-supervised face foundation model, fine-tuned with ensemble of deepfake datasets. Uses triplet loss variants for separable embeddings and attribution-based supervision by manipulation type/source dataset.

Result: Extensive experiments show effectiveness in challenging real-world scenarios with strong generalization capabilities.

Conclusion: The framework demonstrates robust deepfake detection with improved generalization through foundation models and specialized training techniques.

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [49] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: POEv2 is a robust line segment detection framework that works for both generic and wireframe detection, achieving state-of-the-art performance by combining with efficient edge detectors.


<details>
  <summary>Details</summary>
Motivation: Existing line segment detectors are specialized for either generic detection (all meaningful segments) or wireframe detection (geometrically meaningful segments with large spatial support), but none work well for both tasks simultaneously.

Method: Improved version of Pixel Orientation Estimation (POE) method that detects line segments from edge strength maps and can be combined with any edge detector.

Result: Achieves state-of-the-art performance on three publicly available datasets when combined with an efficient edge detector.

Conclusion: POEv2 provides a unified framework that effectively handles both generic and wireframe line segment detection tasks, overcoming the limitations of specialized detectors.

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [50] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM is a novel model that enhances light field salient object detection by incorporating self-prompting mechanisms and frequency-domain analysis to better handle multiple objects of varying sizes and prevent small objects from being overwhelmed by noise.


<details>
  <summary>Details</summary>
Motivation: Existing models neglect prompt information extraction in light field SOD tasks and ignore frequency-domain analysis, causing small objects to be overwhelmed by noise.

Method: Proposes SPLF-SAM with Unified Multi-scale Feature Embedding Block (UMFEB) to identify multiple objects of varying sizes, and Multi-scale Adaptive Filtering Adapter (MAFA) to learn frequency features and prevent noise interference.

Result: Extensive experiments show superiority over ten state-of-the-art LF SOD methods.

Conclusion: SPLF-SAM effectively addresses limitations of existing models by integrating prompt information extraction and frequency-domain analysis, achieving superior performance in light field salient object detection.

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [51] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar is a feedforward 3D avatar reconstruction framework that uses a Large Gaussian Reconstruction Transformer to create high-quality 3D Gaussian Splatting models from various input types (single image, multi-view, or video) within seconds.


<details>
  <summary>Details</summary>
Motivation: Current 3D avatar reconstruction methods suffer from high time complexity, sensitivity to data quality, and low data utilization efficiency, limiting their practical usability.

Method: Uses a VGGT-style transformer architecture with multi-granular guidance encoding (camera pose, FLAME expression, head pose) and incremental Gaussian aggregation via landmark tracking and sliced fusion losses to predict canonical 3DGS representations.

Result: FastAvatar achieves higher quality reconstruction with competitive speed compared to existing methods, enabling incremental quality improvement with more observations.

Conclusion: The framework provides a quality-speed-tunable paradigm for highly usable avatar modeling that efficiently leverages diverse input data without wasting information.

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [52] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet is a new large-scale dataset of 7,856 high-resolution pollinator images with over 8,000 annotated instances across honeybees, bumblebees, and unidentified insects, collected in real agricultural field conditions to support automated pollinator monitoring.


<details>
  <summary>Details</summary>
Motivation: Pollinator insects are vital to global food production but their populations are declining due to environmental stressors. There is a need for scalable, automated monitoring solutions to track pollinator populations.

Method: Created BuzzSet dataset with manually verified labels using YOLOv12 model and human verification. Images were preprocessed into 256x256 tiles. Used RF-DETR transformer-based object detector for baseline performance evaluation.

Result: Achieved high F1-scores: 0.94 for honeybees and 0.92 for bumblebees, with minimal misclassification between these categories. Best mAP@0.50 of 0.559. Unidentified class was more challenging due to label ambiguity.

Conclusion: BuzzSet provides a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision applications, enabling better automated pollinator monitoring.

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [53] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: AIM (Adaptive Intra-Network Modulation) addresses optimization bias in imbalanced multimodal learning by adaptively modulating parameters across network depths, achieving balanced learning without hindering any modality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for imbalanced multimodal learning typically hinder dominant modalities to promote weaker ones, which negatively impacts overall performance. The paper identifies optimization bias within networks as an overlooked problem causing this issue.

Method: Proposes AIM which: 1) Decouples under-optimized parameters of dominant modality into Auxiliary Blocks, 2) Encourages reliance on these degraded blocks for joint training with weaker modalities, 3) Adaptively assesses modality imbalance across network depths and adjusts modulation strength at each depth.

Result: AIM outperforms state-of-the-art methods across multiple benchmarks and shows strong generalizability across different backbones, fusion strategies, and optimizers.

Conclusion: AIM successfully addresses optimization bias in multimodal networks, enabling balanced learning without performance degradation of any modality, representing a significant advancement in imbalanced multimodal learning.

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [54] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: This paper introduces a structural recognition approach for handwritten math expressions that provides explicit symbol-to-trace alignment, enabling better error analysis and interpretability compared to traditional encoder-decoder models.


<details>
  <summary>Details</summary>
Motivation: Current encoder-decoder architectures with large language models excel at LaTeX generation but lack explicit symbol-to-trace alignment, which is critical for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates.

Method: Two innovations: 1) automatic annotation system using neural network to map LaTeX equations to raw traces for generating symbol segmentation, classification, and spatial relation annotations; 2) modular structural recognition system that independently optimizes segmentation, classification, and relation prediction using graph-based trace sorting, hybrid convolutional-recurrent network, and transformer-based correction.

Result: Achieves competitive performance on the CROHME-2023 benchmark and generates a complete graph structure that directly links handwritten traces to predicted symbols.

Conclusion: The structural recognition system enables transparent error analysis and interpretable outputs, addressing critical limitations of existing approaches for handwritten mathematical expression recognition.

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [55] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo introduces motion-aware partitioning of 3D Gaussians to improve dynamic scene reconstruction by separating high/low dynamic regions and using specialized deformation networks with cross-frame consistency loss.


<details>
  <summary>Details</summary>
Motivation: Existing deformation-based methods for dynamic 3D Gaussian Splatting produce blurred renderings and lose fine motion details in highly dynamic regions due to limitations of unified motion modeling.

Method: Dynamic score-based partitioning strategy separates high/low dynamic 3D Gaussians. High-dynamic Gaussians are recursively partitioned temporally with duplicated deformation networks per segment, while low-dynamic ones remain static. Cross-frame consistency loss ensures visual continuity.

Result: MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly excelling in regions with complex or rapid motions.

Conclusion: The motion-aware partitioning approach with specialized modeling for high-dynamic regions and cross-frame consistency effectively captures intricate motion details and improves dynamic scene reconstruction quality.

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [56] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic is a one-step diffusion model for multi-view material estimation that produces high-quality material parameters with low variance, outperforming previous multi-step diffusion methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based material estimation methods use multi-step denoising which is time-consuming and produces high variance results, conflicting with the deterministic nature of material estimation tasks.

Method: Uses a one-step diffusion model with pixel-space losses designed based on material properties, and introduces a Detail Injection Network (DIN) to eliminate detail loss from VAE encoding and enhance prediction sharpness.

Result: Achieves 9.9% improvement in PSNR of albedo, and reduces MSE for metallic and roughness by 44.4% and 60.0% respectively compared to state-of-the-art methods.

Conclusion: StableIntrinsic demonstrates that one-step diffusion models can effectively solve material estimation tasks with higher quality, lower variance, and faster inference than multi-step approaches.

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [57] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Text-to-image models struggle with multi-color prompts. This paper introduces a dedicated editing technique that significantly improves semantic alignment for multi-object color prompts across various diffusion models.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation methods fail to accurately capture precise semantics in complex multi-object prompts, particularly with multiple color attributes. Existing evaluation metrics are coarse and human evaluations are difficult to scale.

Method: The authors perform a case study on color attributes and introduce a dedicated image editing technique specifically designed to address multi-object semantic alignment issues for prompts containing multiple colors.

Result: Pretrained models struggle significantly with multi-color prompts compared to single-color prompts. The proposed editing technique significantly boosts performance across a wide range of metrics and works with various text-to-image diffusion techniques.

Conclusion: Multi-color semantic alignment remains a challenging problem that existing inference-time techniques cannot reliably solve. The dedicated editing approach effectively mitigates these issues and improves faithfulness to complex color prompts.

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [58] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: Enhanced neural architecture with attention mechanisms and data fusion for improved waste sorting accuracy using multi-modal image data


<details>
  <summary>Details</summary>
Motivation: Automating waste sorting is challenging due to complex and variable waste streams, requiring more accurate and efficient systems

Method: Encoder-Decoder architecture with Comprehensive Attention Block, Mamba attention mechanism, and Data Fusion Block using PCA for dimensionality reduction of multi-channel images

Result: Outperforms existing methods significantly across RGB, hyperspectral, multispectral, and combined RGB-hyperspectral data

Conclusion: The proposed architecture with integrated attention mechanisms and data fusion effectively improves waste sorting accuracy and efficiency

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [59] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: A bag of tricks training approach for robust, real-time mitotic figure detection using RTMDet single-stage object detector, achieving high F1 scores (0.78-0.84) across diverse domains while maintaining clinical deployment speed.


<details>
  <summary>Details</summary>
Motivation: Mitotic figure detection faces challenges due to variations in slide scanners, staining protocols, tissue types, and artifacts, requiring robust solutions for clinical use.

Method: Built on RTMDet single-stage object detector with multi-domain training data, balanced sampling, careful augmentation, and targeted hard negative mining on necrotic/debris tissue to reduce false positives.

Result: Achieved F1 scores between 0.78-0.84 in grouped 5-fold cross-validation, and 0.81 F1 on MIDOG 2025 preliminary test set, outperforming larger models with domain adaptability.

Conclusion: The solution provides practical accuracy-speed trade-off suitable for real-world clinical adoption, demonstrating robust performance across diverse and unfamiliar domains.

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [60] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: CSSL framework uses context-aware thresholding to achieve high neuronal sparsity in event-based vision tasks without explicit constraints, matching state-of-the-art performance while being efficient for neuromorphic processing.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer advantages for robot perception but existing deep learning methods don't fully leverage event data sparsity, and neuromorphic approaches struggle to match performance in complex tasks while maintaining sparsity.

Method: Proposed Context-aware Sparse Spatiotemporal Learning (CSSL) with context-aware thresholding that dynamically regulates neuron activations based on input distribution to naturally reduce activation density.

Result: CSSL achieves comparable or superior performance to state-of-the-art methods in event-based object detection and optical flow estimation while maintaining extremely high neuronal sparsity.

Conclusion: CSSL enables efficient event-based vision for neuromorphic processing by naturally achieving high activation sparsity without manual tuning of sparsity constraints.

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [61] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS is an unsupervised video instance segmentation framework that uses quality-guided self-training to bridge the synthetic-to-real domain gap without human annotations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Video Instance Segmentation requires pixel-level masks and temporal consistency labels, which are challenging to annotate. Existing unsupervised methods suffer from synthetic-to-real domain gap limitations.

Method: Quality-guided self-training with a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos.

Result: Achieved 52.6 AP50 on YouTubeVIS-2019 val set, surpassing previous state-of-the-art VideoCutLER by 4.4% without human annotations.

Conclusion: Quality-aware self-training is viable for unsupervised VIS, effectively bridging the synthetic-to-real domain gap and achieving competitive performance.

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [62] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: ERSR: A novel semi-supervised framework for fetal head ultrasound segmentation using dual-scoring filtering, ellipse-constrained refinement, and symmetry-based consistency regularization to address poor image quality and limited annotated data.


<details>
  <summary>Details</summary>
Motivation: Automated fetal head segmentation in ultrasound is critical for prenatal monitoring but faces challenges due to poor image quality and lack of annotated data. Existing semi-supervised methods struggle with the unique characteristics of fetal head ultrasound images.

Method: Proposed ERSR framework with three components: 1) Dual-scoring adaptive filtering strategy using boundary consistency and contour regularity criteria, 2) Ellipse-constrained pseudo-label refinement via least-squares ellipse fitting, 3) Symmetry-based multiple consistency regularization across perturbed images, symmetric regions, and predictions.

Result: State-of-the-art performance: HC18 dataset - 92.05% Dice with 10% labeled data, 95.36% with 20% labeled data; PSFH dataset - 91.68% with 10% labeled data, 93.70% with 20% labeled data.

Conclusion: The ERSR framework effectively addresses challenges in fetal head ultrasound segmentation by generating reliable pseudo-labels and enforcing effective consistency constraints, achieving superior performance with limited labeled data.

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [63] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: A novel calibration framework that improves model reliability under distribution shift without requiring target domain information, using low-frequency filtering and gradient-based rectification.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks produce overconfident predictions that become worse under distribution shift, and existing methods require impractical access to target domain data.

Method: Uses frequency-domain analysis to identify that distribution shifts distort high-frequency cues, applies low-frequency filtering to focus on domain-invariant features, and adds gradient-based rectification to maintain in-distribution calibration.

Result: Significantly improves calibration under distribution shift on CIFAR-10/100-C and WILDS datasets while maintaining strong in-distribution performance.

Conclusion: The proposed framework effectively addresses calibration under distribution shift without target domain access, making it practical for real-world applications.

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [64] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: Proposes a machine-centric image quality assessment (MIQA) framework to evaluate how image degradations affect machine vision systems, including a large database (MIQD-2.5M) and a region-aware model (RA-MIQA) that outperforms traditional human-centric metrics.


<details>
  <summary>Details</summary>
Motivation: Machine vision systems are vulnerable to performance degradation under adverse visual conditions, and traditional human visual system-based quality metrics are inadequate for assessing machine-centric image quality.

Method: Established an end-to-end MIQA paradigm, constructed a 2.5M-sample database (MIQD-2.5M) with 75 vision models and 250 degradation types, and developed a region-aware MIQA (RA-MIQA) model for fine-grained spatial degradation analysis.

Result: RA-MIQA achieved significant performance gains (13.56% SRCC on consistency and 13.37% on accuracy for image classification) over traditional HVS-based metrics and revealed task-specific degradation sensitivities. HVS-based metrics proved inadequate for MVS quality prediction.

Conclusion: The study advances MVS reliability and establishes foundations for machine-centric image processing and optimization, demonstrating that specialized MIQA models are needed as traditional human-centric approaches are insufficient for machine vision applications.

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [65] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: A unified two-stage framework for joint action prediction and visual future generation in egocentric scenarios, using hand trajectories and multi-modal fusion with causal cross-attention.


<details>
  <summary>Details</summary>
Motivation: Existing approaches either predict actions without modeling visual consequences (VLA models) or generate future frames without action conditioning (video prediction models), leading to incomplete understanding of human-object interactions.

Method: Two-stage approach: 1) Consecutive state modeling processes visual observations, language, and action history to predict future hand trajectories; 2) Causal cross-attention fuses multi-modal cues to guide a Latent Diffusion Model for frame-by-frame video generation.

Result: Outperforms state-of-the-art baselines on Ego4D, BridgeData, and RLBench datasets in both action prediction and future video synthesis tasks.

Conclusion: The proposed framework successfully bridges the gap between action prediction and visual outcome modeling, providing a unified solution for egocentric human activity understanding and robotic manipulation tasks.

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [66] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN is a multimodal conditional mesh-to-mesh GAN for 3D aortic aneurysm growth prediction, combining local KNN-based convolutional networks with global graph convolutional networks to overcome over-smoothing limitations and enable personalized disease trajectory modeling.


<details>
  <summary>Details</summary>
Motivation: Personalized prediction of aortic aneurysm progression is challenging due to the need to model both subtle local deformations and global anatomical changes within complex 3D geometries, requiring accurate and clinically deployable solutions.

Method: Dual-branch architecture with local KNN-based convolutional network (KCN) for fine-grained geometric details and global graph convolutional network (GCN) for long-range structural context. Dedicated condition branch encodes clinical attributes and target time interval for anatomically plausible predictions.

Result: Outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation on the TAAMesh dataset (590 multimodal records from 208 patients).

Conclusion: MCMeshGAN provides a robust framework for clinically deployable, personalized 3D disease trajectory modeling of aortic aneurysm progression, with publicly available source code.

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [67] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: Self-supervised learning approach that builds structured visual representations using semantic grouping, instance separation, and hierarchical structuring with a novel ProtoScale module, outperforming state-of-the-art methods in object detection tasks.


<details>
  <summary>Details</summary>
Motivation: Current SSL approaches excel at global image understanding but lack structured scene representation capabilities needed for dense prediction tasks like object detection.

Method: Proposes a self-supervised approach with ProtoScale module that combines semantic grouping, instance level separation, and hierarchical structuring while preserving full scene context across augmented views.

Result: Achieves superior performance in object detection tasks on COCO and UA-DETRAC datasets, even with limited annotated data and fewer fine-tuning epochs compared to state-of-the-art methods.

Conclusion: The method successfully learns object-centric representations that enhance supervised object detection, demonstrating the value of structured visual representation learning in SSL.

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [68] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet is a transformer-based model that combines predicted pedestrian trajectories and vehicle speeds to predict crossing intention, achieving state-of-the-art performance with low inference time.


<details>
  <summary>Details</summary>
Motivation: With autonomous vehicles on public roads, accurately predicting pedestrian crossing intention is crucial for safety. Existing approaches need improvement in both accuracy and computational efficiency.

Method: Uses two-branch transformer architecture: Sequence Attention Module (SAM) for sequential trajectory/speed data, and Visual Attention Module (VAM) for visual representation of predicted trajectories overlaid on scene images.

Result: Achieves state-of-the-art results across three major datasets and has the lowest total inference time (including preprocessing) among current approaches.

Conclusion: TrajFusionNet effectively combines trajectory prediction and visual attention with lightweight modalities, providing both high accuracy and computational efficiency for pedestrian crossing intention prediction.

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [69] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: SMI: A sky background estimation model using mutual information and incremental training to improve sky subtraction in fiber spectra by leveraging all fibers rather than just sky fibers.


<details>
  <summary>Details</summary>
Motivation: Current sky background subtraction relies mainly on sky fiber spectra to build Super Sky, which lacks modeling of the environment surrounding objects, leading to insufficient background estimation.

Method: SMI uses two networks: 1) wavelength calibration module to extract sky features and solve feature shift problems, 2) incremental training to maximize mutual information between different spectra for common components and minimize mutual information between adjoining spectra for individual components.

Result: Experiments on LAMOST spectra show SMI obtains better object sky background during observation, especially in the blue end of the spectrum.

Conclusion: SMI effectively addresses the limitations of traditional sky subtraction methods by utilizing all fiber spectra and mutual information techniques, providing more accurate sky background estimation particularly in challenging spectral regions.

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [70] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: MS-LiDAR combined with deep learning models (SPT, PTv3, PTv1) achieves high-accuracy urban tree extraction, with SPT showing best performance (85.28% mIoU) and pNDVI integration reducing error by 10.61%.


<details>
  <summary>Details</summary>
Motivation: To improve urban tree monitoring for greening policies and electrical infrastructure risk reduction by addressing challenges in complex urban environments using advanced sensing and AI techniques.

Method: Used multispectral LiDAR to capture 3D spatial and spectral data, evaluated three deep learning models (Superpoint Transformer, Point Transformer V3, Point Transformer V1) for tree point extraction, and incorporated pseudo NDVI with spatial data.

Result: SPT demonstrated notable time efficiency and accuracy with 85.28% mIoU. Adding pNDVI to spatial data reduced error rate by 10.61 percentage points compared to spatial-only approach.

Conclusion: MS-LiDAR combined with deep learning shows strong potential for improving urban tree extraction and inventory management, with spectral data integration significantly enhancing detection accuracy.

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [71] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: PersonaAnimator: A novel framework for video-to-video motion personalization that learns personalized motion patterns from unconstrained videos, addressing limitations in style transfer, data dependency, and physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation methods have three key limitations: (1) they replicate motion without learning style characteristics, (2) rely heavily on motion capture data that's difficult to obtain, and (3) sometimes generate physically implausible motions.

Method: Proposes PersonaAnimator framework that learns personalized motion patterns directly from unconstrained videos. Introduces PersonaVid dataset with 20 motion content and 120 style categories. Uses Physics-aware Motion Style Regularization to ensure physical plausibility.

Result: Extensive experiments show PersonaAnimator outperforms state-of-the-art motion transfer methods and establishes a new benchmark for video-to-video motion personalization.

Conclusion: The paper pioneers the video-to-video motion personalization task and demonstrates successful learning of personalized motion patterns from videos while ensuring physical plausibility, setting a new standard in the field.

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [72] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: First comprehensive review of hyperspectral imaging for automotive applications, revealing significant gap between research potential and commercial readiness - only 4 cameras meet performance thresholds and none comply with automotive temperature standards.


<details>
  <summary>Details</summary>
Motivation: HSI offers transformative sensing for ADAS/autonomous driving with material-level scene understanding beyond RGB capabilities, but its practical automotive integration needs systematic evaluation.

Method: Comprehensive qualitative review plus quantitative analysis of 216 commercial HSI/multispectral cameras against automotive criteria (frame rate, spatial resolution, spectral dimensionality, AEC-Q100 compliance), plus review of HSI datasets and applications.

Result: Only 4 cameras meet performance thresholds, none comply with AEC-Q100. Current HSI datasets are limited in scale, spectral consistency, channel count, and environmental diversity, posing challenges for algorithm development and validation.

Conclusion: Establishes current state of HSI in automotive (2025) and outlines key research directions needed for practical integration of spectral imaging in ADAS/autonomous systems.

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [73] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: OSS metric enables efficient AL method evaluation without detector training and robust validation set selection using object similarity.


<details>
  <summary>Details</summary>
Motivation: Address computational costs and reliability issues in active learning for object detection, particularly in autonomous driving where training detectors is expensive (up to 282 GPU hours) and method rankings vary across validation sets.

Method: Introduces object-based set similarity (OSS) metric that measures similarity between training sets and target domains using object-level features, eliminating need for detector training. Validated on KITTI, BDD100K, CODA datasets with uncertainty-based AL methods and EfficientDet/YOLOv3 architectures.

Result: OSS enables elimination of ineffective AL methods before training and selection of representative validation sets for robust evaluation. It's detector-agnostic, requires only labeled object crops, and integrates with existing AL pipelines.

Conclusion: OSS provides a practical framework for deploying active learning in real-world applications where computational efficiency and evaluation reliability are critical, unifying AL training and evaluation strategies based on object similarity.

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [74] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: A novel 3D semantic segmentation method that leverages 2D foundation models to augment sparse 3D annotations by propagating 2D segmentation masks into 3D space and using consistency regularization to generate reliable pseudo labels.


<details>
  <summary>Details</summary>
Motivation: Current 3D segmentation methods don't leverage complementary 2D-3D data and fail to fully utilize available labels. The emergence of powerful 2D foundation models provides an opportunity to enhance 3D segmentation with limited annotations.

Method: Propagates 2D segmentation masks from foundation models into 3D space using geometric correspondences, extends sparse 3D annotations with 3D masks, applies confidence- and uncertainty-based consistency regularization on 3D point cloud augmentations, and selects reliable pseudo labels to generate more training data.

Result: The approach substantially augments available labels and bridges the gap between limited 3D annotations and powerful 2D foundation models.

Conclusion: This innovative strategy improves 3D weakly supervised segmentation performance by maximizing utility of sparse 3D annotations through integration with 2D foundation model capabilities.

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [75] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR: A hierarchical transformer with wavelet transform for image super-resolution that uses adaptive windows and multi-frequency decomposition to capture long-range dependencies while reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Transformer-based SR methods suffer from quadratic computational complexity in window self-attention, forcing small fixed windows that limit receptive field and long-range dependency modeling.

Method: Embed wavelet transform within hierarchical transformer framework with adaptive hierarchical windows, decomposing images into frequency subbands to focus on global/local features while preserving structural details through progressive reconstruction.

Result: Achieves state-of-the-art SR results with higher efficiency - fewer parameters, lower FLOPs, and faster speeds compared to SwinIR-Light, SwinIR-NG, and SRFormer-Light.

Conclusion: WaveHiT-SR effectively addresses computational complexity limitations while maintaining performance through hierarchical processing and wavelet-based frequency decomposition.

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [76] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA is a new benchmark for Korean text-rich visual question answering that addresses the lack of comprehensive evaluation resources for low-resource languages, featuring diverse visual contexts and a semi-automated data generation pipeline.


<details>
  <summary>Details</summary>
Motivation: There's a critical gap in text-rich VQA benchmarks for low-resource languages like Korean, which hinders robust model evaluation and comparison, while high-resource languages like English have well-established datasets.

Method: Developed KRETA benchmark with 15 domains and 26 image types, using a semi-automated VQA generation pipeline optimized for text-rich settings with refined stepwise image decomposition and seven-metric evaluation protocol for quality assurance.

Result: Created a comprehensive Korean text-rich VQA benchmark that enables in-depth evaluation of visual text understanding and reasoning capabilities across diverse visual contexts.

Conclusion: KRETA fills the critical gap for Korean language VQA evaluation while providing an adaptable pipeline that can facilitate similar benchmark development for other languages, accelerating multilingual VLM research.

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [77] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: A study of Chan-Vese algorithm for image segmentation with a proposed functional segmentation loss based on active contours, implemented in MATLAB and PyTorch, with performance comparison against classical methods.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive analysis of the Chan-Vese algorithm and develop a modern functional segmentation loss based on active contours for improved image segmentation performance.

Method: Employed discretized scheme from Chan-Vese model's functional energy and PDE level set function, implemented in MATLAB, and proposed PyTorch-based functional segmentation loss using pytorch.nn.ModuleLoss with Chan-Vese level set approach.

Result: Compared results with common computer vision segmentation datasets and evaluated performance against classical loss functions, with all code and materials made publicly available.

Conclusion: The study provides both theoretical proof and practical implementation of Chan-Vese algorithm, offering a modern functional loss approach for image segmentation tasks with publicly accessible resources for reproducibility.

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [78] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: Comprehensive assessment of 25 state-of-the-art Vision-Language Models' geolocation capabilities reveals high accuracy (61%) on social media-like images, raising urgent privacy concerns despite poor performance on generic street-level images.


<details>
  <summary>Details</summary>
Motivation: Geo-localization using VLMs poses significant privacy risks (stalking, surveillance) due to widespread AI model usage and photo sharing on social media, yet there's little systematic evaluation of their geolocation precision and limits.

Method: Conducted comprehensive assessment of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments to evaluate geolocation capabilities.

Result: Current VLMs perform poorly on generic street-level images but achieve notably high accuracy (61%) on images resembling social media content.

Conclusion: The findings highlight significant and urgent privacy concerns as VLMs demonstrate strong geolocation capabilities on social media-style images, raising risks for unintended inferences and societal privacy issues.

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [79] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim is a training-free framework that combines global and local embedding similarity signals to detect object hallucinations in vision-language models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Object hallucination poses safety risks for real-world deployment of vision-language models, and current detection methods using either global or local perspectives alone have limited reliability.

Method: GLSim leverages complementary global and local embedding similarity signals between image and text modalities without requiring training, enabling more accurate hallucination detection.

Result: GLSim achieves superior detection performance, significantly outperforming competitive baselines in comprehensive benchmarking of object hallucination detection methods.

Conclusion: The proposed GLSim framework provides more accurate and reliable object hallucination detection by effectively combining global and local perspectives, addressing limitations of existing approaches.

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [80] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS (Generative Segmentation) formulates image segmentation as a generative task using label diffusion, directly generating segmentation masks from noise conditioned on images and language descriptions, achieving state-of-the-art performance on panoptic narrative grounding.


<details>
  <summary>Details</summary>
Motivation: Traditional segmentation methods treat the task discriminatively, while existing diffusion approaches remain image-centric. The authors propose to make segmentation itself the primary generative modeling target rather than treating it as an auxiliary process.

Method: GS reverses the typical generative process - instead of generating images from label maps, it directly generates segmentation masks from noise conditioned on both input image and language description using label diffusion, enabling end-to-end training with explicit spatial and semantic control.

Result: GS significantly outperforms existing discriminative and diffusion-based methods on Panoptic Narrative Grounding (PNG) benchmark, setting a new state-of-the-art for language-driven segmentation.

Conclusion: Formulating segmentation as a generative task via label diffusion proves highly effective, demonstrating that making label generation the primary modeling target enables superior performance in language-driven image segmentation tasks.

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [81] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: A novel framework called SegAssist for Incremental Test Time Adaptation of Vision Language Models that handles both unseen classes and domains during testing using segmentation-assisted active labeling without requiring training.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of dynamic environments where unfamiliar objects and distribution shifts occur during testing, requiring models to continuously adapt to both covariate and label shifts as new classes emerge.

Method: Proposes SegAssist - a training-free segmentation-assisted active labeling module that repurposes VLMs' segmentation capabilities to refine active sample selection, prioritizing samples likely from unseen classes and querying an oracle for labeling.

Result: Extensive experiments on benchmark datasets demonstrate SegAssist's effectiveness in enhancing VLM performance in real-world scenarios requiring continuous adaptation to emerging data.

Conclusion: SegAssist provides a practical solution for incremental test time adaptation, enabling VLMs to better handle dynamic environments with continuously appearing unseen classes and domains through improved active sample selection.

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [82] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D is a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations, using 2D-induced voxel features and achieving superior accuracy and speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary 3D object detection through image-based methods remains limited compared to 3D point cloud-based methods, creating a need for efficient annotation-free approaches.

Method: Single-stage detector using 2D-induced voxel features from ImGeoNet, trained with class-agnostic 3D localization loss and voxel-semantic alignment loss. Uses 3D Pseudo Box Generation with graph embedding to combine 2D segments into coherent 3D structures.

Result: Achieves higher precision and recall than other methods, including OV-3DET. Demonstrates superior accuracy and speed (0.3 sec per scene) on ScanNet200 and ARKitScenes benchmarks, outperforming two-stage methods and baselines.

Conclusion: OpenM3D provides an efficient, annotation-free solution for open-vocabulary 3D object detection that requires only multi-view images as input and delivers state-of-the-art performance in both accuracy and speed.

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [83] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: This paper presents a top-10 ranking solution for the MARIO challenge on AMD progression monitoring using OCT scans, featuring a fusion CNN for classification and a novel masked autoencoder for future OCT generation and progression prediction.


<details>
  <summary>Details</summary>
Motivation: Timely diagnosis and consistent monitoring of neovascular AMD are crucial for effective anti-VEGF treatment outcomes, requiring better tools to track disease progression in OCT scans.

Method: For Task 1 (classification between consecutive OCT pairs): fusion CNN network with model ensembling. For Task 2 (3-month progression prediction): Patch Progression Masked Autoencoder that generates future OCT scans and then classifies evolution using the Task 1 solution.

Result: Achieved Top 10 ranking in both tasks of the MARIO challenge, though ineligible for prizes due to organizational affiliations with challenge organizers.

Conclusion: The proposed methods effectively address AMD progression monitoring challenges, demonstrating strong performance in both classification and predictive tasks using OCT imaging data.

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [84] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: PAUL framework addresses noisy correspondence in cross-view geo-localization by using uncertainty learning to partition and augment training data, achieving superior performance in handling GPS drift and misaligned image pairs.


<details>
  <summary>Details</summary>
Motivation: Existing cross-view geo-localization methods assume perfect image pair alignment during training, but real-world factors like GPS drift, urban canyon effects, and adverse weather cause systematic alignment shifts with only partial correspondences, creating noisy data that current research overlooks.

Method: Proposes PAUL (Partition and Augmentation by Uncertainty Learning) - a framework that uses uncertainty-aware co-augmentation and evidential co-training to estimate data uncertainty, selectively augment high-confidence regions, and refine feature learning to suppress noise from misaligned pairs.

Result: Comprehensive experiments show PAUL consistently achieves superior performance over other competitive noisy-correspondence-driven methods across various noise ratios, validating the effectiveness of its individual components.

Conclusion: PAUL successfully bridges the gap between idealized benchmarks and practical applications by addressing the NC-CVGL problem through targeted partitioning and augmentation based on uncertainty estimation, providing robust supervision for noisy samples without traditional filtering or label correction.

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [85] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: Discrete Diffusion VLA is a unified transformer policy that uses discrete diffusion to model action chunks, achieving adaptive decoding order and robust error correction while maintaining compatibility with VLM backbones.


<details>
  <summary>Details</summary>
Motivation: Current VLA decoders either use fixed autoregressive order or continuous diffusion heads that require specialized training and iterative sampling, hindering unified and scalable architectures.

Method: Single-transformer policy that models discretized action chunks with discrete diffusion, trained with cross-entropy objective, featuring adaptive decoding order and secondary remasking for error correction.

Result: Achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal, and 49.3% overall on SimplerEnv Bridge, outperforming both autoregressive and continuous diffusion baselines.

Conclusion: Discrete-diffusion action decoder enables precise action modeling and consistent training, providing foundation for scaling VLA to larger models and datasets.

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [86] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: A novel calibration framework that integrates dual-fisheye camera modeling into 3D Gaussian splatting to transform imperfect 360-degree inputs into seamless renderings.


<details>
  <summary>Details</summary>
Motivation: Consumer-grade dual-fisheye systems produce imperfect panoramas due to lens separation and angular distortions, limiting the quality of 360-degree content for VR, robotics, and autonomous navigation applications.

Method: Incorporates a dual-fisheye camera model into 3D Gaussian splatting pipeline, jointly optimizing 3D Gaussian parameters with calibration variables that emulate lens gaps and angular distortions.

Result: Extensive evaluations show the method produces seamless renderings from imperfect images and outperforms existing 360-degree rendering models.

Conclusion: The framework successfully transforms imperfect omnidirectional inputs into flawless novel view synthesis, addressing inherent limitations of dual-fisheye camera systems.

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory is a unified framework that integrates LLMs with text-to-audio systems to generate coherent long-form audio narratives, addressing limitations of current TTA methods in temporal coherence and compositional reasoning.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio generation systems excel at short audio clips but struggle with long-form narrative audio that requires temporal coherence, scene transitions, and emotional consistency.

Method: Uses LLMs to decompose narrative queries into temporally ordered sub-tasks, features a decoupled bridging mechanism (bridging query for intra-event alignment and residual query for cross-event coherence), and employs end-to-end training to unify instruction comprehension with audio generation.

Result: Outperforms prior TTA baselines in both instruction-following ability and audio fidelity, with extensive experiments demonstrating superiority in single-audio and narrative audio generation.

Conclusion: AudioStory provides an effective solution for long-form audio narrative generation by leveraging LLM reasoning capabilities and specialized bridging mechanisms, establishing a new benchmark (AudioStory-10K) for diverse audio narrative domains.

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: Lightweight moth classification using knowledge distillation from BioCLIP2 to ConvNeXt-tiny achieves comparable accuracy to large models with reduced computational cost for insect monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate species identification of moths from automated camera systems is crucial for understanding insect declines, but challenging due to domain shifts between curated images and noisy field imagery.

Method: Proposed lightweight classification approach combining limited expert-labelled field data with knowledge distillation from BioCLIP2 foundation model into ConvNeXt-tiny architecture.

Result: BioCLIP2 substantially outperforms other methods, and the distilled lightweight model achieves comparable accuracy with significantly reduced computational cost on 101 Danish moth species from AMI camera systems.

Conclusion: The approach offers practical guidelines for developing efficient insect monitoring systems and bridging domain gaps for fine-grained classification in ecological applications.

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA is a trainable compositional framework that combines a generalist planner with specialist executors for GUI automation in scientific computing, using a two-stage training approach to achieve both robust execution and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between generalist agents (good at planning but poor execution) and specialized agents (good execution but poor planning) in GUI automation for scientific computing, where existing compositional frameworks are static and non-trainable.

Method: Two-stage pipeline: 1) Specialization - train expert planners for each scientific application using decoupled GRPO approach bootstrapped from small task trajectories; 2) Generalization - aggregate successful trajectories to build consolidated dataset for supervised fine-tuning of final planner.

Result: CODA significantly outperforms baselines and establishes new state of the art on four challenging applications from ScienceBoard benchmark among open-source models.

Conclusion: CODA successfully bridges the planning-execution gap in GUI automation for scientific domains through its trainable compositional framework, demonstrating both robust execution and cross-domain generalization capabilities.

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>
