<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: Comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data, evaluating trade-offs between model complexity, optimization methods, and approximation quality.


<details>
  <summary>Details</summary>
Motivation: To review and assess optimization-based methods for generating tessellation models (Voronoi, Laguerre, GBPDs) that approximate voxel-based grain structures in materials like polycrystals and foams.

Method: Analyzed linear/nonlinear programming, stochastic optimization (cross-entropy method), and gradient descent for fitting tessellation models. Evaluated fit quality using discrepancy measures (volume, surface area, topology).

Result: Results show trade-offs between model complexity, optimization routine complexity, and approximation quality, aiding method selection based on data and application needs.

Conclusion: Provides guidance for selecting appropriate tessellation fitting methods, balancing complexity and accuracy for real-world datasets.

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: The paper explores efficient models for scene understanding via semantic segmentation in self-driving cars, using the BDD100k dataset and various backbones as encoders, showing improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: To address the need for better scene understanding in autonomous vehicles by leveraging deep learning and semantic segmentation.

Method: Proposes several models for semantic segmentation, tested on the BDD100k dataset, with different backbones as encoders.

Result: Appropriate backbone selection significantly impacts model performance, improving accuracy, mean IoU, and loss function metrics.

Conclusion: The study demonstrates the importance of backbone choice in semantic segmentation models for enhanced scene understanding in autonomous driving.

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA is a gradient-based test-time adaptation method for vision-language models, using a soft contrastive loss to align with CLIP's pre-training, improving performance and stability under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP struggle with generalization under distribution shifts, and entropy minimization for test-time adaptation is misaligned with their contrastive training.

Method: CLIPTTA employs a soft contrastive loss aligned with CLIP's pre-training, with a batch-aware design to prevent collapse. It also includes an Outlier Contrastive Exposure (OCE) loss for open-set scenarios.

Result: CLIPTTA outperforms entropy-based objectives and competes with state-of-the-art TTA methods across 75 datasets, showing stable performance under diverse shifts.

Conclusion: CLIPTTA effectively adapts vision-language models at test time, addressing limitations of entropy minimization and improving generalization under distribution shifts.

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: The paper introduces Attention Focusing (AF) to address distracted attention in Generalized Category Discovery (GCD), improving performance by up to 15.4%.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods often focus on irrelevant background regions, leading to suboptimal feature extraction. AF aims to sharpen the model's focus.

Method: AF consists of Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP), which measure and prune non-informative tokens, respectively.

Result: AF improves performance by up to 15.4% when integrated into SimGCD, with minimal computational overhead.

Conclusion: AF is a lightweight, effective solution for distracted attention in GCD, easily integrable into existing methods.

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: The paper addresses hallucination artifacts in generative super-resolution (GSR) models, proposing a method to measure and mitigate them using a multimodal large language model (MLLM) and deep feature alignment.


<details>
  <summary>Details</summary>
Motivation: GSR models produce perceptual artifacts where generated details mismatch the low-resolution or ground-truth images, limiting practical use. Existing metrics fail to capture these hallucinations.

Method: The authors introduce a 'Hallucination Score' (HS) using an MLLM to assess artifacts. They also identify deep feature distances correlated with HS and use them as differentiable rewards to align GSR models.

Result: HS aligns well with human evaluations and complements existing SR metrics. Deep feature distances show strong correlation with HS.

Conclusion: The proposed method effectively measures and mitigates hallucinations in GSR, improving model alignment and perceptual quality.

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack is a semi-automated toolkit combining deep learning and optical flow for robust point tracking in B-mode ultrasound videos, outperforming zero-shot trackers and matching specialized methods.


<details>
  <summary>Details</summary>
Motivation: Accurate tissue motion tracking in B-mode ultrasound is hindered by speckle noise, low edge contrast, and out-of-plane movement, necessitating a reliable solution for clinical and biomechanical research.

Method: DUSTrack integrates deep learning with optical flow, features a GUI for training data generation, and employs optical-flow-based filtering to reduce noise while preserving motion.

Result: DUSTrack achieves superior accuracy compared to zero-shot trackers and matches specialized methods, demonstrated in cardiac, muscle, and fascicle tracking use cases.

Conclusion: DUSTrack is a versatile, open-source tool for quantifying tissue motion in ultrasound videos, with potential for widespread clinical and research applications.

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT is a neuro-symbolic framework for interpretable affordance grounding, combining commonsense priors and visual evidence to identify action-enabling objects in scenes.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and accuracy in scene understanding by grounding symbolic and perceptual structures transparently.

Method: Integrates ConceptNet, language models, and CLIP in an energy-based reasoning loop for iterative refinement.

Result: Enhances accuracy and interpretability in multi-object, label-free settings.

Conclusion: CRAFT advances robust and trustworthy scene understanding through transparent, goal-driven decisions.

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: A novel framework for streaming 3D Gaussian splatting (3DGS) videos addresses challenges like large data volume and compression complexity, using Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling for efficient transmission.


<details>
  <summary>Details</summary>
Motivation: 3DGS videos offer superior volumetric representation but face streaming challenges due to high data volume and compression complexity.

Method: The framework uses Gaussian deformation fields for video construction, hybrid saliency tiling, and differentiated quality modeling to optimize compression and bandwidth adaptation.

Result: The method outperforms existing approaches in video quality, compression efficiency, and transmission rate.

Conclusion: The proposed framework effectively solves 3DGS video streaming challenges, ensuring high-quality transmission.

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT is a multi-modal large language model for real-world infrared images, leveraging a novel dataset (IR-TD) and a bi-cross-modal curriculum transfer learning strategy to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on synthetic infrared images, limiting their ability to capture real-world infrared characteristics due to scarce aligned text data.

Method: Proposes IRGPT, built on IR-TD (260K real image-text pairs), and a bi-cross-modal curriculum transfer learning strategy to transfer knowledge from visible to infrared domains.

Result: IRGPT achieves state-of-the-art performance on 9 benchmark tasks, surpassing larger-scale models.

Conclusion: IRGPT addresses the limitations of synthetic data and advances infrared vision-language modeling with real-world data and effective transfer learning.

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: A novel Gestalt-guided Parallel Interaction Network (GPI-Net) is proposed for high-quality point cloud registration by fusing local and global features using Gestalt principles, orthogonal integration, and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Accurate point cloud registration requires handling feature redundancy and complex spatial relationships between local and global features. Gestalt principles offer advantages in analyzing these relationships.

Method: GPI-Net uses an orthogonal integration strategy to reduce redundancy and a Gestalt Feature Attention block for geometric features. A Dual-path Multi-Granularity block enhances information exchange.

Result: Extensive experiments show GPI-Net outperforms existing methods in challenging tasks.

Conclusion: GPI-Net effectively integrates local and global features for high-quality correspondences, demonstrating superior performance.

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: The paper proposes adaptive tiling, quality assessment, and bitrate adaptation solutions for 3D Gaussian splatting video streaming, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing fundamental challenges in 3DGS video streaming, such as tiling, quality assessment, and bitrate adaptation, to enhance immersive experiences.

Method: Introduces adaptive tiling guided by saliency, a quality assessment framework, and a meta-learning-based bitrate algorithm.

Result: Proposed methods significantly outperform state-of-the-art techniques in experiments.

Conclusion: The solutions effectively tackle key challenges in 3DGS streaming, improving performance and adaptability.

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS is a Mixture-of-Experts framework for autonomous driving, combining a Global Expert and Scene-Adaptive Experts with a Dual-aware Router to handle diverse scenarios robustly and adaptively.


<details>
  <summary>Details</summary>
Motivation: Single-mode planning struggles with diverse driving scenarios, so GEMINUS aims to improve adaptability and robustness.

Method: Uses a Global Expert for overall performance, Scene-Adaptive Experts for specific scenarios, and a Dual-aware Router to dynamically activate experts.

Result: Outperforms existing methods in Bench2Drive, achieving top Driving Score and Success Rate with monocular vision.

Conclusion: GEMINUS effectively combines experts for adaptive, robust autonomous driving, showing significant improvements over single-expert baselines.

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard is a tamper-resistant framework for embedding metadata links in visualization images, ensuring recoverability after tampering and enabling applications like interactive chart reconstruction and copyright protection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for embedding metadata in visualization images are fragile to common tampering (e.g., cropping, editing), limiting their practicality for Visualization Image Data Retrieval (VIDR).

Method: VisGuard uses repetitive data tiling, invertible information broadcasting, and an anchor-based scheme for crop localization to embed metadata links robustly.

Result: VisGuard outperforms in data retrieval accuracy, embedding capacity, and security against tampering and steganalysis.

Conclusion: VisGuard effectively safeguards visualization dissemination by providing tamper-resistant metadata embedding, enhancing information conveyance and practical applications.

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet introduces a sequence modeling framework for VPR, combining spatial feature extraction and temporal differencing into an end-to-end trainable module, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of VPR in dynamic and perceptually aliased environments by leveraging temporal coherence in image sequences, often neglected by single-frame embedding methods.

Method: Uses a lightweight 1D convolutional encoder and a learnable differential temporal operator (DSD) to capture spatial and temporal context, enhanced by a quadruplet loss for better inter-class separability.

Result: Outperforms state-of-the-art baselines on public benchmarks, especially under seasonal and viewpoint variations.

Conclusion: OptiCorNet effectively integrates sequence-level embeddings for VPR, offering robustness to dynamic changes and perceptual aliasing.

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT improves data-free quantization for Vision Transformers by enhancing synthetic data quality and aligning activations, outperforming existing methods without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing DFQ methods for ViTs struggle with synthetic data quality and activation distribution mismatches, leading to performance degradation.

Method: DFQ-ViT synthesizes samples by difficulty and uses an activation correction matrix to align quantized and full-precision model activations.

Result: DFQ-ViT outperforms state-of-the-art DFQ methods, e.g., improving DeiT-T 3-bit quantization by 4.29%, and matches real-data quantization performance.

Conclusion: DFQ-ViT offers a computationally efficient, fine-tuning-free solution for deploying ViTs on resource-constrained devices, aligning with Green Learning principles.

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: A novel retrieval-augmented framework for 3D point cloud completion leverages cross-modal retrieval and hierarchical feature fusion to enhance structural learning and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D point cloud completion are limited by class-specific focus and lack structural priors. Cross-modal learning with instance images is insufficient for diverse or sparse data.

Method: Proposes a retrieval-augmented framework with a Structural Shared Feature Encoder (SSFE) for cross-modal feature extraction and a Progressive Retrieval-Augmented Generator (PRAG) for hierarchical feature fusion.

Result: Demonstrates effectiveness in generating fine-grained point clouds and handling sparse data or unseen categories in evaluations.

Conclusion: The framework improves completion quality and generalization by integrating reference priors and suppressing irrelevant information.

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA introduces token compression for WSI VQA, reducing computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-resolution WSIs in MLLMs, which require long context lengths and high resources, and lack generative capabilities for VQA.

Method: Proposes TCP-LLaVA, using trainable compression tokens to aggregate visual/textual info, reducing input length for the LLM.

Result: Outperforms baselines in VQA accuracy on TCGA tumor subtypes and reduces training resource use.

Conclusion: TCP-LLaVA is an efficient and accurate solution for WSI VQA in pathology.

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: A robust framework for motion segmentation and egomotion estimation using event-based normal flow for neuromorphic vision sensors, avoiding full optical flow computation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on optical flow or depth estimation, which can be computationally intensive. The paper aims to leverage sparse, high-temporal-resolution event data for more efficient and accurate motion analysis.

Method: The approach uses event over-segmentation, isolates moving objects via residual analysis, and refines segmentations with hierarchical clustering based on motion similarity and temporal consistency.

Result: Validated on the EVIMO2v2 dataset, the method achieves accurate segmentation and translational motion estimation without full optical flow.

Conclusion: The framework shows advantages at object boundaries and potential for scalable, real-time robotic and navigation applications.

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: A survey on feed-forward deep learning techniques for 3D reconstruction and view synthesis, covering representation architectures, key tasks, datasets, and future challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for 3D reconstruction and view synthesis are computationally intensive, limiting real-world applicability. Feed-forward deep learning approaches offer faster, generalizable solutions.

Method: The paper reviews feed-forward techniques, categorizing them by representation architectures (e.g., point clouds, 3DGS, NeRF) and examining tasks like pose-free reconstruction and dynamic 3D reconstruction.

Result: The survey highlights applications in digital humans, SLAM, robotics, and more, along with datasets and evaluation protocols.

Conclusion: Feed-forward approaches hold promise for advancing 3D vision, though open challenges remain for future research.

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM improves multiview pedestrian detection by ensuring depth consistency and reducing noise in human modeling without relying on costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human modeling in multiview pedestrian detection introduce noise and lack generalization, often requiring expensive 3D annotations.

Method: Proposes Depth-Consistent Human Modeling (DCHM) with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.

Result: DCHM significantly reduces noise, outperforms state-of-the-art baselines, and is the first to reconstruct pedestrians in challenging sparse-view, large-scale, crowded scenarios.

Conclusion: DCHM eliminates reliance on human-labeled annotations and achieves precise human modeling for pedestrian localization, setting a new benchmark.

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse is a new MLLM-based IAA model offering joint scoring and expert-level understanding, paired with the ArtiMuse-10K dataset for advancing image aesthetics assessment.


<details>
  <summary>Details</summary>
Motivation: The rise of AI-generated content and educational/artistic applications demands better IAA methods with quantitative scoring and professional insights, addressing limitations of current MLLM-based approaches.

Method: Proposes ArtiMuse, an MLLM-based IAA model with joint scoring and expert-level understanding, and introduces ArtiMuse-10K, a 10,000-image dataset with expert annotations.

Result: ArtiMuse enhances IAA by combining scoring and detailed attribute analysis, supported by a high-quality dataset.

Conclusion: The model and dataset aim to advance IAA research, addressing modality bias and lack of fine-grained analysis in existing methods.

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: A browser extension is proposed to translate sign language to subtitles in video calls, using a large dataset of ASL videos to bridge communication gaps for the hearing impaired.


<details>
  <summary>Details</summary>
Motivation: The communication barrier for the hearing impaired, especially during video calls, and the preference for signing over typing in such scenarios.

Method: Utilizing a large-scale dataset of over 2000 Word-Level ASL videos from 100+ signers to develop a browser extension for real-time sign language translation.

Result: The proposed system aims to enable seamless communication by converting sign language into subtitles during video calls.

Conclusion: The browser extension has the potential to significantly improve accessibility and communication for the hearing impaired in virtual settings.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: The paper presents a VQA pipeline using the Florence model for gastrointestinal endoscopy, achieving strong results on the KASVIR dataset.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of visual question answering (VQA) in medical contexts, specifically for gastrointestinal endoscopy, leveraging large multimodal models.

Method: Utilizes the Florence model with domain-specific augmentations to enhance training diversity while preserving medical features.

Result: Fine-tuning Florence yields accurate responses on the KASVIR dataset, demonstrating its potential for medical VQA.

Conclusion: The approach provides a strong baseline for future work on explainability, robustness, and clinical integration in medical VQA.

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: The study explores the link between ANN decision boundaries and human perceptual variability in emotion categorization, revealing shared computational principles and enabling personalized emotion modeling.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding inter-individual differences in human emotion perception and the potential alignment with ANN behavior.

Method: Introduces a perceptual boundary sampling method to create ambiguous facial expression stimuli, tested via large-scale human experiments and ANN fine-tuning.

Result: ANN-confusing stimuli also cause perceptual uncertainty in humans, and fine-tuning aligns ANN predictions with human perceptual patterns.

Conclusion: The findings connect ANN decision boundaries to human perceptual variability, advancing personalized emotion modeling.

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: A camera guidance system helps identify and remove clutter in photos, improving aesthetics and photo quality through interactive tools and algorithms.


<details>
  <summary>Details</summary>
Motivation: Clutter in photos distracts from intended emotions or stories, especially for amateurs who lack experience in decluttering scenes.

Method: The system uses a clutter distinguishment algorithm with aesthetics evaluations and an iterative image inpainting algorithm based on GANs to remove clutter.

Result: User studies show the system helps users identify distractions and take higher quality photos more efficiently.

Conclusion: The system effectively aids photographers in decluttering scenes, enhancing photo aesthetics and storytelling.

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D enhances 3D scene understanding by using natural language to encode object relationships, outperforming baselines on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene-language models lack relational understanding due to reliance on visual embeddings alone.

Method: Descrip3D integrates textual descriptions of objects' attributes and relationships via embedding fusion and prompt-level injection.

Result: Outperforms baselines on five benchmarks (ScanRefer, Multi3DRefer, ScanQA, SQA3D, Scan2Cap).

Conclusion: Language-guided relational representation improves 3D scene understanding across tasks.

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD is a finetuning-aligned method using logits to model nonlinear fine-tuning dynamics, outperforming linear methods in predicting model transferability.


<details>
  <summary>Details</summary>
Motivation: The challenge of efficiently selecting pre-trained models for downstream tasks due to the lack of accurate transferability prediction methods.

Method: LEAD models fine-tuning dynamics via logits, using an ODE framework for nonlinear optimization and class-aware decomposition.

Result: Outperforms existing methods on 24 pre-trained models across 10 datasets, even in low-data scenarios.

Conclusion: LEAD effectively bridges the optimization gap in model transferability prediction, offering a practical and adaptable solution.

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: The paper benchmarks GANs, diffusion models, and flow matching for T1w-to-T2w MRI synthesis, finding Pix2Pix (GAN-based) superior in fidelity, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Reducing MRI scan time and cost by synthesizing missing contrasts from acquired ones.

Method: Comparative evaluation of GANs, diffusion models, and flow matching on three public MRI datasets.

Result: Pix2Pix outperforms diffusion and FM methods in structural fidelity, image quality, and efficiency.

Conclusion: GANs are currently more practical for MRI I2I translation, with flow-based models needing more data to compete.

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: The paper compares TensorFlow, PyTorch, and JAX for blood cell image classification, analyzing inference time and accuracy, with JAX and PyTorch performing well.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed performance analysis of deep learning frameworks in blood cell image classification.

Method: Comparison of TensorFlow, PyTorch, and JAX on the BloodMNIST dataset, focusing on inference time and classification performance for varying image sizes.

Result: JAX and PyTorch showed comparable accuracy to benchmarks, with performance variations due to image resolution and framework optimizations.

Conclusion: JAX and PyTorch are efficient for medical image classification, with framework choice impacting performance.

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D is a novel method for 3D Open-Vocabulary Sub-concepts Discovery, combining unsupervised segmentation with weak open-vocabulary guidance for adaptable semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional methods are limited to either task-specific goals or scene content, lacking adaptability to both scene and user queries.

Method: DiSCO-3D uses Neural Fields representations to integrate unsupervised segmentation with weak open-vocabulary guidance.

Result: DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and excels in edge cases of open-vocabulary and unsupervised segmentation.

Conclusion: DiSCO-3D addresses a broader problem in 3D semantic segmentation, offering adaptability and state-of-the-art performance.

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph is a graph-based framework for facial expression recognition, using facial landmarks and vision transformers to model structural relationships, achieving high accuracy across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition is vital for applications like human-computer interaction and affective computing, requiring structural information for accuracy.

Method: Exp-Graph represents facial attributes as a graph (landmarks as vertices, proximity and appearance similarity as edges), using vision transformers and graph convolutional networks to capture dependencies.

Result: Achieved accuracies of 98.09%, 79.01%, and 56.39% on Oulu-CASIA, eNTERFACE05, and AFEW datasets, demonstrating strong generalization.

Conclusion: Exp-Graph effectively integrates structural and semantic information, proving robust for practical facial expression recognition in varied environments.

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2 is an efficient adaptation framework for SAM2, enhancing medical video segmentation with minimal computational overhead and superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation methods lack adaptability and require large datasets for retraining, leading to high costs and risks like catastrophic forgetting.

Method: Proposes DD-SAM2, incorporating a Depthwise-Dilated Adapter (DD-Adapter) for multi-scale feature extraction, enabling fine-tuning with limited data.

Result: Achieves Dice scores of 0.93 (TrackRad2025) and 0.97 (EchoNet-Dynamic), outperforming existing methods.

Conclusion: DD-SAM2 offers a scalable, efficient solution for medical video segmentation and tracking, with public release of code and datasets.

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ is a novel framework for cross-modal detection of synthetic media, using reinforcement learning and hybrid reasoning to outperform single-modality methods.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI has increased misinformation risks, but current detection systems are limited by single-modality designs.

Method: BusterX++ employs reinforcement learning post-training with multi-stage training, thinking reward, and hybrid reasoning.

Result: The framework achieves stable and substantial performance improvements, validated on the GenBuster++ benchmark.

Conclusion: BusterX++ effectively addresses cross-modal synthetic media detection, offering enhanced transparency and interpretability.

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion is a novel multispectral feature fusion framework using a state space model (SSM) to address limitations in object detection by balancing complementary features and shared semantics, achieving superior performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current multispectral feature fusion methods overly focus on local complementary features and struggle with the trade-off between receptive field size and computational complexity, limiting generalization and scalability.

Method: MS2Fusion employs a dual-path parametric interaction mechanism: one branch for cross-modal complementary features via cross-attention, and another for shared semantics via parameter sharing, both optimized within SSM.

Result: MS2Fusion outperforms state-of-the-art methods on benchmarks like FLIR, M3FD, and LLVIP, and shows strong performance in RGB-T semantic segmentation and RGBT salient object detection.

Conclusion: MS2Fusion effectively addresses key limitations in multispectral feature fusion, offering a scalable, generalizable solution with superior performance across multiple tasks.

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai, an AI-powered framework, enhances sports officiating by automating real-time head kick detection in Taekwondo, reducing latency and improving fairness. Its adaptable methodology can extend to other sports.


<details>
  <summary>Details</summary>
Motivation: Traditional officiating systems suffer from latency, subjectivity, and inconsistency, undermining fairness and trust in sports like Taekwondo.

Method: The framework uses computer vision, deep learning, and edge inference for pose estimation, motion classification, and impact analysis to automate action detection.

Result: FST.ai reduces decision time from minutes to seconds, improves consistency, and demonstrates scalability for other sports.

Conclusion: FST.ai's robust and adaptable framework has the potential to transform officiating standards across multiple sports disciplines.

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: A cost-effective computer vision framework using semantic segmentation estimates food waste in institutional dining, achieving high accuracy with lightweight models, though performance varies by food type.


<details>
  <summary>Details</summary>
Motivation: Quantifying food waste in dining settings is crucial for data-driven sustainability strategies.

Method: Four fully supervised models (U-Net, U-Net++, and lightweight variants) trained with dynamic loss and evaluated using metrics like Pixel Accuracy, Dice, IoU, and DPA.

Result: Models achieved high performance (≥90% DPA for some foods), with lighter models enabling real-time inference. Dry/rigid foods segmented better than complex/viscous ones.

Conclusion: The framework is scalable and pioneering for real-time waste monitoring, despite limitations like 2D imaging and manual data collection, offering actionable insights for reducing institutional food waste.

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML is a framework that improves gene expression prediction from histopathology images by aligning cross-modal representations at multiple levels.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize cross-modal alignment between histopathology images and gene expression, limiting prediction performance.

Method: Gene-DML uses Dual-pathway Multi-Level discrimination: multi-scale instance-level and cross-level instance-group discrimination to align morphological and transcriptional modalities.

Result: Gene-DML achieves state-of-the-art performance in gene expression prediction on public datasets.

Conclusion: The framework enhances predictive accuracy and generalization by robustly aligning cross-modal representations.

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: The paper introduces Doc-750K, a high-quality dataset for multimodal document understanding, and Docopilot, a native multimodal model that outperforms retrieval-augmented methods in coherence, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex document comprehension due to lack of quality datasets, and RAG methods have limitations like fragmented contexts and error accumulation.

Method: Developed Doc-750K dataset with diverse document structures and cross-page dependencies, then built Docopilot, a native multimodal model avoiding RAG.

Result: Docopilot excels in coherence, accuracy, and efficiency, setting a new benchmark for document-level multimodal understanding.

Conclusion: The work advances document-level multimodal understanding by providing a robust dataset and model, with resources publicly released.

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents is a collaborative multi-agent system for multi-modal WSI analysis, improving accuracy and versatility over existing MLLMs.


<details>
  <summary>Details</summary>
Motivation: Address underperformance of multi-modal large language models (MLLMs) in WSI analysis compared to task-specific models, and explore untapped potential of multi-agent systems in pathology.

Method: Proposes WSI-Agents with three components: task allocation, verification mechanism, and summary module, leveraging specialized agents and validation.

Result: Outperforms current WSI MLLMs and medical agent frameworks in multi-modal WSI benchmarks.

Conclusion: WSI-Agents effectively balances accuracy and versatility in WSI analysis, demonstrating its potential in digital pathology.

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: The paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) and proposes Multimodal Interactive Prompt Distillation (MIPD) to transfer knowledge from a teacher MLLM to a smaller GSR model, enhancing generalization and zero-shot abilities.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with complex GSR tasks and are resource-heavy, while conventional GSR models lack generalization for unseen and rare situations.

Method: MIPD distills multimodal knowledge from a teacher MLLM using LLM-based rationales and scene-aware prompts, aligning them via Negative-Guided Multimodal Prompting Alignment (NMPA).

Result: MIPD achieves superior performance on seen, rare, and unseen situations in the Ov-SWiG dataset and improves unseen detection on HICO-DET.

Conclusion: The MIPD framework effectively enhances generalization and zero-shot abilities in GSR models, bridging gaps between seen and unseen scenarios.

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: The paper introduces GTPBD, a fine-grained terraced parcel dataset, addressing gaps in existing datasets by covering complex terrains and offering multi-level labels for various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack representation of complex terraced terrains, limiting precision agriculture research. GTPBD aims to fill this gap with a comprehensive, manually annotated dataset.

Method: GTPBD includes 47,537 high-resolution images with three-level labels (boundary, mask, parcel) across diverse terrains and climatic regions. It supports four tasks: semantic segmentation, edge detection, parcel extraction, and UDA.

Result: The dataset benchmarks eight semantic segmentation, four edge extraction, three parcel extraction, and five UDA methods, using a multi-dimensional evaluation framework.

Conclusion: GTPBD provides infrastructure for fine-grained agricultural terrain analysis and cross-scenario knowledge transfer, advancing terraced remote sensing research.

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet improves diabetic retinopathy staging by combining retinal imaging, socioeconomic data, and comorbidities, using multimodal fusion and a deferral system for early detection in underserved populations.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy (DR) disproportionately affects lower-income communities due to limited screening access, leading to late-stage diagnoses. Comorbid conditions worsen progression.

Method: Proposes MultiRetNet, integrating retinal imaging, socioeconomic factors, and comorbidities. Tests three fusion methods, uses contrastive learning for deferral system training, and synthesizes adversarial images.

Result: Fully connected layer fusion is most versatile. The system maintains accuracy on suboptimal images and identifies cases needing clinician review.

Conclusion: MultiRetNet enhances early detection, reduces healthcare costs, and addresses care disparities, promoting equity in underserved populations.

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: The paper introduces InterAct VideoQA, a dataset to benchmark VideoQA models for traffic monitoring, addressing challenges in real-world traffic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with complex real-world traffic scenes, necessitating a domain-specific dataset for improvement.

Method: The InterAct VideoQA dataset includes 8 hours of traffic footage, segmented into 10-second clips with 25,000 QA pairs, covering spatiotemporal dynamics and vehicle interactions.

Result: Evaluation shows challenges in reasoning over spatiotemporal dependencies, but fine-tuning on InterAct VideoQA improves model performance.

Conclusion: InterAct VideoQA serves as a benchmark for advancing VideoQA models in intelligent transportation systems.

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA improves VideoQA by refining queries with LLMs and grounding them temporally, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA methods struggle with irrelevant content and lack causal-temporal reasoning.

Method: LeAdQA uses LLMs for query refinement, temporal grounding for segment retrieval, and adaptive fusion for evidence integration.

Result: Achieves SOTA on NExT-QA, IntentQA, and NExT-GQA with enhanced reasoning and efficiency.

Conclusion: LeAdQA effectively addresses flaws in current VideoQA methods, improving accuracy and efficiency.

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS is a framework for reliable and efficient spatial-spectral interpretability in Vision Transformers (ViTs) for hyperspectral imaging (HSI), addressing challenges like spectral cue capture and computational feasibility.


<details>
  <summary>Details</summary>
Motivation: Existing saliency methods fail to capture meaningful spectral cues in HSI, and full-spectrum ViTs are computationally prohibitive for interpretability.

Method: FOCUS introduces class-specific spectral prompts and a learnable [SINK] token to guide attention and absorb noise, enabling stable 3D saliency maps without gradient backpropagation.

Result: FOCUS improves band-level IoU by 15%, reduces attention collapse by 40%, and aligns saliency results with expert annotations.

Conclusion: FOCUS makes high-resolution ViT interpretability practical for HSI applications, bridging the gap between black-box modeling and trustworthy decision-making.

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: The paper introduces Hybrid Pooling Downsampling (HPD), a method to improve semantic segmentation by retaining spatial information, outperforming traditional downsampling methods.


<details>
  <summary>Details</summary>
Motivation: Traditional downsampling methods in CNNs lose key spatial information, affecting pixel-by-pixel prediction accuracy in semantic segmentation.

Method: Proposes HPD, replacing traditional methods with MinMaxPooling to retain image contrast and detail features.

Result: HPD increases the DSC coefficient by 0.5% on average in experiments on ACDC and Synapse datasets.

Conclusion: HPD is an efficient solution for semantic segmentation tasks, enhancing performance over traditional methods.

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [47] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: The paper introduces the Ensemble Parallel Direction solver (EPD), a novel ODE solver for diffusion models that reduces truncation errors and maintains low-latency sampling by leveraging parallel gradient evaluations.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high sampling latency due to sequential denoising, and existing acceleration methods degrade image quality under low-latency constraints.

Method: EPD incorporates multiple parallel gradient evaluations per ODE step, which are independent and fully parallelizable. It optimizes learnable parameters via distillation with minimal training overhead and can be used as a plugin for existing ODE samplers.

Result: EPD achieves superior performance (e.g., FID scores of 4.47 on CIFAR-10, 7.97 on FFHQ) at the same latency level (5 NFE), outperforming existing learning-based solvers.

Conclusion: EPD effectively balances high-quality and low-latency sampling in diffusion models, offering a practical solution for accelerating generative tasks.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [48] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: The paper evaluates DUSt3R, MASt3R, and VGGT models for 3D reconstruction on aerial images, showing improved performance with sparse inputs but limitations with high-resolution or large sets.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of transformer-based models (DUSt3R, MASt3R, VGGT) in handling sparse, unordered aerial images for pose estimation and dense 3D reconstruction.

Method: Pre-trained DUSt3R, MASt3R, and VGGT models were tested on the UseGeo dataset's aerial blocks, focusing on sparse image sets (fewer than 10 images) and low-resolution inputs.

Result: The models achieved +50% completeness gains over COLMAP with sparse inputs, and VGGT showed higher efficiency and pose reliability. However, performance declined with high-resolution images and larger sets.

Conclusion: Transformer-based models are promising for sparse, low-resolution scenarios but cannot fully replace traditional SfM and MVS. They serve as complementary tools in challenging conditions.

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [49] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: The paper proposes VPIP, a framework for unified low-level vision tasks using visual prompts, and introduces GenLV, a model evaluated across diverse tasks, showing scalability and strong adaptability.


<details>
  <summary>Details</summary>
Motivation: The diversity in low-level vision tasks (e.g., restoration, enhancement) requires a unified modeling approach to handle varying formulations and outputs.

Method: VPIP uses input-target image pairs as visual prompts, integrating a backbone, prompt encoder, and interaction module. GenLV is developed and tested on 100+ tasks.

Result: GenLV achieves strong performance across tasks, with scalability in model capacity and task diversity. Joint training improves generalization, especially for data-limited tasks.

Conclusion: VPIP and GenLV demonstrate effectiveness, scalability, and adaptability, offering a unified foundation for low-level vision modeling.

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [50] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: A novel framework, HICOM, improves multi-face deepfake detection by leveraging human-inspired cues like scene-motion coherence and face-body consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with multi-face scenarios due to lack of contextual awareness.

Method: Human studies identify key detection cues; HICOM framework integrates these cues for multi-face detection, enhanced by an LLM for interpretability.

Result: HICOM improves accuracy by 3.3% in-dataset and 2.8% under perturbations, outperforming others by 5.8% on unseen datasets.

Conclusion: Incorporating human factors enhances deepfake defense, with HICOM offering improved accuracy and interpretability.

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [51] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: A novel, efficient, and lightweight approach for robot action prediction using InstructPix2Pix, reducing computational cost and latency compared to traditional video prediction models.


<details>
  <summary>Details</summary>
Motivation: Predicting future motion trajectories is crucial for safer and smarter decision-making in robotics, autonomous systems, and human activity forecasting.

Method: Adapts InstructPix2Pix for future visual frame prediction in robotics, using a single image and textual instruction as input. The model is fine-tuned for multimodal prediction.

Result: Achieves superior SSIM and PSNR on the RoboTWin dataset, outperforming state-of-the-art baselines with faster inference and lower GPU demands.

Conclusion: The lightweight design enables efficient multimodal control, prioritizing motion trajectory precision over visual fidelity, making it ideal for robotics and sports analytics.

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [52] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant is a unified quantization framework for diffusion models, combining segment-aware and dual-scale techniques to improve efficiency without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for diffusion models lack generalizability and compatibility with industrial pipelines, limiting their deployment.

Method: SegQuant uses SegLinear for structural semantics and DualScale for preserving activations, ensuring visual fidelity.

Result: SegQuant achieves strong performance and broad applicability across diffusion models.

Conclusion: SegQuant offers a versatile and efficient solution for quantizing diffusion models, compatible with mainstream tools.

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [53] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench is a new benchmark for evaluating LVLMs on financial charts, revealing key limitations in their performance.


<details>
  <summary>Details</summary>
Motivation: Financial charts are complex and underexplored in LVLM research, necessitating a dedicated benchmark.

Method: FinChart-Bench includes 1,200 financial charts with 7,016 annotated questions (TF, MC, QA). 25 LVLMs were evaluated.

Result: Findings show narrowing gaps between open/closed-source models, performance degradation in upgrades, struggles with instruction following, spatial reasoning limitations, and unreliability as evaluators.

Conclusion: Current LVLMs have significant limitations in financial chart understanding, highlighting the need for further research.

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [54] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet improves dehazing by transferring haze patterns from unseen domains to source images, enhancing model adaptability with new losses.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing models struggle with unseen real-world hazy images due to limited training data, prompting a need for better domain adaptation.

Method: Proposes PHATNet, which transfers haze patterns to source-domain images for fine-tuning, aided by Haze-Transfer-Consistency and Content-Leakage Losses.

Result: PHATNet significantly improves state-of-the-art dehazing models on real-world datasets.

Conclusion: PHATNet's domain adaptation approach effectively enhances dehazing performance for unseen real-world images.

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [55] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: A paired image generation method for DBT images improves lesion segmentation by generating high-quality paired images and annotations without external conditions.


<details>
  <summary>Details</summary>
Motivation: High concealment of mass lesions in DBT images makes manual annotation difficult, leading to a lack of annotated data for training. Existing diffusion models struggle with lesion feature learning and lack annotation generation.

Method: Proposes a paired image generation method using a diffusion guider for conditional diffusion models, generating paired DBT slices and lesion masks.

Result: Improved generation quality and usability for supervised training, enhancing downstream segmentation performance.

Conclusion: The method effectively addresses data scarcity and improves lesion segmentation without external dependencies.

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [56] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: A novel self-supervised depth completion method using only sparse depth and single images, eliminating the need for dense labels or multi-frame data.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of costly dense annotations and multi-frame dependencies in existing depth completion methods.

Method: Proposes a self-supervised paradigm with novel loss functions for depth propagation and leverages segmentation maps from vision foundation models.

Result: Demonstrates effectiveness through extensive experiments.

Conclusion: The method offers a practical solution for depth completion without dense labels or additional frames.

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [57] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: Proposes an all-in-one video restoration framework using foundation models for interpretable guidance, introduces new benchmarks, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability and flexibility in video restoration, and the absence of standardized benchmarks for all-in-one video restoration.

Method: Uses foundation models to ground degradation-aware semantic context in natural language, learns an approximation to avoid extra inference cost, and introduces new benchmarks.

Result: Achieves state-of-the-art performance on all proposed benchmarks, including multi-degradation and time-varying composite degradation settings.

Conclusion: The framework offers interpretable and flexible video restoration without prior degradation knowledge, and the new benchmarks standardize evaluation in the field.

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [58] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: Proposes an uncertainty-aware DETR-based object detection framework to improve localization accuracy and model prediction uncertainty, achieving state-of-the-art results on COCO, LISC, and WBCDD datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional detectors lack uncertainty modeling, limiting robustness. This work aims to enhance DETR-based detectors by explicitly addressing uncertainty.

Method: Models bounding boxes as Gaussian distributions, uses Gromov-Wasserstein distance for alignment, and introduces a Bayes Risk formulation for reliability. Also quantifies uncertainty via confidence intervals.

Result: Improves performance on COCO benchmark and achieves state-of-the-art results on LISC and WBCDD datasets.

Conclusion: The framework is scalable for general and domain-specific tasks, enhancing detection reliability and accuracy.

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [59] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: The paper proposes a hypergraph-enhanced Transformer framework for emotion recognition using micro-gestures, achieving state-of-the-art results on public datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-gestures can reveal human emotions but lack sufficient modeling. This work aims to bridge this gap by leveraging advanced deep learning techniques.

Method: A hybrid-supervised framework with hypergraph-enhanced Transformer, combining self-supervised reconstruction and supervised emotion recognition, is proposed.

Result: The method outperforms existing approaches on iMiGUE and SMG datasets under multiple metrics.

Conclusion: The framework effectively models micro-gestures for emotion recognition, demonstrating superior performance and potential for affective computing.

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [60] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: A non-learning-based method uses sparse depth measurements to convert relative-scale depth predictions from foundation models into metric-scale depth, avoiding retraining or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Foundation models for depth prediction often output relative-scale depth, limiting real-world application. Existing scale adaptation methods are costly and reduce generalization.

Method: Leverages sparse depth measurements to adapt relative-scale predictions to metric-scale without retraining or fine-tuning.

Result: Effectively bridges the gap between relative and metric depth without additional computational costs or loss of generalization.

Conclusion: The approach preserves foundation models' generalization while enabling metric-scale depth, offering a practical solution for real-world deployment.

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [61] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer combines deep learning and handcrafted methods for robust rPPG estimation, using spectral attention and contrastive learning for efficiency and label-free training.


<details>
  <summary>Details</summary>
Motivation: Existing rPPG methods either rely on large datasets (deep learning) or have limited performance (handcrafted). A hybrid approach is needed to leverage both strengths.

Method: BeatFormer integrates zoomed orthonormal complex attention and frequency-domain energy measurement. Spectral Contrastive Learning (SCL) enables training without PPG or HR labels.

Result: BeatFormer shows robustness and performance in cross-dataset evaluations, especially in motion scenarios, validated on PURE, UBFC-rPPG, and MMPD datasets.

Conclusion: BeatFormer offers a lightweight, efficient, and label-free solution for rPPG estimation, outperforming existing methods in challenging conditions.

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [62] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: A unified 2D pre-trained multi-modal network simplifies 3D visual grounding by processing RGB images, text, and point clouds together, reducing model complexity and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing 3D visual grounding methods are inefficient due to separate encoders for each modality and reliance on 3D encoders, leading to large, complex models.

Method: Proposes a unified 2D pre-trained multi-modal network using CLIP with adapter-based fine-tuning, a GARF module for feature fusion, and a multi-modal decoder for cross-modal understanding.

Result: Reduces trainable parameters by ~58%, improves 3D detection by 6.52%, and enhances 3D visual grounding by 6.25%.

Conclusion: The method simplifies architecture, improves efficiency, and achieves better performance in 3D visual grounding tasks.

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [63] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: Proposes Semantic-Aware Representation Learning (SARL) for multi-label image classification, improving precision and reducing noise in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods using attention or GCNs may introduce noise and imprecise object localization.

Method: Uses label semantic-related feature learning, optimal transport-based attention, and regional score aggregation.

Result: Outperforms existing methods on PASCAL VOC 2007 and MS-COCO datasets.

Conclusion: SARL effectively enhances multi-label image classification by aligning semantics and improving representation.

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [64] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: A disentangled framework, \method, is proposed for efficient 3D Gaussian prediction, reducing computational demands and improving reconstruction quality without relying on camera parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D Gaussian Splatting reconstruction are computationally intensive, slow, and rely heavily on data-driven priors and camera parameters, limiting practicality.

Method: The framework uses a stereo vision backbone to extract features from local image pairs, fuses them via global attention blocks, and employs dedicated heads for geometry and appearance prediction, refined for high-quality output.

Result: The method achieves pose-free 3D reconstruction, improving robustness and efficiency while maintaining high-quality outputs.

Conclusion: \method offers an efficient, scalable solution for real-world 3D content generation by reducing resource demands and enhancing practicality.

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [65] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: The paper introduces a robust method for pose estimation in cryo-EM using multi-dimensional scaling (MDS) and a joint optimization framework, improving accuracy and reconstruction fidelity.


<details>
  <summary>Details</summary>
Motivation: The very low SNR in cryo-EM poses challenges for accurate pose estimation and shift correction, directly affecting 3D reconstruction quality.

Method: The approach uses MDS for rotation matrix estimation, a robust joint optimization framework with ℓ₁-norm objectives, and an iterative shift correction algorithm.

Result: The method outperforms prior techniques in Euler angle accuracy and reconstruction fidelity, as measured by Fourier Shell Correlation (FSC).

Conclusion: The proposed pipeline addresses noise sensitivity and geometric constraints more effectively, leading to superior performance in low-SNR cryo-EM scenarios.

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [66] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: A probabilistic framework for Multiple Instance Learning (MIL) in medical imaging improves predictive performance and provides interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: Address the deterministic treatment of attention values in deep MIL methods, which overlooks uncertainty in instance contributions.

Method: Proposes a probabilistic framework estimating distributions over attention values, capturing global and local interactions.

Result: Achieves top predictive performance across three medical datasets and outperforms eleven baselines.

Conclusion: The probabilistic approach enhances interpretability and performance in medical imaging classification.

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [67] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: The paper introduces Open-set Cross Modal Generalization (OSCMG), a more challenging task than CMG, and proposes MICU with FCMI and CUJP components to address open-set limitations in multimodal representation learning.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks open-set evaluation in multimodal learning, limiting real-world applicability. OSCMG aims to bridge this gap by requiring generalization to unseen classes and modalities.

Method: Proposes MICU with two components: FCMI (contrastive learning with masking for alignment) and CUJP (modality-agnostic feature selection with self-supervised learning for diversity and uncertainty handling).

Result: Experiments on CMG and OSCMG validate MICU's effectiveness in open-set conditions.

Conclusion: MICU successfully addresses open-set challenges in multimodal learning, enhancing generalization and robustness.

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [68] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph is a framework for efficient real-time multi-label video classification on embedded devices, leveraging label sparsity and co-occurrence to reduce energy and latency.


<details>
  <summary>Details</summary>
Motivation: Addressing the constraints of limited compute and energy budgets for real-time video classification on embedded devices by exploiting structural properties of video streams.

Method: Uses context-aware Low Rank Adapters (LoRA) specialized for subsets of classes, dynamically selecting and composing adapters per frame to avoid full-model switching.

Result: Achieves 40% lower energy consumption and a 9-point mAP improvement on the TAO dataset.

Conclusion: Polymorph offers a scalable, efficient solution for real-time video classification, balancing performance and resource constraints.

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [69] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: A deep learning-based classifier is proposed to evaluate low-overlap point cloud registration (PCR) quality, improving traditional metrics and enhancing state-of-the-art PCR methods.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation metrics fail under low inlier ratios, necessitating a new approach for reliable PCR quality assessment.

Method: A data-driven approach using a deep learning classifier trained on a 3DMatch-derived dataset to evaluate registration quality.

Result: Integration with existing PCR methods, like GeoTransformer, achieves 86.97% registration recall on 3DLoMatch and generalizes well on ETH dataset.

Conclusion: The proposed classifier effectively addresses the Decision PCR task, significantly improving registration performance and generalization.

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [70] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL is a hierarchical cross-modal prompt learning framework addressing modality isolation and semantic decay in VLMs, improving generalization through bidirectional knowledge flow between text and vision.


<details>
  <summary>Details</summary>
Motivation: Adapting large-scale VLMs to downstream tasks while preserving generalization is challenging due to modality isolation and hierarchical semantic decay.

Method: HiCroPL establishes bidirectional knowledge flow between text and vision, using a hierarchical knowledge mapper and lightweight layer-specific proxies.

Result: Achieves state-of-the-art results on 11 benchmarks across four tasks.

Conclusion: HiCroPL effectively enhances generalization in VLMs by refining semantics bidirectionally and retaining transferable shallow semantics.

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [71] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: Current MLLM approaches for image-based regression underperform due to generic prompts and preset vocabularies. RvTC, a bin-based method, outperforms by eliminating manual vocabularies and using data-specific prompts.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM methods fail to leverage textual input effectively, performing no better than image-only models. The goal is to improve regression tasks by incorporating meaningful textual context.

Method: Proposes Regression via Transformer-Based Classification (RvTC), replacing vocabulary constraints with a flexible bin-based approach and using data-specific prompts.

Result: RvTC achieves state-of-the-art performance on four datasets. Semantic prompts (e.g., challenge titles) boost correlations from 0.83 to 0.90 on AVA.

Conclusion: Semantic prompts and flexible binning in RvTC enhance MLLM performance, proving the importance of meaningful textual context in multimodal regression.

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [72] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: The paper introduces an axis-aligned geometric constraint for document dewarping, improving performance and robustness on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on supervised regression without leveraging geometric properties of documents. The paper aims to utilize the axis-aligned nature of documents for better dewarping.

Method: Proposes an axis-aligned geometric constraint during training and an axis alignment preprocessing strategy for inference. Introduces the Axis-Aligned Distortion (AAD) metric for evaluation.

Result: Achieves SOTA results on benchmarks with 18.2%~34.5% improvements on the AAD metric.

Conclusion: The method effectively leverages geometric properties for superior document dewarping, validated by robust performance and the new AAD metric.

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [73] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: The paper introduces a B-Spline curve fitting method to refine jagged edges in FastSAM, improving segmentation accuracy while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: FastSAM's jagged edges limit its accuracy in real-time applications, prompting the need for a refinement technique.

Method: A four-stage refining process using B-Spline curve fitting is applied to smooth edges in FastSAM.

Result: The method enhances edge quality and analytical accuracy without losing geometric details or real-time capabilities.

Conclusion: The refinement improves FastSAM's practicality for real-world applications like industrial automation and medical imaging.

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [74] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces Video-TT, a benchmark to assess video LLMs' correctness and robustness in video understanding, revealing a significant performance gap compared to humans.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to capture the gap between video LLMs and human intelligence in maintaining correctness and robustness in video interpretation.

Method: Video-TT includes 1,000 YouTube Shorts videos with open-ended and adversarial questions to evaluate visual and narrative complexity.

Result: The evaluation shows a significant performance gap between video LLMs and humans.

Conclusion: Video-TT highlights the limitations of current video LLMs in matching human-level video understanding and robustness.

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [75] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS introduces a large-scale wave equation dataset for realistic breast imaging, enabling benchmarking of neural operators for forward and inverse tasks, and demonstrates in vivo imaging.


<details>
  <summary>Details</summary>
Motivation: Traditional wave equation solvers are computationally intensive and unstable, while neural operators lack realistic datasets for practical imaging.

Method: Developed OpenBreastUS, a dataset with 8,000 breast phantoms and 16M frequency-domain simulations, to benchmark neural operators.

Result: Enables efficient in vivo breast imaging using neural operators, showcasing performance and scalability.

Conclusion: OpenBreastUS bridges theory and practice, advancing neural PDE solvers for real-world medical imaging.

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [76] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI is an ethics-guided, bias-aware AI framework for underwater image enhancement, addressing dataset bias, computational costs, and transparency issues.


<details>
  <summary>Details</summary>
Motivation: To improve marine conservation efforts by mitigating AI model biases, reducing computational costs, and enhancing transparency in underwater image enhancement.

Method: Leverages CLIP embeddings for bias detection and mitigation, integrates adaptive processing for energy efficiency, and employs uncertainty estimation and explainability techniques.

Result: Achieves competitive enhancement quality with controlled PSNR drop (1.0 dB), significant GPU savings, and validated effectiveness against benchmarks like CycleGAN and WaterNet.

Conclusion: EBA-AI advances sustainable, bias-aware, and efficient underwater image processing, supporting large-scale marine monitoring.

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [77] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON is a training-free universal VTON framework that decouples garment and pose conditioning for high fidelity and consistency across diverse settings.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing VTON methods, which either lack cross-domain generalization or suffer from data biases, the paper aims to create a unified solution.

Method: OmniVTON uses garment prior generation and boundary stitching for texture fidelity, and DDIM inversion for pose alignment, disentangling garment and pose constraints.

Result: OmniVTON outperforms existing methods across datasets, garment types, and scenarios, including multi-human VTON.

Conclusion: OmniVTON is the first training-free universal VTON framework, achieving superior performance and enabling multi-human garment transfer.

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [78] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: PanTiny is a lightweight, single-step pan-sharpening framework designed for efficiency and robust performance, trained on multiple satellite datasets to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and poor generalization of large, complex pan-sharpening models trained on single datasets.

Method: Proposes PanTiny, a compact model trained on three satellite datasets (WV2, WV3, GF2) with a universal composite loss function.

Result: PanTiny outperforms larger models, achieving superior performance-to-efficiency balance and better generalization.

Conclusion: Advocates for efficient, generalizable models in pan-sharpening, validated by principled design and training paradigms.

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [79] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ is a novel ID-preserving video diffusion framework with learnable pose alignment, addressing identity consistency issues in human image animation.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with maintaining identity consistency when reference images and driving videos differ in body size or position.

Method: StableAnimator++ uses learnable pose alignment, SVD-guided similarity transformations, and a distribution-aware ID Adapter. It also integrates a HJB-based face optimization during inference.

Result: The framework achieves high-quality, ID-consistent video generation without post-processing, validated by benchmarks.

Conclusion: StableAnimator++ effectively addresses identity inconsistency in human image animation, outperforming existing methods.

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [80] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper evaluates state-of-the-art generative models for text image generation and editing, identifying weaknesses and advocating for integrating these skills into general-domain models.


<details>
  <summary>Details</summary>
Motivation: To assess if advanced generative models (like GPT-4o) can handle the complexities of text image generation and editing, given their success in other areas.

Method: Evaluates 33 OCR tasks across five categories using six models, with tailored inputs and prompts.

Result: Identifies weaknesses in current models for OCR tasks and argues for foundational integration of text generation skills.

Conclusion: Photorealistic text image generation should be a core skill in general-domain models, not just specialized solutions.

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [81] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: The paper introduces LASED, a large-scale aerial dataset, and steerable CNNs to improve Visual Place Recognition (vPR) for UAVs, addressing dataset scarcity and rotational ambiguity.


<details>
  <summary>Details</summary>
Motivation: Aerial vPR lacks large-scale datasets and struggles with rotational variance in UAV imagery, limiting model generalization.

Method: LASED dataset (1M images from 170K locations) and steerable CNNs are proposed to handle geographic/temporal diversity and rotational ambiguity.

Result: Models trained on LASED achieve higher recall; steerable CNNs outperform non-steerable ones by 12% recall improvement.

Conclusion: Combining structured datasets with rotation-equivariant networks enhances aerial vPR robustness and generalization.

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [82] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper introduces BleedOrigin-Bench, a dataset for bleeding source detection in ESD, and BleedOrigin-Net, a dual-stage detection-tracking framework, achieving high accuracy in bleeding onset and source tracking.


<details>
  <summary>Details</summary>
Motivation: Current AI methods for ESD focus on bleeding region segmentation but lack accurate bleeding source detection and tracking, exacerbated by the absence of specialized datasets.

Method: The authors propose BleedOrigin-Net, a dual-stage detection-tracking framework, and introduce BleedOrigin-Bench, a dataset with expert-annotated bleeding sources.

Result: BleedOrigin-Net achieves 96.85% frame-level accuracy for onset detection, 70.24% pixel-level accuracy for initial source detection, and 96.11% pixel-level accuracy for point tracking.

Conclusion: The work addresses critical gaps in AI-assisted ESD, providing a robust dataset and framework for bleeding source localization and tracking.

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [83] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet improves SLAM loop closure detection with multitasking ResNet, online retraining, and DISK descriptors, addressing accuracy and real-time constraints.


<details>
  <summary>Details</summary>
Motivation: To enhance loop closure detection accuracy and meet real-time computation demands in SLAM systems.

Method: Uses a multitasking ResNet variant with online retraining (few-shot learning) and DISK descriptors for feature extraction.

Result: Outperforms handcrafted features and traditional deep learning methods under varying conditions.

Conclusion: LoopNet offers a robust solution for real-time SLAM loop closure, with code and a new dataset (LoopDB) provided.

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [84] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VideoPlan introduces Auxiliary Task Augmentation and Multi-token Prediction to improve long-horizon visual planning, achieving state-of-the-art performance on COIN and CrossTask datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in training MLLMs for video-based planning: scarcity of procedural annotations and inefficiency of next-token prediction for structured action spaces.

Method: Uses Auxiliary Task Augmentation (e.g., goal prediction) and Multi-token Prediction (predicting multiple future tokens) to enhance planning ability.

Result: Achieves 7.3% and 3.4% improvements on COIN and CrossTask datasets, respectively, and performs competitively on Ego4D without specialized features.

Conclusion: VideoPlan effectively tackles data scarcity and structured action modeling, advancing visual planning performance.

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [85] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: A novel spatiotemporal multigraph representation improves event-based object detection by decoupling spatial and temporal dynamics, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard dense tensor conversions for event-based data lose sparsity and low-latency advantages, while existing graph methods poorly model spatiotemporal dynamics.

Method: Proposes a spatiotemporal multigraph with decoupled spatial (B-spline basis) and temporal (motion vector-based attention) graphs, replacing 3D kernels with efficient 2D ones.

Result: Achieves 6% higher detection accuracy on Gen1 and eTraM datasets, 5x speedup, fewer parameters, and no added computational cost.

Conclusion: Structured graph modeling effectively preserves event-based data advantages, enhancing performance in asynchronous vision tasks.

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [86] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba is a neural network using Mamba-SSMs for efficient 3D articulated mesh learning, enabling large-scale mesh generation and reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and scalability challenges in handling large 3D mesh models, especially for articulated shapes like human bodies with clothing and hands.

Method: Serializes mesh vertices into ordered sequences using body part annotations or 3D template locations, then applies Mamba-SSMs. Introduces MambaDiff3D for mesh generation and Mamba-HMR for single-image mesh recovery.

Result: MambaDiff3D outperforms prior methods in generating dense 3D human meshes. Mamba-HMR extends capabilities to whole-body reconstruction with competitive real-time performance.

Conclusion: MeshMamba, with its serialization and Mamba-SSM approach, effectively scales 3D mesh learning, advancing articulated shape modeling and reconstruction.

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [87] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: The paper proposes N-JEPA, a method combining diffusion noise with masked image modeling (MIM) in self-supervised learning (SSL) to enhance representation learning for recognition tasks.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SSL (effective for discriminative tasks) and generative models (superior in image generation) by leveraging diffusion noise for semantic understanding.

Method: Introduces N-JEPA, integrating diffusion noise into MIM via position embedding of masked tokens, using a multi-level noise schedule for feature augmentation.

Result: Demonstrates effectiveness in downstream classification tasks.

Conclusion: Combining diffusion noise with SSL improves recognition model performance, with code to be released.

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [88] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: A hierarchical part-based framework for 3D blood vessel generation separates global topology from local geometry, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurately representing complex blood vessel geometry and topology is challenging due to intricate branching patterns and irregular shapes.

Method: The approach involves three stages: key graph generation for hierarchical structure, vessel segment generation based on geometric properties, and hierarchical assembly.

Result: Validated on real-world datasets, the framework demonstrates superior performance in modeling complex vascular networks.

Conclusion: This is the first successful part-based generative approach for 3D vessel modeling, setting a new benchmark.

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [89] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: The paper introduces Sparse Autoencoder (SAE)-based interpretability to breast imaging using Mammo-CLIP, identifying clinically relevant features and confounding factors.


<details>
  <summary>Details</summary>
Motivation: Interpretability is crucial in medical imaging for clinical adoption, especially in understanding model decisions.

Method: A patch-level Mammo-SAE is trained on Mammo-CLIP to probe latent features linked to breast concepts like mass and suspicious calcification.

Result: Top activated latent neurons align with ground truth regions, and confounding factors are identified. The study also reveals which neurons aid in downstream prediction.

Conclusion: SAE-based interpretability offers deeper insights into foundation models for breast imaging.

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [90] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: A new method, Coalescent Projection (CP), combined with pseudo-class generation and Self-Supervised Transformations (SSTs), outperforms SOTA in CD-FSL by addressing overfitting in transformers.


<details>
  <summary>Details</summary>
Motivation: Overcoming overfitting in transformers due to scarce labeled samples in Cross-Domain Few-Shot Learning (CD-FSL).

Method: Proposes Coalescent Projection (CP) as a successor to soft prompts and introduces pseudo-class generation with SSTs using only base domain data.

Result: Outperforms latest SOTA methods, validated on the BSCD-FSL benchmark.

Conclusion: The proposed CP and SSTs effectively address overfitting and improve performance in extreme domain shift scenarios.

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [91] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus is a training-free framework for subject-driven text-to-image generation using diffusion transformers, leveraging attention sharing, dynamic shifting analysis, and MLLMs for zero-shot synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on training procedures, limiting practical applications and failing to utilize the zero-shot potential of diffusion transformers.

Method: Proposes three innovations: attention sharing for layout integrity, dynamic shifting analysis for feature extraction, and MLLM integration for cross-modal semantics.

Result: Achieves state-of-the-art or comparable results without training, with seamless compatibility for inpainting and control modules.

Conclusion: FreeCus successfully unlocks DiT's zero-shot potential for consistent subject synthesis, offering a practical and flexible solution.

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [92] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: The paper proposes MinCD-PnP, a robust method for image-to-point-cloud registration by simplifying blind PnP into minimizing Chamfer distance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Differential PnP is sensitive to noise and outliers, limiting correspondence learning. Blind PnP's robustness inspired a more effective solution.

Method: Simplified blind PnP into MinCD-PnP, minimizing Chamfer distance between 2D-3D keypoints, and introduced MinCD-Net, a lightweight multi-task module.

Result: MinCD-Net achieves higher inlier ratio and registration recall across multiple datasets, including 7-Scenes and ScanNet.

Conclusion: The proposed MinCD-PnP and MinCD-Net offer a robust, efficient solution for I2P registration, improving performance in noisy and outlier-prone scenarios.

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [93] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: A video compression framework using conditional diffusion models for perceptually optimized reconstruction, outperforming traditional and neural codecs.


<details>
  <summary>Details</summary>
Motivation: To leverage conditional diffusion models for video compression, aligning reconstructed content with human visual perception.

Method: Reframes compression as conditional generation, using multi-granular conditioning, compact representations, and multi-condition training with modality dropout.

Result: Significantly outperforms traditional and neural codecs on perceptual quality metrics like FVD and LPIPS, especially at high compression ratios.

Conclusion: The proposed framework effectively combines conditional diffusion models with novel modules for superior perceptual video compression.

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [94] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: The paper proposes a Vision Language Model (VLM) framework for detecting physical and digital attacks in biometric systems, outperforming traditional CNNs without extensive training.


<details>
  <summary>Details</summary>
Motivation: Biometric systems face evolving attack techniques, and traditional deep learning models struggle with adaptability and data requirements.

Method: The study introduces an in-context learning framework using VLMs for attack detection, evaluated on open-source models and databases.

Result: The framework achieves competitive performance in detecting attacks, surpassing some CNNs without resource-heavy training.

Conclusion: VLMs show promise for improving generalization in biometric attack detection, offering a resource-efficient alternative.

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [95] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: The paper proposes DMD, a minutiae-anchored local dense representation for robust fingerprint matching under diverse conditions, achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fingerprint matching under varied capture conditions is challenging; existing methods lack robustness and accuracy.

Method: DMD extracts descriptors from minutiae-centered patches, forming a 3D tensor for fine-grained ridge and minutiae features, using segmentation masks for efficient matching.

Result: DMD achieves top accuracy on rolled, plain, partial, contactless, and latent fingerprint datasets, with high computational efficiency.

Conclusion: DMD is effective, generalizable, and efficient, suitable for large-scale fingerprint recognition.

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [96] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: The paper addresses challenges in few-shot object detection (FSOD) by proposing a Spatial-Channel State Space Modeling (SCSM) module to improve feature extraction using inter-channel correlations, inspired by Mamba for temporal sequences.


<details>
  <summary>Details</summary>
Motivation: Current FSOD methods struggle with extracting effective features due to limited training samples, where high-weight channels may not be effective and low-weight channels may still hold value.

Method: The SCSM module includes Spatial Feature Modeling (SFM) for spatial-channel balance and Channel State Modeling (CSM) based on Mamba for channel correlation.

Result: Experiments on VOC and COCO datasets show SCSM improves feature representation and achieves state-of-the-art performance.

Conclusion: The SCSM module effectively enhances feature extraction in FSOD by leveraging inter-channel correlations, leading to superior performance.

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [97] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: BenchDepth introduces a new benchmark for evaluating depth foundation models (DFMs) using five downstream tasks, avoiding biases of traditional alignment-based metrics.


<details>
  <summary>Details</summary>
Motivation: Current depth estimation evaluation protocols are inconsistent and biased, favoring certain representations and complicating fair comparisons.

Method: Proposes BenchDepth, evaluating DFMs through five proxy tasks (e.g., depth completion, SLAM) to assess practical utility without alignment.

Result: Benchmarked eight state-of-the-art DFMs, providing key insights into their performance and practical applicability.

Conclusion: BenchDepth aims to improve depth model evaluation practices and inspire future research in depth estimation.

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [98] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD (Explicit Dual Distribution) is a novel framework for industrial defect detection that models dual feature distributions, uses synthetic data generation, and achieves high performance.


<details>
  <summary>Details</summary>
Motivation: One-class anomaly detection paradigms struggle with uniform outlier assumptions and data scarcity in real-world manufacturing.

Method: ExDD uses parallel memory banks for normality and anomaly patterns, latent diffusion models for synthetic data, and a neighborhood-aware scoring mechanism.

Result: Achieves 94.2% I-AUROC and 97.7% P-AUROC on KSDD2, optimal with 100 synthetic samples.

Conclusion: ExDD effectively addresses limitations of traditional anomaly detection, improving industrial defect detection.

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [99] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion addresses pavement defect detection challenges using synthetic anomaly generation and dual-path feature adaptation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Overcome challenges like limited annotated data, domain shift, and variability in defect appearances for robust pavement defect detection.

Method: Uses a latent diffusion model for synthetic anomaly generation and dual-path feature adaptors for specialized representations, with a lightweight discriminator for fine-grained defect patterns.

Result: Consistently strong performance on six benchmark datasets in classification and localization tasks.

Conclusion: RoadFusion sets new state-of-the-art metrics, proving effective for real-world road inspection.

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [100] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: The paper shows that high-fidelity synthetic datasets can train models as accurately as large real datasets, with added benefits like cost efficiency, data control, and fairness.


<details>
  <summary>Details</summary>
Motivation: To address the high costs and data challenges of training large human-centric vision models.

Method: Uses procedural synthetic datasets for training, ensuring detail, perfect labels, and control over diversity.

Result: Models achieve comparable accuracy on depth, surface normal, and segmentation tasks at lower cost.

Conclusion: Synthetic data is a viable, efficient alternative to large real datasets for human-centric vision tasks.

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [101] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet improves FER by using multi-modal semantic guidance, a multi-scale fusion module, and dynamic adversarial loss to handle occlusion and biases, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing FER models struggle with occlusion and dataset biases, leading to inaccurate classifications.

Method: ORSANet uses semantic segmentation maps and facial landmarks as priors, a Multi-scale Cross-interaction Module for fusion, and a Dynamic Adversarial Repulsion Enhancement Loss.

Result: ORSANet achieves state-of-the-art performance on public benchmarks and the new Occlu-FER dataset.

Conclusion: ORSANet effectively addresses occlusion and bias challenges in FER, demonstrating superior performance.

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [102] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX is a concept-based explanation framework to improve interpretability in surgical phase recognition models by linking neurons to relevant concepts.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for surgical phase recognition lack interpretability, hindering trust and debugging.

Method: SurgX selects example sequences for neurons, constructs a concept set, associates neurons with concepts, and identifies key neurons for predictions.

Result: Validated on two models, SurgX effectively explains predictions, enhancing interpretability.

Conclusion: SurgX demonstrates potential for making surgical phase recognition models more transparent and trustworthy.

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [103] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune is a training-free token pruning method for efficient egomotion video reasoning, outperforming prior methods in reducing computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Egomotion videos are crucial for embodied AI agents, but existing token pruning methods are inefficient for their spatiotemporal continuity and motion constraints.

Method: EgoPrune includes a keyframe selector, Perspective-Aware Redundancy Filtering (PARF), and an MMR-based token selector to prune redundant tokens efficiently.

Result: EgoPrune outperforms prior methods in benchmarks, reducing FLOPs, memory usage, and latency, and proves effective on an edge device.

Conclusion: EgoPrune is a practical solution for efficient on-device egomotion video reasoning.

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [104] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda is a method for fine-tuning VLMs by dynamically adjusting fused representations, improving performance with minimal changes.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect fused representations in VLMs, limiting their downstream potential.

Method: RAda uses a learned mask from a lightweight attention layer to calibrate the rational matrix.

Result: RAda improves baselines and performs comparably to state-of-the-art methods in various settings.

Conclusion: RAda is a versatile and efficient fine-tuning technique for VLMs.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [105] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: A crowd-search initiative created a dataset of hard-to-detect anomalies in dense forests, serving as a benchmark for improving anomaly detection in complex environments.


<details>
  <summary>Details</summary>
Motivation: Authorities struggled to locate a suspect in dense forest terrain, highlighting the need for better anomaly detection methods in occluded, real-world conditions.

Method: High-resolution aerial imagery was captured, but automated analysis failed due to dense vegetation. A crowd-search initiative was launched, producing a labeled dataset.

Result: Existing anomaly detection methods performed poorly, emphasizing the need for context-aware approaches. The dataset is openly accessible for further research.

Conclusion: The dataset and interactive web interface provide valuable resources for improving anomaly detection in complex environments, aiding future manhunts and rescue operations.

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [106] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: A LiDAR-Visual odometry framework integrating LiDAR and images for accurate pose estimation, using depth completion and attention mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Odometry is crucial for autonomous systems' self-localization and navigation, requiring robust and accurate solutions.

Method: Combines LiDAR point clouds and images via depth completion, uses multi-scale feature extraction with attention, and refines pose hierarchically.

Result: Achieves superior accuracy and robustness on the KITTI benchmark compared to existing methods.

Conclusion: The proposed framework effectively integrates LiDAR and visual data, offering robust odometry for dynamic environments.

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [107] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR is a framework for interactive text-to-video retrieval that minimizes uncertainties (text ambiguity, mapping uncertainty, frame uncertainty) using principled metrics, improving retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Current interactive TVR systems lack explicit uncertainty quantification, limiting effectiveness. UMIVR addresses this gap by quantifying and minimizing uncertainties.

Method: UMIVR uses semantic entropy (TAS), Jensen-Shannon divergence (MUS), and a Temporal Quality-based Frame Sampler (TQFS) to measure uncertainties and guide clarifying questions.

Result: UMIVR achieves 69.2% Recall@1 after 10 rounds on MSR-VTT-1k, outperforming heuristic-based methods.

Conclusion: UMIVR establishes a principled, uncertainty-minimizing approach for interactive TVR, validated by significant performance gains.

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [108] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer is a Transformer-based framework for low-light enhancement, excelling in non-uniform lighting scenarios with spatially-adaptive illumination modeling and guided attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-uniform lighting (e.g., backlit, shadow), leading to over-exposure or inadequate brightness restoration.

Method: Proposes SAIGFormer with dynamic integral image representation, Spatially-Adaptive Integral Illumination Estimator (SAI²E), and Illumination-Guided Multi-head Self-Attention (IG-MSA).

Result: Outperforms state-of-the-art on five datasets and LOL-Blur benchmark, excelling in non-uniform illumination and generalization.

Conclusion: SAIGFormer advances low-light enhancement with superior performance and adaptability, especially in challenging lighting conditions.

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [109] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: A self-supervised framework for learning procedures from unlabeled videos, addressing challenges like order variations and redundant frames using fused Gromov-Wasserstein optimal transport and contrastive regularization.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous methods suffering from order variations, background/redundant frames, and repeated actions in procedural video learning.

Method: Proposes a framework combining fused Gromov-Wasserstein optimal transport with a structural prior for temporal alignment and contrastive regularization to avoid degenerate solutions.

Result: Demonstrates superior performance on benchmarks (EgoProceL, ProceL, CrossTask) compared to prior methods like OPEL.

Conclusion: The proposed approach effectively learns key steps and their order from unlabeled videos, outperforming existing methods.

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [110] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: The paper introduces Endoscapes-SG201 dataset and SSG-Com, a graph-based method, to enhance surgical scene understanding by modeling tool-action-target combinations and hand identity.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based representations of surgical scenes lack exploration of tool-action-target combinations and hand identity, which are crucial for comprehensive scene understanding.

Method: Proposes the Endoscapes-SG201 dataset with annotations for tool-action-target and hand identity, and introduces SSG-Com, a graph-based method to learn these elements.

Result: Experiments show SSG-Com's effectiveness in tasks like critical view of safety assessment and action triplet recognition, proving the importance of the new components.

Conclusion: Incorporating tool-action-target and hand identity into graph representations significantly improves surgical scene understanding, as demonstrated by SSG-Com's performance.

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [111] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa improves zero-shot HOI detection by decomposing VLM text features into class-shared basis and adaptable weights, enhancing generalization and action distinction.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with distinguishing actions involving the same object or generalizing to unseen classes.

Method: HOLa uses low-rank factorization of VLM text features, adapts weights for each HOI class, and enriches visual representations with human-object tokens.

Result: Achieves state-of-the-art unseen-class mAP of 27.91 on HICO-DET.

Conclusion: HOLa effectively enhances generalization and action distinction in zero-shot HOI detection.

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [112] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: The paper introduces Dynamic-Image (DynImg), a method to improve video understanding by integrating temporal prompts for fast-moving objects and using 4D Rotary Position Embedding.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with accurately representing spatial information of fast-moving objects, leading to poor spatio-temporal interaction.

Method: DynImg uses non-key frames as temporal prompts to highlight fast-moving objects and employs 4D Rotary Position Embedding to maintain spatio-temporal order.

Result: DynImg outperforms state-of-the-art methods by ~2% on video understanding benchmarks.

Conclusion: DynImg effectively enhances video comprehension by addressing temporal integration challenges.

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [113] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix is a two-stage framework using class-conditional GANs for realistic image augmentation in medical applications, outperforming traditional mixup in COVID-19 detection.


<details>
  <summary>Details</summary>
Motivation: Traditional mixup produces unrealistic images, hindering learning in medical applications. GeMix aims to improve semantic fidelity and regularization.

Method: Uses StyleGAN2-ADA to generate images conditioned on soft labels from Dirichlet priors and Beta-distributed blending. Tested on COVIDx-CT-3 with ResNet and EfficientNet backbones.

Result: GeMix improves macro-F1 and reduces false negatives in COVID-19 detection compared to traditional mixup.

Conclusion: GeMix is a drop-in replacement for pixel-space mixup, offering better regularization and semantic fidelity without disrupting training pipelines.

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [114] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: Proposes an onboard satellite framework for real-time change detection using a deep neural network with three submodules: compression, co-registration, and change detection.


<details>
  <summary>Details</summary>
Motivation: Overcome latency in traditional satellite change detection by shifting the workflow onboard to enable real-time applications.

Method: Uses a deep neural network with three interlinked submodules: image compression, lightweight co-registration, and a temporally-invariant change detection model.

Result: Achieves compelling F1 scores with a throughput of 0.7 Mpixel/s on low-power hardware (15W accelerator).

Conclusion: The framework successfully addresses onboard processing constraints and outperforms state-of-the-art methods.

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [115] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT, a diffusion transformer-based model for medical image segmentation, achieves state-of-the-art results on skin lesion datasets, offering fast inference and low hardware requirements.


<details>
  <summary>Details</summary>
Motivation: Improving skin lesion segmentation for better disease diagnosis and treatment planning, especially in resource-limited settings.

Method: SegDT combines diffusion transformers with Rectified Flow for efficient, high-quality segmentation on low-cost hardware.

Result: State-of-the-art performance on three datasets with fast inference speeds.

Conclusion: SegDT advances medical image analysis, providing a practical tool for healthcare professionals.

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [116] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0 is a Vision-Language-Action model trained on human videos to address dexterity and generalization gaps in manipulation tasks, using physical instruction tuning and part-level motion tokenization.


<details>
  <summary>Details</summary>
Motivation: Existing VLAs struggle with complex manipulation tasks due to reliance on synthetic or limited teleoperated data. Human hand data offers rich dexterity and scalability.

Method: Combines VLA pretraining from human videos, physical space alignment, and post-training adaptation. Introduces part-level motion tokenization for precise hand trajectory modeling.

Result: Achieves millimeter-level reconstruction accuracy, excels in hand motion generation and instruction following, and scales well with model and data sizes.

Conclusion: Being-H0 demonstrates improved performance in real-world robotic manipulation, validating the effectiveness of physical instruction tuning.

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: A hybrid method combining SDF and 3DGS improves surface reconstruction and novel view rendering, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SDF (lacking fine details) and 3DGS (lacking global coherence) in sparse-view image tasks.

Method: Combines SDF for coarse geometry and 3DGS for rendering, with mutual refinement between the two.

Result: Outperforms state-of-the-art methods on DTU and MobileBrick datasets.

Conclusion: The hybrid approach effectively leverages the strengths of SDF and 3DGS for superior performance.

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: The paper introduces CylinderPlane, a cylindrical coordinate-based representation to address multi-face artifacts and improve 360° view image generation, outperforming Tri-plane methods.


<details>
  <summary>Details</summary>
Motivation: Tri-plane representations cause multi-face artifacts due to shared symmetric features, limiting 360° view generation.

Method: Proposes CylinderPlane, a cylindrical coordinate system, and nested cylinders for multi-scale feature capture.

Result: Achieves high-quality, artifact-free 360° image synthesis and outperforms previous methods.

Conclusion: CylinderPlane effectively resolves feature ambiguity and enhances multi-view consistency, adaptable to various rendering pipelines.

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: This survey reviews efficiency optimization techniques for DNNs in video analytics, covering hardware, data processing, and deployment, while highlighting challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of video data demands efficient and accurate analytics, but improving DNN efficiency remains a challenge.

Method: The survey organizes methods bottom-up, examining hardware support, data processing, and operational deployment.

Result: It provides a comprehensive review of efficiency-focused DNN optimization techniques in video analytics.

Conclusion: The paper identifies key challenges and problems in optimizing DNN performance for video analytics.

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: The paper explores Active Learning (AL) and Sequential Learning (SL) for OMR in medieval music manuscripts, using YOLOv8 to minimize manual labeling. Results show comparable accuracy to full supervision with fewer labels, but uncertainty-based AL was ineffective for the dataset.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of annotated data and complexity of historical manuscripts in Optical Music Recognition (OMR).

Method: Uses YOLOv8 for object detection and layout recognition, selecting uncertain samples for iterative labeling and retraining, starting with one annotated image.

Result: Achieves comparable accuracy to fully supervised training with fewer labeled examples, but uncertainty-based AL was ineffective for the dataset.

Conclusion: Advocates for more usable methods in data-scarcity scenarios, as uncertainty-based AL did not perform well for the manuscript studied.

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: The study applies the Lottery Ticket Hypothesis (LTH) to deepfake detection, identifying efficient subnetworks (winning tickets) that maintain high accuracy even at high sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection methods are resource-intensive and poorly understood; this work aims to improve efficiency and deployability.

Method: Uses LTH-based iterative magnitude pruning on MesoNet, CNN-5, and ResNet-18 architectures, tested on OpenForensic and FaceForensics++ datasets.

Result: Pruned networks retain high accuracy (e.g., MesoNet at 56.2% accuracy with 80% sparsity) and outperform one-shot pruning methods. Winning tickets are transferable across datasets.

Conclusion: LTH-based pruning enables efficient, deployable deepfake detection systems while maintaining performance.

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: EVA is a training-free method to reduce hallucinations in MLLMs by dynamically selecting intermediate layers with strong visual factual information and correcting output logits.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with object hallucinations due to suppressed visual information by prior knowledge in deep layers. The unclear suppression mechanism in intermediate layers motivates the study.

Method: EVA selects intermediate layers with significant visual factual information, contrasts distributions from original and pure-text inputs, and corrects final layer logits.

Result: EVA significantly reduces hallucination rates in benchmarks compared to baselines.

Conclusion: EVA is an effective, model-agnostic solution for mitigating hallucinations in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA is a new benchmark for Multilingual Handwritten Visual Question Answering, addressing gaps in current models by including 1,600 handwritten pages and 2,400 Q&A pairs, and evaluating OCR models in real-world contexts.


<details>
  <summary>Details</summary>
Motivation: Current MLVQA models underperform with handwritten documents, highlighting the need for a dedicated benchmark to improve multilingual handwritten document comprehension.

Method: HW-MLVQA introduces a dataset of 1,600 handwritten pages and 2,400 Q&A pairs, evaluated across text, image, and combined modalities, and tests OCR models without ground truth transcriptions.

Result: The benchmark provides a framework for assessing multilingual handwritten document interpretation, enabling advancements in this niche area.

Conclusion: HW-MLVQA aims to drive innovation in multilingual handwritten document understanding by offering a comprehensive evaluation tool.

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: A knowledge distillation method using CLIP for Image Quality Assessment (IQA) reduces model complexity and improves performance.


<details>
  <summary>Details</summary>
Motivation: Address CLIP's excessive parameters and poor local distortion identification in IQA tasks.

Method: Design quality-graded prompts, fine-tune CLIP, and use modality-adaptive distillation to transfer knowledge to a student model.

Result: Outperforms existing IQA methods with reduced complexity, validated on multiple datasets.

Conclusion: The method shows strong practical potential for efficient and effective IQA.

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: LINR-PCGC is the first INR-based lossless point cloud geometry compression method, improving encoding speed and reducing bitstream size compared to traditional and AI-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based methods are limited by training data dependencies, while INR methods face encoding time and decoder size constraints, restricting them to lossy compression.

Method: Proposes a group-level coding framework with network initialization to speed up encoding, and a lightweight network using multiscale SparseConv for fast inference and compact decoder size.

Result: Reduces encoding time by 60% and bitstream size by ~21.21% (vs. G-PCC) and ~21.95% (vs. SparsePCGC).

Conclusion: LINR-PCGC achieves lossless compression with significant efficiency gains, outperforming existing methods.

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS introduces wavelet-space losses for sparse-view 3D Gaussian Splatting, improving generalization by focusing on low-frequency subbands and reducing high-frequency overfitting.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS struggles with overfitting to high-frequency details in training views, necessitating a better regularization approach.

Method: DWTGS uses wavelet-space losses, supervising low-frequency subbands and enforcing sparsity on high-frequency subbands self-supervised.

Result: DWTGS outperforms Fourier-based methods, improving generalization and reducing high-frequency hallucinations.

Conclusion: Wavelet-space losses in DWTGS offer a superior alternative to Fourier-based frequency regularization for sparse-view 3DGS.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: A computationally efficient FIQA method using teacher-student distillation and self-training, achieving high performance with low overhead.


<details>
  <summary>Details</summary>
Motivation: Address the computational complexity of FIQA algorithms for scalable and practical deployment in real-world systems.

Method: Two-stage approach: (1) Train a teacher model using labeled data and self-training with pseudo-labels, (2) Distill a lightweight student model using labeled and pseudo-labeled data.

Result: Student model matches teacher performance with minimal computational cost; won ICCV 2025 VQualA FIQA Challenge.

Conclusion: The method successfully balances efficiency and performance, making it suitable for real-world deployment.

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: The paper clarifies transformer-based spatially-controlled image generation, comparing paradigms and introducing control token prefilling and sampling enhancements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed and fair scientific comparison in spatially-controlled image generation, and to provide clear takeaways for practitioners.

Method: Controlled experiments on ImageNet with diffusion-based, flow-based, and autoregressive models, focusing on control token prefilling and sampling enhancements like classifier-free guidance and softmax truncation.

Result: Control token prefilling is a strong baseline; sampling enhancements improve control-generation consistency; adapter-based approaches mitigate forgetting but underperform in consistency.

Conclusion: The study offers practical insights for transformer-based spatially-controlled generation, bridging gaps in the literature and highlighting effective techniques.

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen is a two-stage framework using condensed tokens to generate long videos with improved consistency and smooth transitions, addressing memory and coherence challenges.


<details>
  <summary>Details</summary>
Motivation: Extending diffusion-based models for long videos often causes memory bottlenecks and inconsistency. TokensGen aims to solve these issues.

Method: TokensGen decomposes the task into three parts: inner-clip control, long-term consistency, and smooth transitions. It uses To2V for short clips and T2To for global token generation, with FIFO-Diffusion for seamless connections.

Result: The method enhances long-term coherence and smooth transitions without excessive computational cost.

Conclusion: TokensGen offers a scalable, modular solution for long video generation, benefiting storytelling and cinematic production.

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: A transformer-based method predicts bilateral grids to correct photometric inconsistencies in multi-view scenes, improving novel view synthesis without scene-specific retraining.


<details>
  <summary>Details</summary>
Motivation: Photometric inconsistencies from camera pipelines degrade multi-view consistency and novel view synthesis quality. Existing methods increase complexity and slow training.

Method: Proposes a transformer-based approach to predict spatially adaptive bilateral grids for photometric correction, integrated into the 3D Gaussian Splatting pipeline.

Result: Outperforms or matches scene-specific optimization methods in reconstruction fidelity and convergence speed.

Conclusion: The method enables robust cross-scene generalization and maintains high training efficiency.

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: A novel framework, HDF, is proposed for Dynamic Facial Expression Recognition (DFER) to address performance degradation from sample heterogeneity. It includes two modules: DAM for time-frequency modeling and DSM for adaptive optimization, achieving improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing DFER methods suffer from performance degradation due to multi-source data and individual expression variability. The paper aims to enhance tolerance to inconsistencies and improve representation learning.

Method: The HDF framework introduces two modules: Time-Frequency Distributional Attention Module (DAM) for dual-branch attention and Distribution-aware Scaling Module (DSM) for dynamic loss balancing.

Result: HDF outperforms on DFEW and FERV39k datasets, achieving higher WAR and UAR with strong generalization.

Conclusion: The proposed HDF framework effectively addresses heterogeneity challenges in DFER, offering improved accuracy and robustness.

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: Proposes tree-based semantic loss functions for medical image segmentation to improve accuracy by leveraging hierarchical label organization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods penalize all segmentation errors equally, ignoring inter-class semantics, which is problematic for rich label spaces.

Method: Introduces two tree-based semantic loss functions and integrates them with sparse, background-free annotation training.

Result: Achieves state-of-the-art performance in head MRI for whole brain parcellation and neurosurgical hyperspectral imaging tasks.

Conclusion: The proposed method effectively exploits label hierarchy for better segmentation, proving its utility in medical imaging.

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: A novel PEFT method for medical image segmentation dynamically adjusts rank during adaptation, outperforming standard LoRA and other PEFT methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of selecting a fixed rank in LoRA for medical imaging tasks by introducing dynamic rank adjustment.

Method: Uses an l_1 sparsity regularizer on singular value decomposition to automatically find task-adapted ranks, optimized with a proximal optimizer.

Result: Significant performance improvements in few-shot fine-tuning, especially for base and novel organ segmentation tasks.

Conclusion: The method is efficient, robust against suboptimal rank initialization, and publicly available.

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: The paper explores low-parameter deep neural networks for computer vision, focusing on bottleneck architectures and superlinear activations. It finds that reducing interference in feature maps improves scaling and accuracy in small networks (<1.5M parameters). A new architecture, NoDepth Bottleneck, is proposed, showing strong performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and accuracy of low-parameter deep neural networks by addressing interference in feature maps, a challenge in bottleneck architectures.

Method: Examines various bottleneck architectures to identify design elements that reduce interference. Proposes the NoDepth Bottleneck architecture based on experimental insights.

Result: Limiting interference improves scaling and accuracy in small networks. The NoDepth Bottleneck demonstrates robust performance on ImageNet.

Conclusion: The findings contribute to more efficient and scalable neural networks for low-parameter applications and deepen understanding of bottlenecks in computer vision.

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM leverages SEEM, a foundational segmentation model, to generate masks for unlabeled data and uses conformal prediction to calibrate and filter unreliable labels, improving semi-supervised semantic segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost of pixel-level annotation by using foundational models to generate labels for unlabeled data, while mitigating their shortcomings.

Method: Proposes ConformalSAM, which calibrates SEEM using labeled target data, filters unreliable labels via conformal prediction, and employs self-reliance training to avoid overfitting.

Result: Outperforms recent SSSS methods on benchmarks and enhances their performance as a plug-in.

Conclusion: ConformalSAM effectively leverages foundational models for SSSS, balancing early-stage reliability with later-stage adaptability.

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: The paper addresses the limitations of Multimodal Large Language Models (MLLMs) in effectively leveraging visual information during Multimodal In-Context Learning (MICL). It introduces Dynamic Attention Reallocation (DARA) and TrueMICL to enhance and evaluate true multimodal adaptation.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs over-rely on textual patterns and neglect visual cues, making MICL unimodal and limiting its practical utility. This issue is often masked by performance on tasks not requiring visual understanding.

Method: The authors propose DARA, a fine-tuning strategy to rebalance attention between visual and textual tokens, and introduce TrueMICL, a dataset requiring multimodal integration for task completion.

Result: Experiments show significant improvements in true multimodal in-context learning capabilities with the proposed solutions.

Conclusion: The paper successfully enhances MICL by addressing visual neglect and provides a reliable evaluation framework with TrueMICL.

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: Diffusion models improve multivariate subsurface modeling and probabilistic inversion, outperforming VAEs and GANs. Proposed corrections to Diffusion Posterior Sampling enhance robustness and reduce costs.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing generative models (VAEs, GANs) in multivariate subsurface modeling and probabilistic inversion.

Method: Introduces corrections to Diffusion Posterior Sampling, including a noise-contamination-aware likelihood approximation. Tests involve facies and acoustic impedance with local and seismic data.

Result: Improved statistical robustness, better posterior sampling, and reduced computational costs compared to original methods.

Conclusion: Diffusion models with proposed corrections offer efficient, robust solutions for subsurface modeling and inversion, handling both hard and indirect data.

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench is a benchmark to evaluate physical commonsense in text-to-video models, using 383 prompts and a three-stage evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Current T2V models lack physical reasoning, producing videos that violate commonsense expectations.

Method: The benchmark uses prompts focusing on tool use, material properties, and procedural interactions, with a three-stage evaluation: grounded physics questions, video captioning, and language model answers.

Result: PhysVidBench offers a structured, interpretable framework to assess physical commonsense in T2V models, addressing overlooked areas like tool-mediated actions.

Conclusion: The benchmark fills a gap in T2V evaluation by systematically testing physical plausibility, aiding model improvement.

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper introduces Segment Concept (SeC), a concept-driven framework for Video Object Segmentation (VOS) that leverages Large Vision-Language Models (LVLMs) for robust object tracking and segmentation, outperforming SAM 2.1 by 11.8 points on the new SeCVOS benchmark.


<details>
  <summary>Details</summary>
Motivation: Current VOS methods rely on appearance matching, lacking human-like conceptual understanding, which limits performance in complex scenarios like occlusions and visual variations.

Method: SeC constructs high-level, object-centric representations using LVLMs, integrating visual cues and semantic reasoning, and dynamically balances feature matching with computational effort based on scene complexity.

Result: SeC achieves an 11.8-point improvement over SAM 2.1 on the SeCVOS benchmark, setting a new state-of-the-art in concept-aware VOS.

Conclusion: SeC demonstrates the effectiveness of concept-driven approaches in VOS, particularly in complex scenarios, and introduces a challenging benchmark (SeCVOS) for future research.

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: The paper proposes aligning tokenizer embeddings with the denoising objective to improve generative modeling, introducing the Latent Denoising Tokenizer (l-DeTok), which outperforms standard tokenizers.


<details>
  <summary>Details</summary>
Motivation: Modern generative models use denoising (reconstructing clean signals from corrupted inputs) as a training objective, suggesting that tokenizers could be more effective if aligned with this objective.

Method: The authors introduce the Latent Denoising Tokenizer (l-DeTok), trained to reconstruct clean images from corrupted latent embeddings using interpolative noise and random masking.

Result: Experiments on ImageNet 256x256 show l-DeTok consistently outperforms standard tokenizers across six generative models.

Conclusion: Denoising is a fundamental design principle for tokenizers, offering new perspectives for future tokenizer development.

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>
