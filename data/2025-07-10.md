<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: A UIE method using CLIP perception loss and curriculum contrastive regularization improves underwater image quality by aligning with human perception and enhancing constraints.


<details>
  <summary>Details</summary>
Motivation: Underwater images suffer from quality degradation due to light absorption and scattering, and existing deep learning methods often ignore human perception and lack solution space constraints.

Method: Proposes a UIE method with a CLIP perception loss module and curriculum contrastive regularization to align with human perception and enhance constraints.

Result: Outperforms state-of-the-art methods in visual quality and generalization.

Conclusion: The method effectively improves underwater image quality by integrating human perception and robust regularization.

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [2] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: SPARC introduces a unified latent space for AI models, improving cross-model interpretability with shared concepts.


<details>
  <summary>Details</summary>
Motivation: Existing methods create isolated concept spaces, limiting cross-model interpretability.

Method: SPARC uses Global TopK sparsity and Cross-Reconstruction Loss to align latent spaces across models.

Result: Achieves 0.80 Jaccard similarity, tripling alignment compared to prior methods.

Conclusion: SPARC enables direct comparison of model representations and practical applications like cross-modal retrieval.

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [3] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF introduces a probabilistic framework for uncertainty quantification in neural implicit SDF models, addressing computational inefficiencies and geometric inconsistencies in 3D representations.


<details>
  <summary>Details</summary>
Motivation: The need for precise surface geometry and uncertainty awareness in scientific simulations (e.g., fluid flow through forests) drives the development of BayesSDF.

Method: BayesSDF uses a Laplace approximation with Hessian-based metrics for efficient, surface-aware uncertainty estimation in SDF models.

Result: The method outperforms existing approaches in calibration and geometric consistency, providing actionable confidence measures.

Conclusion: BayesSDF establishes a robust foundation for uncertainty-aware 3D scene reconstruction, simulation, and robotic decision-making.

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [4] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: LIRA addresses LMMs' segmentation and comprehension issues by combining visual and semantic features (SEFE) and fine-grained supervision (ILVC), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with inaccurate segmentation and hallucinated comprehension due to weak visual understanding and lack of fine-grained perception.

Method: LIRA uses SEFE for better segmentation by fusing semantic and pixel-level features and ILVC for fine-grained supervision via local descriptions.

Result: LIRA outperforms in segmentation and comprehension tasks, with segmentation precision linked to latent semantics of the <seg> token.

Conclusion: LIRA effectively mitigates LMMs' limitations, demonstrating superior performance and introducing the AttrEval dataset for semantic evaluation.

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [5] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: A survey on offline handwritten text recognition (HTR) data augmentation and generation techniques, covering traditional methods and deep learning advances like GANs, diffusion models, and transformers, while addressing challenges in script authenticity and data scarcity.


<details>
  <summary>Details</summary>
Motivation: Limited annotated training data for HTR systems, especially in low-resource languages and complex scripts, hinders performance. This survey aims to improve accuracy and robustness by exploring augmentation and generation techniques.

Method: Follows PRISMA methodology, analyzing 1,302 studies filtered to 848 from IEEE, Springer, Science Direct, and ACM. Evaluates datasets, metrics, and methodologies.

Result: Identifies research gaps and proposes future directions for handwritten text generation across diverse scripts and languages.

Conclusion: The survey highlights the need for diverse, realistic handwriting samples and suggests advancements in generation techniques to address data scarcity and script authenticity.

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [6] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: CCPDA is a data augmentation method for improving fire-class segmentation in wildland fire science by copying and pasting refined fire clusters onto target images.


<details>
  <summary>Details</summary>
Motivation: The scarcity of labeled datasets for fire segmentation makes training models costly and challenging.

Method: CCPDA involves identifying fire clusters, centralizing them, and pasting onto target images to enhance dataset diversity.

Result: CCPDA improves fire-class segmentation metrics and outperforms other augmentation methods.

Conclusion: CCPDA effectively addresses dataset limitations and enhances fire-class segmentation performance.

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [7] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: AR2 improves CNN robustness to corruptions by aligning class activation maps between clean and corrupted images, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks degrade under common corruptions, limiting real-world reliability.

Method: AR2 aligns CAMs between clean and corrupted images, using iterative CAM-guided refinement and fine-tuning.

Result: AR2 outperforms state-of-the-art methods on CIFAR-10-C, CIFAR-100-C, and ImageNet-C benchmarks.

Conclusion: AR2 offers a robust, scalable solution for enhancing model reliability in corrupted environments.

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [8] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: The paper introduces MFT25, a dataset for underwater multiple fish tracking, and SU-T, a tracking framework with UKF and FishIoU, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Underwater tracking is underexplored despite its importance for marine ecology and aquaculture.

Method: Proposes SU-T with Unscented Kalman Filter for non-linear fish motion and FishIoU for morphological matching.

Result: SU-T achieves 34.1 HOTA and 44.6 IDF1 on MFT25, outperforming terrestrial tracking methods.

Conclusion: MFT25 and SU-T advance underwater tracking research, benefiting marine biology and conservation.

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [9] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: SImpHAR introduces a simulation pipeline and two-stage training strategy for bio-impedance-based HAR, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of labeled data in bio-impedance sensing for HAR.

Method: Simulation pipeline for bio-impedance signals and two-stage training strategy.

Result: 22.3% and 21.8% improvements in accuracy and macro F1 score.

Conclusion: Simulation-driven augmentation and modular training are promising for impedance-based HAR.

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [10] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: PCL-Former, a hierarchical multi-stage transformer architecture, improves temporal action localization by using dedicated transformer modules for proposal, classification, and localization, outperforming state-of-the-art methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Leveraging transformers and multi-stage architectures for better spatio-temporal modeling in temporal action localization (TAL).

Method: Hierarchical multi-stage transformer architecture (PCL-Former) with specialized modules (Proposal-Former, Classification-Former, Localization-Former) and loss functions.

Result: Outperforms state-of-the-art TAL methods by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS datasets, respectively.

Conclusion: PCL-Former is effective for TAL, validated by superior performance and ablation studies.

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [11] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: THOR is a real-time adaptive RGB frame sampling method using thermal sensing to efficiently capture and classify hand-object interactions, reducing data and power usage while maintaining high activity recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Continuous RGB image processing in wearable cameras is power-intensive, generates excessive data, raises privacy concerns, and demands high computational resources.

Method: THOR uses low-resolution thermal data to detect activity transitions, adjusting RGB frame sampling rates and localizing hand-object interactions for targeted processing.

Result: The method captures all activity segments using only 3% of original RGB data, achieving 95% F1-score for activity recognition, comparable to full RGB processing (94%).

Conclusion: THOR enables practical, real-time monitoring of hand-related activities with reduced resource usage, supporting longitudinal health behavior studies.

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [12] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mul√©,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: Proposes an event autoencoder for efficient event data compression and reconstruction, achieving high-speed, low-power object detection with comparable accuracy to YOLO-v4.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of traditional frame-based vision systems (motion blur, latency, redundancy) and challenges of event cameras (sparse, noisy data) for real-time perception.

Method: Uses convolutional encoding, adaptive threshold selection, and a lightweight classifier to enhance accuracy and reduce computational complexity.

Result: Achieves comparable accuracy to YOLO-v4 with 35.5x fewer parameters, high frame rates (8-44.8 FPS), and 87.84x better FPS than state-of-the-art.

Conclusion: The model improves event-based vision performance, making it suitable for real-time edge computing in low-power, high-speed applications.

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [13] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-RTS improves video reasoning efficiency by combining data-efficient RL and adaptive test-time scaling, outperforming existing models with minimal training data.


<details>
  <summary>Details</summary>
Motivation: Addressing the high cost and scalability issues of traditional RL-based video reasoning methods that rely on large-scale supervised fine-tuning and annotations.

Method: Skipping SFT, using pure-RL training with output-based rewards, and introducing a sparse-to-dense video TTS strategy for efficient inference.

Result: Video-RTS achieves 2.4% higher accuracy on average using only 3.6% of training samples, with notable improvements on benchmarks like Video-Holmes and MMVU.

Conclusion: Video-RTS demonstrates strong reasoning performance by leveraging pure RL training and adaptive TTS, offering a scalable and efficient alternative to traditional methods.

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [14] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: Mask6D, a novel pre-training strategy for 6D pose estimation, uses pose-aware 2D-3D correspondence and visible mask maps to improve accuracy in cluttered or occluded scenes.


<details>
  <summary>Details</summary>
Motivation: Current pose estimation networks struggle with limited RGB information due to occlusion, needing better feature extraction.

Method: Proposes Mask6D, combining RGB images with pose-aware 2D-3D correspondence and visible mask maps for pre-training, followed by fine-tuning.

Result: Outperforms previous end-to-end pose estimation methods in experiments.

Conclusion: Mask6D effectively enhances pose estimation in challenging conditions by leveraging additional modal information and focused pre-training.

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [15] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: The paper introduces BC-HOI, a framework for open vocabulary HOI detection, using Attention Bias Guidance and LLM-based Supervision Guidance to improve fine-grained interaction features.


<details>
  <summary>Details</summary>
Motivation: Existing methods use coarse-grained VLM features, which are unsuitable for detection tasks. The goal is to enhance interaction representation for open vocabulary HOI detection.

Method: Proposes BC-HOI with ABG for fine-grained instance-level features and LSG for token-level supervision, leveraging LLMs.

Result: Achieves superior performance on HICO-DET and V-COCO benchmarks in both open and closed settings.

Conclusion: BC-HOI effectively addresses the limitations of VLM features, improving HOI detection performance.

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [16] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: A survey categorizing critical traffic elements into anomalies and normal but critical entities, analyzing 35 vision-driven tasks and 73 datasets, aiming to unify standards and optimize resources.


<details>
  <summary>Details</summary>
Motivation: To improve road safety by leveraging advances in vision-based sensors and computer vision algorithms, providing a unified framework for traffic scenario analysis.

Method: Systematic categorization of traffic entities into two main groups (anomalies and normal but critical), integrating ten categories and twenty subclasses, and analyzing tasks and datasets.

Result: A taxonomy connecting related fields, analysis of 35 tasks, and examination of 73 datasets, highlighting pros, cons, and research gaps.

Conclusion: The survey offers a holistic overview, guides resource selection, and identifies critical research gaps, contributing to the field's advancement.

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [17] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: FIFA is a unified framework for evaluating and correcting hallucinations in VideoMLLMs, outperforming existing methods and aligning with human judgment.


<details>
  <summary>Details</summary>
Motivation: VideoMLLMs suffer from hallucinations, and current evaluation methods are limited and fail to assess open-ended responses.

Method: FIFA extracts descriptive facts, models dependencies via a Spatio-Temporal Semantic Dependency Graph, and verifies them using VideoQA models. Post-Correction revises hallucinations.

Result: FIFA aligns better with human judgment than existing methods, and Post-Correction improves factual consistency.

Conclusion: FIFA effectively addresses hallucinations in VideoMLLMs, enhancing evaluation and correction.

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [18] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: The paper introduces Key Step Concept Unlearning (KSCU), a method to unlearn harmful concepts in text-to-image diffusion models while preserving generative quality by targeting key denoising steps.


<details>
  <summary>Details</summary>
Motivation: Addressing the misuse of text-to-image diffusion models (e.g., Stable Diffusion) by balancing unlearning effectiveness with generative retainability, as existing methods struggle with this trade-off.

Method: KSCU focuses on pivotal denoising steps in diffusion models, fine-tuning only at these steps to unlearn concepts efficiently without compromising generative capabilities.

Result: Benchmark experiments show KSCU effectively prevents undesirable image generation while retaining the model's generative performance.

Conclusion: KSCU offers a targeted and efficient solution for concept unlearning in diffusion models, outperforming conventional approaches.

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [19] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: A pipeline converts English speech into 3D sign language animations using speech-to-text, gloss translation, and motion generation, achieving high BLEU scores and natural animations.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked task of converting spoken English into sign language animations, enabling easier communication for deaf and hard-of-hearing individuals.

Method: The system uses Whisper for speech-to-text, MarianMT for gloss translation (enhanced with Word2Vec/FastText), and a 3D keypoint-based motion system trained on Sign3D-WLASL.

Result: Achieves BLEU scores of 0.7714 and 0.8923 for gloss translation and produces smooth, realistic 3D animations.

Conclusion: The pipeline integrates audio, text, and motion into a unified framework, advancing the field beyond recognition-focused or single-data-type approaches.

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [20] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: ILNet introduces Inverse Learning attention and Dynamic Anchor Selection for multi-agent trajectory prediction, improving accuracy and adaptability in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Human drivers dynamically adjust decisions based on others' intentions, inspiring a need for better spatio-temporal interaction modeling in trajectory prediction.

Method: ILNet uses Inverse Learning attention for dynamic spatio-temporal coordination and a Dynamic Anchor Selection module for adaptable trajectory keypoints.

Result: ILNet achieves state-of-the-art performance on INTERACTION and Argoverse datasets, with higher accuracy and multimodal distributions in complex scenarios.

Conclusion: ILNet effectively captures complex interactions and adapts to future environments, outperforming existing methods with fewer parameters.

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [21] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: Proposes a model-agnostic active learning method for wildlife detection in camera trap data, achieving high performance with only 30% of training data.


<details>
  <summary>Details</summary>
Motivation: Wildlife data from camera traps is voluminous and costly to label. Active learning can optimize labeling efforts, but existing methods require full model access, limiting applicability.

Method: Integrates uncertainty and diversity metrics at object and image levels for sample selection in active learning.

Result: With 30% of training data, the approach matches or exceeds performance of using the full dataset.

Conclusion: The model-agnostic method effectively reduces labeling effort while maintaining detection accuracy, aiding wildlife monitoring.

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [22] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: ToBo is a self-supervised learning pipeline that compresses scenes into bottleneck tokens and predicts future scenes using minimal hints, excelling in sequential tasks like video tracking and robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the need for compact and temporally aware visual representations in dynamic scenes for tasks like tracking and manipulation.

Method: ToBo squeezes a scene into a bottleneck token and predicts subsequent scenes using minimal patches as hints, embedding temporal dependencies.

Result: Outperforms baselines in tasks like video label propagation and robotic manipulation, with robustness in real-world robot deployments.

Conclusion: ToBo effectively learns sequential scene representations and scales across model sizes, proving versatile and robust.

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [23] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: The paper introduces Concept-TRAK, a method for concept-level attribution in diffusion models, addressing copyright and transparency concerns by isolating contributions to specific image elements.


<details>
  <summary>Details</summary>
Motivation: Growing adoption of diffusion models raises copyright and transparency issues, with existing methods failing to attribute influences to specific image elements like styles or objects.

Method: Concept-TRAK extends influence functions with a reformulated diffusion training loss and a concept-aware reward function for robust, sample-specific attribution.

Result: Concept-TRAK outperforms prior methods on the AbC benchmark and provides actionable insights for responsible AI development through diverse case studies.

Conclusion: Concept-level attribution via Concept-TRAK enhances transparency and governance in generative AI, addressing critical concerns in copyright and model accountability.

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [24] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: Proposes a divergence-based similarity function (DSF) to model joint structure across multiple augmented views, improving performance and efficiency in tasks like kNN classification and linear evaluation.


<details>
  <summary>Details</summary>
Motivation: Prior methods fail to capture joint structure across all views, limiting their effectiveness in contrastive learning.

Method: Introduces DSF, which represents sets of augmented views as distributions and measures similarity via divergence between distributions.

Result: DSF consistently improves performance in tasks like kNN classification and linear evaluation, and is more efficient than other multi-view methods.

Conclusion: DSF effectively models joint structure without needing a temperature hyperparameter, outperforming cosine similarity.

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [25] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: The paper introduces the Edge-Boundary-Texture (EBT) loss, a novel objective for edge detection that categorizes pixels into edge, boundary, and texture, improving precision and contextual localization over the traditional WBCE loss.


<details>
  <summary>Details</summary>
Motivation: Current edge detection methods, using WBCE loss, treat non-edge pixels uniformly, leading to blurred predictions due to ignored structural nuances near edges.

Method: The EBT loss divides pixels into edge, boundary, and texture categories, assigning distinct weights to each, enabling structured learning and better edge localization.

Result: Experiments show EBT loss outperforms WBCE loss quantitatively and perceptually, with minimal fine-tuning due to robust hyperparameters.

Conclusion: The EBT loss is a superior, easily deployable alternative to WBCE for edge detection, offering improved precision and contextual awareness.

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [26] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST introduces a motion diffusion model using temporal clip Banzhaf interaction to improve human motion generation from rare language prompts by addressing coarse-grained matching and semantic cue issues.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in generating human motion from rare language prompts, particularly coarse-grained matching and overlooked semantic cues due to motion redundancy.

Method: Uses temporal clip Banzhaf interaction for fine-grained clip-level textual-motion coherence, followed by a motion prompt module for generation.

Result: Achieves state-of-the-art performance in text-to-motion retrieval and generation, especially for rare prompts.

Conclusion: MOST effectively addresses previous limitations and demonstrates superior performance in generating semantically consistent human motions from text.

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [27] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: The paper introduces AMContrast3D and AMContrast3D++, adaptive margin contrastive learning methods for 3D semantic segmentation on point clouds, addressing ambiguities in transition regions and improving segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods penalize all points equally, ignoring ambiguities in transition regions and unreliable human labels for highly ambiguous points, leading to sub-optimal models.

Method: AMContrast3D integrates contrastive learning with ambiguity estimation, adapting objectives per point. AMContrast3D++ adds a parallel branch with an ambiguity prediction module and masked refinement for reliable embeddings.

Result: The method improves segmentation performance and robustness, validated on S3DIS and ScanNet datasets.

Conclusion: The proposed adaptive margin contrastive learning effectively handles ambiguities, enhancing 3D semantic segmentation accuracy and reliability.

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [28] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: A dual-camera system (DCS) and exposure-adaptive fusion network (EAFNet) are proposed to address flickering in HDR video reconstruction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Exposure fluctuations in reference images cause flickering in HDR video reconstruction, necessitating a robust solution.

Method: DCS uses one camera for consistent reference sequences and another for supplementary data. EAFNet includes pre-alignment, asymmetric cross-feature fusion, and reconstruction subnetworks to enhance and fuse features.

Result: The method outperforms others on various datasets, demonstrating DCS's potential.

Conclusion: The proposed DCS and EAFNet effectively mitigate flickering and improve HDR video quality, with code and data made available for further use.

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [29] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: The paper proposes CMDCL, a cross-modal dual-causal learning method for long-term action recognition (LTAR), addressing biases and confounders in vision-language models (VLMs) through causal interventions.


<details>
  <summary>Details</summary>
Motivation: LTAR is challenging due to temporal spans and visual confounders. Existing VLMs rely on statistical correlations, lacking causal mechanisms, and current causality-based methods miss cross-modal modeling.

Method: CMDCL introduces a structural causal model for video-label text relationships, using textual and visual causal interventions to debias text embeddings and remove visual confounders.

Result: Experiments on Charades, Breakfast, and COIN benchmarks show CMDCL's effectiveness in robust action representation.

Conclusion: CMDCL successfully addresses LTAR challenges by leveraging cross-modal causal learning, outperforming existing methods.

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [30] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: Omni-Fuse, a novel spatial-spectral fusion network, improves hyperspectral image segmentation by integrating cross-dimensional features and attention mechanisms, achieving a 5.73% DSC improvement.


<details>
  <summary>Details</summary>
Motivation: Medical Hyperspectral Imaging (MHSI) offers rich spectral data for disease diagnosis but struggles with high dimensionality and spectral redundancy.

Method: Omni-Fuse uses cross-dimensional feature fusion, bidirectional attention, spectral-guided spatial query selection, and a two-stage decoder.

Result: Achieves a 5.73% improvement in DSC on two datasets.

Conclusion: Omni-Fuse effectively addresses MHSI challenges, enhancing segmentation performance efficiently.

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [31] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: Proposes View-Dependent Projection (VDP) for efficient 3D-to-2D mapping in point cloud segmentation, addressing limitations of view-independent methods.


<details>
  <summary>Details</summary>
Motivation: Existing projection-based methods lack adaptability to view variations, leading to limited point awareness and computational inefficiency.

Method: Designs VDP to generate data-driven projections inspired by fireworks, with color regularization to optimize feature emphasis.

Result: PointVDP achieves competitive results on S3DIS and ScanNet benchmarks with minimal computational overhead.

Conclusion: VDP offers a lightweight, resource-efficient solution for semantic understanding in point cloud segmentation.

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [32] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0 introduces a pathology foundation model using slide-level supervision for patch-level representations, outperforming SSL methods in biomarker prediction with high data efficiency.


<details>
  <summary>Details</summary>
Motivation: Patch-level SSL in digital pathology overlooks domain-specific features and is less data-efficient, requiring large datasets.

Method: EXAONE Path 2.0 learns patch-level representations under direct slide-level supervision, using 37k WSIs.

Result: Achieves state-of-the-art performance across 10 biomarker prediction tasks with remarkable data efficiency.

Conclusion: EXAONE Path 2.0 addresses limitations of SSL in pathology, offering a more efficient and effective solution for biomarker prediction.

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [33] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: The paper addresses learning dense keypoint localization from sparse pixel-level annotations in medical imaging, proposing a novel loss function (Crag and Tail loss) to improve accuracy despite limited annotations.


<details>
  <summary>Details</summary>
Motivation: Sparse labels in medical imaging are common due to high annotation costs, especially for dense tasks like pixel-level keypoint localization. This work aims to enable learning from few annotations to advance research in such scenarios.

Method: The problem is framed as sparse heatmap regression. A new loss function, Crag and Tail loss, is introduced to efficiently use sparse labels while mitigating false negatives.

Result: The approach achieves accurate dense localization of carcinosis keypoints, validated through extensive ablation studies.

Conclusion: The proposed method shows promise for advancing research in medical imaging where dense annotations are impractical, offering a practical solution for sparse label challenges.

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [34] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: ClipGS is a Gaussian splatting framework for interactive cinematic visualization of medical data, addressing high computing costs and low speed with learnable truncation and adaptive adjustment.


<details>
  <summary>Details</summary>
Motivation: Enhancing diagnostic accuracy and surgical planning through high-quality volumetric medical data visualization, despite computational challenges.

Method: Introduces ClipGS with a learnable truncation scheme and adaptive adjustment model for dynamic interactions and rendering performance.

Result: Achieves 36.635 PSNR, 156 FPS, and 16.1 MB model size, outperforming state-of-the-art methods.

Conclusion: ClipGS effectively improves rendering quality and efficiency for medical data visualization.

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [35] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: Diff$^2$I2P is a differentiable I2P registration framework using a diffusion prior to bridge the modality gap, improving cross-modal feature learning and registration accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with accurate cross-modal correspondences due to the modality gap between images and point clouds.

Method: Proposes Control-Side Score Distillation (CSD) and Deformable Correspondence Tuning (DCT) for differentiable correspondence estimation and transformation optimization.

Result: Achieves over 7% improvement in registration recall on the 7-Scenes benchmark.

Conclusion: Diff$^2$I2P outperforms state-of-the-art methods by effectively leveraging diffusion models to bridge the modality gap.

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [36] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: Proposes CDR-CA for refining diversity in Text-to-Image Retrieval, introducing Multi-Source DPPs and Tangent Normalization for context-aware diversity.


<details>
  <summary>Details</summary>
Motivation: Conventional RD methods lack adaptability to varying application contexts, limiting their utility.

Method: Extends DPP to multi-sources (MS-DPP) with a unified similarity matrix and introduces Tangent Normalization for context reflection.

Result: Demonstrates effectiveness through extensive experiments.

Conclusion: CDR-CA and MS-DPP offer a robust, context-aware solution for diversity refinement in retrieval tasks.

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [37] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: The paper analyzes gradient dynamics in diffusion models for image restoration, identifies instabilities, and proposes SPGD to stabilize the process, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address underexplored interactions and instabilities between denoising and likelihood guidance steps in diffusion models for image restoration.

Method: Proposes SPGD, combining progressive likelihood warm-up and adaptive directional momentum smoothing to manage gradients.

Result: SPGD improves stability and performance, outperforming existing methods in quantitative metrics and visual quality.

Conclusion: SPGD effectively mitigates gradient conflicts and fluctuations, enhancing restoration performance in diffusion models.

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [38] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: MK-Pose, a multimodal framework for category-level object pose estimation, integrates RGB, point clouds, and textual descriptions, outperforming state-of-the-art methods in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with occlusion and generalization across instances and categories, necessitating a more robust solution.

Method: Uses self-supervised keypoint detection with attention-based queries, soft heatmap matching, graph-based relational modeling, and a graph-enhanced feature fusion module.

Result: Outperforms state-of-the-art methods in IoU and average precision on CAMERA25, REAL275, and HouseCat6D datasets.

Conclusion: MK-Pose demonstrates superior performance and generalization, offering a promising solution for category-level pose estimation.

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [39] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian introduces a flexible, training-free method for compressing 3D Gaussian splatting models using mixed-precision quantization and pruning, achieving high compression ratios without retraining.


<details>
  <summary>Details</summary>
Motivation: The need for efficient compression of large-scale 3D Gaussian models for mobile and edge devices, avoiding retraining and adapting to varying constraints.

Method: Combines mixed-precision quantization and attribute-discriminative pruning for training-free compression.

Result: Achieves up to 96.4% compression with minimal quality loss (<1 dB PSNR drop), 1.7-2.1x faster than training-free methods, and 10-100x faster than training-involved approaches.

Conclusion: FlexGaussian is a fast, flexible, and effective solution for compressing 3D Gaussian models without retraining, suitable for resource-limited devices.

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [40] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: QUANet improves object counting by using quantity-oriented text prompts and a dual-stream decoder with T2C-adapters, achieving strong zero-shot performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack quantity awareness in text prompts, limiting accuracy in object counting tasks.

Method: Introduces quantity-oriented text prompts, a vision-text alignment loss, and a dual-stream decoder with T2C-adapters for density map prediction.

Result: Demonstrates strong generalizability on benchmarks like FSC-147, CARPK, PUCPR+, and ShanghaiTech.

Conclusion: QUANet enhances quantity awareness and achieves superior zero-shot class-agnostic counting performance.

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [41] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: StixelNExT++ is a novel monocular perception method that enhances 3D Stixel representation and object segmentation, achieving real-time performance and high scene compression.


<details>
  <summary>Details</summary>
Motivation: To improve scene representation for monocular perception systems by enhancing 3D Stixel inference and object segmentation.

Method: Clusters smaller 3D Stixel units, uses a lightweight neural network trained on LiDAR-based ground truth, and supports point cloud and bird's-eye-view representations.

Result: Achieves real-time performance (10 ms/frame) and competitive accuracy on the Waymo dataset within 30 meters.

Conclusion: StixelNExT++ shows promise for collective perception in autonomous systems due to its efficiency and adaptability.

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [42] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: STG-Mamba is a novel method for music-guided dance video synthesis, using spatial-temporal graph Mamba blocks and a self-supervised regularization network, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To translate input music into dance videos by effectively capturing spatial-temporal dependencies in skeleton sequences and generating realistic videos.

Method: Uses two mappings: music-to-skeleton (with STGM blocks) and skeleton-to-video (with a self-supervised network). A new dataset of 54,944 clips is collected.

Result: STG-Mamba significantly outperforms existing methods in experiments.

Conclusion: The proposed method effectively synthesizes dance videos from music, demonstrating superior performance.

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [43] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: SpatialReasoner enhances 3D visual grounding by integrating LLM-driven spatial reasoning and hierarchical feature fields, improving accuracy in localizing objects based on spatial relations.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with spatial relation understanding in language queries for 3D visual grounding, limiting their effectiveness in complex environments.

Method: Proposes SpatialReasoner, a framework combining LLM-driven spatial reasoning and hierarchical feature fields, leveraging CLIP and SAM for feature extraction.

Result: Outperforms baseline models in 3D visual grounding, demonstrating improved spatial reasoning and localization accuracy.

Conclusion: SpatialReasoner effectively addresses spatial relation challenges, enhancing 3D visual grounding for embodied AI applications.

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [44] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: A novel hierarchical pre-training strategy for gloss-free Sign Language Translation (SLT) improves translation quality by aligning video features with pseudo-glosses and spoken sentences.


<details>
  <summary>Details</summary>
Motivation: Existing SLT methods face challenges in bridging visual-textual disparities, with gloss-based and gloss-free approaches each having limitations.

Method: Hierarchical pre-training extracts features at frame, segment, and video levels, aligning them with pseudo-glosses and spoken sentences.

Result: Improved BLEU-4 and ROUGE scores, maintaining efficiency.

Conclusion: The proposed method effectively enhances SLT performance without relying on gloss annotations.

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [45] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: A novel method combining visual adapters, prompt learning, Partial Optimal Transport (POT), and contrastive learning (CL) enhances CLIP's adaptability for medical anomaly detection, achieving top results in few-shot, zero-shot, and cross-dataset scenarios.


<details>
  <summary>Details</summary>
Motivation: Medical anomaly detection is challenging due to diverse imaging modalities, anatomical variations, and limited labeled data.

Method: Uses visual adapters and prompt learning with POT and CL to align multiple prompts with local features for detecting subtle abnormalities. CL ensures intra-class cohesion and inter-class separation.

Result: Achieves state-of-the-art performance in few-shot, zero-shot, and cross-dataset settings without synthetic data or memory banks.

Conclusion: The proposed method effectively improves CLIP's adaptability for medical AD, offering robust performance across diverse scenarios.

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [46] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: RPFNet is a Residual Prior-driven Frequency-aware Network for image fusion, addressing computational costs and lack of ground-truth by using residual priors and frequency-domain convolution.


<details>
  <summary>Details</summary>
Motivation: To overcome high computational costs of spatial modeling and the challenge of capturing complementary features without ground-truth.

Method: Uses a dual-branch framework: Residual Prior Module (RPM) for modality-specific differences and Frequency Domain Fusion Module (FDFM) for efficient global feature integration. Includes Cross Promotion Module (CPM) for local-global synergy and auxiliary training losses.

Result: RPFNet effectively integrates features, enhances textures and salient objects, and aids high-level vision tasks.

Conclusion: RPFNet successfully addresses fusion challenges, improving performance and computational efficiency.

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [47] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: The paper introduces CHDL, a semiconductor wafer dicing dataset, and DIFFUMA, a dual-path prediction model, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: The lack of specialized datasets for high-precision industrial processes like semiconductor manufacturing limits research.

Method: Proposes DIFFUMA, a dual-path architecture combining a Mamba module for temporal context and a diffusion module for spatial detail enhancement.

Result: DIFFUMA reduces MSE by 39% and improves SSIM to 0.988 on CHDL, also generalizing well to natural datasets.

Conclusion: The work provides a SOTA model and a valuable dataset to advance industrial AI research.

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [48] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: Proposes PCA caching and DynCFGCache to improve video generation speed by adaptively reusing outputs based on scene complexity and dynamic CFG reuse, achieving 2.79x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Fixed-frequency caching degrades quality in complex scenes, and manual tuning is inefficient.

Method: PCA caching adjusts reuse thresholds using prompt-derived complexity; DynCFGCache dynamically reuses CFG outputs.

Result: Achieves 2.79x speedup on Wan2.1 model while maintaining visual fidelity.

Conclusion: The approach effectively balances speed and quality in video generation.

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [49] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: A novel local-and-global dual-granularity identity association mechanism improves weakly supervised text-to-person image matching by addressing complex one-to-many identity relationships and enhancing model sensitivity.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to predict complex one-to-many identity relationships, limiting performance in weakly supervised text-to-person image matching.

Method: Proposes local (cross-modal identity relationships within a batch) and global (dynamic cross-modal identity association network) mechanisms, along with confidence-based dynamic adjustment and information-asymmetric sample pair construction.

Result: Significantly improves cross-modal matching accuracy and model robustness.

Conclusion: The method offers an efficient and practical solution for text-to-person image matching, addressing key challenges in weakly supervised learning.

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [50] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: The study develops high-performing OCR systems for the endangered Manchu language using fine-tuned vision-language models, achieving exceptional accuracy on both synthetic and real-world documents.


<details>
  <summary>Details</summary>
Motivation: Manchu, a critically endangered language, lacks effective OCR systems for historical documents, hindering digital humanities research.

Method: Fine-tuned three open-source vision-language models (LLaMA-3.2-11B, Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using parameter-efficient training.

Result: LLaMA-3.2-11B achieved 98.3% word accuracy on synthetic data and 93.1% on real-world handwritten documents, outperforming traditional CRNN approaches.

Conclusion: The work provides a cost-effective, transferable framework for endangered language OCR, enabling historians and linguists to process archives without specialized resources.

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [51] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FOLC-Net is a lightweight federated-optimized framework for MRI disease diagnosis, outperforming SOTA models in multi-view and individual anatomical plane analysis, especially in the sagittal view.


<details>
  <summary>Details</summary>
Motivation: Address performance degradation in SOTA models when processing axial, coronal, and sagittal anatomical planes in MRI diagnosis.

Method: Introduces FOLC-Net with federated-optimized lightweight architecture (1.217M parameters, 0.9MB storage), MRFO for model structure, global model cloning, and ConvNeXt for client adaptability.

Result: Achieves 92.44% accuracy in sagittal view, surpassing DL models (88.37%-88.95%), and improves accuracy across all views.

Conclusion: FOLC-Net provides a robust, adaptable solution for decentralized medical image analysis, addressing SOTA limitations with MRFO, cloning, and ConvNeXt.

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [52] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: A novel pipeline generates synthetic thermal aerial images to expand datasets for deep learning, improving object detection performance in UAV applications.


<details>
  <summary>Details</summary>
Motivation: The lack of diverse thermal aerial datasets hinders deep learning progress in UAV-based thermal imaging for search, rescue, and wildlife monitoring.

Method: A procedural pipeline creates synthetic thermal images by integrating new object classes into existing thermal backgrounds, controlling position, scale, and orientation.

Result: Enhanced datasets (HIT-UAV and MONET) show strong object detection performance, with thermal detectors outperforming visible-light models.

Conclusion: Synthetic thermal data generation effectively expands datasets and improves UAV-based thermal imaging applications, emphasizing the need for aerial viewpoint replication.

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [53] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: GreenHyperSpectra is a pretraining dataset for plant trait prediction using hyperspectral data, addressing label scarcity and domain shifts with semi- and self-supervised methods.


<details>
  <summary>Details</summary>
Motivation: Conventional field sampling lacks scalability for ecologically meaningful spatial scales, necessitating machine learning solutions for trait prediction.

Method: Uses GreenHyperSpectra dataset to pretrain label-efficient multi-output regression models, evaluated in in-distribution and out-of-distribution scenarios.

Result: Outperforms state-of-the-art supervised baselines, improving spectral representation learning for trait prediction.

Conclusion: Provides a methodological framework to advance research in representation learning and plant trait assessment.

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [54] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: A lightweight framework for co-speech gesture video generation uses 2D skeletons and a diffusion model to synthesize realistic, audio-aligned videos, supported by a new public dataset (CSG-405).


<details>
  <summary>Details</summary>
Motivation: The task is challenging due to one-to-many audio-visual mapping, lack of large datasets, and high computational demands.

Method: Uses 2D skeletons and a diffusion model conditioned on audio segments and reference skeletons for motion prediction, combined with an off-the-shelf video generation model.

Result: Outperforms state-of-the-art in visual quality and synchronization, generalizing across speakers and contexts.

Conclusion: The proposed method and dataset (CSG-405) advance co-speech gesture generation, offering high-fidelity results and accessibility.

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [55] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper introduces a new color space (HVI) and a network (HVI-CIDNet+) for low-light image enhancement, addressing color bias and noise artifacts while improving content restoration and color correction.


<details>
  <summary>Details</summary>
Motivation: Existing methods in sRGB and HSV color spaces suffer from color bias, brightness artifacts, and noise issues, prompting the need for a more effective solution.

Method: Proposes the HVI color space to mitigate red and black noise artifacts and introduces HVI-CIDNet+, leveraging pre-trained vision-language models and a Prior-guided Attention Block for content restoration and color correction.

Result: HVI-CIDNet+ outperforms state-of-the-art methods on 10 benchmark datasets, demonstrating superior performance in low-light image enhancement.

Conclusion: The HVI color space and HVI-CIDNet+ effectively address limitations of existing methods, offering improved performance in low-light image enhancement.

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [56] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: A new framework combines symbolic regression and trajectory-guided I2V models to improve physical alignment in video generation by forecasting accurate motion trajectories.


<details>
  <summary>Details</summary>
Motivation: Current video generation models lack physical accuracy due to reliance on statistical correlations instead of physical laws.

Method: The framework extracts motion trajectories, enhances symbolic regression with retrieval-based pre-training, and uses discovered equations to guide video generation.

Result: The method recovers ground-truth equations and improves physical alignment in generated videos for Classical Mechanics scenarios.

Conclusion: The proposed approach advances physics-grounded video forecasting without fine-tuning existing models.

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [57] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: Proposes an end-to-end method using Vision Transformer (ViT) attention maps for Weakly Supervised Semantic Segmentation (WSSS), outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of WSSS by leveraging ViT's self-attention maps for accurate pseudo-mask generation, reducing reliance on external modules and fine-grained labeled data.

Method: Trains a sparse ViT with multiple [CLS] tokens (one per class) using random masking. Aggregates self-attention maps of each [CLS] token at inference to generate pseudo-masks.

Result: Outperforms related works on standard benchmarks and specialized datasets, generating accurate pseudo-masks for training segmentation models.

Conclusion: The method enhances interpretability of self-attention maps and achieves results comparable to fully-supervised models, significantly reducing labeled data dependency.

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [58] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: IAP is a new attack framework for generating invisible adversarial patches, balancing model susceptibility and human perception, outperforming existing methods in stealth and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Prior adversarial patch methods lack stealth and context coherence, making them detectable by humans or automated defenses.

Method: IAP uses perceptibility-aware localization and perturbation optimization, leveraging classwise localization, sensitivity maps, and perceptibility-regularized adversarial loss.

Result: IAP achieves high attack success rates with improved invisibility, outperforming baselines and evading state-of-the-art defenses.

Conclusion: IAP is a highly effective and stealthy adversarial patch framework, advancing the field of adversarial attacks.

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [59] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Bl√ºmel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: Long-term biometric evaluations show daily fluctuations in comparison scores, emphasizing the need for extended testing in controlled environments.


<details>
  <summary>Details</summary>
Motivation: To understand the variability of biometric data over time and its implications for biometric systems.

Method: Conducted long-term evaluations with 400+ participants using diverse biometric tools and state-of-the-art face recognition algorithms.

Result: Comparison scores fluctuate more between days than over the entire period.

Conclusion: Extended testing in controlled environments is crucial for accurate biometric analysis and future advancements.

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [60] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: SemRaFiner improves radar-based panoptic segmentation by optimizing feature extraction and training procedures, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Enhancing semantic scene understanding for autonomous vehicles, especially in adverse weather, where cameras and LiDARs face limitations.

Method: Proposes SemRaFiner, which optimizes feature extraction for sparse radar point clouds and refines training with dedicated data augmentation.

Result: Outperforms existing radar-based panoptic segmentation methods.

Conclusion: SemRaFiner effectively addresses the challenges of sparse radar data, improving scene understanding for autonomous driving.

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [61] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: APL introduces adaptive part discovery and learning to improve GCD by leveraging DINO part priors and a novel contrastive loss for better discriminability and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods rely on global representations, causing a trade-off between discriminability and generalization. APL addresses this by focusing on object parts.

Method: APL uses shared learnable part queries and DINO part priors to discover consistent object parts. It employs an all-min contrastive loss for part representation learning.

Result: APL enhances GCD frameworks by replacing CLS token features with part representations, showing significant improvements on fine-grained datasets.

Conclusion: APL effectively balances discriminability and generalization in GCD by leveraging part-based learning and adaptive contrastive loss.

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [62] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper introduces MCCD, a multi-attribute Chinese calligraphy dataset, addressing the scarcity of detailed calligraphy datasets and enabling diverse research tasks.


<details>
  <summary>Details</summary>
Motivation: Existing calligraphy datasets lack attribute information, hindering in-depth study. The evolution of calligraphy styles and scarcity of annotated data pose challenges.

Method: A novel dataset (MCCD) with 7,765 categories and 329,715 samples is created, annotated with script styles, dynasties, and calligraphers. Benchmark experiments are conducted.

Result: Recognition is challenging due to stroke complexity and attribute interplay. MCCD fills a gap and supports diverse research tasks.

Conclusion: MCCD advances Chinese calligraphy research and related fields, providing a valuable resource for future studies.

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [63] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,C√©sar Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hern√°ndez,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: A deep learning model identifies palm clusters from satellite imagery to estimate ancient human management areas, revealing larger impacts than previously thought.


<details>
  <summary>Details</summary>
Motivation: To understand long-term effects of ancient human management on Neotropical forests at high-resolution scales.

Method: Combines deep learning (for palm identification) and clustering algorithms (for palm clusters) on satellite imagery, applied to 765 km¬≤ in Colombia.

Result: Palms were significantly more abundant near archaeological sites, suggesting ancient management areas may be much larger than archaeological evidence indicates.

Conclusion: Pre-Columbian populations influenced vegetation, leaving lasting ecological footprints, and AI can reveal fine-scale human-environment interactions.

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [64] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: CheXPO, a strategy for optimizing vision-language models in medical applications, reduces hallucinations by combining confidence-similarity mining and counterfactual rationales, achieving significant performance gains with minimal data.


<details>
  <summary>Details</summary>
Motivation: Vision-language models (VLMs) suffer from hallucinations in medical contexts, and existing preference optimization methods face challenges like irrelevant samples, imbalanced data, and high expert costs.

Method: CheXPO synthesizes a multi-task chest X-ray dataset for supervised fine-tuning, identifies hard examples via token-level confidence analysis, and uses similarity-based retrieval and synthetic counterfactual rationales to balance preferences.

Result: CheXPO achieves an 8.93% relative performance gain using only 5% of SFT samples, reaching state-of-the-art performance in clinical tasks.

Conclusion: CheXPO offers a scalable, interpretable solution for reducing hallucinations in VLMs for radiology, minimizing reliance on expert annotations.

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [65] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: A deep learning method (ProRSeg) was developed for domain-invariant MR-MR registration in prostate cancer MRgART, showing strong cross-domain performance and feasibility for dose accumulation.


<details>
  <summary>Details</summary>
Motivation: Accurate deformable image registration (DIR) is needed for contour propagation and dose accumulation in MR-guided adaptive radiotherapy (MRgART).

Method: ProRSeg was trained with 262 pairs of 3T MR scans using weighted segmentation consistency loss and tested on same-, cross-, and mixed-domain datasets for contour propagation and dose accumulation.

Result: ProRSeg generalized well for bladder (DSCs ~0.87) but showed domain-dependent performance for rectum and CTV. Dose accumulation met CTV coverage for 83.3% of patients.

Conclusion: ProRSeg demonstrated reasonable multi-domain registration performance and preliminary feasibility for evaluating treatment compliance.

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [66] [Hallucinating 360¬∞: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: Percep360 is a novel panoramic generation method for autonomous driving, focusing on coherence and controllability. It uses LSDM for continuous diffusion and PPM for dynamic control, improving image quality and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires comprehensive 360¬∞ views, but current data acquisition is complex and limited. Existing models lack controllability and coherence.

Method: Proposes Local Scenes Diffusion Method (LSDM) for continuous panorama generation and Probabilistic Prompting Method (PPM) for controllability.

Result: Generated images outperform stitched ones in quality metrics and enhance BEV segmentation.

Conclusion: Percep360 advances panoramic generation for autonomous driving with improved coherence, controllability, and downstream utility.

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [67] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttop√§√§,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: The paper introduces the MassID45 dataset, combining DNA barcoding and high-resolution imaging for automatic classification of bulk insect samples, aiding large-scale ecological studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of classifying insects in bulk samples, which are common in ecological surveys, by integrating molecular and imaging data.

Method: Human annotators, aided by AI, segmented and labeled over 17,000 specimens in bulk images, combining DNA barcodes with imaging data.

Result: The MassID45 dataset enables precise taxonomic classification and abundance estimation for bulk insect samples.

Conclusion: This dataset advances tiny object detection and segmentation, benefiting ecological and machine learning research.

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [68] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: FreeTTA is a training-free test-time adaptation method for Vision-Language Models (VLMs) that enhances predictions by modeling test data distribution without relying on historical data or costly processes.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of VLMs in handling domain shifts and distributional changes during test time, FreeTTA aims to improve flexibility and performance without unrealistic assumptions or costly training.

Method: FreeTTA uses an online EM algorithm, leveraging zero-shot predictions from VLMs as priors to iteratively update posterior probabilities and parameters for test samples.

Result: FreeTTA outperforms state-of-the-art methods across 15 datasets in cross-domain and out-of-distribution settings, demonstrating stable and significant improvements.

Conclusion: FreeTTA provides a practical and effective solution for test-time adaptation in VLMs, eliminating reliance on training data or annotations while enhancing prediction accuracy.

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [69] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,J√∂rg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: A study introduces DenoiseCP-Net, a multi-task architecture for LiDAR-based collective perception in adverse weather, improving denoising, reducing bandwidth, and maintaining detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Automated vehicles' perception systems are vulnerable to adverse weather. Collective perception can help, but its performance in such conditions is understudied.

Method: Proposed DenoiseCP-Net, integrating voxel-level noise filtering and object detection into a unified sparse convolution backbone. Tested on an extended OPV2V dataset with simulated adverse weather.

Result: Achieves near-perfect denoising, reduces bandwidth by 23.6%, maintains detection accuracy, and lowers inference latency.

Conclusion: DenoiseCP-Net effectively addresses adverse weather challenges in collective perception, enhancing performance and efficiency.

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [70] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: MCA-RG is a knowledge-driven framework for radiology report generation that aligns visual features with medical concepts, improving accuracy and clinical relevance.


<details>
  <summary>Details</summary>
Motivation: Challenges in mapping pathological/anatomical features to text descriptions and semantic agnostic feature extraction hinder clinical adoption of LLMs for RRG.

Method: MCA-RG uses pathology and anatomy concept banks, aligns visual features with medical concepts, employs contrastive learning for anatomy, and a matching loss for pathology. A feature gating mechanism filters low-quality features.

Result: MCA-RG outperforms benchmarks on MIMIC-CXR and CheXpert Plus, demonstrating superior performance in radiology report generation.

Conclusion: MCA-RG effectively enhances radiology report generation by aligning visual features with medical concepts, addressing key clinical adoption challenges.

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [71] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: A novel framework for multi-modal feature fusion improves NSCLC survival prediction by integrating 3D CT images and clinical data using a cross-modality masked learning approach.


<details>
  <summary>Details</summary>
Motivation: Accurate prognosis for NSCLC patients undergoing immunotherapy is crucial for personalized treatment, but limited datasets and ineffective feature fusion hinder progress.

Method: A large-scale dataset with 3D CT images and clinical records is introduced. A cross-modality masked learning framework, featuring a Slice-Depth Transformer for CT images and a graph-based Transformer for clinical data, is proposed.

Result: The framework outperforms existing methods, enhancing multi-modal integration and setting a new benchmark for NSCLC survival prediction.

Conclusion: The proposed approach advances prognostic models for NSCLC by effectively fusing multi-modal data, improving prediction accuracy and patient outcomes.

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [72] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: The paper introduces the Deliberate-to-Intuitive (D2I) framework to enhance multimodal reasoning in LLMs without extra annotations or complex rewards, improving performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the high training costs and scalability issues in multimodal reasoning research caused by additional data annotations and rule-based rewards.

Method: Proposes D2I, which uses deliberate reasoning strategies with rule-based format rewards during training and shifts to intuitive reasoning during evaluation.

Result: D2I outperforms baselines in both in-domain and out-of-domain benchmarks, demonstrating improved reasoning skills.

Conclusion: The framework highlights the effectiveness of format rewards for transferable reasoning skills and suggests decoupling training-time reasoning depth from test-time flexibility.

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [73] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: A novel GNN-ViTCap framework improves WSI classification and captioning by addressing redundant patches and unknown positions, achieving high performance metrics.


<details>
  <summary>Details</summary>
Motivation: Challenges in WSI classification and captioning, like redundant patches and subjective captures, hinder accurate cancer diagnosis.

Method: Uses a GNN-ViTCap framework: patch embeddings, deep clustering for redundancy removal, GNN for context, and fine-tuning a language model.

Result: Achieves F1=0.934, AUC=0.963 for classification; BLEU-4=0.811, METEOR=0.569 for captioning, outperforming state-of-the-art.

Conclusion: GNN-ViTCap provides a reliable, efficient solution for microscopy-based diagnosis, advancing computer-aided pathology.

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [74] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: A lightweight method predicts cellular composition from H&E-stained histology images using pre-trained pathology foundation models, avoiding costly spatial transcriptomics.


<details>
  <summary>Details</summary>
Motivation: To leverage pathology foundation models for predicting cell-type compositions from histology images without expensive spatial transcriptomics.

Method: Uses pre-trained pathology foundation models to extract features, then trains a lightweight MLP regressor on cell-type abundances derived via cell2location.

Result: Competitive performance compared to existing methods like Hist2Cell, with reduced computational complexity.

Conclusion: The approach efficiently predicts cell-type compositions from histology images, offering a cost-effective alternative to spatial transcriptomics.

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [75] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: MST-Distill is a novel cross-modal knowledge distillation framework using a mixture of specialized teachers and adaptive routing to address distillation path selection and knowledge drift, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional distillation methods struggle in cross-modal settings due to data and statistical heterogeneities, failing to leverage complementary prior knowledge from cross-modal teacher models.

Method: Proposes MST-Distill, featuring an ensemble of cross-modal and multimodal teacher models, an instance-level routing network, and a plug-in masking module to suppress discrepancies and reconstruct representations.

Result: Extensive experiments on five multimodal datasets show MST-Distill significantly outperforms state-of-the-art knowledge distillation methods.

Conclusion: MST-Distill effectively addresses limitations of traditional methods, enhancing cross-modal knowledge transfer with dynamic and adaptive distillation.

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [76] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: An OCR-powered pipeline for efficient table extraction from invoices, using Tesseract OCR and custom post-processing to improve accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extracting structured tabular data from noisy and non-standard invoice formats for automated financial workflows and digital archiving.

Method: Leverages Tesseract OCR for text recognition, dynamic preprocessing, table boundary detection, and row-column mapping.

Result: Significantly improves data extraction accuracy and consistency.

Conclusion: The pipeline is effective for real-world use cases like automated financial workflows and digital archiving.

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [77] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: The paper explores how contextual metadata (location, meal type, food items) enhances Large Multimodal Models (LMMs) for nutrition analysis, introduces the ACETADA dataset, and shows improved accuracy with metadata integration.


<details>
  <summary>Details</summary>
Motivation: Existing work focuses on proprietary models, leaving open LMMs underexplored. The impact of contextual metadata and reasoning modifiers on nutrition analysis is unclear.

Method: Investigates metadata (GPS, timestamps, food items) integration with LMMs for nutrition estimation. Uses the ACETADA dataset and evaluates eight LMMs with reasoning modifiers.

Result: Metadata integration reduces Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in nutritional predictions.

Conclusion: Context-aware LMMs show promise for enhanced nutrition analysis, with metadata and reasoning modifiers improving accuracy.

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [78] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: A geometric deep learning framework predicts the Laplace-Beltrami (LB) spectrum efficiently from CAD meshes, reducing computation time by 5x compared to FEM without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: The FEM method for LB spectrum estimation is inefficient for large CAD databases or frequent quality control tasks, necessitating a faster solution.

Method: A Graph Neural Network (GNN) architecture leverages mesh features like Gaussian curvature, mean curvature, and principal curvatures to predict the LB spectrum.

Result: The method achieves a 5x speedup over FEM while maintaining competitive accuracy.

Conclusion: The LB spectrum is learnable, and the proposed GNN framework offers a practical solution for efficient LB spectrum estimation in real-world applications.

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [79] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet is a deep learning framework for robustly inferring real-world scale from images by treating ruler reading as a keypoint-detection problem, using geometric-progression parameters for invariance to perspective, and leveraging synthetic data for training.


<details>
  <summary>Details</summary>
Motivation: Accurate pixel-to-real-world scale conversion is critical for applications like biomedicine, forensics, and e-commerce, but remains a challenge due to diverse ruler types and imaging conditions.

Method: RulerNet reformulates ruler reading as a keypoint-detection problem with geometric-progression parameters for perspective invariance. It uses a synthetic-data pipeline with ControlNet for photorealistic context and introduces DeepGP for efficient parameter regression.

Result: RulerNet achieves accurate, consistent, and efficient scale estimation under real-world conditions, demonstrating strong generalization across ruler types and imaging scenarios.

Conclusion: RulerNet is a generalizable, scalable solution for real-world measurement tasks, with potential for integration into automated vision systems.

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [80] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: The paper introduces L-VQAScore, a novel metric combining visual localization and VQA to better evaluate Text-to-Image models, outperforming existing methods in capturing fine-grained entity-attribute associations.


<details>
  <summary>Details</summary>
Motivation: Current T2I evaluation methods struggle with assessing complex entity-attribute semantics, particularly attribute confusion.

Method: Proposes L-VQAScore, leveraging VQA localization to target single entities and measure correct and miss-localized attributes.

Result: L-VQAScore outperforms state-of-the-art methods in correlating with human judgments on a challenging dataset.

Conclusion: L-VQAScore is a reliable, scalable alternative to subjective evaluations for fine-grained T2I assessment.

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [81] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: The paper introduces MotionMillion, the largest human motion dataset, and a benchmark for zero-shot text-to-motion generation, achieving strong generalization.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in zero-shot generalization and lack of evaluation frameworks in text-to-motion generation.

Method: Developed an annotation pipeline, created MotionMillion dataset, proposed MotionMillion-Eval benchmark, and scaled a model to 7B parameters.

Result: Demonstrated strong generalization to out-of-domain and complex motions.

Conclusion: A significant step toward zero-shot human motion generation, with code publicly available.

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [82] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: The paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, leveraging pretrained models to reduce data and cost requirements for state-of-the-art captioning.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and data demands of training Vision-Language Models (VLMs) for captioning.

Method: Uses a VLV auto-encoder with frozen pretrained components (vision encoder, T2I diffusion decoder, LLM) and fine-tunes an LLM for captioning.

Result: Achieves SoTA captioning comparable to GPT-4o and Gemini 2.0 Flash, with training costs under $1,000 USD.

Conclusion: The VLV framework is cost-efficient and reduces reliance on massive paired datasets, offering a scalable solution for VLM training.

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [83] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent is a unified system for upscaling any image to 4K resolution, handling severe degradations and enhancing facial details, outperforming benchmarks across diverse domains.


<details>
  <summary>Details</summary>
Motivation: To create a versatile, high-performance agentic system for super-resolution tasks, addressing diverse imaging needs with a unified approach.

Method: Combines Profiling, Perception Agent (vision-language models and quality assessment), and Restoration Agent (recursive execution-reflection with expert policy).

Result: Achieves state-of-the-art performance across 26 benchmarks in 11 categories, excelling in perceptual and fidelity metrics.

Conclusion: 4KAgent introduces a novel agentic paradigm for low-level vision, encouraging innovation in vision-centric autonomous agents.

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [84] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper explores using pre-trained text-to-image diffusion models as visual encoders for MLLMs, addressing CLIP's limitations in fine-grained detail capture. It identifies diffusion features' semantic richness and alignment potential, proposes a fusion strategy with CLIP, and mitigates leakage issues, showing improved performance in vision-centric tasks.


<details>
  <summary>Details</summary>
Motivation: CLIP's inability to capture fine-grained details in images for MLLMs motivates the exploration of diffusion models as alternative visual encoders.

Method: Analyzes diffusion models' internal representations, leverages text conditioning for focus, aligns features with LLMs, and proposes a fusion strategy combining CLIP and diffusion features.

Result: Demonstrates diffusion models' effectiveness in visual understanding, especially for tasks requiring spatial and compositional reasoning, outperforming CLIP in benchmarks.

Conclusion: Diffusion models show promise as instruction-aware visual encoders, offering richer semantics and better alignment for MLLMs, with potential for vision-centric applications.

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>
