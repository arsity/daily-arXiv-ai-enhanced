<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: A deep learning-based real-time smoking detection system using CCTV surveillance for fire exit areas, achieving high recall and mAP with optimized YOLOv8 architecture.


<details>
  <summary>Details</summary>
Motivation: Critical safety requirements in fire exit areas necessitate real-time smoking detection to prevent fire hazards and ensure public safety through automatic regulatory compliance.

Method: Evaluated YOLOv8, YOLOv11, and YOLOv12 models, then developed a custom YOLOv8-based model with added structures for challenging surveillance contexts. Used dataset of 8,124 images from 20 scenarios with 2,708 low-light samples. Tested on edge devices with multithreaded operations.

Result: Proposed model achieved 78.90% recall and 83.70% mAP at 50, outperforming other models. Jetson Xavier NX processed data at 52-97ms per inference, suitable for real-time operations.

Conclusion: The system provides a robust and adaptable platform for real-time smoking detection in surveillance contexts, effectively addressing public safety needs in fire exit areas.

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: Procedural data-trained representation models achieve near-real performance on visual tasks using visual memory databases without further training, with performance gaps explained by object part representation dissimilarities.


<details>
  <summary>Details</summary>
Motivation: To achieve full compartmentalization from real-world images while maintaining strong performance on visual tasks by using procedural data only and visual memory databases.

Method: Train representation models exclusively on procedural data, then apply them to visual tasks using visual memory - an explicit database of reference image embeddings without additional training.

Result: Within 1% of Places-trained model on NIGHTS visual similarity, outperforms by 8-15% on fine-grained classification (CUB200, Flowers102), within 10% on ImageNet-1K classification, and strong zero-shot segmentation on COCO (R² within 10% of real-data models).

Conclusion: Procedural models can achieve competitive performance to real-data models, with remaining gaps attributed to dissimilar representations of object parts in procedural models causing incorrect memory searches.

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: Systematic evaluation of four ophthalmic foundation models (RETFound, VisionFM, RetiZero, DINORET) shows DINORET and RetiZero perform best, with RetiZero having better generalization. Gating-based fusion provides modest improvements for some diseases, but systemic disease prediction remains challenging.


<details>
  <summary>Details</summary>
Motivation: Foundation models show promise in medical imaging but there's no clear understanding of which ophthalmic FMs perform best, their task-specific performance, or the benefits of combining multiple FMs together.

Method: Proposed FusionFM evaluation suite with two fusion approaches to integrate different ophthalmic FMs. Evaluated four state-of-the-art FMs on standardized datasets from multiple countries using AUC and F1 metrics for both ophthalmic disease detection and systemic disease prediction.

Result: DINORET and RetiZero achieved superior performance in both ophthalmic and systemic disease tasks. RetiZero showed stronger generalization on external datasets. Gating-based fusion provided modest improvements for glaucoma, AMD, and hypertension prediction.

Conclusion: The study provides evidence-based evaluation of ophthalmic FMs, demonstrates benefits of model fusion, and identifies strategies for enhancing clinical applicability, though systemic disease prediction (especially hypertension) remains challenging.

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF is a unified deep learning framework that reconstructs multiple dentocraniofacial hard tissues using multimodal fusion of point clouds and multi-view images, achieving superior geometric precision and clinical efficiency.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models are limited to single-tissue scenarios and modality-specific inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability for dentocraniofacial reconstruction.

Method: UniDCF uses multimodal fusion encoding of point clouds and multi-view images, leveraging complementary strengths of each modality with a score-based denoising module to refine surface smoothness. The framework was trained on the largest multimodal dataset with 54,555 annotated instances from 6,609 patients.

Result: UniDCF outperforms state-of-the-art methods in geometric precision, structural completeness, and spatial accuracy. It reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%.

Conclusion: UniDCF enables rapid, automated, high-fidelity reconstruction of multiple dentocraniofacial tissues, supporting personalized restorative treatments, streamlining clinical workflows, and enhancing patient outcomes.

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5 is an advanced multimodal model featuring native-resolution vision processing and reflection-based reasoning, achieving state-of-the-art performance in the sub-40B parameter range with significant improvements over its predecessor.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed-resolution image processing that degrades fine details and global layout, particularly for visually dense content like complex charts, and to enhance reasoning capabilities beyond linear chain-of-thought approaches.

Method: Integrates native-resolution vision transformer, reflection-based reasoning with self-checking and revision, trained via five-phase curriculum (visual/multimodal pretraining, instruction tuning, DPO/GRPO alignment), using multimodal data packing and hybrid parallelism for efficiency.

Result: Ovis2.5-9B scores 78.3 on OpenCompass leaderboard (substantial improvement over Ovis2-8B), Ovis2.5-2B scores 73.9 (SOTA for its size), achieves leading results on STEM benchmarks, strong grounding/video capabilities, and SOTA for complex chart analysis.

Conclusion: Ovis2.5 successfully combines native-resolution visual perception with advanced reasoning capabilities, establishing new state-of-the-art performance for open-source MLLMs in sub-40B parameter range while maintaining efficiency for resource-constrained applications.

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE is the first publicly available video-to-text e-commerce attribute value extraction dataset covering 14 domains and 172 attributes, with 224k training and 25k evaluation samples filtered by CLIP-MoE system.


<details>
  <summary>Details</summary>
Motivation: Existing AVE datasets lack support for product videos, diverse attribute coverage, and public availability, creating a gap in video-based e-commerce product structuring.

Method: Created VideoAVE dataset with CLIP-based Mixture of Experts filtering to remove mismatched video-product pairs, then benchmarked state-of-the-art video VLMs on attribute-conditioned value prediction and open attribute-value extraction tasks.

Result: Video-to-text AVE remains challenging, especially in open settings, with current VLMs showing room for improvement in leveraging temporal information effectively.

Conclusion: VideoAVE provides a valuable benchmark for advancing video-based attribute value extraction, highlighting the need for more sophisticated video VLMs that can better utilize temporal cues for e-commerce applications.

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: Curvature-based geometric features alone enable MLP to achieve high accuracy (97% on MNIST, 89% on EMNIST) for handwritten character recognition, showing deep learning benefits can be achieved with interpretable handcrafted features.


<details>
  <summary>Details</summary>
Motivation: To investigate whether second-order geometric cues (curvature magnitude, sign, and gradient orientation) are sufficient on their own to drive effective handwritten character recognition, providing an alternative to convolutional neural networks.

Method: Using three handcrafted feature maps (planar curvature magnitude, curvature sign, and gradient orientation) as inputs to a multilayer perceptron (MLP) classifier for handwritten character recognition tasks.

Result: The curvature-orientation MLP achieved 97% accuracy on MNIST digits and 89% accuracy on EMNIST letters, demonstrating strong performance with interpretable features.

Conclusion: Curvature-based representations have strong discriminative power for handwritten character images, and the advantages of deep learning can be realized even with interpretable, hand-engineered features rather than complex CNN architectures.

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: A dual approach combining prompt optimization and multimodal data augmentation improves hateful meme detection by enhancing model robustness and reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: Modern web has increasing multimodal hate content disguised as humor, and current Vision-Language Models lack fine-grained supervision and are vulnerable to implicit hate speech.

Method: 1) Prompt optimization framework varying structure, supervision granularity, and training modality; 2) Multimodal data augmentation pipeline generating 2,479 counterfactually neutral memes using multi-agent LLM-VLM setup.

Result: Structured prompts improve robustness even in small models, InternVL2 achieves best F1-scores, and augmentation pipeline successfully reduces spurious correlations and improves classifier generalization.

Conclusion: Prompt structure and data composition are as critical as model size, and targeted augmentation enables more trustworthy and context-sensitive hate detection.

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: The paper investigates Gaussian curvature as a geometric prior for 3D surface reconstruction, showing it provides compact surface descriptions and can improve stereo and monocular depth methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for 3D vision lack explicit geometric models that can be analyzed, transferred, or systematically modified. The authors want to explore Gaussian curvature as an invariant geometric quantity that could address these limitations.

Method: The researchers investigate Gaussian curvature's role in 3D surface modeling using the Middlebury stereo dataset. They analyze how it provides sparse surface descriptions and examine its implicit use in state-of-the-art methods.

Result: Gaussian curvature offers: (i) compact 3D surface descriptions, (ii) appears to be implicitly considered by current methods, (iii) serves as a geometric prior that can improve reconstruction, and (iv) can function as an unsupervised metric for stereo evaluation.

Conclusion: Gaussian curvature represents a valuable geometric invariant that can enhance 3D reconstruction methods by providing explicit, analyzable geometric models that current data-driven approaches lack.

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [10] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: Systematic evaluation of image-to-graph transformation methods for graph-level anomaly detection in dermoscopic images, comparing segmentation schemes, edge strategies, and feature descriptors.


<details>
  <summary>Details</summary>
Motivation: No previous study has rigorously compared the numerous potential image-to-graph transformation approaches for GNN-based graph-level anomaly detection, despite GNNs being powerful for graph-based ML tasks.

Method: Systematically evaluated multiple segmentation schemes, edge construction strategies, and node feature sets (color, texture, shape descriptors) using state-of-the-art GLAD models on dermoscopic images across unsupervised, weakly supervised, and fully supervised regimes.

Result: Color descriptors provided best standalone performance, while combining shape and texture features consistently enhanced detection. Best unsupervised configuration achieved AUC-ROC of 0.805, increasing to 0.872 with sparse labels and 0.914 with full supervision.

Conclusion: Comprehensive image-to-graph transformation evaluation provides effective graph representations for anomaly detection, with feature combinations outperforming single descriptors and achieving competitive results without pretrained backbones.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [11] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: A comprehensive review of Transformer-based models in UAV systems, covering architectures, applications, benchmarks, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of Transformer architectures has significantly impacted UAV technology, but there's a need for systematic categorization and evaluation of these developments to guide researchers and practitioners.

Method: Systematic categorization and evaluation of Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and LLMs, with comparative analyses through structured tables and performance benchmarks.

Result: Presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications in precision agriculture and autonomous navigation, reviews key datasets and evaluation metrics, and identifies existing gaps in literature.

Conclusion: This comprehensive synthesis provides guidance for understanding and advancing Transformer-driven UAV technologies while outlining critical challenges in computational efficiency and real-time deployment for future research.

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [12] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat is the first black-box attack that exploits 3D Gaussian Splatting shading methods to create viewpoint-specific adversarial camouflage visible only from specific angles, successfully attacking various object detectors without requiring model access.


<details>
  <summary>Details</summary>
Motivation: As 3DGS gains adoption in safety-critical tasks like autonomous navigation, there's a need to understand how adversaries might tamper with images to cause harm through novel attack vectors.

Method: Exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle to embed adversarial content visible only from specific viewpoints, without requiring access to model architecture or weights.

Result: Successfully attacks a variety of popular detectors including single-stage, multi-stage, and transformer-based models on both real-world physical object captures and synthetic scenes.

Conclusion: This exposes a novel safety risk for applications like autonomous navigation and mission-critical robotic systems, demonstrating the first black-box attack on downstream object detectors using 3DGS.

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [13] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: Prostate MRI foundation model's label efficiency depends on image quality distribution alignment between finetuning and test sets. Performance varies significantly with quality mismatch, requiring sufficient high-quality data for optimal results.


<details>
  <summary>Details</summary>
Motivation: To evaluate how variable image quality affects label-efficient finetuning of medical imaging foundation models, specifically investigating the impact of quality distribution mismatches between finetuning and deployment datasets.

Method: Used ProFound (domain-specific vision foundation model pretrained on large-scale prostate MRI datasets) and systematically varied high-/low-quality image ratios in both finetuning and evaluation sets to measure generalizability.

Result: Image quality distribution and finetune-test mismatch significantly affect performance. Consistent quality ratios enable far less labeled data than training from scratch, but without enough high-quality finetuning data, pretrained models may fail to outperform non-pretrained ones.

Conclusion: Assessing and aligning quality distributions between finetuning and deployment is crucial, highlighting the need for quality standards in finetuning data to fully realize foundation models' efficiency benefits.

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [14] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing: A tensor ring decomposition framework for ultra-light parameter-efficient VLM adaptation that reduces training parameters by 90% while achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-based fine-tuning methods have limited compression rates due to cross-layer redundancy and limited representational capacity across homogeneous adapters.

Method: Cross-layer tensor ring decomposition to formulate adapters as layer-shared tensor cores and layer-specific slices, with diverse rank-driven adapters guided by generalization-aware fine-tuning.

Result: Achieves state-of-the-art performance while reducing average training parameters by 90% across various tasks.

Conclusion: AdaRing provides an effective solution for ultra-light parameter-efficient adaptation of vision-language models by addressing cross-layer redundancy and enhancing representational diversity.

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [15] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: EVTP-IV is a visual token pruning method that achieves 3.5-5X speedup for Instructed Visual Segmentation while maintaining accuracy using only 20% of tokens.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have strong performance on Instructed Visual Segmentation but suffer from high inference costs, especially in video applications.

Method: A k-center based token pruning method that selects spatially representative token subsets by integrating spatial information to ensure better coverage.

Result: Achieves up to 5X speed-up on video tasks and 3.5X on image tasks while maintaining comparable accuracy using only 20% of tokens. Outperforms state-of-the-art pruning baselines.

Conclusion: The method effectively reduces inference costs for IVS tasks while preserving segmentation performance through intelligent visual token selection.

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [16] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN is a pure CNN-based image super-resolution model that uses large kernel modulation to achieve better performance than Transformers while maintaining fast inference speed.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between CNNs (fast but limited non-local feature capture) and Transformers (good non-local modeling but slow inference) in resource-constrained super-resolution scenarios.

Method: Proposes Large Kernel Modulation Network with Enhanced Partial Large Kernel Block (channel shuffle, attention, and large kernel convolutions) and Cross-Gate Feed-Forward Network (dynamic feature adjustment and fusion).

Result: Outperforms state-of-the-art lightweight SR models, achieving 0.23 dB PSNR improvement over DAT-light on Manga109 dataset with 4.8x faster inference at 4x upscale.

Conclusion: LKMN successfully balances quality and efficiency in image super-resolution, demonstrating that pure CNN architectures can achieve superior non-local feature capture while maintaining fast inference speeds.

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: Using only horizontal and vertical Sobel derivatives as input, an MLP achieves near-CNN performance on handwritten character recognition with smaller memory footprint and transparent features.


<details>
  <summary>Details</summary>
Motivation: To investigate whether first-order edge maps (Sobel derivatives) are sufficient for handwritten character recognition as an alternative to convolutional neural networks, exploring simpler and more interpretable models.

Method: Train a multilayer perceptron (MLP) using only horizontal and vertical Sobel derivatives as input features on MNIST and EMNIST Letters datasets.

Result: The MLP achieved 98% accuracy on MNIST digits and 92% on EMNIST letters, approaching CNN performance while offering smaller memory footprint and more transparent features.

Conclusion: First-order gradients capture most class-discriminative information in handwritten characters, making edge-aware MLPs a compelling alternative to CNNs for HCR tasks.

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: Proposes OVG-HQ, a new online video grounding task with hybrid-modal queries (text, images, video segments), and introduces OVG-HQ-Unify framework with Parametric Memory Block and cross-modal distillation to handle modality imbalance and limited context in online settings.


<details>
  <summary>Details</summary>
Motivation: Traditional video grounding struggles with streaming video scenarios and queries using visual cues, creating a gap for online processing with multiple input modalities.

Method: OVG-HQ-Unify framework with Parametric Memory Block (PMB) to retain learned knowledge and cross-modal distillation to guide learning of non-dominant modalities. Constructed QVHighlights-Unify dataset and adapted online evaluation metrics.

Result: Experiments show OVG-HQ-Unify outperforms existing models, providing robust solution for online hybrid-modal video grounding with both accuracy and efficiency.

Conclusion: The proposed framework effectively addresses challenges of limited context in online settings and modality imbalance, enabling single model to handle diverse hybrid-modal queries in real-time video grounding applications.

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl is a lightweight plugin that localizes unsafe content in text-to-image generation and suppresses harmful semantics using DPO training, achieving better safety and fidelity than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods for text-to-image models create trade-offs between safety and fidelity, and concept replacement approaches can cause semantic incongruity. A more flexible detect-then-suppress approach is needed.

Method: SafeCtrl uses a novel training strategy with Direct Preference Optimization (DPO) on image-level preference data to learn suppression behaviors. It precisely localizes unsafe content and suppresses harmful semantics rather than hard substitution.

Result: Extensive experiments show SafeCtrl significantly outperforms state-of-the-art methods in both safety efficacy and fidelity preservation.

Conclusion: Decoupled, suppression-based control is an effective and scalable direction for building more responsible generative models without requiring costly pixel-level annotations.

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP is a lightweight vision-language framework that uses single pixel temporal-spectral data from Sentinel-2 imagery for land-use classification, eliminating need for large spatial tiles and text supervision.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for remote sensing rely on large spatial tiles (computationally expensive) and text-based supervision (often unavailable), limiting scalability for large-scale applications.

Method: Leverages spectral and temporal information from single Sentinel-2 pixels combined with cross-view learning using geo-tagged ground-level photos from LUCAS and Sen4Map datasets, minimizing caption-based training requirements.

Result: Demonstrates that single pixel inputs with temporal and spectral cues are sufficient for thematic mapping tasks including LULC, crop type, and ecosystem type classification.

Conclusion: Provides a scalable and efficient alternative to traditional approaches, enabling large-scale remote sensing applications with reduced computational costs and minimal text supervision requirements.

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: Synthetic MRI data from GANs can improve brain tumor segmentation boundary delineation when combined with real data (40% real + 60% synthetic optimal), but class imbalance issues persist for tumor core regions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in brain tumor segmentation including tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets by using synthetic data to improve dataset diversity.

Method: Used U-Net segmentation network trained on: 1) real data from BraTS 2020 dataset, 2) synthetic data generated with medigan library GAN, and 3) hybrid datasets with varying proportions of real and synthetic samples.

Result: Quantitative performance (Dice, IoU, precision, recall, accuracy) was comparable between real-only and hybrid models. Qualitative analysis showed hybrid datasets (40% real + 60% synthetic) improved whole tumor boundary delineation, but tumor core and enhancing tumor regions still had lower accuracy due to persistent class imbalance.

Conclusion: Synthetic data is feasible for brain tumor segmentation augmentation, but future work needs larger-scale experiments, volumetric data consistency, and better class imbalance mitigation strategies.

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: This paper provides the first comprehensive survey of deep learning-based point cloud denoising methods, categorizing them into outlier removal and surface noise restoration, and discusses challenges, contributions, and future directions.


<details>
  <summary>Details</summary>
Motivation: Real-world point clouds contain various noise types that degrade downstream task performance. Despite deep learning models outperforming traditional methods, no systematic survey exists to summarize developments in DL-based point cloud denoising.

Method: The authors formulate point cloud denoising as a two-step process (outlier removal and surface noise restoration), create a taxonomy tailored to denoising tasks, and systematically compare existing methods based on similarities, differences, and advantages.

Result: The survey identifies key challenges in DL-based point cloud denoising, summarizes main contributions of existing methods, and provides a comprehensive framework for understanding the field.

Conclusion: The paper fills a critical gap by offering the first systematic survey of DL-based point cloud denoising, providing valuable insights for future research directions and advancements in the field.

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose is a retraining-free 6D pose tracking framework that handles fast-moving camera and object scenarios using visual-inertial odometry, depth-informed 2D tracking, and VIO-guided Kalman filtering in a closed-loop system.


<details>
  <summary>Details</summary>
Motivation: Previous 6D pose tracking methods work well only in static or quasi-static scenes but fail dramatically when both camera and objects move rapidly, requiring a solution for robust tracking in dynamic scenarios.

Method: Three synergistic components: (1) Visual-inertial odometry for camera motion ROI compensation, (2) Depth-informed 2D tracker for object translation ROI correction, (3) VIO-guided Kalman filter for rotation prediction and hierarchical pose refinement in a closed-loop system.

Result: The framework achieves real-time and robust 6D pose tracking for fast-moving cameras and objects, as demonstrated through both simulation and real-world experiments.

Conclusion: DynamicPose successfully overcomes the limitations of previous methods by providing accurate pose initialization and precise tracking in highly dynamic scenarios without requiring retraining.

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: This paper proposes a lightweight object detection method for point clouds that approximates multi-scale features from single neighborhoods using knowledge distillation, employs class-aware statistics as transferable features, and introduces central weighted IoU for better localization.


<details>
  <summary>Details</summary>
Motivation: Multi-scale feature learning in point cloud object detection typically requires multiple neighborhood searches and scale-aware layers, which increases computational costs and hinders development of lightweight models, especially under limited computational resources.

Method: The method approximates multi-scale features from a single neighborhood using knowledge distillation, designs a transferable feature embedding mechanism using class-aware statistics, and introduces central weighted intersection over union for improved localization to address center offset issues.

Result: Extensive experiments on public datasets demonstrate the effectiveness of the proposed method, which successfully saves computational costs while maintaining detection performance.

Conclusion: The proposed approach provides an effective solution for lightweight object detection from point clouds by approximating multi-scale features efficiently and addressing localization challenges through innovative techniques like transferable feature embedding and central weighted IoU.

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG is the first unified framework for 3D understanding and generation that uses an LLM to process sentences and 3D representations, featuring a spatial decoder with latent diffusion for high-quality 3D generation and a geometric-semantic learning strategy for enhanced spatial understanding.


<details>
  <summary>Details</summary>
Motivation: Despite progress in unified architectures for 2D image understanding and generation, integrating 3D tasks remains challenging and largely unexplored. The paper aims to bridge this gap by creating a comprehensive framework for 3D modalities.

Method: Proposes UniUGG framework with an LLM for sentence and 3D representation processing, a spatial decoder using latent diffusion model for 3D generation, and a geometric-semantic learning strategy to pretrain the vision encoder for capturing both semantic and geometric cues.

Result: Extensive experimental results demonstrate superiority in visual representation, spatial understanding, and 3D generation. The method supports both 3D scene generation from reference images with view transformations and spatial visual question answering tasks.

Conclusion: UniUGG successfully addresses the integration challenge of 3D tasks into unified architectures, providing a comprehensive solution for both 3D understanding and generation with state-of-the-art performance across multiple benchmarks.

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH is a moment-aware RVOS framework that introduces temporal moment annotations and selective supervision to address semantic misalignment in referring video object segmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods suffer from semantic misalignment due to indiscriminate frame sampling and supervision of all visible objects regardless of their relevance to the textual expression.

Method: Proposes SAMDWICH framework with Moment-guided Dual-path Propagation (MDP) for moment-aware object grounding and tracking, and Object-level Selective Supervision (OSS) to supervise only temporally aligned objects. Introduces MeViS-M dataset with manual temporal moment annotations.

Result: Achieves state-of-the-art performance on challenging MeViS benchmark, particularly excelling in complex scenarios with diverse expressions.

Conclusion: The moment-aware framework with selective supervision significantly enhances video-text alignment and referential understanding in RVOS tasks.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++ is a collaborative learning framework for efficient edge detection that balances accuracy and computational efficiency by leveraging cross-information from heterogeneous architectures, training moments, and parameter samplings.


<details>
  <summary>Details</summary>
Motivation: Edge detection is crucial for computer vision applications but existing deep learning methods have high computational costs that limit deployment on resource-constrained devices. There's a need for models that maintain high accuracy while reducing complexity.

Method: Proposes PEdger++, a collaborative learning framework that extracts cross-information from heterogeneous architectures, diverse training moments, and multiple parameter samplings to enhance learning from an ensemble perspective.

Result: Extensive experiments on BSDS500, NYUD and Multicue datasets show clear improvements over existing methods both quantitatively and qualitatively, with multiple model versions available for different computational requirements.

Conclusion: PEdger++ effectively addresses the accuracy-efficiency trade-off in edge detection, providing adaptable solutions for various resource constraints while maintaining high performance.

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: Novel multi-modal micro-expression dataset with synchronized RGB and event cameras, showing event-based data significantly outperforms RGB for Action Unit classification (51.23% vs 23.12%) and achieves high-quality frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: Micro-expression analysis is important for applications like Human-Robot Interaction and Driver Monitoring Systems, but RGB cameras have limitations in temporal resolution and sensitivity to motion blur for capturing subtle facial movements.

Method: Created a multi-resolution, multi-modal dataset with synchronized RGB and event cameras under variable lighting conditions. Evaluated two baseline tasks: Action Unit classification using Spiking Neural Networks and frame reconstruction using Conditional Variational Autoencoders.

Result: Event-based data achieved 51.23% accuracy for Action Unit classification vs 23.12% with RGB data. Frame reconstruction achieved SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input.

Conclusion: Event cameras show promising results for micro-expression recognition and frame reconstruction, outperforming traditional RGB cameras due to their microsecond-level precision, high dynamic range, and low latency.

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON is the first generative MLLM-based model for product representation learning that addresses multimodal alignment challenges, background noise in product images, and lack of standardized benchmarks through guided MoE modules, semantic region detection, and specialized negative sampling.


<details>
  <summary>Details</summary>
Motivation: Existing discriminative dual-flow architectures struggle with many-to-one alignment between multiple product images and texts. Generative MLLMs show promise but face challenges including lack of multimodal modeling modules, background noise in product images, and absence of standardized evaluation benchmarks.

Method: Proposes MOON with: (1) guided Mixture-of-Experts module for multimodal and aspect-specific modeling, (2) core semantic region detection to mitigate background noise, and (3) specialized negative sampling strategy for increased difficulty and diversity. Also releases MBE benchmark.

Result: Demonstrates competitive zero-shot performance on both the proposed MBE benchmark and public datasets, showing strong generalization across cross-modal retrieval, product classification, and attribute prediction tasks.

Conclusion: MOON effectively addresses key challenges in product representation learning and establishes a new standard through comprehensive benchmarking, with case studies and visualizations confirming its effectiveness for product understanding tasks.

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive: An instance-aware 3D Gaussian Splatting framework for dynamic driving scene reconstruction that achieves 3D instance segmentation without pre-processed instance IDs or complex pipelines.


<details>
  <summary>Details</summary>
Motivation: Current methods unify background elements into single representations, hindering instance-level understanding and scene editing. Existing approaches rely on pre-processed instance IDs or complex pipelines and are designed for indoor scenes, making them unsuitable for outdoor driving scenarios.

Method: Uses SAM-generated masks as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. Introduces 3D regularization to implicitly encode instance identities with voxel-based loss consistency. Employs a lightweight static codebook to bridge continuous features and discrete identities without pre-processing.

Result: Quantitative and qualitative experiments demonstrate effectiveness. First framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.

Conclusion: InstDrive successfully addresses the limitations of existing methods by providing instance-aware reconstruction for dynamic driving scenes without complex pre-processing requirements, enabling better scene understanding and editing capabilities.

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: WiseLVAM is a fully automated framework for left ventricular linear measurements that combines B-mode structure awareness with AMM motion awareness, eliminating the need for manual scanline placement while maintaining clinical guideline compliance.


<details>
  <summary>Details</summary>
Motivation: Existing automated methods for LV measurements are unreliable due to small landmark prediction errors causing significant measurement deviations. Clinical guidelines require specific scanline placement at the basal level perpendicular to the LV long axis, which most methods don't properly address.

Method: Proposes a contour-aware scanline placement approach using weakly supervised B-mode landmark detection to infer LV long axis and basal level. Builds on EnLVAM by automating scanline placement and performing measurements in Anatomical Motion Mode (AMM) for enhanced robustness.

Result: The method enables fully automated yet manually adaptable LV linear measurements that mimic clinical guidelines, combining structure-awareness from B-mode images with motion-awareness from AMM mode.

Conclusion: WiseLVAM provides a practical solution for routine clinical application by enhancing measurement accuracy and robustness through automated scanline placement and dual-mode (B-mode + AMM) analysis.

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU combines frequency domain processing with quantum-inspired retrieval for medical VQA, achieving superior performance on complex image-text reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical questions requiring both image and text understanding remain challenging in healthcare AI, needing improved accuracy and knowledge-based reasoning.

Method: Uses Fast Fourier Transform to shift features to frequency domain for noise filtering, combined with quantum-inspired retrieval system to fetch medical facts from external sources using quantum-based similarity techniques.

Result: Outperforms earlier models on VQA-RAD dataset, especially on complex cases requiring image-text reasoning, with improved performance and explainability.

Conclusion: The approach offers a promising way to build smart, clear, and helpful AI tools for doctors by combining frequency processing with quantum-inspired information retrieval.

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG is a video-based retrieval-augmented generation framework that enhances motion LLMs by retrieving relevant 2D human motion signals from large-scale video databases to address data limitations.


<details>
  <summary>Details</summary>
Motivation: Motion LLMs face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, requiring external knowledge from video databases to improve 3D motion generation.

Method: Develops Gemini Motion Video Retriever for effective motion-centered video retrieval and Motion-centric Dual-alignment DPO Trainer to mitigate error propagation from suboptimal retrieval results.

Result: Experimental results show VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.

Conclusion: VimoRAG successfully addresses key bottlenecks in video-based motion RAG and enhances motion generation capabilities for text-only motion LLMs.

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: AutoEval framework using Prediction Consistency and Reliability (PCR) to estimate object detection performance without ground-truth labels by analyzing spatial consistency and confidence reliability of bounding boxes before/after NMS.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for evaluating object detectors is costly and time-consuming, creating a need for automated performance assessment methods.

Method: Proposes PCR metric that measures spatial consistency between boxes before/after non-maximum suppression and reliability via confidence scores of overlapping boxes. Uses meta-dataset with varying image corruptions for realistic evaluation.

Result: PCR provides more accurate performance estimates than existing AutoEval methods, and the meta-dataset covers a wider range of detection performance scenarios.

Conclusion: The AutoEval framework with PCR enables efficient and effective automated evaluation of object detectors without requiring ground-truth annotations, addressing the limitations of manual assessment.

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD is a diffusion-based model for generic event boundary detection that generates diverse plausible boundaries rather than deterministic predictions, using temporal self-similarity encoding and classifier-free guidance.


<details>
  <summary>Details</summary>
Motivation: Previous GEBD methods focused on deterministic predictions but overlooked the inherent subjectivity and diversity of plausible event boundaries in videos.

Method: Proposes a diffusion-based model that encodes temporal self-similarity across frames and iteratively decodes random noise into plausible boundaries using classifier-free guidance to control diversity.

Result: Achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries.

Conclusion: The diffusion-based generative approach effectively addresses the subjectivity of event boundary detection and produces high-quality diverse predictions.

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: MSCNN method reduces 3D laser scanner uncertainty by 70% MSE and improves PSNR by 6dB using high-end scanner references and neural network correction.


<details>
  <summary>Details</summary>
Motivation: High-end and low-end laser scanners have positional errors due to equipment limitations and environmental factors in rough indoor environments, affecting 3D point accuracy for geometric modeling.

Method: Multi-stage CNN approach that pairs high-accuracy scanners as references with low-accuracy scanners in identical environments, establishes statistical relationships between measurement discrepancies and spatial distribution, and combines geometric processing with neural network refinement.

Result: 70% reduction in mean square error (MSE) and approximately 6 decibels improvement in peak signal-to-noise ratio (PSNR) on rough indoor rooms dataset.

Conclusion: Low-end laser scanners can achieve measurement uncertainty levels approaching high-end devices without hardware modifications through this integrated correction framework.

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: A theoretical framework for analyzing quantization error propagation in diffusion models, with a timestep-aware compensation scheme that significantly improves post-training quantization performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face deployment challenges due to computationally intensive iterative denoising processes. Post-training quantization causes stepwise quantization errors to accumulate progressively during generation, compromising output fidelity.

Method: Developed a theoretical framework that mathematically formulates error propagation in diffusion models, deriving per-step quantization error propagation equations and establishing the first closed-form solution for cumulative error. Proposed a timestep-aware cumulative error compensation scheme.

Result: Extensive experiments across multiple image datasets demonstrate that the compensation strategy effectively mitigates error propagation, significantly enhancing existing PTQ methods to achieve state-of-the-art performance on low-precision diffusion models.

Conclusion: The proposed theoretical framework and compensation scheme provide an effective solution for reducing quantization error accumulation in diffusion models, enabling more efficient deployment while maintaining output quality.

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med is a novel vision-language pre-training framework for medical 3D CT scans and radiology reports that achieves state-of-the-art performance with limited data through innovative self-supervised learning, a TriBERT language encoder, and hierarchical contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Medical volumetric data like CT scans paired with radiology reports is scarce and difficult to curate at scale, limiting vision-language model performance in medical applications compared to general domain 2D image-text pairs.

Method: Proposes VELVET-Med framework with: 1) uni-modal self-supervised learning integration, 2) TriBERT language encoder for multi-level textual semantics, 3) hierarchical contrastive learning for multi-level vision-language correspondence, using only 38,875 scan-report pairs.

Result: The framework achieves state-of-the-art performance across multiple downstream tasks including 3D segmentation, cross-modal retrieval, visual question answering, and report generation, demonstrating strong transferability.

Conclusion: VELVET-Med effectively addresses the challenge of limited medical volumetric data by focusing on innovative pre-training objectives and architectures rather than large-scale data collection, successfully capturing rich spatial and semantic relationships in medical images and clinical narratives.

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3 is an end-to-end MLLM framework that integrates dynamic visual tools (cropping, zooming, reusing) into interleaved vision-language reasoning through supervised fine-tuning, achieving superior performance on multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models have shown impressive performance but their long Chain-of-Thought capabilities in multimodal scenarios remain underexplored, particularly the ability to emulate human-like 'thinking with image' through iterative visual transformations.

Method: Proposed Simple o3 framework with scalable data synthesis pipeline generating high-quality interleaved vision-language reasoning chains via 'observe-reason-act' cycle, creating TWI-Tools-146K dataset with executable visual operations and rigorous verification.

Result: Experimental results demonstrate superior performance on diverse benchmarks, outperforming existing approaches. Additional visual tokens for interleaved reasoning with image reuse and magnification significantly improve visual reasoning, while precise cropping enhances focus on key regions.

Conclusion: Simple o3 establishes a computationally affordable paradigm for advancing multimodal reasoning, providing first in-depth analysis of different interleaved reasoning strategies and their impact on model performance.

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit is a two-stage virtual try-on method that combines warping for detail preservation with diffusion-based synthesis for realistic results, specifically maintaining fine garment details like logos and text.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based virtual try-on methods often fail to preserve critical fine-grained garment details such as logos and printed text, which are essential for brand integrity and customer trust in fashion retail.

Method: Two-stage hybrid approach: 1) Warp target garment using learned flow field for high-fidelity preservation, 2) Fidelity-preserving try-on module blends warped garment with preserved human regions using preserved-region input and inpainting mask to retain key areas and regenerate only necessary regions.

Result: Extensive qualitative results show DualFit achieves visually seamless try-on results while faithfully maintaining high-frequency garment details, effectively balancing reconstruction accuracy and perceptual realism.

Conclusion: DualFit successfully addresses the limitation of detail preservation in virtual try-on by combining warping and diffusion approaches, providing both high-fidelity garment detail preservation and realistic synthesis results.

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef is a tri-level quantization-aware defense framework that reduces patch-based adversarial attack transferability across different bit-width QNNs by disrupting semantic and gradient alignment through feature disalignment and gradient perceptual dissonance penalties.


<details>
  <summary>Details</summary>
Motivation: Quantized Neural Networks (QNNs) provide limited robustness against patch-based adversarial attacks that remain transferable across different bit-widths, and existing defenses either overfit to fixed quantization settings or fail to address cross-bit generalization vulnerability.

Method: TriQDef consists of three components: (1) Feature Disalignment Penalty (FDP) for semantic inconsistency in intermediate representations, (2) Gradient Perceptual Dissonance Penalty (GPDP) for misaligning input gradients across bit-widths using Edge IoU and HOG Cosine metrics, and (3) Joint Quantization-Aware Training Protocol that unifies these penalties in a shared-weight training scheme across multiple quantization levels.

Result: Extensive experiments on CIFAR-10 and ImageNet show TriQDef reduces Attack Success Rates (ASR) by over 40% on unseen patch and quantization combinations while preserving high clean accuracy.

Conclusion: The findings highlight the importance of disrupting both semantic and perceptual gradient alignment to mitigate patch transferability in QNNs, demonstrating that TriQDef effectively addresses cross-bit generalization vulnerability in quantized neural networks.

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: A fine-tuning method that balances domain adaptation with retention of pretrained VLM capabilities, preventing catastrophic forgetting while achieving strong retrieval performance without text data during fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Pretrained VLMs underperform in fine-grained open-set retrieval and suffer from catastrophic forgetting when fine-tuned on domain-specific data, losing their general-purpose multimodal capabilities.

Method: Combines continual learning regularization techniques with systematic validation set design and hyperparameter tuning to optimize knowledge retention while adapting to specific domains.

Result: Achieves strong performance on both fine-grained and coarse-grained retrieval benchmarks while maintaining visual-text alignment without using text data during fine-tuning.

Conclusion: Proposed method effectively balances domain adaptation with knowledge retention, providing robust generalization across datasets and pretrained models while preserving multimodal capabilities.

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR is a dual-branch implicit neural representation method for cardiac cine MRI reconstruction that combines positional embeddings with local multi-scale k-space features to achieve improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for cardiac cine MRI reconstruction focus only on coordinate-based positional embeddings while ignoring important feature representations of target points and their neighboring context, limiting reconstruction quality.

Method: Proposes KP-INR with two branches: one processes positional embeddings of k-space coordinates, and the other learns from local multi-scale k-space feature representations at those coordinates, with cross-branch interaction to approximate target k-space values.

Result: Experiments on CMRxRecon2024 dataset confirm improved performance over baseline models, demonstrating strong performance on challenging Cartesian k-space data.

Conclusion: KP-INR shows potential in cardiac cine MRI reconstruction by effectively combining positional and feature information, offering a promising approach for high-quality image recovery from undersampled data.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: FB-Mem is a novel segmentation-based metric that quantifies partial memorization in diffusion models, revealing memorization is more pervasive than previously thought and showing current mitigation methods fail to eliminate local memorization.


<details>
  <summary>Details</summary>
Motivation: Current detection methods only identify verbatim memorization but fail to capture partial memorization in small image regions and complex memorization patterns beyond specific prompt-image pairs.

Method: Proposed Foreground Background Memorization (FB-Mem), a segmentation-based metric that classifies and quantifies memorized regions within generated images using a clustering approach.

Result: Revealed that memorization is more pervasive: (1) individual generations may link to clusters of similar training images, and (2) existing mitigation methods fail to eliminate local memorization, especially in foreground regions.

Conclusion: Established an effective framework for measuring memorization in diffusion models, demonstrated inadequacy of current mitigation approaches, and proposed a stronger mitigation method using clustering.

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk is a novel framework for emotional talking head synthesis that uses VAE-generated 3D landmarks combined with emotion embeddings and a tri-plane attention NeRF to achieve superior emotion accuracy, controllability, and identity preservation compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current talking head generation methods excel at lip sync and image quality but fail to produce accurate and controllable emotional expressions while preserving subject identity, which is critical for artificial social intelligence.

Method: Uses VAE to generate 3D facial landmarks from audio, concatenates with emotion-label embeddings via ResNet-based landmark deformation model, then conditions a novel tri-plane attention Neural Radiance Field (NeRF) with landmarks and facial blendshape coefficients to synthesize emotional talking heads.

Result: Extensive experiments show RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation.

Conclusion: RealTalk advances the development of socially intelligent AI systems by enabling highly realistic emotional talking head synthesis with improved emotional expressiveness and identity consistency.

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse is a prompt-based framework that generates realistic RF signals from simulated indoor scenes with human motions, enabling scalable RF data generation for perception tasks.


<details>
  <summary>Details</summary>
Motivation: Collecting high-quality RF data in dynamic indoor environments is challenging, and there's a need for privacy-preserving alternatives to vision-based methods.

Method: Uses language-guided 4D world generator with state-aware causal transformer for human motion generation, and phase-coherent ray tracing simulator for accurate RF signal simulation.

Result: Effective conditioned human motion generation, successful phase coherence application to beamforming and respiration monitoring, and performance gains in RF imaging and activity recognition tasks.

Conclusion: WaveVerse enables scalable RF data generation, achieves performance improvements in both data-limited and data-adequate scenarios, and opens new possibilities for RF-based perception applications.

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: A unified kernel- and feature-agnostic formulation for feature lifting in 3D scene understanding that solves as a sparse linear inverse problem with provable optimal error bounds and regularization strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimally assigning rich image feature descriptors to 3D primitives while handling inconsistency issues from multi-view images in feature lifting for 3D scene understanding.

Method: Formulates feature lifting as a sparse linear inverse problem solved efficiently in closed form. Uses Tikhonov Guidance for numerical stability and Post-Lifting Aggregation for filtering noisy inputs via feature clustering.

Result: Achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic-forward baselines while producing lifted features in minutes.

Conclusion: The approach provides a unified, efficient solution for high-quality feature lifting with provable error bounds and effective regularization strategies for multi-view consistency.

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: Optimized YOLOv11 for cotton disease detection with improved small-target feature extraction, dynamic category weighting, and enhanced data augmentation, achieving 8-10.5% mAP improvement and 158 FPS inference speed.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in cotton disease detection: low precision in early spot detection (35% leakage rate for sub-5mm² spots), performance degradation in field conditions (25% accuracy drop), and high error rates (34.7%) in multi-disease scenarios.

Method: Proposed C2PSA module for enhanced small-target feature extraction, dynamic category weighting to handle sample imbalance, and improved data augmentation via Mosaic-MixUp scaling. Based on YOLOv11 architecture.

Result: Experimental results on 4,078-image dataset show: mAP50: 0.820 (+8.0% improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS. Mobile-deployed system enables real-time monitoring.

Conclusion: The optimized YOLOv11 system successfully addresses key challenges in cotton disease detection, providing high-precision real-time monitoring and enabling precision treatment applications in agriculture.

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: A generative neural physics framework that combines deep learning with wave physics simulation for fast 3D ultrasound computed tomography, enabling high-resolution musculoskeletal imaging with quantitative tissue parameter mapping in under 10 minutes.


<details>
  <summary>Details</summary>
Motivation: Conventional ultrasound computed tomography (USCT) is limited for musculoskeletal imaging due to ray-based reconstructions that neglect strong scattering effects, preventing accurate quantitative tissue characterization.

Method: Proposes a generative neural physics framework that couples generative networks with physics-informed neural simulation, learning ultrasonic wave propagation from cross-modality images to merge wave modeling accuracy with deep learning efficiency.

Result: Reconstructs 3D tissue parameter maps in under 10 minutes on synthetic and in vivo data (breast, arm, leg), achieving MRI-comparable resolution and sensitivity to biomechanical properties in muscle and bone.

Conclusion: Overcomes computational bottlenecks in strongly scattering regimes, advancing USCT toward routine clinical assessment of musculoskeletal disease through fast, high-fidelity quantitative imaging.

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: This paper introduces WXSOD dataset for weather-affected salient object detection and proposes WFANet model that uses weather prediction to enhance detection performance in adverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: Salient object detection struggles in complex weather conditions due to lack of datasets with weather noise annotations. Most existing methods work well only in clean natural scenes and few address weather-induced performance degradation.

Method: Created WXSOD dataset with 14,945 RGB images containing diverse weather noise and annotations. Proposed WFANet - a two-branch network with weather prediction branch that extracts weather features and saliency detection branch that fuses semantic and weather features.

Result: WFANet achieves superior performance compared to 17 existing SOD methods on the new WXSOD benchmark, demonstrating effectiveness of weather-aware feature aggregation.

Conclusion: The WXSOD dataset fills an important gap in weather-affected SOD research, and WFANet provides an effective baseline approach that leverages weather information to improve detection robustness in adverse conditions.

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [51] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: SCTR is a superpixel-informed continuous tensor representation that overcomes limitations of traditional low-rank tensor methods by using superpixels as modeling units and neural network parameterization for better handling of spatial variations and non-grid data.


<details>
  <summary>Details</summary>
Motivation: Traditional low-rank tensor representation methods assume holistic data is low-rank and are limited to discrete meshgrid data, which doesn't hold in real-world scenarios with spatial variations and diverse data forms.

Method: Uses superpixels as basic modeling units and proposes asymmetric low-rank tensor factorization with superpixel-specific factor matrices parameterized by a shared neural network with specialized heads, separating global pattern learning from local adaptation.

Result: Achieves 3-5 dB PSNR improvements over existing LRTR-based methods across multispectral images, videos, and color images on benchmark datasets.

Conclusion: SCTR provides a continuous and flexible framework for multi-dimensional data modeling that effectively captures both cross-superpixel commonalities and within-superpixel variations, offering superior performance and adaptability.

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [52] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: Proposes Region-level Context-aware Multimodal Understanding (RCMU) to integrate visual object information with textual context, introduces RCVIT training method, RCMU dataset, RC&P-Bench benchmark, and RC-Qwen2-VL models that achieve strong performance on RCMU tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on general visual understanding but lack the ability to integrate textual context associated with specific objects/regions for context-aware multimodal understanding.

Method: Proposes Region-level Context-aware Visual Instruction Tuning (RCVIT) that incorporates object information and bounding box coordinates to associate visual content with textual information. Creates RCMU dataset and RC&P-Bench benchmark for training and evaluation.

Result: RC-Qwen2-VL models achieve outstanding performance on multiple RCMU tasks and demonstrate successful applications in multimodal RAG and personalized conversation.

Conclusion: The proposed RCMU framework effectively enables MLLMs to integrate region-level visual and textual context, with RC-Qwen2-VL models showing strong performance and practical applications in multimodal understanding tasks.

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [53] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: SNNSIR is a fully spike-driven Spiking Neural Network for stereo image restoration that achieves competitive performance with significantly reduced computational overhead through spike-compatible modules like SRBB, SSCM, and SSCA.


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks offer high computational efficiency and low energy consumption but existing hybrid SNN-ANN models still use operations incompatible with SNNs' binary nature. The authors aim to create a fully spike-driven architecture for stereo image restoration.

Method: Proposed SNNSIR with three key components: 1) Spike Residual Basic Block (SRBB) for enhanced information flow, 2) Spike Stereo Convolutional Modulation (SSCM) for simplified nonlinearity and noise-sensitive region detection, 3) Spike Stereo Cross-Attention (SSCA) for efficient bidirectional feature interaction across stereo views.

Result: Extensive experiments on rain streak removal, raindrop removal, low-light enhancement, and super-resolution tasks demonstrate competitive restoration performance while significantly reducing computational overhead compared to existing methods.

Conclusion: The proposed fully spike-driven SNNSIR architecture shows potential for real-time, low-power stereo vision applications by maintaining competitive performance while being hardware-friendly and energy-efficient.

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [54] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: Dynamic semantic segmentation network adaptation for autonomous driving with three-tier control mechanism and Bayesian optimization for efficient hyperparameter search under computational constraints.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving platforms face diverse scenarios with varying hardware resources and precision requirements, requiring computational cost consideration for deployment on embedded devices like NVIDIA DRIVE PX 2.

Method: Three-tier control mechanism (width multiplier, classifier depth, classifier kernel) for fine-grained model component control, plus Bayesian Optimization with surrogate modeling for efficient hyperparameter exploration under tight computational budgets.

Result: Enables broad model scaling, targeted refinement of final layers, scenario-specific kernel optimization, and Task-Specific Learning Adaptation (TSLA) configurations that maximize computational capacity and model accuracy.

Conclusion: The approach provides dynamic adaptability for autonomous driving semantic segmentation networks, optimizing hardware utilization while accommodating diverse computational complexity and accuracy requirements across different self-driving tasks.

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [55] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: CLAIR: A novel framework for weakly supervised zero-shot cross-domain image retrieval that refines noisy CLIP-generated pseudo-labels and uses contrastive learning with cross-domain mapping to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Large foundation models like CLIP can generate pseudo-labels for unlabeled data, making unsupervised zero-shot cross-domain retrieval less relevant. The paper focuses on weakly supervised approach with noisy pseudo-labels from CLIP.

Method: Proposes CLAIR with confidence score refinement for noisy pseudo-labels, inter-instance/inter-cluster contrastive losses, inter-domain contrastive loss, closed-form cross-domain mapping function using CLIP text embeddings, and learnable prompts for zero-shot generalization.

Result: Extensive experiments on TUBerlin, Sketchy, Quickdraw, and DomainNet datasets show CLAIR consistently outperforms existing state-of-the-art methods.

Conclusion: CLAIR effectively addresses weakly supervised zero-shot cross-domain image retrieval by refining noisy pseudo-labels and aligning features across domains, demonstrating superior performance across multiple benchmark datasets.

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [56] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: Improved 3D Gaussian Splatting densification with edge-aware selection, long-axis splitting, and anti-overfitting techniques for better reconstruction quality with fewer Gaussians.


<details>
  <summary>Details</summary>
Motivation: 3DGS's current densification strategy produces suboptimal reconstruction quality, needing improvements in when/how to densify and overfitting mitigation.

Method: Proposes Edge-Aware Score for candidate selection, Long-Axis Split strategy to reduce geometric distortions, and anti-overfitting techniques including Recovery-Aware Pruning, Multi-step Update, and Growth Control.

Result: Achieves state-of-the-art performance with fewer Gaussians, enhancing rendering fidelity without additional training or inference overhead.

Conclusion: The comprehensive densification pipeline improvements significantly boost 3DGS reconstruction quality while maintaining efficiency.

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [57] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: NCA-WSS: A weakly supervised segmentation method using neural cellular automata that extracts segmentation masks from classification feature maps without needing segmentation labels, outperforming existing approaches on white blood cell datasets.


<details>
  <summary>Details</summary>
Motivation: White blood cell detection and segmentation is crucial for medical diagnostics but requires large labeled datasets that are time-consuming and expensive to acquire. There's a need for efficient weakly supervised methods that can reduce labeling requirements.

Method: Proposes neural cellular automata for weakly supervised segmentation (NCA-WSS) that leverages feature maps generated during classification to extract segmentation masks without retraining with segmentation labels.

Result: The method significantly outperforms existing weakly supervised approaches on three white blood cell microscopy datasets, demonstrating strong segmentation performance without segmentation-specific training.

Conclusion: NCA shows great potential for both classification and segmentation in weakly supervised frameworks, providing a scalable and efficient solution for medical image analysis tasks.

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [58] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: Integration of attention pooling with Neural Cellular Automata (NCA) improves microscopy image classification performance while maintaining parameter efficiency and explainability.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between NCA and larger architectures for microscopy image analysis by enhancing feature extraction capabilities.

Method: Combine attention pooling mechanism with Neural Cellular Automata to refine focus on informative regions and improve classification accuracy.

Result: Significantly outperforms existing NCA methods on eight microscopy datasets, shows improved performance compared to lightweight CNNs and vision transformers while maintaining lower parameter count.

Conclusion: NCA-based models with attention pooling offer a promising alternative for explainable image classification, balancing performance and interpretability.

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [59] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive is a Doppler-driven temporal aggregation method that enhances radar point cloud density for autonomous driving by reducing scatter from dynamic objects through radial shifting based on Doppler components and adaptive aggregation durations.


<details>
  <summary>Details</summary>
Motivation: Radar's long detection range is crucial for autonomous driving, but sparse radar point clouds at long range make accurate detection challenging. Existing temporal aggregation methods introduce scatter from dynamic objects, degrading detection performance.

Method: Proposes DoppDrive which shifts points from previous frames radially according to their dynamic Doppler component to eliminate radial scatter, and assigns each point a unique aggregation duration based on Doppler and angle to minimize tangential scatter.

Result: DoppDrive significantly improves object detection performance across various detectors and datasets, enhancing radar point cloud density while minimizing scatter.

Conclusion: DoppDrive is an effective pre-detection step that enhances radar point cloud density and improves object detection performance, compatible with any radar detector for autonomous driving applications.

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [60] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: A framework to remove HMD occlusions and reconstruct 3D facial geometry from single-view RGB videos using GAN-based inpainting guided by landmarks and reference frames.


<details>
  <summary>Details</summary>
Motivation: HMDs obscure facial expressions crucial for social XR applications like teleconferencing, creating a need to remove occlusions while preserving facial identity.

Method: Combines GAN-based video inpainting guided by dense facial landmarks and reference frames with SynergyNet-based 3DMM parameter regression for 3D reconstruction, using landmark optimization throughout.

Result: Successfully removes HMDs while maintaining facial identity and realism, producing photorealistic 3D geometry outputs that remain robust across different landmark densities.

Conclusion: The framework effectively addresses HMD occlusion issues in XR applications through joint inpainting and 3D reconstruction, enabling better facial expression communication in social XR scenarios.

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [61] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: A novel Semantic Discrepancy-aware Detector (SDD) that uses reconstruction learning to align forgery and semantic concept spaces for improved fake image detection.


<details>
  <summary>Details</summary>
Motivation: The misalignment between forgery and semantic concept spaces hinders detection performance of pre-trained models in identifying fake images, despite their learned semantic concepts being critical for detection.

Method: Proposes SDD with semantic token sampling to mitigate space shifts, concept-level forgery discrepancy learning through visual reconstruction, and low-level forgery feature enhancement to minimize redundant information.

Result: Achieves superior results compared to existing methods on two standard image forgery datasets, demonstrating the efficacy of the proposed approach.

Conclusion: The SDD framework effectively addresses the space misalignment problem and enhances forgery detection performance by leveraging semantic concepts and reconstruction learning.

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [62] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat is a plug-and-play feature enhancement module that improves underwater object detection by optimizing features specifically for detection tasks, achieving state-of-the-art performance while maintaining real-time processing speeds.


<details>
  <summary>Details</summary>
Motivation: Underwater environments cause severe image degradation that impairs object detection models, and traditional image enhancement methods are not optimized for downstream detection tasks.

Method: A multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring enhancement is explicitly guided to refine features most relevant to the detection task. Integrated with YOLOv8m.

Result: State-of-the-art Precision (0.877) and Recall (0.624), with competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421) while maintaining 46.5 FPS processing speed.

Conclusion: AquaFeat provides an effective and computationally efficient solution for real-world underwater applications like marine ecosystem monitoring and infrastructure inspection.

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [63] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: MBMamba improves Mamba architecture for image deblurring by adding memory buffer mechanism and Ising-inspired regularization to address local pixel forgetting and channel redundancy issues without changing the original architecture.


<details>
  <summary>Details</summary>
Motivation: The Mamba architecture shows promise for image deblurring but suffers from local pixel forgetting and channel redundancy due to its flatten-and-scan strategy, while existing solutions increase computational complexity and hinder real-time performance.

Method: Proposes MBMamba with two key components: 1) memory buffer mechanism to preserve historical information for fusion and model relevance between adjacent features, and 2) Ising-inspired regularization loss that simulates energy minimization of pixel mutual attraction to maintain image structure.

Result: Experimental results demonstrate that MBMamba outperforms state-of-the-art approaches on widely used benchmarks for image deblurring.

Conclusion: The proposed method effectively addresses Mamba's limitations in image deblurring without architectural changes, achieving superior performance while maintaining computational efficiency.

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [64] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot approach for temporal interaction localization that identifies precise hand-object contact and separation moments in egocentric videos without needing object masks or verb-noun taxonomies.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on 'how to interact' but neglects the critical problem of 'when to interact' - identifying exact contact and separation moments between hand and object, which is crucial for VR/AR applications and robotic policy transfer.

Method: Proposes EgoLoc with hand-dynamics-guided sampling to generate visual prompts, leverages vision-language models to identify contact/separation attributes and localize timestamps, and uses closed-loop feedback for refinement.

Result: Comprehensive experiments show EgoLoc achieves plausible temporal interaction localization on public datasets and novel benchmarks, and effectively facilitates downstream applications in egocentric vision and robotic manipulation.

Conclusion: EgoLoc provides a generalizable zero-shot solution for precise hand-object interaction timing localization, eliminating the need for object masks and verb-noun taxonomies while demonstrating practical utility in multiple applications.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [65] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: A simple two-step data augmentation method using diffusion models to generate synthetic training data for offline RL, improving generalization in vision-based tasks without algorithmic changes.


<details>
  <summary>Details</summary>
Motivation: Offline RL policies struggle to generalize due to limited diverse state exposure in visual data, with challenges like noise, distractions, and spurious correlations leading to overfitting.

Method: Two-step process: first augment original offline data for diversity, then use diffusion model to generate additional synthetic data in latent space.

Result: Significantly improves generalization across continuous (Visual D4RL) and discrete (Procgen) action spaces, increases data diversity, reduces generalization gap, and maintains computational efficiency.

Conclusion: This approach enables better utilization of vision-based offline data and could fuel progress in synthetic data generation for training more general agents.

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [66] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: IPGPhormer is a novel interpretable graph-transformer framework for cancer survival analysis from whole-slide images that balances spatial relationships and local context while providing tissue and cellular-level interpretability without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing survival analysis methods struggle to balance long-range spatial relationships with local contextual dependencies and lack inherent interpretability, limiting their clinical utility in cancer prognosis.

Method: Proposes Interpretable Pathology Graph-Transformer (IPGPhormer) framework that captures tumor microenvironment characteristics and models spatial dependencies across tissue, providing interpretability at both tissue and cellular levels without post-hoc annotations.

Result: Comprehensive evaluations on four public benchmark datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in both predictive accuracy and interpretability.

Conclusion: IPGPhormer offers a promising tool for cancer prognosis assessment, paving the way for more reliable and interpretable decision-support systems in pathology.

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [67] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: ViT-EnsembleAttack enhances adversarial transferability by applying adversarial augmentation to Vision Transformer surrogate models using multi-head dropping, attention score scaling, and MLP feature mixing strategies optimized with Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble attacks focus on refining weights or paths but overlook exploring ensemble models themselves to enhance transferability, particularly for Vision Transformers which receive less attention in ensemble-based attacks.

Method: Proposes three adversarial augmentation strategies for ViTs: Multi-head dropping, Attention score scaling, and MLP feature mixing with Bayesian optimization. Includes Automatic Reweighting and Step Size Enlargement modules to boost transferability.

Result: Extensive experiments show ViT-EnsembleAttack significantly enhances adversarial transferability of ensemble-based attacks on ViTs, outperforming existing methods by a substantial margin.

Conclusion: The proposed method effectively addresses the gap in ensemble-based attacks for Vision Transformers and demonstrates superior performance through adversarial augmentation of surrogate models.

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [68] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT is a framework that uses LLMs to decompose complex text instructions into structured semantic units, then integrates them into optimized prompts for T2I models, significantly improving performance on complex image generation tasks.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with complex, long-form textual instructions, failing to accurately render intricate details, spatial relationships, and specific constraints as revealed by benchmarks like LongBench-T2I.

Method: Two-stage framework: 1) Complex Instruction Decomposition and Semantic Enhancement using LLMs to break down instructions into structured semantic units, 2) Multi-Stage Prompt Integration and Adaptive Generation to transform units into hierarchical or optimized prompts for T2I models.

Result: DeCoT consistently improves T2I model performance across all dimensions, achieving average score of 3.52 with Infinity-8B (vs baseline 3.44), with particular gains in "Text" and "Composition" aspects. Human evaluations confirm superior perceptual quality and instruction fidelity.

Conclusion: DeCoT effectively bridges the gap between high-level user intent and T2I model requirements, enabling more faithful and accurate image generation from complex textual instructions through systematic decomposition and prompt optimization.

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [69] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP is a federated learning framework that enhances CLIP's performance by leveraging multi-scale visual features and client-specific style indicators to generate robust, context-aware prompts, achieving superior accuracy and generalization across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional prompt learning methods using only final-layer features miss rich multi-scale visual cues and domain-specific style variations in decentralized client data, limiting performance in federated learning settings.

Method: FedCSAP extracts low, mid, and high-level features from CLIP's vision encoder combined with client-specific style indicators from batch-level statistics, merging visual details with textual context to generate distinct, non-redundant prompt tokens.

Result: Comprehensive experiments show FedCSAP outperforms existing federated prompt learning methods in both accuracy and generalization across multiple image classification datasets.

Conclusion: The framework effectively handles non-IID class distributions and diverse domain styles while maintaining data privacy through local training and global aggregation, demonstrating superior performance in federated learning environments.

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [70] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR is an inference-time strategy that enhances Large Vision-Language Models' reasoning by generating multiple diverse perspectives, integrating them with the original question, and using the enriched context for final answer generation without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: LVLMs struggle with complex visual reasoning tasks requiring deep contextual understanding and multi-angle analysis due to limitations of single-shot image encoding and prompts.

Method: Three-stage approach: 1) Generate N diverse descriptions/reasoning paths from various angles, 2) Intelligently integrate these with original question to create comprehensive context-augmented prompt, 3) Use enriched prompt for final reasoning and answer generation.

Result: Significant accuracy gains on challenging VQA datasets (GQA, VQA-CP v2, ScienceQA), particularly for tasks requiring robust contextual understanding. Human evaluations confirm improved coherence and completeness.

Conclusion: MPCAR effectively leverages LVLMs' generative capabilities to enrich input contexts, unlocking latent reasoning potential for complex multimodal tasks without parameter fine-tuning.

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [71] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD is a novel vision-language framework that enhances autonomous driving by incorporating comprehensive scene understanding and specialized expert adapters into VLMs, significantly improving driving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning methods for autonomous driving lack holistic scene recognition and spatial awareness needed for complex driving situations, creating a gap in explainable driving systems.

Method: Proposes LMAD framework that emulates end-to-end driving paradigms with preliminary scene interaction and specialized expert adapters within the same task structure, designed to be compatible with existing VLMs and planning systems.

Result: Extensive experiments on DriveLM and nuScenes-QA datasets show LMAD significantly boosts VLM performance on driving reasoning tasks, setting new standards for explainable autonomous driving.

Conclusion: LMAD successfully addresses the limitations of current VLM approaches by providing better scene understanding and spatial awareness, making it a promising framework for explainable autonomous driving systems.

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [72] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: S5 is a scalable semi-supervised semantic segmentation framework for remote sensing that leverages large-scale unlabeled data through data selection strategies and foundation model pre-training, achieving state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised semantic segmentation methods in remote sensing rely on small datasets and models, limiting practical applicability. There's a need to utilize vast unlabeled Earth observation data that is typically underutilized due to expensive pixel-level annotations.

Method: Proposed S5 framework with data selection strategy combining entropy-based filtering and diversity expansion to create RS4P-1M dataset. Pre-trained RS foundation models of varying sizes on this dataset, and incorporated Mixture-of-Experts-based multi-dataset fine-tuning for efficient adaptation to multiple benchmarks.

Result: The resulting RS foundation models achieved state-of-the-art performance across all benchmarks in land cover segmentation and object detection tasks, demonstrating significant performance improvements.

Conclusion: Scaling semi-supervised learning through large-scale data utilization and foundation model pre-training is viable and effective for remote sensing applications, enabling better generalization and versatility across diverse benchmarks.

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [73] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: SRMA-Mamba is a novel Mamba-based network for 3D liver cirrhosis segmentation in MRI volumes that integrates spatial anatomical relationships and uses selective scanning with reverse attention to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Early detection of liver cirrhosis is critical for reducing mortality, but existing methods underutilize spatial anatomical details in volumetric MRI data, limiting clinical effectiveness and explainability.

Method: Proposes SRMA-Mamba with Spatial Anatomy-Based Mamba module (SABMamba) that performs selective scans within cirrhotic tissues and combines anatomical information from sagittal, coronal, and axial planes. Also introduces Spatial Reverse Attention module (SRMA) to progressively refine segmentation details using coarse maps and hierarchical encoding features.

Result: Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation.

Conclusion: The proposed SRMA-Mamba network effectively addresses the challenge of modeling spatial relationships in complex liver anatomical structures, achieving superior volumetric segmentation performance for liver cirrhosis detection in MRI data.

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [74] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN is a text-to-dynamic panorama generation framework that creates 360-degree immersive 4D scenes with fine-grained content control and motion-rich, geometry-consistent results.


<details>
  <summary>Details</summary>
Motivation: Existing VR/AR generation works focus on static scenes or narrow perspective-view dynamic scenes, lacking true 360-degree immersive experiences from any viewpoint.

Method: Combines panorama video generation (using dual-branch model with global panorama and local perspective branches with cross-attention) and dynamic scene reconstruction (using geometry-aligned 3D Gaussian Splatting with metric depth maps and estimated camera poses).

Result: Extensive experiments demonstrate effectiveness and superiority in generating visually compelling and motion-coherent dynamic panoramic scenes.

Conclusion: TiP4GEN successfully addresses the gap in 360-degree immersive dynamic scene generation with fine-grained control and geometric consistency.

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [75] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: Comparison of human and AI visual perception through illusions reveals critical differences, showing AI has unique vulnerabilities like pixel sensitivity and hallucinations, while also exhibiting some human-like illusion effects.


<details>
  <summary>Details</summary>
Motivation: To understand how artificial vision systems differ from human perception and identify AI-specific vulnerabilities that could impact trust and safety in AI systems performing human-like visual tasks.

Method: Systematic comparison of human and AI responses to classic visual illusions involving color, size, shape, and motion, analyzing both targeted training effects and emergent pattern recognition behaviors.

Result: AI exhibits some illusion-like effects similar to humans but also has unique AI-specific illusions (pixel-level sensitivity, hallucinations) that reveal perceptual vulnerabilities invisible to humans.

Conclusion: The findings highlight alignment gaps between human and AI perception and provide insights for developing more robust, interpretable AI vision systems that preserve beneficial human perceptual biases while avoiding dangerous distortions.

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [76] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: This paper exposes vulnerabilities in VQA-NLE systems where models produce inconsistent explanations and make decisions without genuine understanding. The authors propose adversarial attacks on both questions and images, and introduce a knowledge-based defense method to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing VQA-NLE systems can produce inconsistent explanations and reach conclusions without genuinely understanding the underlying context, exposing weaknesses in their decision-making processes and explanation mechanisms.

Method: Leverage existing adversarial strategies to perturb questions and propose a novel strategy that minimally alters images to induce contradictory outputs. Introduce a mitigation method that uses external knowledge to alleviate inconsistencies.

Result: Extensive evaluations on two standard benchmarks and two widely used VQA-NLE models show the effectiveness of the proposed attacks and demonstrate the potential of knowledge-based defenses.

Conclusion: The research reveals pressing security and reliability concerns in current VQA-NLE systems and highlights the need for more robust explanation mechanisms in visual question answering.

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [77] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT is a novel framework that uses Vision-Language Large Models to provide interpretable chest X-ray diagnosis by simulating radiologists' chain-of-thought reasoning, achieving competitive accuracy while generating detailed explainable reports.


<details>
  <summary>Details</summary>
Motivation: Chest X-ray interpretation requires extensive clinical experience and suffers from variability, while existing deep learning models are black-box systems that hinder clinical adoption in high-stakes medical settings.

Method: Proposes X-Ray-CoT framework that extracts multi-modal features and visual concepts, then uses LLM-based component with structured Chain-of-Thought prompting strategy to reason and generate detailed natural language diagnostic reports.

Result: Achieves Balanced Accuracy of 80.52% and F1 score of 78.65% on CORDA dataset, slightly surpassing black-box models while uniquely generating high-quality explainable reports as validated by human evaluations.

Conclusion: The framework represents a significant step towards trustworthy and clinically actionable AI systems in medical imaging, with ablation studies confirming the necessity of multi-modal fusion and CoT reasoning for robust and transparent medical AI.

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [78] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA eliminates alignment pre-training by mapping text to visual space instead of visual to text, achieving better reasoning performance while reducing computation by 45%.


<details>
  <summary>Details</summary>
Motivation: Challenge traditional multimodal learning that requires expensive alignment pre-training and projects visual features to text space, seeking more efficient approaches.

Method: Maps text embeddings into continuous visual representation space, performs fusion within transformer intermediate layers using selective additive components in attention mechanisms.

Result: Improves reasoning-intensive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%) but decreases perception tasks (celebrity recognition: -49.5%, OCR: -21.3%).

Conclusion: Alignment pre-training is unnecessary for effective multimodal learning, especially for complex reasoning. Establishes new paradigm with 45% computational reduction and opens efficient multimodal architecture research.

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [79] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: A hybrid AI system combining fine-tuned vision-language models and reasoning LLMs for automated H-reflex EMG waveform interpretation, improving standardization and accuracy in neuromuscular assessment.


<details>
  <summary>Details</summary>
Motivation: Traditional H-reflex EMG analysis suffers from variability and interpretation bias among clinicians, limiting reliability and standardization in sports science and clinical neurology.

Method: Fine-tuned multiple VLMs on curated H-reflex EMG waveform images with clinical annotations, then aggregated outputs using consensus method and refined with specialized reasoning LLM for decision support.

Result: The hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing automation and standardization of neuromuscular diagnostics.

Conclusion: First integration of fine-tuned VLM consortium with reasoning LLM for image-based H-reflex analysis, laying foundation for next-generation AI-assisted neuromuscular assessment platforms.

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [80] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: Hybrid CNN-Transformer models with Convolutional Kolmogorov-Arnold Network achieve state-of-the-art skin cancer classification performance across multiple datasets, demonstrating effective integration of local spatial features and global contextual dependencies.


<details>
  <summary>Details</summary>
Motivation: Precise differentiation between malignant and non-malignant skin lesions is crucial for early diagnosis and treatment, requiring advanced models that can effectively capture both spatial and contextual features in medical images.

Method: Sequential and Parallel Hybrid CNN-Transformer models integrated with Convolutional Kolmogorov-Arnold Network (CKAN), using transfer learning and extensive data augmentation. CNNs extract local spatial features, Transformers model global dependencies, and CKAN facilitates nonlinear feature fusion.

Result: Achieved 92.81% accuracy and 92.47% F1-score on HAM10000, 97.83% accuracy and F1-score on PAD-UFES, and 91.17% accuracy with 91.79% F1-score on BCN20000 dataset, demonstrating strong generalization across diverse datasets.

Conclusion: The hybrid CNN-Transformer architecture with CKAN integration effectively captures both spatial and contextual features, enhancing feature fusion and representation learning for robust skin cancer classification across varying data distributions and class imbalances.

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [81] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR is a responsible AI system for diabetic retinopathy screening that outperforms FDA-approved EyeArt system with significant improvements in accuracy, F1 scores, and fairness metrics while addressing data quality and bias issues.


<details>
  <summary>Details</summary>
Motivation: Diabetic Retinopathy is a leading cause of vision loss, but early detection is hindered by shortage of specialists and challenges in timely examination. AI solutions face adoption barriers due to low-quality data and biases that may lead to learning unintended features.

Method: Developed RAIS-DR, a Responsible AI System that incorporates ethical principles across the AI lifecycle. It integrates efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models.

Result: Evaluated on 1,046 unseen patients, RAIS-DR showed F1 scores increased by 5-12%, accuracy by 6-19%, and specificity by 10-20% compared to EyeArt. Fairness metrics (Disparate Impact, Equal Opportunity Difference) indicated equitable performance across demographic subgroups.

Conclusion: RAIS-DR is a robust and ethically aligned solution for DR screening that can reduce healthcare disparities. The system and code are publicly available with RAIL licensing.

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [82] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: LangVision-LoRA-NAS integrates Neural Architecture Search with LoRA to dynamically optimize Vision Language Models by finding optimal rank configurations for specific multimodal tasks, improving performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current LoRA implementations use fixed ranks for fine-tuning, which limits flexibility and efficiency across diverse multimodal tasks. There's a need for adaptive rank selection to better balance performance and computational requirements.

Method: The framework combines Neural Architecture Search (NAS) with LoRA to dynamically search for optimal rank configurations. It uses LLaMA-3.2-11B model and experiments on multiple datasets to find task-specific optimal LoRA ranks.

Result: Extensive experiments show notable improvement in model performance while reducing fine-tuning costs. The approach demonstrates better efficiency and effectiveness compared to fixed-rank LoRA implementations.

Conclusion: LangVision-LoRA-NAS provides a flexible and efficient framework for Vision Language Model adaptation, enabling dynamic rank optimization that outperforms traditional fixed-rank LoRA approaches in both performance and computational efficiency.

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [83] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: Cross-View Transformers effectively map camera images to Bird's-Eye View maps for autonomous driving, showing strong generalization to unseen environments with optimal performance using L1 loss and four-camera setup.


<details>
  <summary>Details</summary>
Motivation: Bird's-Eye View maps provide crucial top-down abstraction for autonomous driving perception, but effective mapping from camera images to BEV representations needs better methods.

Method: Used Cross-View Transformers (CVT) to map camera images to three BEV channels (road, lane markings, planned trajectory) using a realistic urban driving simulator. Evaluated generalization to unseen towns, different camera layouts, and compared focal vs L1 loss formulations.

Result: Four-camera CVT trained with L1 loss delivered the most robust test performance when evaluated in a new town, showing strong generalization capabilities from training data from only one town.

Conclusion: Cross-View Transformers show promise for mapping camera inputs to reasonably accurate BEV maps, with L1 loss and four-camera configuration providing optimal performance for autonomous driving applications.

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [84] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo is a multi-modal subject-specific adaptation method for expression recognition that uses co-training to leverage complementary information across modalities and source domains, outperforming existing UDA and MSDA methods.


<details>
  <summary>Details</summary>
Motivation: Current MSDA approaches for personalized expression recognition often overlook multimodal information or blend sources into single domains, failing to capture unique subject-specific characteristics needed for applications like patient-specific stress or pain assessment.

Method: MuSACo selects relevant source subjects, generates pseudo-labels using dominant modality for class-aware learning, uses class-agnostic loss for less confident samples, and aligns source features while combining only confident target features across modalities.

Result: Experimental results on BioVid and StressID datasets show MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.

Conclusion: MuSACo effectively addresses limitations of existing MSDA approaches by leveraging multimodal information and preserving subject diversity, making it particularly suitable for affective computing applications in digital health where subject-level nuances are crucial.

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [85] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL is a prompt-driven framework that uses large vision-language models for generalized image forgery detection through holistic scene evaluation and region-wise anomaly analysis across multiple domains.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative models has created challenges in detecting visual forgeries, with existing methods struggling with generalization across domains and lacking reasoning capabilities.

Method: Proposes REVEAL framework that uses large vision-language models for prompt-driven visual reasoning. Includes two approaches: (1) Holistic scene-level evaluation assessing physics, semantics, perspective, and realism, and (2) Region-wise anomaly detection by analyzing multiple image regions.

Result: Conducted experiments across multiple domains (Photoshop, DeepFake, AIGC editing) and compared vision language models against competitive baselines while analyzing their reasoning capabilities.

Conclusion: The framework demonstrates effective forgery detection through semantic alignment of vision-language models, providing both detection and reasoning capabilities across diverse manipulation domains.

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [86] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: SFAC is a CNN-based algorithm that colorizes old photos using only two images (reference and target), eliminating need for large datasets while preserving structure through feature alignment and perceptual constraints.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning colorization methods struggle with old photos due to lack of ground truth and domain gap between natural gray images and historical photos, requiring a specialized approach.

Method: Structure-preserving Feature Alignment Colorizer (SFAC) uses semantic correspondence between two images with feature distribution alignment loss, plus structure-preserving mechanisms at feature and pixel levels.

Result: Extensive experiments show SFAC effectively colorizes old photos with both qualitative and quantitative improvements, overcoming domain gap issues.

Conclusion: SFAC provides an effective solution for old photo colorization that works with minimal data (just two images) while maintaining structural integrity through novel alignment and preservation techniques.

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [87] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: USDRL is a unified skeleton-based foundation model that uses transformer architecture with multi-grained feature decorrelation and multi-perspective consistency training to achieve state-of-the-art performance across 25 benchmarks in 9 different action understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing skeleton-based action understanding methods lack scalability and generalization, with no foundation model capable of handling diverse tasks. The need for a unified approach that can adapt to various action understanding tasks including coarse, dense, and transferred prediction.

Method: Transformer-based Dense Spatio-Temporal Encoder (DSTE) with parallel streams for temporal and spatial features, Multi-Grained Feature Decorrelation (MG-FD) across temporal/spatial/instance domains, and Multi-Perspective Consistency Training (MPCT) with multi-view and multi-modal self-supervised learning.

Result: Significantly outperforms current state-of-the-art methods across 25 benchmarks covering 9 different skeleton-based action understanding tasks, demonstrating superior scalability and generalization capabilities.

Conclusion: USDRL serves as an effective foundation model for skeleton-based human action understanding, encouraging broader research scope and more attention to dense prediction tasks in this field.

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [88] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: MCOUT enables multimodal reasoning in a joint latent space using continuous hidden vectors instead of natural language, achieving up to 8.23% accuracy gains over traditional Chain-of-Thought methods.


<details>
  <summary>Details</summary>
Motivation: Traditional language-based reasoning methods like Chain-of-Thought are suboptimal for multimodal contexts as they struggle to dynamically align audio, visual, and textual information, requiring an alternative paradigm.

Method: Proposes Multimodal Chain of Continuous Thought (MCOUT) with two variants: MCOUT-Base reuses language model's last hidden state for iterative reasoning, and MCOUT-Multi integrates multimodal latent attention to strengthen cross-modal alignment between visual and textual features.

Result: Experiments on MMMU, ScienceQA, and MMStar benchmarks show consistent improvements in multimodal reasoning with up to 8.23% accuracy gains over strong baselines and up to 8.27% BLEU score improvements across multiple-choice and open-ended tasks.

Conclusion: Latent continuous reasoning represents a promising direction for advancing large multimodal models beyond language-bound approaches, offering a scalable framework for human-like reflective multimodal inference.

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [89] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD is a novel Large Vision Language Diffusion framework that uses parallel generation instead of autoregressive methods for autonomous driving, achieving faster inference, bidirectional reasoning, and near-zero failure rates.


<details>
  <summary>Details</summary>
Motivation: Autoregressive Vision Language Models for autonomous driving suffer from high inference latency and inability to perform bidirectional reasoning, making them unsuitable for safety-critical real-world applications.

Method: ViLaD uses a masked diffusion model that enables parallel generation of entire driving decision sequences, supports bidirectional reasoning, and progressive easy-first generation for iterative decision improvement.

Result: On nuScenes dataset, ViLaD outperforms state-of-the-art autoregressive VLM baselines in planning accuracy and inference speed, achieving near-zero failure rate. Real-world deployment confirmed practical viability.

Conclusion: ViLaD represents a paradigm shift in autonomous driving systems, offering superior performance, reduced latency, and practical viability for safety-critical applications through parallel diffusion-based generation.

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [90] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: ViDA-UGC introduces the first large-scale visual distortion assessment dataset for UGC images with fine-grained quality annotations, using a CoT framework to enhance MLLMs' image quality analysis capabilities.


<details>
  <summary>Details</summary>
Motivation: Current explainable IQA methods inadequately evaluate both UGC and AIGC images using the same distortion criteria and lack detailed quality analysis for monitoring and restoration guidance.

Method: Constructed ViDA-UGC dataset with 11K images through distortion-oriented pipeline using human annotation and CoT framework with GPT-4o to generate quality descriptions. Created ViDA-UGC-Bench benchmark with 476 images and 6,149 QA pairs professionally validated.

Result: Experimental results show ViDA-UGC and CoT framework consistently enhance various MLLMs' image quality analysis abilities on ViDA-UGC-Bench and Q-Bench, even surpassing GPT-4o performance.

Conclusion: The study successfully establishes the first comprehensive UGC distortion assessment dataset and benchmark, demonstrating significant improvements in explainable IQA capabilities for multimodal language models.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [91] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: OpenMoCap introduces a robust motion capture model and CMU-Occlu dataset to address marker occlusion challenges in optical motion capture systems, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current optical motion capture systems suffer severe performance degradation under large-scale marker occlusions common in real-world applications, due to lack of realistic training datasets and training strategies for long-range marker dependencies.

Method: Proposes OpenMoCap model with marker-joint chain inference mechanism for simultaneous optimization and construction of deep constraints between markers and joints, plus CMU-Occlu dataset using ray tracing to simulate realistic occlusion patterns.

Result: Extensive experiments show OpenMoCap consistently outperforms competing methods across diverse scenarios. The CMU-Occlu dataset enables future robust motion solving studies.

Conclusion: OpenMoCap provides a robust solution for motion capture under significant occlusions, with practical deployment integration and open-sourced code for community use.

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [92] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES is a wavelet-based visual primitive that achieves high-quality rendering with fast inference by leveraging wavelet spatial-frequency localization, outperforming both INR-based and Gaussian-based representations.


<details>
  <summary>Details</summary>
Motivation: Existing visual representations suffer from spectrum loss or slow rendering due to frequency guidance requirements or complex neural network decoding. There's a need for a representation that offers flexible frequency modulation and fast rendering speed.

Method: Proposes WIPES, a universal wavelet-based visual primitive that captures both low and high-frequency details using wavelet spatial-frequency localization. Also develops a wavelet-based differentiable rasterizer for fast visual rendering.

Result: Experimental results on 2D image representation, 5D static and 6D dynamic novel view synthesis show WIPES offers higher rendering quality and faster inference than INR-based methods, and better rendering quality than Gaussian-based representations.

Conclusion: WIPES demonstrates that wavelet-based visual primitives can effectively address the limitations of existing representations, providing superior rendering quality with fast inference across various visual tasks.

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [93] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: A novel explainable creative assessment system using MLLMs that selects superior ad images by comparative reasoning and user interest consideration, achieving strong results through CoT-SFT and GRPO training.


<details>
  <summary>Details</summary>
Motivation: Advertisers can generate large quantities of creative images with AIGC but lack methods to assess quality and make explainable selections, while existing methods only focus on ranking without explanation.

Method: Proposes Creative4U system using multimodal LLMs, constructs CreativePair dataset with 8k annotated image pairs, employs Chain-of-Thought supervised fine-tuning and Group Relative Policy Optimization reinforcement learning.

Result: Both offline and online experiments demonstrate the effectiveness of the approach in accurately evaluating and selecting creative images.

Conclusion: The first paradigm for explainable creative assessment and selection that successfully integrates assessment and selection into natural language generation tasks using MLLMs.

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [94] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: A novel cloud-edge collaborative paradigm called Context Transfer that uses delayed LVLM outputs as historical context to guide real-time SVLM inference, improving performance while handling cloud latency fluctuations.


<details>
  <summary>Details</summary>
Motivation: Existing cloud-edge collaborative architectures for VLMs fail to accommodate cloud latency fluctuations and overlook the potential of delayed but accurate LVLM responses in real-time applications like autonomous driving and human-computer interaction.

Method: Proposes Context Transfer paradigm that treats delayed LVLM outputs as historical context for SVLM guidance. Designs SpotVLM with context replacement and visual focus modules to refine textual input and enhance visual grounding consistency.

Result: Extensive experiments on three real-time vision tasks across four datasets demonstrate the effectiveness of the proposed framework.

Conclusion: The new paradigm lays the groundwork for more effective and latency-aware collaboration strategies in future VLM systems.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [95] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: Two-stage PMRF pipeline synthesizes contrast-enhanced brain MRI from non-contrast inputs using patch-based 3D U-Net and rectified flow refinement, achieving significant quality improvements while maintaining structural fidelity.


<details>
  <summary>Details</summary>
Motivation: Eliminate need for gadolinium-based contrast agents in MRI which add cost, scan time, environmental concerns, and potential patient risks.

Method: Two-stage approach: 1) Patch-based 3D U-Net predicts voxel-wise posterior mean (MSE minimization), 2) Time-conditioned 3D rectified flow refines initial estimate to incorporate realistic textures while preserving structural accuracy.

Result: Achieved axial FID of 12.46 and KID of 0.007 (68.7% lower FID than posterior mean) with volumetric MSE of 0.057 (27% higher than posterior mean). Successfully restores lesion margins and vascular details realistically.

Conclusion: Method effectively navigates perception-distortion trade-off, producing clinically viable contrast-enhanced MRI synthesis without requiring contrast agents.

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [96] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: A mean teacher framework called BEE that balances exploration (rapid adaptation to new domains) and exploitation (retaining historical knowledge) in Continual Test-Time Adaptation through Multi-level Consistency Regularization and Complementary Anchor Replay.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods struggle with balancing exploration and exploitation - they adjust predictions based on deep-layer outputs which is inefficient for domain shifts affecting shallow features, and single models forget previous domain knowledge during adaptation.

Method: Proposes a mean teacher framework with: 1) Multi-level Consistency Regularization (MCR) loss to align intermediate features between student and teacher models for faster adaptation, and 2) Complementary Anchor Replay (CAR) mechanism to reuse historical checkpoints to recover diverse domain knowledge.

Result: Significantly outperforms state-of-the-art methods on several benchmarks, demonstrating effectiveness for CTTA tasks.

Conclusion: The proposed BEE framework successfully addresses the exploration-exploitation trade-off in CTTA by combining feature-level consistency regularization and historical knowledge replay, achieving superior performance compared to existing approaches.

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [97] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd is a framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from large-scene videos, addressing occlusion challenges through group-guided motion optimization and VAE-based motion priors.


<details>
  <summary>Details</summary>
Motivation: Current 3D crowd reconstruction methods work from static images, lacking temporal consistency and unable to handle occlusions effectively, which is crucial for applications like city surveillance and crowd analysis.

Method: Coarse-to-fine group-guided motion optimization strategy with VAE-based human motion prior and segment-level optimization. Uses collective crowd behavior to handle long-term occlusions, joint optimization of similar motion segments, and Asynchronous Motion Consistency (AMC) loss for robust motion recovery.

Result: Achieves state-of-the-art performance in large-scene dynamic crowd reconstruction. Also contributes VirtualCrowd benchmark dataset for evaluation.

Conclusion: The proposed framework successfully addresses temporal consistency and occlusion challenges in dynamic crowd reconstruction, enabling robust 3D reconstruction of hundreds of individuals from large-scene videos with superior performance.

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [98] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: A two-stage diffusion-based approach for human de-occlusion that first completes occluded body masks using body priors and joint heatmaps, then reconstructs RGB appearance using Stable Diffusion with human-specific textual features and decoder fine-tuning to preserve visible regions.


<details>
  <summary>Details</summary>
Motivation: Humans can infer missing parts of occluded objects using prior knowledge, but deep learning models struggle with accurate occluded region prediction. Human de-occlusion is particularly challenging for recovering both body structure and appearance under occlusion.

Method: Two-stage approach: 1) Mask completion using diffusion-based human body prior and occluded joint heatmaps for spatial cues, 2) RGB completion using Stable Diffusion conditioned on reconstructed mask, enhanced with human-specific textual features from VQA+CLIP, with decoder fine-tuning to prevent pixel degradation in visible areas.

Result: Effectively reconstructs human appearances under severe occlusions, outperforms existing methods in both mask and RGB completion, and improves downstream human-centric tasks like 2D pose estimation and 3D human reconstruction.

Conclusion: The proposed method successfully addresses human de-occlusion by combining structural priors with appearance generation, demonstrating superior performance over existing approaches and practical utility for enhancing human-centric computer vision tasks.

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [99] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: Fine-tuned CLIP model (WP-CLIP) successfully predicts Wölfflin's five principles of art style analysis, addressing limitations of pre-trained models in capturing nuanced stylistic elements.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to effectively predict all five Wölfflin's principles for formal art analysis, and pre-trained vision-language models like CLIP don't inherently capture these nuanced stylistic elements.

Method: Fine-tuned CLIP on annotated datasets of real art images to predict scores for each of Wölfflin's principles, creating WP-CLIP model.

Result: WP-CLIP demonstrates ability to generalize across diverse artistic styles when evaluated on GAN-generated paintings and Pandora-18K art dataset.

Conclusion: Vision-language models show strong potential for automated art analysis and can be effectively adapted to understand complex stylistic principles in visual art.

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [100] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: Vision-G1 is a visual reasoning VLM trained on a comprehensive multi-domain dataset using influence-based data selection and curriculum RL, achieving SOTA performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLMs have limited reasoning generalization due to narrow training focus on mathematical/logical tasks and lack of diverse, verifiable reward data across domains.

Method: Built comprehensive RL-ready dataset from 46 sources across 8 domains, used influence function data selection and difficulty filtering, trained with multi-round RL and data curriculum.

Result: Achieves state-of-the-art performance across various visual reasoning benchmarks, outperforming similar-sized VLMs and proprietary models like GPT-4o and Gemini-1.5 Flash.

Conclusion: The comprehensive multi-domain dataset and curriculum RL approach enable superior visual reasoning generalization, with model and dataset publicly available.

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [101] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV is a novel framework for multi-UAV collaborative 3D detection that learns adaptive instance-aware BEV representations through refine-and-contrast paradigm, achieving superior accuracy-computation trade-offs.


<details>
  <summary>Details</summary>
Motivation: Multi-UAV collaborative 3D detection offers advantages in coverage and occlusion handling but faces computation challenges on resource-constrained UAV platforms, requiring efficient methods that maintain performance.

Method: Introduces Box-Guided Refinement Module (BG-RM) that refines only foreground-associated BEV grids using 2D supervision and spatial subdivision, and Instance-Background Contrastive Learning (IBCL) to enhance feature separation via contrastive learning in BEV space.

Result: Extensive experiments on Air-Co-Pred dataset show AdaBEV achieves superior accuracy-computation trade-offs, outperforming state-of-the-art methods at low resolutions and approaching upper bound performance with negligible overhead.

Conclusion: AdaBEV effectively addresses computational constraints in multi-UAV 3D detection by focusing refinement on foreground instances and enhancing feature discriminability, maintaining high performance with low-resolution inputs.

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [102] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME improves test-time adaptation for driving scenes by using source domain augmentation, domain discrimination, and multiple detector fusion with NMS, achieving significant performance gains on SHIFT Benchmark.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of dynamic domain shifts in real-world driving scenes, particularly weather and day-night transitions, where models need to adapt continuously during test time.

Method: Leverages source domain data augmentation into target domains, introduces domain discriminator and specialized domain detector to handle drastic shifts, trains multiple detectors and consolidates predictions using Non-Maximum Suppression.

Result: Empirical validation shows significant performance enhancements on the SHIFT Benchmark, demonstrating effectiveness in handling dynamic domain changes.

Conclusion: TTA-DAME provides an effective solution for test-time adaptation in driving scenarios, successfully addressing frequent domain shifts through augmented training and multi-detector fusion approaches.

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [103] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: Proposes multi-level knowledge distillation and dynamic self-supervised learning to improve class-incremental learning with repetition using unlabeled external data.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with repetition (CIR) is more realistic than traditional setups but requires handling repeated classes and leveraging abundant unlabeled data from external sources.

Method: Uses multi-level knowledge distillation (MLKD) to preserve knowledge from previous models across features and logits, and dynamic self-supervised loss (SSL) to accelerate new class learning while maintaining task focus.

Result: Significantly improves performance in CIR setup, achieving 2nd place in CVPR 5th CLVISION Challenge.

Conclusion: The proposed components effectively utilize unlabeled data to ensure model stability and plasticity in class-incremental learning with repetition scenarios.

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [104] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: The paper investigates cross-sensor domain gap in autonomous vehicles, introduces CamShift dataset to simulate sensor differences between vehicle types, shows BEVFormer is most robust architecture, and proposes neural rendering-based sensor adaptation pipeline to mitigate performance degradation.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles have varying camera sensor setups due to different vehicle types, causing cross-sensor domain gap that degrades perception model accuracy when trained on one setup and evaluated on another.

Method: Created CamShift dataset in CARLA simulating sensor differences between subcompact vehicles and SUVs. Evaluated state-of-the-art 3D object detectors, identified BEVFormer as most robust, and proposed neural rendering-based sensor adaptation pipeline to transform datasets between different camera setups.

Result: Significant cross-sensor performance degradation observed. BEVFormer with dense Bird's Eye View representation and backward projection showed best robustness. The proposed sensor adaptation pipeline improved performance across all detectors, mitigating domain gap by large margin.

Conclusion: Cross-sensor domain gap is a critical issue in autonomous driving. BEV-based architectures are most robust, and neural rendering-based sensor adaptation enables efficient data reusability across different vehicle sensor setups without requiring new data collection.

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [105] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI-driven news diversity causes multi-level drift that significantly degrades LVLM-based misinformation detection systems, with performance dropping 14.8% on average and reasoning becoming unstable.


<details>
  <summary>Details</summary>
Motivation: The proliferation of multimodal misinformation and rise of GenAI tools create highly varied content that challenges current detection systems, requiring systematic study of these vulnerabilities.

Method: Introduce DriftBench benchmark with 16,000 news instances across six diversification categories, and evaluate six state-of-the-art LVLM detectors on three tasks: robustness to drift, susceptibility to adversarial evidence, and reasoning consistency analysis.

Result: Experiments show substantial performance drops (average F1 -14.8%), increasingly unstable reasoning traces, and severe failures under adversarial evidence injection, revealing fundamental vulnerabilities.

Conclusion: Current MMD systems have critical vulnerabilities to GenAI-driven diversity, indicating an urgent need for more resilient approaches in the GenAI era.

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [106] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: Real-time sign language translation system using CNN to convert gestures to text and speech, enabling communication for hearing/speech impaired individuals.


<details>
  <summary>Details</summary>
Motivation: Address communication barriers for individuals with hearing and speech impairments by creating an accessible assistive technology solution.

Method: Uses convolution neural networks (CNN) trained on Sign Language MNIST dataset to classify hand gestures captured via webcam in real-time, with text-to-speech synthesis for output.

Result: High model accuracy and robust real-time performance demonstrated, though with some latency. System proves practical and reliable for real-world use.

Conclusion: The system serves as an accessible, user-friendly tool that enhances autonomy and social integration for sign language users across diverse settings.

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [107] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: Dual Contrastive Denoising Score framework for real image editing using text-to-image diffusion models with dual contrastive loss to preserve structure while enabling flexible content modification.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with real image editing due to difficulty in creating perfect text prompts and unwanted alterations in regions that should remain unchanged.

Method: Leverages self-attention layers in latent diffusion models with dual contrastive loss inspired by contrastive learning, without requiring auxiliary networks or additional training.

Result: Outperforms existing methods in real image editing while maintaining structure preservation and enabling zero-shot image-to-image translation.

Conclusion: The proposed framework effectively addresses real image editing challenges by combining generative priors with contrastive learning principles for better structure preservation and content modification.

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [108] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 3DGS suffers from appearance artifacts in sparse-view settings due to Gaussian co-adaptation. Proposed CA metric quantifies this issue and two lightweight plug-and-play solutions (random dropout and opacity noise) effectively mitigate the problem.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting performs well in dense-view novel view synthesis but shows appearance artifacts in sparse-view scenarios, indicating a fundamental limitation in current approaches where Gaussians become overly entangled.

Method: Proposed Co-Adaptation Score (CA) metric to quantify Gaussian entanglement by measuring pixel-wise variance across renderings with different random Gaussian subsets. Introduced two strategies: random Gaussian dropout and multiplicative opacity noise injection.

Result: Analysis shows co-adaptation decreases as training views increase. Both proposed strategies effectively mitigate appearance artifacts in sparse-view 3DGS across various methods and benchmarks.

Conclusion: The co-adaptation effect is a core limitation in sparse-view 3DGS. The proposed lightweight plug-and-play solutions successfully address this issue, and understanding this phenomenon can lead to better sparse-view 3DGS performance.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [109] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: FDIKP network uses frequency-domain features and dual-branch inverse kernel prediction to improve single image defocus deblurring, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with severely blurry regions where local high-frequency details are missing, limiting their kernel estimation accuracy for defocus deblurring.

Method: Proposes Frequency-Driven Inverse Kernel Prediction (FDIKP) network with Dual-Branch Inverse Kernel Prediction strategy, Position Adaptive Convolution, and Dual-Domain Scale Recurrent Module for progressive deblurring.

Result: Extensive experiments show the method outperforms existing approaches in single image defocus deblurring.

Conclusion: Incorporating frequency-domain representations enhances structural identifiability in kernel modeling, leading to superior defocus deblurring performance.

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [110] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: Proposes DCSCR network combining traditional and deep learning methods for few-shot image set classification, learning both frame-level and concept-level features with adaptive similarity measurement.


<details>
  <summary>Details</summary>
Motivation: Existing methods either use raw pixel features (traditional) or fail to adaptively adjust features during distance measurement (deep learning), limiting performance in few-shot scenarios.

Method: DCSCR network with three modules: fully convolutional deep feature extractor, global feature learning, and class-specific collaborative representation-based metric learning with new contrastive loss function.

Result: Extensive experiments on well-known few-shot ISC datasets demonstrate effectiveness compared to state-of-the-art algorithms.

Conclusion: The proposed DCSCR approach successfully addresses limitations of existing methods by simultaneously learning multi-level feature representations and adaptive similarity measurements for improved few-shot image set classification.

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [111] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: A novel Mamba-based network with dual-scale fusion and dual-path scanning for shadow removal, leveraging non-shadow regions as guidance and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Shadow removal requires different transformations for shadowed vs well-lit regions, making uniform correction strategies ineffective. The method aims to effectively integrate non-local contextual cues and adaptively model region-specific transformations.

Method: Proposes Dual-Scale Fusion Mamba Block (DFMB) for multi-scale feature fusion to reduce boundary artifacts, and Dual-Path Mamba Group (DPMG) with horizontal scanning and mask-aware adaptive scanning for global feature capture and fine-grained region modeling.

Result: Experimental results show the method significantly outperforms existing state-of-the-art approaches on shadow removal benchmarks.

Conclusion: The proposed Mamba-based network with dual-scale fusion and dual-path scanning effectively addresses the challenges of shadow removal by selectively propagating contextual information based on transformation similarity across regions.

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [112] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA is a deep learning framework that classifies image quality in DSA series for stroke treatment, improving downstream segmentation performance from 42% to 69% success rate.


<details>
  <summary>Details</summary>
Motivation: Poor image quality in digital subtraction angiography (DSA) during mechanical thrombectomy for acute ischemic stroke degrades computer vision model performance, requiring automated quality assessment tools.

Method: Uses pre-trained ResNet backbone models fine-tuned to predict nine image properties (contrast presence, projection angle, motion artifacts, etc.) on 1,758 annotated fluoroscopic MinIPs with separate classifiers for each property.

Result: Achieved excellent performance with ROC-AUC 0.91-0.98 and precision 0.70-1.00 across all labels. Filtering poor quality images increased segmentation success rate from 42% to 69% (p < 0.001).

Conclusion: CLAIRE-DSA shows strong potential as an automated tool for image quality classification in stroke DSA series, supporting clinical and research applications with available source code.

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [113] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: Proposes ICAF framework for semi-supervised semantic segmentation of CdZnTe semiconductor images with many-to-one view relationships, achieving 70.6% mIoU with only 0.5% group-annotated data.


<details>
  <summary>Details</summary>
Motivation: CdZnTe semiconductor image labeling is challenging due to low-contrast defect boundaries requiring multi-view annotation. Existing semi-supervised methods are suboptimal because they assume one-to-one image-GT relationships rather than the many-to-one reality, leading to error accumulation in low-contrast regions.

Method: Intra-group Consistency Augmentation Framework (ICAF) with Intra-group View Sampling (IVS) and Pseudo-label Correction Network (PCN). PCN includes View Augmentation Module (VAM) for boundary-aware view synthesis and View Correction Module (VCM) for information interaction between views to emphasize salient regions and reduce noise.

Result: Achieved 70.6% mIoU on CdZnTe dataset using DeepLabV3+ with ResNet-101 backbone, requiring only 2 group-annotated data samples (0.5% of data).

Conclusion: The group-oriented perspective and ICAF framework effectively address the many-to-one annotation challenge in CdZnTe semiconductor imaging, significantly improving segmentation performance with minimal annotated data by leveraging inherent consistency constraints within view groups.

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [114] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack is a novel UAV-based multi-object tracking framework that addresses challenges like small target variations, occlusions, and motion blur in complex urban traffic environments through specialized detection, adaptive filtering, group motion modeling, and spatio-temporal memory prediction.


<details>
  <summary>Details</summary>
Motivation: UAV-based multi-object tracking faces significant challenges in complex urban environments including small target scale variations, occlusions, nonlinear crossing motions, and motion blur, which hinder tracking stability and accuracy.

Method: Proposes SocialTrack framework with: 1) specialized small-target detector with multi-scale feature enhancement, 2) Velocity Adaptive Cubature Kalman Filter for trajectory prediction, 3) Group Motion Compensation Strategy for social group modeling, and 4) Spatio-Temporal Memory Prediction for historical trajectory utilization.

Result: Extensive experiments on UAVDT and MOT17 datasets show SocialTrack outperforms state-of-the-art methods, with significant improvements in MOTA and IDF1 metrics, demonstrating superior robustness and adaptability.

Conclusion: SocialTrack provides an effective solution for UAV-based multi-object tracking in complex urban environments, offering modular and compatible design that can be integrated with existing trackers to enhance performance.

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [115] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: A novel image style transfer method using multiple style images with image prompt adapters and statistical feature alignment during denoising to prevent content leakage and improve style matching accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion models for style transfer struggle with accurate style matching, limited style image usage, and content-style entanglement issues.

Method: Leverages multiple style images with image prompt adapters and statistical alignment of features during denoising, intervening at both cross-attention and self-attention layers of the UNet, using clustering to distill representative attention features.

Result: Achieves state-of-the-art results for stylization as demonstrated in experimental evaluations.

Conclusion: The proposed approach effectively addresses key limitations in current style transfer methods by better representing style features and preventing content leakage through multi-image style representation and statistical feature alignment.

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [116] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: Using deep learning on Google Street View images to estimate global cycling and motorcycling levels with YOLOv4 model achieving 89% detection accuracy and strong correlation with actual mode shares.


<details>
  <summary>Details</summary>
Motivation: Transportation impacts health through physical activity, pollution, and injury risks, but comparative global data on cycling and motorcycling behaviors is scarce. Street view imagery combined with computer vision offers efficient data collection.

Method: Used YOLOv4 model fine-tuned on images from 6 cities to detect cycles and motorcycles in 8000 GSV images per city across 185 global cities. Developed beta regression model with city-level mode shares as outcome and GSV-detected counts as predictors, controlling for population density.

Result: Strong correlation between GSV motorcycle counts and mode share (0.78), moderate correlation for cycling (0.51). Beta regression achieved R² of 0.614 for cycling and 0.612 for motorcycling with median absolute errors of 1.3% and 1.4% respectively.

Conclusion: Computer vision on GSV images effectively captures travel modes and activity, providing valuable insights alongside traditional data sources for global transportation behavior analysis.

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [117] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: Computer vision models (ResNet50 and vision transformers) successfully classify eclipsing binaries into detached/overcontact types with >96% accuracy using polar coordinate hexbin visualization of light curves, but perform poorly on automated spot detection.


<details>
  <summary>Details</summary>
Motivation: To apply computer vision methods for automated classification of eclipsing binary light curves in large-scale astronomical surveys, addressing the need for efficient morphological classification.

Method: Used pre-trained CNN (ResNet50) and vision transformer models fine-tuned on synthetic datasets. Developed novel polar coordinate hexbin visualization of phase-folded light curves. Implemented hierarchical classification: first stage for detached/overcontact types, second stage for spot detection.

Result: High accuracy (>96%) on validation data across multiple passbands (Gaia G, I, TESS). Strong performance (>94%, up to 100% for TESS) on observational data from OGLE, DEBCat, and WUMaCat catalogues. Poor performance on automated spot detection.

Conclusion: Computer vision shows great potential for eclipsing binary morphological classification in large surveys, but automated spot detection requires further research due to poor performance on subtle photometric features.

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [118] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: NVG framework generates images through structured visual granularity sequences, progressively refining from global layout to fine details, achieving better FID scores than VAR models on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To enable fine-grained control over image generation by decomposing images into hierarchical visual granularity sequences that capture different levels of detail.

Method: Next Visual Granularity (NVG) framework that generates images iteratively from empty images, progressing from coarse to fine details using structured sequences with varying token granularity.

Result: NVG models consistently outperform VAR series on ImageNet (FID scores: 3.30->3.03, 2.57->2.44, 2.09->2.06) and show clear scaling behavior.

Conclusion: The NVG framework provides hierarchical control over image generation and demonstrates superior performance compared to existing methods, with potential for further exploration.

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [119] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: Overview of the CVPR 2025 Spatio-temporal Instance Segmentation challenge using event and grayscale camera data for pixel-level object segmentation


<details>
  <summary>Details</summary>
Motivation: To advance research in spatio-temporal instance segmentation by combining event camera and grayscale camera data for more accurate object segmentation

Method: Challenge-based evaluation with multiple teams developing segmentation methods using spatio-temporally aligned event and grayscale camera data

Result: Overview of top-5 ranking teams' methods and challenge results provided, with detailed resources available on GitHub

Conclusion: The challenge successfully advanced spatio-temporal instance segmentation techniques using multi-modal event and grayscale camera data

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [120] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA is a deep learning model that enhances underwater images by preserving both low- and high-frequency information and spatial structures to address visual degradation challenges in marine monitoring.


<details>
  <summary>Details</summary>
Motivation: Underwater environments suffer from light scattering, absorption, and turbidity that degrade image clarity and color information, making accurate marine biodiversity monitoring and ecological assessment difficult.

Method: Proposes a Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator that adaptively refines feature representations in frequency domains while preserving spatial information for better structural consistency.

Result: Comprehensive experiments on EUVP and LSUI datasets demonstrate superiority over state-of-the-art methods in restoring fine-grained image detail and structural consistency.

Conclusion: DEEP-SEA effectively mitigates underwater visual degradation and has the potential to improve reliability of underwater monitoring platforms for ecological observation, species identification, and autonomous navigation.

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [121] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: Winning approach for multimodal deception detection challenge using progressive domain adaptation to handle domain shift across diverse audio-visual datasets


<details>
  <summary>Details</summary>
Motivation: Address the domain shift issue across source and target domains in multimodal deception detection, where knowledge from diverse source domains needs to be effectively transferred to the target domain

Method: Multi-source Multimodal Progressive Domain Adaptation (MMPDA) framework that gradually aligns source and target domains at both feature and decision levels to bridge domain shifts

Result: Achieved Top-2 place in competition with 60.43% accuracy and 56.99% F1-score on stage 2, surpassing 1st place by 5.59% on F1-score and 3rd place by 6.75% on accuracy

Conclusion: The proposed MMPDA framework effectively handles domain shifts in multimodal deception detection, demonstrating strong performance in cross-domain adaptation and securing competitive results in the challenge

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [122] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: CoMuCo is a novel fine-tuning strategy for vision-language models that uses multi-view expert modules with consistency constraints to improve performance on cross-domain few-shot tasks where standard methods struggle.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models like CLIP perform well on standard image datasets but show limited effectiveness when dealing with cross-domain tasks where imaging domains differ from natural images.

Method: Consistency-guided Multi-view Collaborative Optimization (CoMuCo) employs two functionally complementary expert modules to extract multi-view features, incorporating prior knowledge-based consistency constraints and information geometry-based consensus mechanisms.

Result: Extensive evaluations show CoMuCo consistently outperforms current methods in few-shot tasks on both existing and newly proposed cross-domain benchmarks.

Conclusion: CoMuCo effectively addresses the cross-domain limitation of VLMs and establishes a new benchmark for comprehensive evaluation of methods on imaging domains distinct from natural images.

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [123] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: MPS-Tuning is a novel fine-tuning method that preserves the geometric structure of data distribution in vision-language models by constraining intrinsic manifold geometry and enhancing class separability through Gram matrix alignment and pairwise similarity optimization.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning methods neglect the geometric structure of data distribution, leading to semantic representation distortion. The authors aim to overcome this limitation by explicitly preserving the intrinsic geometry of the semantic manifold.

Method: MPS-Tuning treats data distribution as a semantic manifold and preserves both macroscopic and microscopic topological structures by aligning Gram matrices of features before and after fine-tuning. It also optimizes pairwise similarities between image and text modalities to enhance class discriminability.

Result: Extensive experiments demonstrate that MPS-Tuning significantly improves model performance while effectively preserving the structure of the semantic manifold.

Conclusion: The proposed MPS-Tuning method successfully addresses the limitation of existing VLM fine-tuning approaches by explicitly constraining manifold geometry, leading to improved performance and better preservation of semantic structure.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [124] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S^2-Guidance improves diffusion model quality by using stochastic sub-networks to refine suboptimal predictions from Classifier-free Guidance, addressing semantic incoherence issues.


<details>
  <summary>Details</summary>
Motivation: Classifier-free Guidance (CFG) produces suboptimal results that lead to semantic incoherence and low-quality outputs in diffusion models, despite being widely used.

Method: Proposes S^2-Guidance which uses stochastic block-dropping during forward process to create stochastic sub-networks that refine the model's predictions away from low-quality outputs.

Result: Extensive experiments on text-to-image and text-to-video generation show S^2-Guidance consistently outperforms CFG and other advanced guidance strategies.

Conclusion: S^2-Guidance effectively addresses CFG's limitations by leveraging stochastic sub-networks to guide models toward higher quality outputs with better semantic coherence.

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [125] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG is a one-shot pruning method using NMF for initial weight selection and gradient masking to maintain sparsity during training, achieving comparable or better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks face deployment challenges due to large size, and existing pruning methods often involve complex iterative processes or struggle to maintain sparsity effectively during training.

Method: ONG uses Non-negative Matrix Factorization (NMF) for one-shot pruning at training start, then employs gradient masking to ensure only unpruned weights are updated, strictly preserving target sparsity throughout training.

Result: Experiments on CIFAR-10/100 with ResNet architectures show ONG achieves comparable or superior performance at various sparsity levels while maintaining structural integrity post-pruning.

Conclusion: ONG provides an effective one-shot pruning approach with clear sparsity targeting mechanism and maintains performance comparable to established stable sparsification methods.

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [126] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow is a 0.5B latent flow matching transformer model that generates entire 3D CT volumes conditioned on clinical reports, achieving superior performance in temporal coherence, image diversity, and text-image alignment compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To accelerate medical research through data augmentation, enable privacy-preserving synthesis of patient data, reduce regulatory constraints on patient data usage, and preserve diagnostic signals while generating synthetic CT volumes.

Method: Uses a 0.5B latent flow matching transformer conditioned on clinical reports. Leverages A-VAE from FLUX for latent space definition and CT-Clip text encoder for report encoding. Employs custom autoregressive approach where the model predicts sequences of slices iteratively, using previously generated slices and text to predict subsequent sequences.

Result: Demonstrates superiority over state-of-the-art generative CT models in terms of temporal coherence, image diversity, and text-image alignment, as measured by FID, FVD, IS scores, and CLIP score.

Conclusion: CTFlow represents a significant advancement in text-conditioned 3D CT volume generation, enabling high-quality synthetic data creation that preserves diagnostic information while addressing privacy and regulatory concerns in medical imaging research.

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [127] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU is a multi-stage cross-modal fusion framework for 3D detection that effectively integrates LiDAR and camera data through depth completion, bilateral backbone encoding, and iterative refinement with IoU-aware proposal generation.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal 3D detection methods often use single or partial stage fusion, leading to insufficient feature extraction and suboptimal performance. The challenge lies in effectively aligning 3D spatial information from LiDAR with 2D semantic information from cameras.

Method: 1) Depth completion network projects pixel information to 3D space to create pseudo points; 2) Bilateral cross-view enhancement 3D backbone with S2D branch (encoder-decoder for sparse LiDAR) and ResVC branch (3D/2D convolution for inaccurate pseudo points); 3) Iterative voxel-point aware fine-grained pooling; 4) IoU joint prediction branch with novel proposals generation.

Result: Extensive experiments demonstrate superior performance on KITTI, nuScenes and Waymo datasets, showing the framework's effectiveness in multi-modal 3D detection.

Conclusion: CMF-IOU provides an effective multi-stage fusion approach that successfully addresses the alignment challenge between 3D spatial and 2D semantic information, achieving state-of-the-art performance across major autonomous driving datasets.

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [128] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench is the first benchmark that jointly evaluates both semantic and spatial alignment in layout-guided text-to-image generation, addressing the gap in existing benchmarks that only assess text alignment.


<details>
  <summary>Details</summary>
Motivation: Layout-guided text-to-image models provide better control but lack proper evaluation of spatial alignment, which is crucial for applications like synthetic data generation where spatial errors can degrade data quality.

Method: The benchmark features text-and-layout pairs across seven challenging scenarios and proposes an evaluation protocol that incorporates layout alignment score alongside existing text alignment metrics to assess spatial accuracy.

Result: The benchmark was used to evaluate several state-of-the-art diffusion models, revealing their respective strengths and limitations across diverse alignment tasks including object generation, color fidelity, attribute recognition, and spatial control.

Conclusion: 7Bench fills a critical gap in evaluating layout-guided text-to-image models by providing comprehensive assessment of both semantic and spatial alignment, enabling better understanding of model capabilities for real-world applications.

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [129] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD is a dual-branch framework for high-resolution anomaly detection that addresses the limitations of conventional methods by integrating multi-scale anomaly cues and adaptive detector assignment to handle varying anomaly sizes efficiently.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods struggle with high-resolution images due to information loss from downsampling and poor performance of lightweight networks or simple tiling approaches in industrial scenarios.

Method: Uses dual-branch architecture to integrate anomaly cues across different scales, multi-resolution feature fusion for fine-grained texture variations, and a detector pool with adaptive assignment strategies based on patch features.

Result: Superior performance demonstrated on high-resolution benchmarks including MVTec-HD, VisA-HD, and RealIAD-HD, showing effectiveness in detecting both subtle and large-scale anomalies.

Conclusion: HiAD provides an efficient and accurate solution for high-resolution anomaly detection that meets practical industrial demands while controlling computational costs.

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [130] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG is a two-stage ViT framework that sequentially improves encoder and decoder generality through feature boosting and knowledge distillation to mitigate catastrophic forgetting in incremental learning, especially in small-memory scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on enhancing either encoder or decoder but not both, limiting effectiveness against catastrophic forgetting, particularly in small-memory settings where few historical samples can be stored.

Method: Two-stage training: 1) Train ensembled encoder via feature boosting to learn generalized representations that enhance decoder and balance classifier, 2) Use balanced KD and feature KD to compress ensembled encoder into a new more generalized encoder.

Result: Extensive experiments on three benchmark datasets show superior performance compared to existing methods, with ablation studies confirming the effectiveness of all components.

Conclusion: SEDEG effectively addresses catastrophic forgetting in incremental learning by sequentially improving both encoder and decoder generality, demonstrating strong performance even in challenging small-memory scenarios.

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [131] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: Automated U-Net framework for fiber bundle segmentation in macaque tracer data with improved sparse bundle detection and reduced false discovery rates.


<details>
  <summary>Details</summary>
Motivation: Large-scale analysis of anatomic tracer studies is limited by labor-intensive manual annotation of fiber bundles on histological slides, and existing automated methods often miss sparse bundles or require complex post-processing.

Method: U-Net architecture with large patch sizes, foreground aware sampling, and semisupervised pre-training for fully automated fiber bundle segmentation.

Result: Eliminates mislabeling errors, improves sparse bundle detection by over 20%, reduces False Discovery Rate by 40% compared to state-of-the-art, and enables standalone slice analysis.

Conclusion: This framework facilitates automated large-scale analysis of anatomic tracing data, generating more ground-truth data for validating and optimizing dMRI tractography methods.

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [132] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen is an end-to-end video relighting framework that uses large-scale video generative models to replace backgrounds and adjust foreground lighting based on textual descriptions, achieving consistent cinematic results with strict foreground preservation.


<details>
  <summary>Details</summary>
Motivation: Video relighting is challenging but valuable for creating harmonious lighting between foreground and background in videos. Current methods struggle with preserving foreground properties like albedo and maintaining temporal consistency across frames.

Method: Developed an end-to-end framework using large-scale video generative models with textual lighting control. Created a mixed dataset of realistic and synthetic videos using 3D rendering and HDR-based lighting simulation. Implemented joint training with domain-aware adapter to decouple relighting learning from domain appearance distribution.

Result: Experimental results show Lumen effectively edits input videos into cinematic relighted videos with consistent lighting and strict foreground preservation, outperforming existing methods in comprehensive benchmark evaluations.

Conclusion: Lumen successfully addresses video relighting challenges by leveraging large-scale generative models and a carefully constructed mixed dataset, achieving high-quality results with temporal consistency and foreground property preservation through domain-aware training approach.

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [133] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem introduces semantic-guided masking and hybrid high-order motion reconstruction for self-supervised skeleton-based action recognition, improving performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised methods for skeleton-based action recognition focus on limited joints and low-order motion patterns, which restricts their ability to understand complex human motions needed for human-robot collaboration.

Method: Proposes MaskSem framework that uses Grad-CAM based on relative motion to guide joint masking of semantically rich temporal regions, and reconstructs hybrid high-order motion targets (velocity + acceleration) to learn multi-order motion patterns.

Result: Experiments on NTU60, NTU120, and PKU-MMD datasets show improved skeleton-based action recognition performance when combined with a vanilla transformer.

Conclusion: MaskSem enhances the understanding of complex motion patterns through semantic-guided masking and hybrid high-order motion reconstruction, making it more suitable for human-robot interaction applications.

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [134] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed is a novel RL framework for open-ended medical VQA that addresses reward collapse in semantic rewards, achieving significant improvements in accuracy and generalization across medical benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement fine-tuning approaches in medical imaging primarily target closed-ended VQA, limiting real-world clinical applicability. Open-ended medical VQA better reflects clinical practice but suffers from reward collapse where semantically different responses receive similar scores.

Method: ARMed first incorporates domain knowledge through supervised fine-tuning on chain-of-thought data, then applies reinforcement learning with textual correctness and adaptive semantic rewards to enhance reasoning quality.

Result: ARMed achieved a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain benchmarks across six challenging medical VQA benchmarks.

Conclusion: The results highlight the critical role of reward discriminability in medical RL and demonstrate the promise of semantically guided rewards for enabling robust and clinically meaningful multimodal reasoning.

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [135] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: Deep learning pipeline using 3D SegResNet architecture for automated multi-class tooth segmentation in dental CBCT scans, achieving 0.87 Dice score on validation data.


<details>
  <summary>Details</summary>
Motivation: Automated segmentation of dental structures in CBCT can assist in pathology identification and facilitate radiation therapy planning for head and neck cancer patients.

Method: Used MONAI Auto3DSeg framework with 3D SegResNet, trained on 63 CBCT scans with 5-fold cross-validation. Two-phase approach: ensemble fusion for initial segmentation, then tight cropping around mandible for nerve structure segmentation.

Result: Achieved average Dice score of 0.87 on the ToothFairy3 challenge out-of-sample validation set.

Conclusion: The approach demonstrates effective automated dental segmentation that can improve patient care in radiation oncology through efficient 3D structure identification.

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [136] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR is a novel end-to-end architecture with two disentangled decoders that separately handle human head localization and gaze prediction, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end gaze detection models use a single decoder that creates entangled representations for both head localization and gaze prediction, which limits performance. Disentangling these tasks could improve accuracy.

Method: Proposed GazeDETR architecture with two separate decoders - one for human head prediction (using local information) and another for gaze prediction (using both local and global information) with coherent attentive fields for each subtask.

Result: Achieves state-of-the-art results on GazeFollow, VideoAttentionTarget and ChildPlay datasets, outperforming existing end-to-end models by a notable margin.

Conclusion: Disentangling head localization and gaze prediction into separate decoders with specialized attention mechanisms significantly improves gaze communication quantification performance compared to unified multitask approaches.

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [137] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: Compact Attention is a hardware-aware acceleration framework that uses adaptive tiling and temporally varying windows to exploit structured sparsity in video diffusion transformers, achieving 1.6-2.5x speedup while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Self-attention mechanisms in transformer-based video generation are computationally expensive, especially for ultra-long sequences. Current sparse attention methods fail to fully exploit the inherent spatio-temporal redundancies in video data and often impose rigid constraints or introduce significant overhead.

Method: Proposes Compact Attention with three innovations: 1) Adaptive tiling strategies for dynamic tile grouping to approximate diverse spatial interaction patterns, 2) Temporally varying windows that adjust sparsity levels based on frame proximity, and 3) An automated configuration search algorithm that optimizes sparse patterns while preserving critical attention pathways.

Result: Achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines.

Conclusion: Provides a principled approach to unlocking efficient long-form video generation through structured sparsity exploitation, demonstrating significant computational savings without compromising output quality.

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [138] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: A zero-shot NAS method using SVD and extrinsic curvature to predict network performance without labeled data, achieving superior correlation and efficiency across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot NAS proxies require labeled data and focus either on convergence/generalization or expressivity alone, limiting real-world applicability.

Method: Proposes a zero-cost proxy combining convergence, generalization and expressivity using SVD of layer features and extrinsic curvature of network output, computed with a single unlabeled sample.

Result: Superior performance on NAS-Bench-101, NAS-Bench-201, TransNAS-Bench-101-micro, and NAS tasks in DARTS and AutoFormer search spaces, while being highly efficient.

Conclusion: The method successfully addresses limitations of existing proxies by eliminating labeled data requirement and comprehensively optimizing for convergence, generalization and expressivity.

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [139] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: A comprehensive survey of multi-modal visual object tracking (MMVOT) covering data collection, modality alignment, model design, evaluation, and benchmarking across six MMVOT tasks with 338 references.


<details>
  <summary>Details</summary>
Motivation: The rapid development of smart cities has generated massive multi-modal data, creating a need to understand and advance multi-modal visual object tracking for comprehensive infrastructure monitoring.

Method: The paper categorizes existing MMVOT methods based on how they handle visible (RGB) and auxiliary modalities (thermal, depth, event, NIR, language, sonar), analyzing data collection challenges, modality alignment, annotation approaches, and evaluation frameworks.

Result: The survey reveals that MMVOT datasets exhibit pronounced long-tail distributions and lack animal categories compared to RGB datasets, and addresses whether multi-modal tracking always provides superior performance over unimodal tracking.

Conclusion: This comprehensive analysis provides foundational insights into all aspects of MMVOT, highlighting current challenges in data distribution and modality integration while establishing benchmarks for future research in multi-modal visual tracking.

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [140] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: Feature diversity improves open set recognition and continual learning performance by enhancing novel class detection and knowledge retention.


<details>
  <summary>Details</summary>
Motivation: To empirically investigate the role of feature diversity in addressing open set recognition (detecting novel classes) and continual learning (updating models with new classes), as most existing approaches use feature diversity heuristically without direct examination.

Method: Empirical analysis and experiments demonstrating the relationship between enhanced feature diversity and improved performance in both open set recognition and continual learning tasks.

Result: Enhanced feature diversity improves recognition of open set samples and facilitates better retention of previously learned data while integrating new data in continual learning settings.

Conclusion: Feature diversity plays a crucial role in both open set recognition and continual learning, and these findings should inspire further research into practical methods and theoretical understanding in these domains.

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [141] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm reduces communication bandwidth by 90% for collaborative perception using 4D radar Doppler and query-driven sparse feature sharing, maintaining accuracy while overcoming occlusion.


<details>
  <summary>Details</summary>
Motivation: Transmitting dense Bird's-Eye-View feature maps in collaborative autonomous vehicle systems overwhelms communication bandwidth, requiring more efficient methods.

Method: Integrates 4D radar Doppler to build motion-centric dynamic maps, uses reference queries for dynamic/high-confidence regions and exploratory queries for occluded areas, exchanges only query-specific features via multi-scale gated deformable attention.

Result: Achieves up to 90% lower bandwidth than full-map sharing while matching or surpassing prior baselines across varied traffic densities and occlusions.

Conclusion: SlimComm provides a communication-efficient framework that maintains perception accuracy while significantly reducing bandwidth requirements for collaborative autonomous vehicles.

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [142] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0 is a real-time interactive world model that generates minute-long videos at 25 FPS using few-step auto-regressive diffusion, addressing the speed limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing interactive world models suffer from slow inference due to bidirectional attention and lengthy steps, making real-time simulation of dynamic environments impossible.

Method: Three components: scalable data pipeline producing 1200+ hours of annotated video from Unreal Engine/GTA5, action injection module for frame-level inputs, and few-step distillation for real-time streaming generation.

Result: Achieves high-quality minute-level video generation across diverse scenes at 25 FPS, enabling real-time interactive simulations.

Conclusion: The framework advances interactive world modeling by enabling real-time performance while maintaining quality, with open-sourced weights and codebase.

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [143] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: EgoTwin is a novel framework for joint egocentric video and human motion generation that addresses viewpoint alignment and causal interplay challenges using a diffusion transformer architecture with head-centric motion representation and cybernetics-inspired interaction.


<details>
  <summary>Details</summary>
Motivation: Egocentric video generation remains underexplored compared to exocentric video synthesis, requiring modeling of first-person view content with camera motion patterns from the wearer's body movements.

Method: Proposes EgoTwin framework built on diffusion transformer architecture with head-centric motion representation (anchoring motion to head joint) and cybernetics-inspired interaction mechanism that captures causal interplay between video and motion within attention operations.

Result: Extensive experiments demonstrate the effectiveness of the EgoTwin framework, evaluated on a large-scale real-world dataset of synchronized text-video-motion triplets with novel metrics for video-motion consistency.

Conclusion: The proposed EgoTwin framework successfully addresses the challenges of viewpoint alignment and causal interplay in joint egocentric video and human motion generation, representing a significant advancement in this underexplored research area.

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [144] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR is a hierarchical feature adaptation framework for cardiac MRI reconstruction that addresses multi-center domain shifts through parameter-efficient adapters at protocol and center levels, with a universal adapter for unseen centers.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based cardiac MRI reconstruction faces significant domain shift challenges when deployed across multiple clinical centers with heterogeneous scanner configurations and imaging protocols.

Method: Uses hierarchical adapters: Protocol-Level Adapters for sequence-specific characteristics, Center-Level Adapters for scanner-dependent variations, and Universal Adapter for generalization to unseen centers. Built on variational unrolling backbone with multi-scale SSIM loss, frequency domain enhancement, and contrast-adaptive weighting.

Result: Comprehensive evaluation on CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9 modalities demonstrates superior cross-center generalization while maintaining reconstruction quality.

Conclusion: HierAdaptMR effectively addresses multi-level domain variations in cardiac MRI reconstruction through parameter-efficient hierarchical adaptation, enabling robust performance across diverse clinical centers and scanner configurations.

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [145] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: A novel situated visualization technique that guides users during scene scanning by identifying important objects needing extended image coverage through semantic segmentation and vision-language models, improving view synthesis quality.


<details>
  <summary>Details</summary>
Motivation: High-quality view synthesis requires uniform and dense view sampling, but human camera operators often struggle with this due to impatience, lack of scene understanding, or time constraints. Existing guidance methods focus on single objects or ignore view-dependent material characteristics.

Method: Leverages semantic segmentation and category identification ranked by a vision-language model to identify important objects requiring extended image coverage. Generates spherical proxies around highly ranked objects to guide users during scanning at multiple scales.

Result: The method shows superior performance in real scenes compared to conventional view sampling strategies, enabling better representation of view-dependent appearance.

Conclusion: The proposed situated visualization technique effectively addresses the challenge of guiding human operators during image acquisition for high-quality novel view synthesis, particularly for capturing view-dependent material characteristics across multiple scales.

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [146] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: Odo is a diffusion-based method for realistic human body shape editing that preserves identity, clothing, and background while transforming body shapes using semantic attributes and SMPL depth guidance.


<details>
  <summary>Details</summary>
Motivation: Human shape editing remains underexplored compared to pose editing, with current methods suffering from unrealistic proportions, texture distortions, and background inconsistencies due to lack of proper datasets and alignment errors.

Method: End-to-end diffusion-based approach combining a frozen UNet to preserve appearance/background details with a ControlNet guided by target SMPL depth maps for shape transformation, trained on a new large-scale dataset of 18,573 images across 1,523 subjects.

Result: Achieves per-vertex reconstruction error of 7.5mm (significantly lower than baseline 13.6mm), produces realistic results that accurately match target shapes while preserving fine-grained details.

Conclusion: The proposed Odo method with the new dataset enables realistic and intuitive body reshaping, outperforming prior approaches in both quantitative metrics and visual quality.

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [147] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: Two-stage multimodal framework using radiologist eye-tracking data to improve chest X-ray disease classification and generate region-aware radiology reports, achieving significant performance gains in both tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance disease classification and radiology report generation by incorporating radiologist eye-tracking data (gaze information) to better capture clinical expertise and attention patterns in medical image analysis.

Method: Two-stage approach: 1) Gaze-guided contrastive learning for disease classification using visual features, clinical labels, bounding boxes, and eye-tracking signals with multi-term gaze-attention loss; 2) Modular report generation pipeline extracting confidence-weighted keywords, mapping to anatomical regions, and generating region-aligned sentences.

Result: Gaze integration improved F1 score from 0.597 to 0.631 (+5.70%) and AUC from 0.821 to 0.849 (+3.41%), with enhanced precision and recall. Report generation showed improved quality in clinical keyword recall and ROUGE overlap metrics.

Conclusion: Incorporating radiologist gaze data significantly improves both disease classification performance and the interpretability/quality of generated medical reports, demonstrating the value of eye-tracking signals in medical AI systems.

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [148] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: Using Stable Diffusion to generate synthetic bona fide ID card images improves Presentation Attack Detection system performance by addressing data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Current PAD systems face challenges due to limited availability of genuine ID card images for training and increasing diversity of attack methods. Most existing approaches focus on generating attack samples but neglect the scarcity of bona fide images.

Method: Proposes using Stable Diffusion to generate synthetic versions of bona fide ID card images, creating additional training data to enhance detector generalization capabilities.

Result: The synthetic images are successfully identified as bona fide by both a system trained from scratch and a commercial PAD solution, leading to improved detection performance and addressing data restriction limitations.

Conclusion: Synthetic image generation using Stable Diffusion is an effective approach to overcome data scarcity in PAD systems, demonstrating positive impact on detection capabilities while working within data constraints.

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [149] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: A novel Chessboard dataset with 3.1M questions and balanced distribution addresses RSVQA biases, enabling fine-grained visual reasoning through cell-level answer grounding.


<details>
  <summary>Details</summary>
Motivation: Current RSVQA models lack interpretability and suffer from dataset biases leading to shortcut learning, requiring more transparent and trustworthy decision-making systems.

Method: Developed Checkmate model that identifies relevant image cells for decisions, built on the Chessboard dataset with 3,123,253 questions where each answer is linked to specific image cells.

Result: Extensive experiments across multiple architectures show improved transparency and more trustworthy decision-making in RSVQA systems.

Conclusion: The approach successfully addresses interpretability and bias issues in RSVQA through fine-grained visual reasoning and cell-level explainability, supporting more reliable remote sensing question answering.

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [150] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: DMS uses diffusion models to synthesize novel views for self-supervised stereo matching and depth estimation, addressing occlusion issues without requiring labels.


<details>
  <summary>Details</summary>
Motivation: Self-supervised stereo matching and depth estimation suffer from photometric ambiguity in occluded regions where corresponding pixels are missing in the target view.

Method: Finetune Stable Diffusion to synthesize novel views along epipolar direction (left-left, right-right, and center views) using directional prompts to supplement occluded pixels for explicit photometric reconstruction.

Result: 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets using only unlabeled stereo image pairs.

Conclusion: DMS is an effective plug-and-play method that enhances self-supervised stereo and depth estimation by leveraging diffusion models to address occlusion problems.

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [151] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: RT-DETR-L model provides better balance of speed and accuracy for real-time beach litter detection compared to RT-DETR-X, despite slightly lower accuracy.


<details>
  <summary>Details</summary>
Motivation: Coastal pollution requires scalable automated monitoring solutions, necessitating evaluation of state-of-the-art object detection models for beach litter detection.

Method: Comparative analysis of two RT-DETR variants (Large and Extra-Large) trained on coastal debris dataset, evaluating accuracy (mAP metrics) and inference speed.

Result: RT-DETR-X achieved slightly higher accuracy (mAP@50: 0.816, mAP@50-95: 0.612) but RT-DETR-L was significantly faster (20.1ms vs 34.5ms inference time).

Conclusion: RT-DETR-L offers more practical real-time deployment due to better speed-accuracy trade-off, providing insights for Transformer-based environmental conservation applications.

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [152] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: Visual action prompts using skeletons provide precise cross-domain action representation for video generation, balancing geometric precision with transferable dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing action representations for video generation face a precision-generality trade-off - text/masks lack precision while agent-centric signals lack cross-domain transferability.

Method: Render actions into visual skeletons as domain-agnostic representations, construct pipelines from human-object interactions and robotic manipulation data, integrate into pretrained video models via lightweight fine-tuning.

Result: Effective action control of complex interactions while preserving cross-domain dynamics, demonstrated on EgoVid, RT-1 and DROID datasets.

Conclusion: Visual skeletons provide a unified action representation that enables precise action-driven video generation with cross-domain transferability for complex high-DoF interactions.

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [153] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion is a training-free framework that transfers animations between characters with different skeletal topologies using sparse bone correspondences and minimal target examples.


<details>
  <summary>Details</summary>
Motivation: Existing motion retargeting techniques struggle with characters that have substantially different skeletal topologies due to lack of one-to-one bone correspondences and limited paired motion datasets.

Method: Uses a novel training-free framework that works with only one or few example motions on the target skeleton, accessing sparse bone correspondences between source and target skeletons without requiring large datasets.

Result: Achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios, with successful integration in downstream applications and user interfaces.

Conclusion: Motion2Motion provides a practical solution for industrial applications, demonstrating potential for animation transfer across diverse skeletal topologies without extensive training data requirements.

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [154] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5 shows unprecedented spatial intelligence but still falls short of human performance across various spatial reasoning tasks, with proprietary models not having decisive advantages on the most difficult problems.


<details>
  <summary>Details</summary>
Motivation: Multi-modal models have limitations in spatial understanding and reasoning, which are fundamental for artificial general intelligence. With GPT-5's release, it's timely to evaluate leading models' spatial capabilities.

Method: Proposed a comprehensive taxonomy of spatial tasks unifying existing benchmarks, then evaluated state-of-the-art proprietary and open-source models on eight key benchmarks using over one billion total tokens, plus qualitative evaluation on human-intuitive scenarios.

Result: GPT-5 demonstrates unprecedented spatial intelligence strength but still underperforms humans across broad spatial tasks. Identified challenging spatial problems where proprietary models don't show decisive advantages over open-source alternatives.

Conclusion: While GPT-5 represents significant progress in spatial intelligence, multi-modal models still struggle with spatial reasoning that humans find intuitive, indicating substantial room for improvement in achieving human-level spatial understanding.

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse is a novel framework that reconstructs interactive 3D Gaussian scenes by fusing multiple scans with object rearrangement to reveal occluded regions, using segmentation-aware Gaussian fields and bi-directional consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene reconstruction methods struggle with object occlusions and limited sensor coverage, requiring complex multi-stage pipelines or dense per-object scanning that are error-prone and not scalable.

Method: Constructs segmentation-aware Gaussian fields, enforces bi-directional photometric and semantic consistency across scans, uses pseudo-intermediate scene state for unified alignment, and implements collaborative co-pruning strategies to refine geometry.

Result: Enables high-fidelity rendering and object-level scene manipulation without dense observations or complex pipelines, with strong generalization to novel scene configurations.

Conclusion: IGFuse demonstrates effectiveness for real-world 3D reconstruction and real-to-simulation transfer, providing a scalable solution for complete and interactive 3D scene reconstruction.

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX is the first feed-forward framework for generating 4D (dynamic 3D) scene representations from a single image, using a pretrained video diffusion model fine-tuned with novel adaptation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 4D scene generation rely on computationally intensive optimization or require multi-frame video inputs, creating a need for efficient, end-to-end image-to-4D generation from single images.

Method: Fine-tunes pretrained video diffusion models using: 1) 4DNeX-10M dataset with high-quality 4D annotations, 2) unified 6D video representation for RGB+XYZ sequences, and 3) effective adaptation strategies for 4D modeling.

Result: Produces high-quality dynamic point clouds enabling novel-view video synthesis, outperforms existing methods in efficiency and generalizability.

Conclusion: 4DNeX offers a scalable solution for image-to-4D modeling and lays foundation for generative 4D world models that simulate dynamic scene evolution.

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>
