<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: Analysis of positive sampling Kantorovich operators (SK operators) and their convergence properties, including local/global approximation using SK, Gaussian, Bilateral, and wavelet-based operators with error metrics evaluation.


<details>
  <summary>Details</summary>
Motivation: To study the approximation properties of various sampling Kantorovich operators and analyze their performance through mathematical error metrics and image processing applications.

Method: Introduced basic terminology and fundamental theorem of approximation, measured error using MSE, SI, SSI, SMPI, ENL metrics at various resolution levels, conducted numerical examples including 2D Shepp-Logan Phantom analysis.

Result: Different operators have varying significance for different image features due to the uneven nature of images (non-ideal conditions), with some operators performing well for specific features while others do not.

Conclusion: The study justifies the fundamental theorem of approximation and demonstrates that operator selection should be feature-specific as no single operator performs optimally for all image characteristics under non-ideal conditions.

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [2] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: Federated learning framework for skeleton-based human activity recognition that outperforms centralized training while preserving privacy in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Need for accurate real-time worker action recognition in smart manufacturing while addressing privacy concerns that make centralized data collection impractical.

Method: Uses federated learning with two temporal backbones (LSTM and Transformer) trained under four paradigms: centralized, local per-client, FL with FedAvg, and federated ensemble learning on a custom dataset of industrial upper-body gestures.

Result: FL Transformer improved over centralized training by +12.4 percentage points, with FedEnsemble achieving +16.3 points gain on global test set. On unseen external client, FL exceeded centralized accuracy by +52.6 to +58.3 percentage points.

Conclusion: Federated learning not only preserves privacy but significantly enhances cross-user generalization, making it a practical solution for scalable, privacy-aware HAR in industrial environments.

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [3] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: LENS introduces reinforcement learning to optimize chain-of-thought reasoning for text-prompted image segmentation, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing supervised fine-tuning methods ignore explicit chain-of-thought reasoning at test time, limiting generalization to unseen prompts and domains.

Method: Scalable reinforcement-learning framework that jointly optimizes reasoning process and segmentation with unified rewards spanning sentence-, box-, and segment-level cues.

Result: Achieves 81.2% average cIoU on RefCOCO benchmarks, outperforming GLaMM by up to 5.6% using Qwen2.5-VL-3B-Instruct model.

Conclusion: RL-driven CoT reasoning serves as robust prior for text-prompted segmentation and offers practical path toward more generalizable Segment Anything models.

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [4] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC is a compact video multimodal LLM for embodied cognition that achieves SOTA performance in object understanding, segmentation, and spatial reasoning through region-level video interaction.


<details>
  <summary>Details</summary>
Motivation: To develop a general-purpose cognitive core for embodied agents that provides fine-grained perception of the physical world and enables precise interactions, addressing the scarcity of annotated 3D datasets.

Method: Built on a vision-language foundation model, incorporates region encoder and mask decoder for flexible region-level video interaction. Uses egocentric video pipeline for generating embodied cognition data and introduces RynnEC-Bench benchmark.

Result: Achieves state-of-the-art performance in object property understanding, object segmentation, and spatial reasoning despite compact architecture.

Conclusion: RynnEC offers a region-centric video paradigm for embodied agents and is expected to advance development of general-purpose cognitive cores and facilitate generalization across diverse embodied tasks.

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [5] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: A deep equilibrium canonicalizer (DEC) method to improve local scale equivariance in computer vision models, addressing scale variation challenges where objects of the same class can have different sizes and perceived sizes vary with camera distance.


<details>
  <summary>Details</summary>
Motivation: Scale variation is a fundamental challenge in computer vision where objects of the same class can have different sizes, and their perceived size is affected by camera distance. These variations are local to objects, meaning different object sizes may change differently within the same image.

Method: Proposes a deep equilibrium canonicalizer (DEC) to improve local scale equivariance of models. DEC can be easily incorporated into existing network architectures and adapted to pre-trained models.

Result: On the competitive ImageNet benchmark, DEC improves both model performance and local scale consistency across four popular pre-trained deep-nets: ViT, DeiT, Swin, and BEiT.

Conclusion: DEC effectively handles scale variations by improving local scale equivariance, demonstrating performance improvements across multiple state-of-the-art vision transformer architectures on ImageNet.

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [6] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: CLIPSym leverages CLIP's vision-language capabilities with a novel equivariant decoder and semantic-aware prompting to detect rotation and reflection symmetries, outperforming state-of-the-art methods on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Symmetry is a fundamental geometric cue in computer vision, and recent advances in vision-language models like CLIP offer potential to improve symmetry detection by leveraging semantic cues from natural image descriptions.

Method: Proposes CLIPSym with CLIP's image/language encoders and a rotation-equivariant decoder combining Transformer and G-Convolution. Introduces Semantic-Aware Prompt Grouping (SAPG) to aggregate object-based prompts for better semantic integration.

Result: Outperforms current state-of-the-art on three standard symmetry detection datasets (DENDI, SDRW, and LDRS). Ablation studies confirm benefits of CLIP pre-training, equivariant decoder, and SAPG technique.

Conclusion: CLIPSym effectively leverages vision-language pre-training for symmetry detection, demonstrating that semantic cues from natural language descriptions can significantly enhance geometric symmetry detection performance.

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [7] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: A comprehensive survey paper on Video Anomaly Detection (VAD) that systematically organizes literature across supervision levels, adaptive learning methods, and three major application categories, aiming to provide a structured foundation for advancing VAD systems.


<details>
  <summary>Details</summary>
Motivation: Video Anomaly Detection is a pivotal computer vision task with broad relevance, but the field remains fragmented across domains and learning paradigms. The authors aim to consolidate insights and provide a comprehensive perspective to advance both theoretical understanding and real-world applicability.

Method: Systematic organization of VAD literature across various supervision levels (fully supervised, weakly supervised, unsupervised), adaptive learning methods (online, active, continual learning), and three major application categories: human-centric, vehicle-centric, and environment-centric scenarios.

Result: The survey identifies fundamental contributions and limitations of current VAD methodologies, providing a structured foundation for researchers and drawing attention to open challenges in both fundamental research questions and practical deployment obstacles.

Conclusion: This comprehensive survey serves as a useful reference for the research community, supporting advancement in VAD by consolidating fragmented knowledge and highlighting broader open challenges in anomaly detection for real-world applications.

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [8] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: Novel image classification framework combining Graph Convolutional Networks with Voronoi diagrams, achieving faster preprocessing and higher accuracy than state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To leverage GCNs' capability for modeling relational data in image classification, moving beyond conventional CNNs by using graph-based representations of images.

Method: Uses graph-based image representation where pixels/regions are vertices, simplified via Delaunay triangulations. Introduces normalized Voronoi Graph Convolution Network (NVGCN) for faster processing.

Result: Significant improvement in preprocessing time and classification accuracy on benchmark datasets, especially for complex scenes and fine-grained categories. Validated via cross-validation.

Conclusion: Integration of GCNs with Voronoi diagrams advances image classification and opens new avenues for graph-based learning in computer vision and unstructured data domains.

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [9] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: A new learning mechanism for large multimodal models that improves robustness and alignment between visual and textual modalities through shuffling reconstruction tasks and a directed-token approach.


<details>
  <summary>Details</summary>
Motivation: Large multimodal models suffer from fundamental limitations in robustness and generalization due to alignment issues between visual and textual features.

Method: Introduces two new tasks: reconstructing image order and text order during pre-training and fine-tuning. Proposes a directed-token approach to capture visual/textual knowledge and an Image-to-Response Guided loss for better visual understanding.

Result: The approach consistently achieves state-of-the-art performance on academic task-oriented and instruction-following LMM benchmarks.

Conclusion: The proposed shuffling-based learning mechanism effectively improves reasoning capability, visual understanding, and cross-modality alignment in large multimodal models.

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [10] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: Data augmentation strategies significantly impact conformal prediction performance for diabetic retinopathy grading, with Mixup and CutMix improving both accuracy and uncertainty reliability while CLAHE can negatively affect model certainty.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for medical tasks like diabetic retinopathy grading need reliable uncertainty quantification for clinical deployment. Current models lack robust uncertainty estimates, and the interaction between standard training practices (like data augmentation) and conformal prediction guarantees is not well understood.

Method: Systematic evaluation of five data augmentation strategies (no augmentation, geometric transforms, CLAHE, Mixup, CutMix) on two backbone architectures (ResNet-50 and CoaT) using the DDR dataset for diabetic retinopathy grading. Analyzed conformal prediction metrics including empirical coverage, average prediction set size, and correct efficiency.

Result: Sample-mixing strategies (Mixup and CutMix) improved both predictive accuracy and yielded more reliable and efficient uncertainty estimates. Conversely, CLAHE negatively impacted model certainty. The choice of augmentation significantly affects conformal prediction performance.

Conclusion: Augmentation strategies must be co-designed with downstream uncertainty quantification to build trustworthy AI systems for medical imaging. Mixup and CutMix are particularly effective for improving both accuracy and uncertainty reliability in diabetic retinopathy grading.

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [11] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: A novel conditional diffusion framework for 3D dental CBCT scan generation that enables precise control over tooth presence and configuration using tooth-level binary attributes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating anatomically realistic dental CBCT scans with fine-grained control for diagnosis and treatment planning, overcoming limitations in current medical image synthesis approaches.

Method: Integrates wavelet-based denoising diffusion, FiLM conditioning, and masked loss functions to focus learning on relevant anatomical structures. Uses tooth-level binary attributes for precise control over tooth presence and configuration.

Result: Strong fidelity and generalization with low FID scores, robust inpainting performance, and SSIM values above 0.91 even on unseen scans. Successfully demonstrated for tooth addition, removal, and full dentition synthesis tasks.

Conclusion: Enables realistic, localized modification of dentition without rescanning, opening opportunities for surgical planning, patient communication, and targeted data augmentation in dental AI workflows.

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [12] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA is a novel framework for open-vocabulary 3D scene understanding using 3D Gaussian Splatting that distills scene-specific 3D instance features and introduces cross-attention with learnable codebooks for efficient language-aware 3D representations.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene reconstruction methods struggle to capture fine-grained, language-aware 3D representations from 2D images, creating a need for better open-vocabulary 3D understanding.

Method: GALA uses 3D Gaussian Splatting with self-supervised contrastive learning to distill scene-specific 3D instance features. It introduces a cross-attention module with two learnable codebooks that encode view-independent semantic embeddings, avoiding per-Gaussian high-dimensional feature learning.

Result: Extensive experiments on real-world datasets demonstrate GALA's remarkable open-vocabulary performance on both 2D and 3D tasks while reducing memory consumption.

Conclusion: GALA provides an effective framework for open-vocabulary 3D scene understanding that achieves strong performance while being memory-efficient through its innovative cross-attention and codebook design.

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [13] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: A new benchmark and framework for multi-rationale explainable object recognition using CLIP, achieving state-of-the-art results without training.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from weak conditioning on explanatory structures and limited single/noisy rationales that don't capture full feature diversity.

Method: Contrastive Conditional Inference (CCI) framework that models probabilistic relationships among image embeddings, category labels, and rationales without requiring training.

Result: Achieves state-of-the-art results on multi-rationale benchmark with strong zero-shot performance, setting new standards for classification accuracy and rationale quality.

Conclusion: Provides a more complete framework for evaluating explainable object recognition models and enables effective rationale conditioning for accurate category prediction.

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [14] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: OccluNet is a spatio-temporal deep learning model that combines YOLOX object detection with transformer-based temporal attention to automate vascular occlusion detection in DSA sequences for acute ischemic stroke treatment.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of vascular occlusions during endovascular thrombectomy is critical but challenging due to anatomical complexity and time constraints in interpreting DSA sequences.

Method: Proposes OccluNet integrating YOLOX object detector with transformer-based temporal attention mechanisms. Compared with YOLOv11 baseline on individual frames or minimum intensity projections. Explored two variants: pure temporal attention and divided space-time attention.

Result: Achieved precision of 89.02% and recall of 74.87% on DSA images from MR CLEAN Registry. Significantly outperformed baseline models, with both attention variants attaining similar performance.

Conclusion: OccluNet demonstrates capability to capture temporally consistent features for automated occlusion detection in DSA sequences, offering potential to assist in time-critical stroke treatment procedures.

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [15] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: Pixels2Play-0.1 is a foundation model that learns to play 3D video games from pixel input with human-like behavior, using behavior cloning with both labeled human demonstrations and unlabeled public videos.


<details>
  <summary>Details</summary>
Motivation: To create AI agents that can serve as teammates, controllable NPCs, personalized streamers, and assistive testers by learning from the same pixel stream available to human players and generalizing to new games with minimal engineering.

Method: End-to-end behavior cloning using labeled human gameplay demonstrations and unlabeled public videos with action imputation via inverse-dynamics model. Uses decoder-only transformer with auto-regressive action output for large action space handling on consumer GPUs.

Result: Qualitative results show competent play across simple Roblox and classic MS-DOS titles. Ablations demonstrate the value of unlabeled data. The model works on single consumer GPU with low latency.

Conclusion: The approach shows promise for building general game-playing agents, with scaling and evaluation steps outlined to reach expert-level, text-conditioned control in future versions.

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [16] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: A unified diffusion transformer model for generating multi-modal multi-view driving videos (RGB, depth, semantic maps) in autonomous driving scenes, addressing the limitation of existing RGB-only generation approaches.


<details>
  <summary>Details</summary>
Motivation: Existing video generation methods for autonomous driving focus only on RGB video generation and lack multi-modal capability, while multi-modal data like depth maps and semantic maps are crucial for holistic urban scene understanding. Using multiple separate models increases deployment difficulty and misses opportunities for leveraging complementary cues.

Method: Proposes a unified diffusion transformer model with modal-shared components and modal-specific components. Uses diverse conditioning inputs to encode controllable scene structure and content cues for multi-modal multi-view video generation in a single framework.

Result: Experiments on nuScenes dataset show the approach can generate high-fidelity and controllable multi-modal multi-view urban scene videos, surpassing state-of-the-art methods.

Conclusion: The proposed unified framework successfully addresses the multi-modal video generation challenge for autonomous driving, enabling generation of RGB, depth, and semantic maps simultaneously with improved fidelity and controllability compared to existing methods.

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [17] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: Proposes inter-class relational loss to improve small object detection by leveraging spatial relationships between objects, achieving significant mAP improvements on license plate detection.


<details>
  <summary>Details</summary>
Motivation: IoU-based losses fail to properly update gradients for small objects due to flat gradients, causing insufficient learning during multi-object training.

Method: Uses inter-class spatial relationships (e.g., car plate attached to car) to guide small object predictions. Adds loss punishment when predicted small object is not within its related larger object, inversely proportional to their overlapped area.

Result: Achieved 10.3% and 1.6% mAP50 improvements for YOLOv12-T and UAV-DETR respectively without hyperparameter tuning. Also created new SVMLP dataset for license plate detection.

Conclusion: The proposed ICR loss can be easily integrated with existing IoU-based losses to enhance small object detection performance by leveraging inter-class spatial relationships.

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [18] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: A lightweight sign generation model using CMLPe with synthetic data pretraining improves sign language recognition accuracy, achieving SOTA results on LSFB and DiSPLaY datasets.


<details>
  <summary>Details</summary>
Motivation: Sign Language Recognition models suffer from performance limitations due to insufficient training data availability, which this work aims to address.

Method: Novel lightweight sign generation model based on CMLPe coupled with synthetic data pretraining approach, tested with Mamba-SL and Transformer-SL classifiers.

Result: Consistently improves recognition accuracy, establishes new state-of-the-art results for LSFB and DiSPLaY datasets, and outperforms traditional augmentation methods in some cases.

Conclusion: Synthetic data pretraining democratizes sign generation for SLR through computationally efficient methods that achieve significant performance improvements across diverse datasets.

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [19] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: New microscopy dataset and baseline model for automated detection of Taxol effects on cells using deep learning with attention mechanisms and k-NN classification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for monitoring Taxol effects require specialized equipment and are labor-intensive, lacking public datasets for automated morphological analysis of cellular responses to Taxol.

Method: Created a new microscopy image dataset of C6 glioma cells treated with varying Taxol concentrations. Proposed ResAttention-KNN model combining ResNet-50 with Convolutional Block Attention Modules and k-Nearest Neighbors classifier in the learned embedding space.

Result: Developed a publicly available dataset and baseline model that integrates attention-based refinement and non-parametric classification for enhanced robustness and interpretability in Taxol concentration classification.

Conclusion: The dataset and implementation are publicly released to support reproducibility and facilitate future research in vision-based biomedical analysis of chemotherapeutic agent effects.

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [20] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: HRC-Pose is a depth-only framework for category-level object pose estimation that uses contrastive learning to learn continuous pose representations, outperforming state-of-the-art methods on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely solely on 6D pose supervision without capturing pose continuity, leading to prediction inconsistencies and poor generalization to unseen poses.

Method: Decouples pose into rotation and translation components, uses contrastive learning with hierarchical ranking scheme to learn continuous representations, and processes rotation-aware and translation-aware embeddings separately.

Result: Outperforms existing depth-only state-of-the-art methods on REAL275 and CAMERA25 benchmarks, learns continuous feature spaces, and runs in real-time.

Conclusion: HRC-Pose effectively addresses pose continuity issues in category-level object pose estimation and demonstrates strong performance for real-world applications.

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [21] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: A novel method for emotion-controllable talking face generation that uses pre-training strategies to disentangle audio components and quantize videos into visual tokens, then employs emotion-anchor representations and an autoregressive transformer to synthesize emotional videos.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of effectively modeling multimodal relationships for specific emotions and leveraging these relationships to synthesize identity-preserving emotional talking face videos from audio inputs.

Method: Uses two pre-training strategies: disentangling audio into independent components and quantizing videos into visual tokens. Introduces emotion-anchor representations to integrate emotional information into visual tokens. Employs an autoregressive transformer to model global distribution of visual tokens and predict index sequences for video synthesis.

Result: Extensive experiments on the MEAD dataset demonstrate superior performance both qualitatively and quantitatively in emotion-controllable talking face generation.

Conclusion: The proposed method effectively tackles emotion-controllable talking face generation by discretely modeling multimodal relationships and synthesizing high-quality emotional videos while preserving identity.

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [22] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: A generalized multi-object tracking framework that handles multiple object types with focus on vehicle tracking, featuring occlusion-aware re-ID and road-structure-aware refinement, achieving strong performance on both new vehicle benchmarks and conventional MOT datasets.


<details>
  <summary>Details</summary>
Motivation: Conventional MOT systems are designed primarily for pedestrian tracking and lack generalization to other object categories like vehicles in complex traffic scenes.

Method: Proposes two key components: (1) occlusion-aware re-identification mechanism for identity preservation of occluded objects, and (2) road-structure-aware tracklet refinement using semantic scene priors (lane directions, crosswalks, road boundaries) to improve trajectory accuracy.

Result: Achieves robust performance on new vehicle benchmark and public benchmarks, with HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Also introduces a new benchmark dataset for vehicle-focused tracking evaluation.

Conclusion: The framework demonstrates effectiveness in general-purpose object tracking while maintaining strong performance on conventional benchmarks, providing a generalized solution beyond pedestrian-only tracking systems.

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [23] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: TCFNet is a Transformer-based coarse-to-fine network for accurate face-bone point cloud transformations in surgical simulation, addressing limitations of traditional biomechanical and existing deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional biomechanical simulation methods are computationally slow, labor-intensive, and inaccurate. Recent deep learning approaches have issues with large-scale point processing, limited receptive fields causing noise, and complex preprocessing/postprocessing requirements.

Method: Two-stage end-to-end framework: 1) Transformer-based network for global feature extraction, 2) Local Information Aggregation Network (LIA-Net) for local geometric modeling. Uses gated recurrent unit to guide local displacement with global features, and includes auxiliary loss inspired by deformable medical image registration.

Result: TCFNet achieves outstanding evaluation metrics and visualization results compared to state-of-the-art methods on gathered datasets, demonstrating superior performance in face-bone point cloud transformation.

Conclusion: The proposed TCFNet effectively addresses limitations of existing methods by combining Transformer architecture with local geometric modeling, providing an accurate and efficient solution for computer-aided surgical simulation without complex preprocessing/postprocessing.

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [24] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: QuadINR introduces hardware-efficient implicit neural representations using piecewise quadratic activation functions to reduce spectral bias while dramatically cutting hardware overhead compared to previous complex activation approaches.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to address spectral bias in Implicit Neural Representations used complex activation functions that incurred significant hardware overhead, creating a need for more hardware-efficient solutions.

Method: QuadINR utilizes piecewise quadratic activation functions that provide rich harmonic content for enhanced expressivity. The method includes a unified N-stage pipeline framework for efficient hardware implementation and demonstrates both FPGA and ASIC implementations.

Result: QuadINR achieves up to 2.06dB PSNR improvement over prior work with only 1914μm² area and 6.14mW dynamic power, reducing resource consumption by 97%, power by 97%, and improving latency by 93% compared to existing baselines.

Conclusion: QuadINR successfully addresses the hardware efficiency problem in INRs while maintaining superior performance, making it a practical solution for real-world applications requiring efficient neural representations of signals.

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [25] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: Img2ST-Net is a novel framework that uses fully convolutional architecture to generate high-resolution spatial transcriptomics data from histology images in parallel, overcoming computational challenges of conventional spot-by-spot methods.


<details>
  <summary>Details</summary>
Motivation: High-resolution spatial transcriptomics (ST) data acquisition is expensive and time-consuming. Conventional sequential regression frameworks become inefficient and unstable at high resolutions (8um or finer), and the extreme sparsity of high-resolution ST data complicates both prediction and evaluation.

Method: Proposes Img2ST-Net with fully convolutional architecture to generate dense HD gene expression maps in parallel. Reformulates the task as super-content image generation with hundreds/thousands of output channels by modeling HD ST data as super-pixel representations. Also introduces SSIM-ST, a structural-similarity-based evaluation metric for high-resolution ST analysis.

Result: The framework improves computational efficiency while better preserving spatial organization intrinsic to spatial omics data. It provides a scalable, biologically coherent solution for efficient and accurate ST inference at scale.

Conclusion: Img2ST-Net offers a principled solution for next-generation ST modeling that is robust and resolution-aware, laying groundwork for efficient high-resolution spatial transcriptomics prediction from routine histology images.

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [26] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: CTA-Flux is a Chinese text adapter that enables the Flux text-to-image model to better understand Chinese prompts while maintaining compatibility with existing plugins, using a parameter-efficient MMDiT approach.


<details>
  <summary>Details</summary>
Motivation: Flux model performs poorly with non-English prompts due to linguistic and cultural biases in English-centric training data. Existing translation or finetuning methods fail to preserve culturally specific semantics, compromising image authenticity.

Method: Leverages MultiModal Diffusion Transformer (MMDiT) to directly control the Flux backbone, significantly reducing parameters while enhancing Chinese semantic understanding. Avoids extensive retraining while maintaining plugin compatibility.

Result: Empirical evaluations show CTA-Flux supports both Chinese and English prompts, achieving superior image generation quality, visual realism, and faithful depiction of Chinese semantics compared to existing approaches.

Conclusion: CTA-Flux effectively bridges Chinese semantic understanding with English-centric TTI models, improving cultural authenticity and generation quality without massive parameter scaling or full model retraining.

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [27] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: MoCHA-former is a novel transformer-based model that effectively removes moiré patterns from camera-captured screen content by addressing spatially varying artifacts, large-scale structures, channel dependencies, and temporal fluctuations through decoupled adaptive demoiréing and spatio-temporal processing.


<details>
  <summary>Details</summary>
Motivation: Existing demoiréing methods struggle with four key limitations: spatially varying artifact strength, large-scale globally spreading structures, channel-dependent statistics in RAW frames, and rapid temporal fluctuations across video frames, which degrade the quality of camera-captured screen content.

Method: MoCHA-former uses two main components: Decoupled Moiré Adaptive Demoiréing (DMAD) with Moiré Decoupling Block and Detail Decoupling Block to separate content from artifacts, and Spatio-Temporal Adaptive Demoiréing (STAD) with Spatial Fusion Block and Feature Channel Attention to handle large structures and channel dependencies. It performs implicit frame alignment for temporal consistency without explicit modules.

Result: The method consistently outperforms prior approaches across multiple metrics (PSNR, SSIM, LPIPS) on two video datasets covering both RAW and sRGB domains, demonstrating superior moiré pattern removal and video quality enhancement.

Conclusion: MoCHA-former effectively addresses the complex challenges of moiré pattern removal in screen capture scenarios through its innovative decoupled adaptive approach and spatio-temporal processing, setting new state-of-the-art performance for both image and video demoiréing tasks.

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [28] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: HyperDiff combines diffusion models with HyperGCN for monocular 3D human pose estimation, addressing depth ambiguity and occlusion while capturing multi-scale skeleton features through multi-granularity structures.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in monocular 3D HPE including depth ambiguity, occlusion issues in 2D-to-3D lifting, and the limitation of traditional methods that overlook multi-scale skeleton features.

Method: Integrates diffusion models to capture data uncertainty and HyperGCN as a denoiser that uses multi-granularity structures to model high-order correlations between joints for improved denoising capability.

Result: Achieves state-of-the-art performance on Human3.6M and MPI-INF-3DHP datasets, with flexible adaptation to varying computational resources for balancing performance and efficiency.

Conclusion: HyperDiff effectively addresses key challenges in monocular 3D pose estimation through the novel combination of diffusion models and HyperGCN, demonstrating superior performance and computational flexibility.

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [29] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: FOCUS is a frequency-based conditioning approach using diffusion models to preserve task-relevant knowledge during test-time adaptation, achieving state-of-the-art performance on semantic segmentation and depth estimation across diverse corruptions.


<details>
  <summary>Details</summary>
Motivation: Test-time adaptation methods struggle to balance adapting to domain shifts while preserving task-relevant knowledge, often leading to catastrophic forgetting of important semantic information.

Method: Proposes FOCUS - a diffusion-driven input-adaptation framework with frequency-based conditioning. Uses a Y-shaped Frequency Prediction Network (Y-FPN) to disentangle high/low frequency information from noisy images, trained with FrequencyMix data augmentation for robustness.

Result: Achieves state-of-the-art averaged performance across 15 corruption types and three datasets for semantic segmentation and monocular depth estimation. Also complements existing model adaptation methods by providing pseudo labels from denoised images.

Conclusion: FOCUS effectively mitigates catastrophic forgetting in test-time adaptation by preserving task-relevant semantic information through frequency-based conditioning, while being computationally efficient and compatible with existing adaptation methods.

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


### [30] [MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](https://arxiv.org/abs/2508.14440)
*Fei Peng,Junqiang Wu,Yan Li,Tingting Gao,Di Zhang,Huiyuan Fu*

Main category: cs.CV

TL;DR: MUSE is a unified framework for layout-controllable multi-subject synthesis that uses concatenated cross-attention to integrate spatial layout constraints with textual guidance, achieving precise subject placement and identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with multi-subject compositional synthesis that requires both precise spatial control and faithful subject reconstruction simultaneously.

Method: Proposes concatenated cross-attention (CCA) mechanism for bidirectional modality alignment between spatial constraints and text, plus a progressive two-stage training strategy to decompose the task into learnable sub-objectives.

Result: Achieves zero-shot end-to-end generation with superior spatial accuracy and identity consistency compared to existing solutions.

Conclusion: MUSE advances controllable image synthesis by effectively addressing the dual challenges of spatial precision and identity preservation in multi-subject generation.

Abstract: Existing text-to-image diffusion models have demonstrated remarkable
capabilities in generating high-quality images guided by textual prompts.
However, achieving multi-subject compositional synthesis with precise spatial
control remains a significant challenge. In this work, we address the task of
layout-controllable multi-subject synthesis (LMS), which requires both faithful
reconstruction of reference subjects and their accurate placement in specified
regions within a unified image. While recent advancements have separately
improved layout control and subject synthesis, existing approaches struggle to
simultaneously satisfy the dual requirements of spatial precision and identity
preservation in this composite task. To bridge this gap, we propose MUSE, a
unified synthesis framework that employs concatenated cross-attention (CCA) to
seamlessly integrate layout specifications with textual guidance through
explicit semantic space expansion. The proposed CCA mechanism enables
bidirectional modality alignment between spatial constraints and textual
descriptions without interference. Furthermore, we design a progressive
two-stage training strategy that decomposes the LMS task into learnable
sub-objectives for effective optimization. Extensive experiments demonstrate
that MUSE achieves zero-shot end-to-end generation with superior spatial
accuracy and identity consistency compared to existing solutions, advancing the
frontier of controllable image synthesis. Our code and model are available at
https://github.com/pf0607/MUSE.

</details>


### [31] [Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2508.14443)
*Gyusam Chang,Tuan-Anh Vu,Vivek Alumootil,Harris Song,Deanna Pham,Sangpil Kim,M. Khalid Jawed*

Main category: cs.CV

TL;DR: NIRSplat introduces multimodal 3D Gaussian splatting with NIR imagery and text metadata for agricultural scene reconstruction, outperforming existing methods in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting has limited application in agriculture due to challenges like uneven illumination, occlusions, and limited field of view in agricultural scenes.

Method: Proposes NIRSplat - a multimodal Gaussian splatting architecture using cross-attention mechanism with 3D point-based positional encoding, integrating NIR imagery, RGB, depth, LiDAR, and text metadata from vegetation indices.

Result: Outperforms existing methods including 3DGS, CoR-GS, and InstantSplat in comprehensive experiments on challenging agricultural scenarios.

Conclusion: The integration of NIR data and text-based metadata significantly enhances robustness and provides botanical insights beyond visible spectrum, making it effective for agricultural 3D reconstruction.

Abstract: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR

</details>


### [32] [Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448)
*Yangche Yu,Yin Chen,Jia Li,Peng Jia,Yu Zhang,Li Dai,Zhenzhen Hu,Meng Wang,Richang Hong*

Main category: cs.CV

TL;DR: DAPA is a novel framework for generalizable conversational engagement modeling that uses domain prompting and parallel cross-attention to improve cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Accurate engagement estimation is essential for adaptive human-computer interaction, but current methods suffer from poor generalizability across diverse domains and difficulty modeling complex interaction dynamics.

Method: DAPA introduces Domain Prompting with learnable domain-specific vectors to condition the model on data origin, and a Parallel Cross-Attention module that aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM) states between participants.

Result: DAPA achieves state-of-the-art performance on cross-cultural and cross-linguistic benchmarks, with 0.45 CCC improvement on NoXi-J test set, and won first place in Multi-Domain Engagement Estimation Challenge at MultiMediate'25.

Conclusion: The proposed DAPA framework effectively addresses domain generalization challenges in engagement estimation through explicit domain conditioning and interaction synchrony modeling, demonstrating superior performance across diverse conversational contexts.

Abstract: Accurate engagement estimation is essential for adaptive human-computer
interaction systems, yet robust deployment is hindered by poor generalizability
across diverse domains and challenges in modeling complex interaction
dynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel
Attention), a novel framework for generalizable conversational engagement
modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable
domain-specific vectors to the input, explicitly conditioning the model on the
data's origin to facilitate domain-aware adaptation while preserving
generalizable engagement representations. To capture interactional synchrony,
the framework also incorporates a Parallel Cross-Attention module that
explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)
states between participants.Extensive experiments demonstrate that DAPA
establishes a new state-of-the-art performance on several cross-cultural and
cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45
in Concordance Correlation Coefficient (CCC) over a strong baseline on the
NoXi-J test set. The superiority of our method was also confirmed by winning
the first place in the Multi-Domain Engagement Estimation Challenge at
MultiMediate'25.

</details>


### [33] [D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis](https://arxiv.org/abs/2508.14449)
*Yuhang Guo,Kaijun Deng,Siyang Song,Jindong Xie,Wenhui Ma,Linlin Shen*

Main category: cs.CV

TL;DR: D^3-Talker is a novel 3D talking head synthesis method that uses separate Gaussian attribute deformation fields for audio and facial motion signals, achieving better lip sync and image quality with limited training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with poor lip synchronization and image quality when trained on few frames due to audio containing irrelevant information to lip motion, requiring a solution that can work with limited data.

Method: Constructs static 3D Gaussian attribute field and uses separate deformation fields for audio and facial motion signals with similarity contrastive loss for decoupling, plus coarse-to-fine refinement module.

Result: Outperforms state-of-the-art methods in both high-fidelity rendering and accurate audio-lip synchronization with limited training data.

Conclusion: D^3-Talker effectively decouples general and personalized deformations, enabling high-quality 3D talking head synthesis with minimal training data through innovative field separation and refinement techniques.

Abstract: A key challenge in 3D talking head synthesis lies in the reliance on a
long-duration talking head video to train a new model for each target identity
from scratch. Recent methods have attempted to address this issue by extracting
general features from audio through pre-training models. However, since audio
contains information irrelevant to lip motion, existing approaches typically
struggle to map the given audio to realistic lip behaviors in the target face
when trained on only a few frames, causing poor lip synchronization and talking
head image quality. This paper proposes D^3-Talker, a novel approach that
constructs a static 3D Gaussian attribute field and employs audio and Facial
Motion signals to independently control two distinct Gaussian attribute
deformation fields, effectively decoupling the predictions of general and
personalized deformations. We design a novel similarity contrastive loss
function during pre-training to achieve more thorough decoupling. Furthermore,
we integrate a Coarse-to-Fine module to refine the rendered images, alleviating
blurriness caused by head movements and enhancing overall image quality.
Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art
methods in both high-fidelity rendering and accurate audio-lip synchronization
with limited training data. Our code will be provided upon acceptance.

</details>


### [34] [Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](https://arxiv.org/abs/2508.14461)
*Shanlin Sun,Yifan Wang,Hanwen Zhang,Yifeng Xiong,Qin Ren,Ruogu Fang,Xiaohui Xie,Chenyu You*

Main category: cs.CV

TL;DR: Ouroboros is a framework with two single-step diffusion models that handle forward and inverse rendering with mutual reinforcement, achieving faster inference and cycle consistency across diverse scenes.


<details>
  <summary>Details</summary>
Motivation: Existing multi-step diffusion models treat forward and inverse rendering independently, leading to cycle inconsistency and slow inference speed.

Method: Two single-step diffusion models with mutual reinforcement and cycle consistency mechanism, extending intrinsic decomposition to indoor/outdoor scenes.

Result: State-of-the-art performance across diverse scenes with substantially faster inference speed, and training-free video decomposition capability.

Conclusion: Ouroboros provides an efficient framework for coherent forward and inverse rendering with fast inference and video application potential.

Abstract: While multi-step diffusion models have advanced both forward and inverse
rendering, existing approaches often treat these problems independently,
leading to cycle inconsistency and slow inference speed. In this work, we
present Ouroboros, a framework composed of two single-step diffusion models
that handle forward and inverse rendering with mutual reinforcement. Our
approach extends intrinsic decomposition to both indoor and outdoor scenes and
introduces a cycle consistency mechanism that ensures coherence between forward
and inverse rendering outputs. Experimental results demonstrate
state-of-the-art performance across diverse scenes while achieving
substantially faster inference speed compared to other diffusion-based methods.
We also demonstrate that Ouroboros can transfer to video decomposition in a
training-free manner, reducing temporal inconsistency in video sequences while
maintaining high-quality per-frame inverse rendering.

</details>


### [35] [DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing](https://arxiv.org/abs/2508.14465)
*Weitao Wang,Zichen Wang,Hongdeng Shen,Yulei Lu,Xirui Fan,Suhui Wu,Jun Zhang,Haoqian Wang,Hao Zhang*

Main category: cs.CV

TL;DR: DreamSwapV is a mask-guided framework for subject swapping in videos that uses multiple conditions and adaptive masks to achieve high-fidelity customization, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current video subject swapping methods are either domain-specific or rely on indirect editing approaches that compromise fidelity, creating a need for a more general and effective solution.

Method: Mask-guided end-to-end framework with multiple conditions, dedicated condition fusion module, adaptive mask strategy, and two-phase dataset construction and training scheme.

Result: Outperforms existing methods as validated by comprehensive experiments on VBench indicators and the newly introduced DreamSwapV-Benchmark.

Conclusion: DreamSwapV provides an effective solution for subject-agnostic video swapping with improved fidelity and context integration compared to previous approaches.

Abstract: With the rapid progress of video generation, demand for customized video
editing is surging, where subject swapping constitutes a key component yet
remains under-explored. Prevailing swapping approaches either specialize in
narrow domains--such as human-body animation or hand-object interaction--or
rely on some indirect editing paradigm or ambiguous text prompts that
compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided,
subject-agnostic, end-to-end framework that swaps any subject in any video for
customization with a user-specified mask and reference image. To inject
fine-grained guidance, we introduce multiple conditions and a dedicated
condition fusion module that integrates them efficiently. In addition, an
adaptive mask strategy is designed to accommodate subjects of varying scales
and attributes, further improving interactions between the swapped subject and
its surrounding context. Through our elaborate two-phase dataset construction
and training scheme, our DreamSwapV outperforms existing methods, as validated
by comprehensive experiments on VBench indicators and our first introduced
DreamSwapV-Benchmark.

</details>


### [36] [LookOut: Real-World Humanoid Egocentric Navigation](https://arxiv.org/abs/2508.14466)
*Boxiao Pan,Adam W. Harley,C. Karen Liu,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: Predicting 6D head poses from egocentric video to understand active information-gathering behavior during navigation


<details>
  <summary>Details</summary>
Motivation: Predicting collision-free future trajectories from egocentric observations is crucial for humanoid robotics, VR/AR, and assistive navigation applications

Method: A framework that reasons over temporally aggregated 3D latent features to model geometric and semantic constraints of both static and dynamic environment parts

Result: The model learns human-like navigation behaviors (waiting/slowing down, rerouting, looking around for traffic) and generalizes to unseen environments

Conclusion: The approach successfully predicts future 6D head poses and demonstrates real-world navigation behaviors, with a new dataset (Aria Navigation Dataset) created to support this research area

Abstract: The ability to predict collision-free future trajectories from egocentric
observations is crucial in applications such as humanoid robotics, VR / AR, and
assistive navigation. In this work, we introduce the challenging problem of
predicting a sequence of future 6D head poses from an egocentric video. In
particular, we predict both head translations and rotations to learn the active
information-gathering behavior expressed through head-turning events. To solve
this task, we propose a framework that reasons over temporally aggregated 3D
latent features, which models the geometric and semantic constraints for both
the static and dynamic parts of the environment. Motivated by the lack of
training data in this space, we further contribute a data collection pipeline
using the Project Aria glasses, and present a dataset collected through this
approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4
hours of recording of users navigating in real-world scenarios. It includes
diverse situations and navigation behaviors, providing a valuable resource for
learning real-world egocentric navigation policies. Extensive experiments show
that our model learns human-like navigation behaviors such as waiting / slowing
down, rerouting, and looking around for traffic while generalizing to unseen
environments. Check out our project webpage at
https://sites.google.com/stanford.edu/lookout.

</details>


### [37] [Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://arxiv.org/abs/2508.14483)
*Haoran Bai,Xiaoxu Chen,Canqian Yang,Zongyao He,Sibin Deng,Ying Chen*

Main category: cs.CV

TL;DR: Vivid-VR is a DiT-based video restoration method that uses ControlNet for content consistency and proposes concept distillation training to prevent distribution drift, achieving superior texture realism and temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Conventional fine-tuning of controllable video restoration pipelines suffers from distribution drift due to imperfect multimodal alignment, compromising texture realism and temporal coherence.

Method: Proposes concept distillation training using pretrained T2V model to synthesize training samples, redesigned control architecture with control feature projector to filter degradation artifacts, and dual-branch ControlNet connector with MLP-based feature mapping and cross-attention for dynamic control.

Result: Extensive experiments show Vivid-VR outperforms existing approaches on synthetic and real-world benchmarks, achieving impressive texture realism, visual vividness, and temporal consistency.

Conclusion: Vivid-VR successfully addresses distribution drift issues in video restoration through concept distillation and enhanced control architecture, delivering superior visual quality and temporal coherence.

Abstract: We present Vivid-VR, a DiT-based generative video restoration method built
upon an advanced T2V foundation model, where ControlNet is leveraged to control
the generation process, ensuring content consistency. However, conventional
fine-tuning of such controllable pipelines frequently suffers from distribution
drift due to limitations in imperfect multimodal alignment, resulting in
compromised texture realism and temporal coherence. To tackle this challenge,
we propose a concept distillation training strategy that utilizes the
pretrained T2V model to synthesize training samples with embedded textual
concepts, thereby distilling its conceptual understanding to preserve texture
and temporal quality. To enhance generation controllability, we redesign the
control architecture with two key components: 1) a control feature projector
that filters degradation artifacts from input video latents to minimize their
propagation through the generation pipeline, and 2) a new ControlNet connector
employing a dual-branch design. This connector synergistically combines
MLP-based feature mapping with cross-attention mechanism for dynamic control
feature retrieval, enabling both content preservation and adaptive control
signal modulation. Extensive experiments show that Vivid-VR performs favorably
against existing approaches on both synthetic and real-world benchmarks, as
well as AIGC videos, achieving impressive texture realism, visual vividness,
and temporal consistency. The codes and checkpoints are publicly available at
https://github.com/csbhr/Vivid-VR.

</details>


### [38] [WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification](https://arxiv.org/abs/2508.14486)
*Toqi Tahamid Sarker,Khaled R Ahmed,Taminul Islam,Cristiana Bernardi Rankrape,Karla Gage*

Main category: cs.CV

TL;DR: WeedSense is a multi-task learning architecture that simultaneously performs weed segmentation, height estimation, and growth stage classification with real-time performance and high accuracy.


<details>
  <summary>Details</summary>
Motivation: Weed management is critical for sustainable agriculture and crop yields, requiring effective monitoring and analysis strategies for site-specific management.

Method: Dual-path encoder with Universal Inverted Bottleneck blocks and Multi-Task Bifurcated Decoder with transformer-based feature fusion for multi-scale feature generation and simultaneous multi-task prediction.

Result: Achieves 89.78% mIoU for segmentation, 1.67cm MAE for height estimation, 99.99% accuracy for growth stage classification at 160 FPS, with 3× faster inference and 32.4% fewer parameters than sequential approaches.

Conclusion: WeedSense provides a comprehensive, efficient solution for weed analysis that outperforms state-of-the-art models and enables real-time agricultural monitoring.

Abstract: Weed management represents a critical challenge in agriculture, significantly
impacting crop yields and requiring substantial resources for control.
Effective weed monitoring and analysis strategies are crucial for implementing
sustainable agricultural practices and site-specific management approaches. We
introduce WeedSense, a novel multi-task learning architecture for comprehensive
weed analysis that jointly performs semantic segmentation, height estimation,
and growth stage classification. We present a unique dataset capturing 16 weed
species over an 11-week growth cycle with pixel-level annotations, height
measurements, and temporal labels. WeedSense leverages a dual-path encoder
incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated
Decoder with transformer-based feature fusion to generate multi-scale features
and enable simultaneous prediction across multiple tasks. WeedSense outperforms
other state-of-the-art models on our comprehensive evaluation. On our
multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm
MAE for height estimation, and 99.99% accuracy for growth stage classification
while maintaining real-time inference at 160 FPS. Our multitask approach
achieves 3$\times$ faster inference than sequential single-task execution and
uses 32.4% fewer parameters. Please see our project page at
weedsense.github.io.

</details>


### [39] [SATURN: Autoregressive Image Generation Guided by Scene Graphs](https://arxiv.org/abs/2508.14502)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: SATURN is a lightweight extension to VAR-CLIP that translates scene graphs into salience-ordered token sequences, enabling better text-to-image generation with improved layout and object relationship accuracy while maintaining high fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with capturing complex layouts and object relationships from prompts, while previous graph-guided approaches rely on heavy GAN/diffusion pipelines that lag behind modern autoregressive architectures in speed and quality.

Method: SATURN extends VAR-CLIP by translating scene graphs into salience-ordered token sequences, allowing a frozen CLIP-VQ-VAE backbone to interpret graph structure while only fine-tuning the VAR transformer.

Result: On Visual Genome dataset, SATURN reduces FID from 56.45% to 21.62% and increases Inception Score from 16.03 to 24.78, outperforming SG2IM and SGDiff without extra modules or multi-stage training.

Conclusion: SATURN effectively combines structural awareness with state-of-the-art autoregressive fidelity, showing significant improvements in object count fidelity and spatial relation accuracy for text-to-image generation.

Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but
often struggle to capture the layout and object relationships implied by
complex prompts. Scene graphs provide a natural structural prior, yet previous
graph-guided approaches have typically relied on heavy GAN or diffusion
pipelines, which lag behind modern autoregressive architectures in both speed
and fidelity. We introduce SATURN (Structured Arrangement of Triplets for
Unified Rendering Networks), a lightweight extension to VAR-CLIP that
translates a scene graph into a salience-ordered token sequence, enabling a
frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only
the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from
56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,
outperforming prior methods such as SG2IM and SGDiff without requiring extra
modules or multi-stage training. Qualitative results further confirm
improvements in object count fidelity and spatial relation accuracy, showing
that SATURN effectively combines structural awareness with state-of-the-art
autoregressive fidelity.

</details>


### [40] [PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments](https://arxiv.org/abs/2508.14504)
*Bernd Hofmann,Albert Scheck,Joerg Franke,Patrick Bruendl*

Main category: cs.CV

TL;DR: PB-IAD is a prompt-based industrial anomaly detection framework that leverages foundation models' multimodal capabilities to address data sparsity, adaptability, and user-centric requirements in manufacturing environments.


<details>
  <summary>Details</summary>
Motivation: Traditional statistical and data-driven anomaly detection methods are constrained by their dependence on extensive annotated datasets and lack flexibility in dynamic production conditions. Foundation models offer promising opportunities for more adaptable and usable solutions.

Method: The framework uses GPT-4.1 with a specialized prompt template for iterative domain knowledge implementation and a pre-processing module that translates user inputs into effective system prompts. It's designed for domain experts to customize without data science expertise.

Result: PB-IAD demonstrates superior performance compared to state-of-the-art methods like PatchCore, particularly in data-sparse scenarios and low-shot settings, achieving results solely through semantic instructions across three manufacturing scenarios and two data modalities.

Conclusion: The framework successfully addresses key industrial requirements through foundation model adaptation, providing a user-centric solution that outperforms traditional methods while requiring minimal annotated data and enabling domain expert customization.

Abstract: The detection of anomalies in manufacturing processes is crucial to ensure
product quality and identify process deviations. Statistical and data-driven
approaches remain the standard in industrial anomaly detection, yet their
adaptability and usability are constrained by the dependence on extensive
annotated datasets and limited flexibility under dynamic production conditions.
Recent advances in the perception capabilities of foundation models provide
promising opportunities for their adaptation to this downstream task. This
paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel
framework that leverages the multimodal and reasoning capabilities of
foundation models for industrial anomaly detection. Specifically, PB-IAD
addresses three key requirements of dynamic production environments: data
sparsity, agile adaptability, and domain user centricity. In addition to the
anomaly detection, the framework includes a prompt template that is
specifically designed for iteratively implementing domain-specific process
knowledge, as well as a pre-processing module that translates domain user
inputs into effective system prompts. This user-centric design allows domain
experts to customise the system flexibly without requiring data science
expertise. The proposed framework is evaluated by utilizing GPT-4.1 across
three distinct manufacturing scenarios, two data modalities, and an ablation
study to systematically assess the contribution of semantic instructions.
Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly
detection such as PatchCore. The results demonstrate superior performance,
particularly in data-sparse scenarios and low-shot settings, achieved solely
through semantic instructions.

</details>


### [41] [Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles](https://arxiv.org/abs/2508.14527)
*Jiangfan Liu,Yongkang Guo,Fangzhi Zhong,Tianyuan Zhang,Zonglei Jing,Siyuan Liang,Jiakai Wang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: ScenGE is a framework that uses LLMs to generate diverse safety-critical scenarios for autonomous vehicle testing by creating adversarial agents and amplifying threats with complex traffic flows, outperforming state-of-the-art methods by 31.96% in collision detection.


<details>
  <summary>Details</summary>
Motivation: Current approaches for safety-critical scenario generation rely on predefined patterns or rule-based strategies, limiting their ability to expose diverse and unforeseen failure modes in autonomous vehicles.

Method: ScenGE first performs Meta-Scenario Generation using a large language model grounded in driving knowledge to infer plausible adversarial agents, then uses Complex Scenario Evolution with background vehicles to amplify threats through trajectory optimization that reduces maneuvering space and creates critical occlusions.

Result: Extensive experiments show ScenGE uncovers 31.96% more severe collision cases than state-of-the-art baselines, works with different simulators and AV systems, and improves model robustness through adversarial training.

Conclusion: The framework generates plausible and critical scenarios validated through real-world tests and human evaluation, representing a critical step toward building public trust in autonomous vehicle safety.

Abstract: The generation of safety-critical scenarios in simulation has become
increasingly crucial for safety evaluation in autonomous vehicles prior to road
deployment in society. However, current approaches largely rely on predefined
threat patterns or rule-based strategies, which limit their ability to expose
diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a
framework that can generate plentiful safety-critical scenarios by reasoning
novel adversarial cases and then amplifying them with complex traffic flows.
Given a simple prompt of a benign scene, it first performs Meta-Scenario
Generation, where a large language model, grounded in structured driving
knowledge, infers an adversarial agent whose behavior poses a threat that is
both plausible and deliberately challenging. This meta-scenario is then
specified in executable code for precise in-simulator control. Subsequently,
Complex Scenario Evolution uses background vehicles to amplify the core threat
introduced by Meta-Scenario. It builds an adversarial collaborator graph to
identify key agent trajectories for optimization. These perturbations are
designed to simultaneously reduce the ego vehicle's maneuvering space and
create critical occlusions. Extensive experiments conducted on multiple
reinforcement learning based AV models show that ScenGE uncovers more severe
collision cases (+31.96%) on average than SoTA baselines. Additionally, our
ScenGE can be applied to large model based AV systems and deployed on different
simulators; we further observe that adversarial training on our scenarios
improves the model robustness. Finally, we validate our framework through
real-world vehicle tests and human evaluation, confirming that the generated
scenarios are both plausible and critical. We hope our paper can build up a
critical step towards building public trust and ensuring their safe deployment.

</details>


### [42] [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/abs/2508.14537)
*Yonghan Shin,SeungKyu Kim,Won-Ki Jeong*

Main category: cs.CV

TL;DR: WISE-FUSE is an adaptive framework that reduces computational pathology encoding time by 3x+ using vision-language models to selectively process only diagnostically relevant regions in whole slide images.


<details>
  <summary>Details</summary>
Motivation: Whole slide images in computational pathology require processing tens to hundreds of thousands of high-resolution patches per slide, resulting in prohibitive encoding costs that take days or weeks, making WSI encoding the major bottleneck for real-world deployment.

Method: Uses pathology-domain vision-language models and large language models to compute similarity scores between low-resolution patches and class-specific textual descriptions. Selects a small subset of informative regions based on similarity scores, quickly eliminates irrelevant patches, and selectively encodes high-resolution patches fused with textual embeddings.

Result: Reduces WSI encoding time by over threefold while achieving diagnostic performance comparable to or surpassing exhaustive patch processing methods.

Conclusion: WISE-FUSE offers a scalable and practical solution for computational pathology by dramatically reducing computational burden while maintaining diagnostic accuracy through selective processing of relevant regions.

Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major
computational challenge due to their gigapixel scale, often requiring the
processing of tens to hundreds of thousands of high-resolution patches per
slide. This results in prohibitive encoding costs, with preprocessing and
training times extending to days or even weeks-making WSI encoding the most
significant bottleneck in real-world deployment. In this work, we propose
WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain
vision-language models and large language models to address this challenge by
selectively processing diagnostically relevant regions. WISE-FUSE first
computes similarity scores between low-resolution patches and class-specific
textual descriptions using a knowledge distillation mechanism that preserves
fine-grained diagnostic features. Based on these similarity scores, we select a
small subset of informative regions for the target task, which quickly
eliminates irrelevant patches at the coarse level. The corresponding
high-resolution patches are then selectively encoded and fused with textual
embeddings to reinforce diagnostic context. Extensive experiments demonstrate
that WISE-FUSE reduces WSI encoding time by over threefold while achieving
diagnostic performance comparable to or surpassing that of exhaustive patch
processing, offering a scalable and practical solution for CPath.

</details>


### [43] [Improving OCR using internal document redundancy](https://arxiv.org/abs/2508.14557)
*Diego Belzarena,Seginus Mowlavi,Aitor Artola,Camilo Mariño,Marina Gardella,Ignacio Ramírez,Antoine Tadros,Roy He,Natalia Bottaioli,Boshra Rajaei,Gregory Randall,Jean-Michel Morel*

Main category: cs.CV

TL;DR: Unsupervised method using document redundancy to improve OCR accuracy for degraded documents through extended GMM with EM algorithm and statistical testing.


<details>
  <summary>Details</summary>
Motivation: Current OCR systems struggle with low-quality data and don't fully exploit document redundancy, especially for printed documents with high inter-domain variability.

Method: Extended Gaussian Mixture Model with alternating Expectation-Maximization algorithm, intra-cluster realignment process, and normality statistical testing to leverage character shape redundancy within documents.

Result: Demonstrated improvements on documents with various degradation levels, including Uruguayan military archives and 17th-20th century European newspapers.

Conclusion: The proposed unsupervised approach effectively corrects imperfect OCR outputs and suggests better clustering by exploiting document-specific character redundancy.

Abstract: Current OCR systems are based on deep learning models trained on large
amounts of data. Although they have shown some ability to generalize to unseen
data, especially in detection tasks, they can struggle with recognizing
low-quality data. This is particularly evident for printed documents, where
intra-domain data variability is typically low, but inter-domain data
variability is high. In that context, current OCR methods do not fully exploit
each document's redundancy. We propose an unsupervised method by leveraging the
redundancy of character shapes within a document to correct imperfect outputs
of a given OCR system and suggest better clustering. To this aim, we introduce
an extended Gaussian Mixture Model (GMM) by alternating an
Expectation-Maximization (EM) algorithm with an intra-cluster realignment
process and normality statistical testing. We demonstrate improvements in
documents with various levels of degradation, including recovered Uruguayan
military archives and 17th to mid-20th century European newspapers.

</details>


### [44] [A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives](https://arxiv.org/abs/2508.14558)
*Juepeng Zheng,Zi Ye,Yibin Wen,Jianxi Huang,Zhiwei Zhang,Qingmei Li,Qiong Hu,Baodong Xu,Lingyuan Zhao,Haohuan Fu*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of Agricultural Parcel and Boundary Delineation (APBD) methods using remote sensing, categorizing approaches into traditional image processing, machine learning, and deep learning methods, with discussion of current trends and future prospects.


<details>
  <summary>Details</summary>
Motivation: The production of high spatial resolution remote sensing images enables cost-efficient and high-accuracy agricultural inventory automation, creating a need to systematically review and organize the various APBD methods developed for detecting and delineating agricultural parcels.

Method: The authors conduct a comprehensive review of recent APBD papers, building meta-data analysis including algorithms, study sites, crop types, sensor types, and evaluation methods. They categorize methods into three classes: traditional image processing, traditional machine learning, and deep learning-based approaches.

Result: The review provides a clear knowledge map of existing APBD efforts, showing that deep learning-oriented approaches contribute to the majority of recent methods. The paper further discusses specific deep learning techniques like semantic segmentation, object detection, and Transformer-based methods.

Conclusion: The review identifies future prospects and potential hot topics in APBD research, aiming to help researchers track development trends in agricultural parcel delineation using remote sensing data.

Abstract: Powered by advances in multiple remote sensing sensors, the production of
high spatial resolution images provides great potential to achieve
cost-efficient and high-accuracy agricultural inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory of the level
of each agricultural parcel have generated many methods for Agricultural Parcel
and Boundary Delineation (APBD). This review covers APBD methods for detecting
and delineating agricultural parcels and systematically reviews the past and
present of APBD-related research applied to remote sensing images. With the
goal to provide a clear knowledge map of existing APBD efforts, we conduct a
comprehensive review of recent APBD papers to build a meta-data analysis,
including the algorithm, the study site, the crop type, the sensor type, the
evaluation method, etc. We categorize the methods into three classes: (1)
traditional image processing methods (including pixel-based, edge-based and
region-based); (2) traditional machine learning methods (such as random forest,
decision tree); and (3) deep learning-based methods. With deep
learning-oriented approaches contributing to a majority, we further discuss
deep learning-based methods like semantic segmentation-based, object
detection-based and Transformer-based methods. In addition, we discuss five
APBD-related issues to further comprehend the APBD domain using remote sensing
data, such as multi-sensor data in APBD task, comparisons between single-task
learning and multi-task learning in the APBD domain, comparisons among
different algorithms and different APBD tasks, etc. Finally, this review
proposes some APBD-related applications and a few exciting prospects and
potential hot topics in future APBD research. We hope this review help
researchers who involved in APBD domain to keep track of its development and
tendency.

</details>


### [45] [Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization](https://arxiv.org/abs/2508.14561)
*Sukhyun Jeong,Hong-Gi Shin,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: Proposes a method to enhance controllable motion generation by combining discrete pose codes with continuous motion features using residual vector quantization, improving both motion quality and controllability.


<details>
  <summary>Details</summary>
Motivation: Discrete pose codes alone cannot capture fine-grained motion details, limiting expressiveness in controllable motion generation systems.

Method: Augments pose code-based latent representations with continuous motion features using residual vector quantization (RVQ) to preserve interpretability while capturing subtle motion characteristics.

Result: Experiments on HumanML3D show FID reduction from 0.041 to 0.015 and Top-1 R-Precision improvement from 0.508 to 0.510, with qualitative analysis confirming enhanced controllability.

Conclusion: The proposed RVQ-based approach successfully bridges the gap between discrete pose codes and continuous motion features, achieving both high-quality motion generation and precise controllability.

Abstract: Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.

</details>


### [46] [Locality-aware Concept Bottleneck Model](https://arxiv.org/abs/2508.14562)
*Sujin Jeon,Hyundo Lee,Eungseo Kim,Sanghack Lee,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: LCBM improves concept localization in label-free concept bottleneck models using prototype learning with foundation models to ensure accurate spatial concept identification.


<details>
  <summary>Details</summary>
Motivation: Label-free CBMs often fail to localize concepts properly, attending to irrelevant regions when predicting concept presence, which limits their interpretability and reliability.

Method: Uses prototype learning with one prototype per concept, leveraging foundation models to ensure prototype relevance and encourage encoding of similar local regions for accurate concept localization.

Result: LCBM effectively identifies present concepts and exhibits improved localization while maintaining comparable classification performance to existing methods.

Conclusion: The proposed LCBM framework successfully addresses concept localization issues in label-free CBMs through prototype learning, enhancing spatial accuracy without compromising classification performance.

Abstract: Concept bottleneck models (CBMs) are inherently interpretable models that
make predictions based on human-understandable visual cues, referred to as
concepts. As obtaining dense concept annotations with human labeling is
demanding and costly, recent approaches utilize foundation models to determine
the concepts existing in the images. However, such label-free CBMs often fail
to localize concepts in relevant regions, attending to visually unrelated
regions when predicting concept presence. To this end, we propose a framework,
coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes rich
information from foundation models and adopts prototype learning to ensure
accurate spatial localization of the concepts. Specifically, we assign one
prototype to each concept, promoted to represent a prototypical image feature
of that concept. These prototypes are learned by encouraging them to encode
similar local regions, leveraging foundation models to assure the relevance of
each prototype to its associated concept. Then we use the prototypes to
facilitate the learning process of identifying the proper local region from
which each concept should be predicted. Experimental results demonstrate that
LCBM effectively identifies present concepts in the images and exhibits
improved localization while maintaining comparable classification performance.

</details>


### [47] [GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels](https://arxiv.org/abs/2508.14563)
*Xingyuan Yang,Min Wei*

Main category: cs.CV

TL;DR: GOGS is a two-stage framework using 2D Gaussian surfels for inverse rendering of glossy objects, achieving robust surface reconstruction and material decomposition for photorealistic relighting.


<details>
  <summary>Details</summary>
Motivation: NeRF-based methods are computationally expensive while 3D Gaussian Splatting struggles with specular reflections and multi-view inconsistencies, leading to surface noise and poor relighting results.

Method: Two-stage approach: 1) Physics-based rendering with split-sum approximation and geometric priors for surface reconstruction, 2) Material decomposition using Monte Carlo importance sampling, differentiable 2D Gaussian ray tracing, and spherical mipmap-based directional encoding.

Result: State-of-the-art performance in geometry reconstruction, material separation, and photorealistic relighting under novel illuminations, outperforming existing inverse rendering approaches.

Conclusion: GOGS effectively addresses limitations of current methods by combining robust surface reconstruction with advanced material decomposition techniques, enabling high-quality inverse rendering of glossy objects from RGB imagery.

Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

</details>


### [48] [Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset](https://arxiv.org/abs/2508.14567)
*Walter Zimmer,Ross Greer,Xingcheng Zhou,Rui Song,Marc Pavel,Daniel Lehmberg,Ahmed Ghita,Akshay Gopalkrishnan,Mohan Trivedi,Alois Knoll*

Main category: cs.CV

TL;DR: TUMTraf-A dataset with 48K+ labeled frames of highway accidents, plus Accid3nD model combining rule-based and learning-based approaches for accident detection.


<details>
  <summary>Details</summary>
Motivation: Accidents remain unavoidable in transportation networks despite safety improvements. Need real-world accident datasets and detection models to better understand and prevent crashes.

Method: Created TUMTraf-A dataset with 10 highway accident sequences, 294K+ 2D boxes, 93K+ 3D boxes from roadside cameras and LiDARs. Proposed Accid3nD model combining rule-based and learning-based detection approaches.

Result: Dataset contains 48,144 labeled frames with 10 object classes in OpenLABEL format. Experiments show robustness of the proposed Accid3nD detection method.

Conclusion: TUMTraf-A provides valuable real-world accident data for research. The hybrid Accid3nD model demonstrates effective accident detection, with dataset and code publicly available.

Abstract: Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as an unavoidable and sporadic outcome of traffic networks. We
present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of
real-world highway accidents. It contains ten sequences of vehicle crashes at
high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and
track IDs within 48,144 labeled frames recorded from four roadside cameras and
LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the
OpenLABEL format. We propose Accid3nD, an accident detection model that
combines a rule-based approach with a learning-based one. Experiments and
ablation studies on our dataset show the robustness of our proposed method. The
dataset, model, and code are available on our project website:
https://tum-traffic-dataset.github.io/tumtraf-a.

</details>


### [49] [Controllable Latent Space Augmentation for Digital Pathology](https://arxiv.org/abs/2508.14588)
*Sofiène Boutaj,Marin Scalbert,Pierre Marza,Florent Couzinie-Devy,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: HistAug is a generative model for controllable latent space augmentations in digital pathology that efficiently generates realistic augmented embeddings while preserving semantic information, improving multiple instance learning performance especially in low-data scenarios.


<details>
  <summary>Details</summary>
Motivation: WSI analysis faces challenges due to gigapixel resolution and scarce supervision. Traditional patch-level augmentation is computationally expensive, while existing feature-level methods lack transformation control, creating a need for efficient, controllable augmentation techniques.

Method: HistAug uses a generative model conditioned on explicit patch-level transformations (hue, erosion, etc.) to generate realistic augmented embeddings in latent space. It processes large numbers of patches efficiently in a single forward pass while preserving semantic information.

Result: HistAug outperforms existing methods across multiple slide-level tasks and diverse organs, particularly in low-data regimes. Ablation studies confirm benefits of learned transformations over noise-based perturbations and highlight importance of uniform WSI-wise augmentation.

Conclusion: HistAug provides an efficient and effective solution for WSI augmentation, enabling better MIL model performance through controllable generative augmentations that preserve semantic information while being computationally feasible for large-scale pathology datasets.

Abstract: Whole slide image (WSI) analysis in digital pathology presents unique
challenges due to the gigapixel resolution of WSIs and the scarcity of dense
supervision signals. While Multiple Instance Learning (MIL) is a natural fit
for slide-level tasks, training robust models requires large and diverse
datasets. Even though image augmentation techniques could be utilized to
increase data variability and reduce overfitting, implementing them effectively
is not a trivial task. Traditional patch-level augmentation is prohibitively
expensive due to the large number of patches extracted from each WSI, and
existing feature-level augmentation methods lack control over transformation
semantics. We introduce HistAug, a fast and efficient generative model for
controllable augmentations in the latent space for digital pathology. By
conditioning on explicit patch-level transformations (e.g., hue, erosion),
HistAug generates realistic augmented embeddings while preserving initial
semantic information. Our method allows the processing of a large number of
patches in a single forward pass efficiently, while at the same time
consistently improving MIL model performance. Experiments across multiple
slide-level tasks and diverse organs show that HistAug outperforms existing
methods, particularly in low-data regimes. Ablation studies confirm the
benefits of learned transformations over noise-based perturbations and
highlight the importance of uniform WSI-wise augmentation. Code is available at
https://github.com/MICS-Lab/HistAug.

</details>


### [50] [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](https://arxiv.org/abs/2508.14597)
*Nitish Kumar Mahala,Muzammil Khan,Pushpendra Kumar*

Main category: cs.CV

TL;DR: A novel smoke detection framework using optical flow-based motion encoding and a Two-Phase Uncertainty-Aware Shifted Windows Transformer for robust early fire detection from monocular imagery.


<details>
  <summary>Details</summary>
Motivation: Traditional smoke detectors struggle with complex spatiotemporal dynamics, illumination variability, flow kinematics, and environmental noise, requiring a more reliable solution for early fire warning systems.

Method: Uses optical flow estimation with a four-color-theorem-inspired dual-phase level-set fractional-order variational model to preserve motion discontinuities. Fuses color-encoded optical flow maps with appearance cues via Gaussian Mixture Model for segmentation. Employs a Shifted-Windows Transformer with multi-scale uncertainty estimation trained in two phases: first for detection accuracy, then for uncertainty estimation.

Result: Extensive experiments show superior generalization and robustness compared to state-of-the-art approaches across multiple evaluation metrics.

Conclusion: The framework provides a reliable solution for early fire detection in surveillance, industrial safety, and autonomous monitoring applications without complex multi-sensor arrays.

Abstract: Fire outbreaks pose critical threats to human life and infrastructure,
necessitating high-fidelity early-warning systems that detect combustion
precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal
dynamics influenced by illumination variability, flow kinematics, and
environmental noise, undermining the reliability of traditional detectors. To
address these challenges without the logistical complexity of multi-sensor
arrays, we propose an information-fusion framework by integrating smoke feature
representations extracted from monocular imagery. Specifically, a Two-Phase
Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke
detection, leveraging a novel smoke segmentation dataset, constructed via
optical flow-based motion encoding, is proposed. The optical flow estimation is
performed with a four-color-theorem-inspired dual-phase level-set
fractional-order variational model, which preserves motion discontinuities. The
resulting color-encoded optical flow maps are fused with appearance cues via a
Gaussian Mixture Model to generate binary segmentation masks of the smoke
regions. These fused representations are fed into the novel Shifted-Windows
Transformer, which is augmented with a multi-scale uncertainty estimation head
and trained under a two-phase learning regimen. First learning phase optimizes
smoke detection accuracy, while during the second phase, the model learns to
estimate plausibility confidence in its predictions by jointly modeling
aleatoric and epistemic uncertainties. Extensive experiments using multiple
evaluation metrics and comparative analysis with state-of-the-art approaches
demonstrate superior generalization and robustness, offering a reliable
solution for early fire detection in surveillance, industrial safety, and
autonomous monitoring applications.

</details>


### [51] [Incremental Object Detection with Prompt-based Methods](https://arxiv.org/abs/2508.14599)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: Visual prompt-based methods underperform in incremental object detection compared to image classification, but combining prompts with limited data replay achieves best results.


<details>
  <summary>Details</summary>
Motivation: To evaluate the generalizability of visual prompt-based methods from incremental image classification to incremental object detection, as no prior work has applied such methods to IOD.

Method: Analyzed three different prompt-based methods under complex domain-incremental learning setting with comprehensive baseline comparisons. Tested prompt length and initialization variations.

Result: Prompt-based approaches alone underperformed in IOD setting, but combining visual prompts with replaying a small portion of previous data achieved the best performance.

Conclusion: Visual prompts show promise for incremental object detection when combined with limited data replay, providing valuable insights for advancing prompt-based IL in detection tasks.

Abstract: Visual prompt-based methods have seen growing interest in incremental
learning (IL) for image classification. These approaches learn additional
embedding vectors while keeping the model frozen, making them efficient to
train. However, no prior work has applied such methods to incremental object
detection (IOD), leaving their generalizability unclear. In this paper, we
analyze three different prompt-based methods under a complex domain-incremental
learning setting. We additionally provide a wide range of reference baselines
for comparison. Empirically, we show that the prompt-based approaches we tested
underperform in this setting. However, a strong yet practical method, combining
visual prompts with replaying a small portion of previous data, achieves the
best results. Together with additional experiments on prompt length and
initialization, our findings offer valuable insights for advancing prompt-based
IL in IOD.

</details>


### [52] [UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://arxiv.org/abs/2508.14604)
*Peiming Li,Ziyi Wang,Yulin Yuan,Hong Liu,Xiangming Meng,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: UST-SSM extends Selective State Space Models to handle spatio-temporal disorder in point cloud videos through semantic-aware sequence reorganization and feature aggregation.


<details>
  <summary>Details</summary>
Motivation: Point cloud videos are effective for recognizing subtle human actions but suffer from spatio-temporal disorder that hinders unidirectional modeling when directly converted to 1D sequences.

Method: Proposes UST-SSM with Spatial-Temporal Selection Scanning (STSS) for semantic-aware sequence reorganization, Spatio-Temporal Structure Aggregation (STSA) for feature compensation, and Temporal Interaction Sampling (TIS) for enhanced temporal dependencies.

Result: Experimental validation on MSR-Action3D, NTU RGB+D, and Synthia 4D datasets demonstrates the method's effectiveness.

Conclusion: UST-SSM successfully addresses the spatio-temporal disorder challenge in point cloud videos and enables effective utilization of distant yet similar points through semantic-aware modeling.

Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of
lighting and viewpoint variations, making them highly effective for recognizing
subtle and continuous human actions. Although Selective State Space Models
(SSMs) have shown good performance in sequence modeling with linear complexity,
the spatio-temporal disorder of point cloud videos hinders their unidirectional
modeling when directly unfolding the point cloud video into a 1D sequence
through temporally sequential scanning. To address this challenge, we propose
the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the
latest advancements in SSMs to point cloud videos. Specifically, we introduce
Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points
into semantic-aware sequences through prompt-guided clustering, thereby
enabling the effective utilization of points that are spatially and temporally
distant yet similar within the sequence. For missing 4D geometric and motion
details, Spatio-Temporal Structure Aggregation (STSA) aggregates
spatio-temporal features and compensates. To improve temporal interaction
within the sampled sequence, Temporal Interaction Sampling (TIS) enhances
fine-grained temporal dependencies through non-anchor frame utilization and
expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,
and Synthia 4D datasets validate the effectiveness of our method. Our code is
available at https://github.com/wangzy01/UST-SSM.

</details>


### [53] [SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos](https://arxiv.org/abs/2508.14607)
*Pengzhi Zhong,Xinzhe Wang,Dan Zeng,Qihua Zhou,Feixiang He,Shuiwang Li*

Main category: cs.CV

TL;DR: SMTrack is the first directly trained deep Spiking Neural Network framework for end-to-end multi-object tracking on standard RGB videos, achieving performance comparable to ANN-based methods.


<details>
  <summary>Details</summary>
Motivation: While SNNs show promise for low-power computation, their application in visual tasks has been limited to basic tasks like classification and detection. The potential of directly-trained SNNs for complex temporal tasks like multi-object tracking on conventional RGB videos remains underexplored.

Method: Proposes SMTrack with adaptive scale-aware Normalized Wasserstein Distance loss (Asa-NWDLoss) that dynamically adjusts normalization based on average object size per batch, and incorporates TrackTrack identity module for robust association and trajectory consistency.

Result: Extensive evaluations on BEE24, MOT17, MOT20, and DanceTrack datasets show SMTrack achieves performance on par with leading ANN-based multi-object tracking methods.

Conclusion: SMTrack advances robust and accurate SNN-based tracking in complex scenarios, demonstrating the viability of directly-trained SNNs for sophisticated temporal vision tasks on standard RGB video streams.

Abstract: Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential
for low-power computation, yet their application in visual tasks remains
largely confined to image classification, object detection, and event-based
tracking. In contrast, real-world vision systems still widely use conventional
RGB video streams, where the potential of directly-trained SNNs for complex
temporal tasks such as multi-object tracking (MOT) remains underexplored. To
address this challenge, we propose SMTrack-the first directly trained deep SNN
framework for end-to-end multi-object tracking on standard RGB videos. SMTrack
introduces an adaptive and scale-aware Normalized Wasserstein Distance loss
(Asa-NWDLoss) to improve detection and localization performance under varying
object scales and densities. Specifically, the method computes the average
object size within each training batch and dynamically adjusts the
normalization factor, thereby enhancing sensitivity to small objects. For the
association stage, we incorporate the TrackTrack identity module to maintain
robust and consistent object trajectories. Extensive evaluations on BEE24,
MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with
leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking
in complex scenarios.

</details>


### [54] [AnchorSync: Global Consistency Optimization for Long Video Editing](https://arxiv.org/abs/2508.14609)
*Zichi Liu,Yinggui Wang,Tao Wei,Chao Ma*

Main category: cs.CV

TL;DR: AnchorSync is a diffusion-based framework for long video editing that uses sparse anchor frame editing and smooth interpolation to maintain global consistency and temporal coherence across thousands of frames.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with structural drift and temporal artifacts in long videos, making minute-long sequence editing challenging.

Method: Decouples video editing into sparse anchor frame editing and smooth intermediate frame interpolation, using progressive denoising and multimodal guidance to enforce structural consistency and preserve temporal dynamics.

Result: Extensive experiments show AnchorSync produces coherent, high-fidelity edits that surpass prior methods in visual quality and temporal stability.

Conclusion: AnchorSync effectively addresses long video editing challenges by maintaining both global consistency and temporal coherence through its novel diffusion-based framework.

Abstract: Editing long videos remains a challenging task due to the need for
maintaining both global consistency and temporal coherence across thousands of
frames. Existing methods often suffer from structural drift or temporal
artifacts, particularly in minute-long sequences. We introduce AnchorSync, a
novel diffusion-based framework that enables high-quality, long-term video
editing by decoupling the task into sparse anchor frame editing and smooth
intermediate frame interpolation. Our approach enforces structural consistency
through a progressive denoising process and preserves temporal dynamics via
multimodal guidance. Extensive experiments show that AnchorSync produces
coherent, high-fidelity edits, surpassing prior methods in visual quality and
temporal stability.

</details>


### [55] [Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images](https://arxiv.org/abs/2508.14660)
*Muhammad Ibraheem Siddiqui,Muhammad Umer Sheikh,Hassan Abid,Kevin Henry,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: PerSense is a training-free one-shot framework for personalized instance segmentation in dense images, using density maps and point prompts with adaptive filtering. PerSense++ enhances it with diversity-aware exemplar selection, hybrid prompt generation, and mask rejection.


<details>
  <summary>Details</summary>
Motivation: Address challenges in dense visual scene segmentation including occlusions, background clutter, and scale variations that make instance segmentation difficult.

Method: Uses Instance Detection Module with density maps for candidate point prompts, Point Prompt Selection Module with adaptive thresholding, and feedback mechanism. PerSense++ adds diversity-aware exemplar selection, hybrid contour/peak-based prompt generation, and Irrelevant Mask Rejection Module.

Result: Extensive experiments show PerSense++ outperforms existing methods in dense settings across multiple benchmarks.

Conclusion: The framework provides effective training-free segmentation for dense images, with PerSense++ offering enhanced robustness in cluttered scenes through additional components.

Abstract: Segmentation in dense visual scenes poses significant challenges due to
occlusions, background clutter, and scale variations. To address this, we
introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot
framework for Personalized instance Segmentation in dense images. PerSense
employs a novel Instance Detection Module (IDM) that leverages density maps
(DMs) to generate instance-level candidate point prompts, followed by a Point
Prompt Selection Module (PPSM) that filters false positives via adaptive
thresholding and spatial gating. A feedback mechanism further enhances
segmentation by automatically selecting effective exemplars to improve DM
quality. We additionally present PerSense++, an enhanced variant that
incorporates three additional components to improve robustness in cluttered
scenes: (i) a diversity-aware exemplar selection strategy that leverages
feature and scale diversity for better DM generation; (ii) a hybrid IDM
combining contour and peak-based prompt generation for improved instance
separation within complex density patterns; and (iii) an Irrelevant Mask
Rejection Module (IMRM) that discards spatially inconsistent masks using
outlier analysis. Finally, to support this underexplored task, we introduce
PerSense-D, a dedicated benchmark for personalized segmentation in dense
images. Extensive experiments across multiple benchmarks demonstrate that
PerSense++ outperforms existing methods in dense settings.

</details>


### [56] [GeMS: Efficient Gaussian Splatting for Extreme Motion Blur](https://arxiv.org/abs/2508.14682)
*Gopi Raju Matta,Trisha Reddypalli,Vemunuri Divya Madhuri,Kaushik Mitra*

Main category: cs.CV

TL;DR: GeMS is a 3D Gaussian Splatting framework that handles severely motion-blurred images without requiring sharp reference images, using deep learning-based pose estimation and probabilistic scene initialization.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail with extreme motion blur because they assume access to sharp images for camera pose estimation and point cloud generation, which is unrealistic in real-world scenarios with severe blur.

Method: GeMS integrates: 1) VGGSfM for pose estimation from blurred inputs, 2) 3DGS-MCMC for probabilistic scene initialization without heuristic pruning, and 3) joint optimization of camera trajectories and Gaussian parameters. GeMS-E adds event-based EDI deblurring for progressive refinement.

Result: Both GeMS and GeMS-E achieve state-of-the-art performance on synthetic and real-world datasets, successfully reconstructing scenes directly from extremely blurred images.

Conclusion: This is the first framework to address extreme motion blur within 3D Gaussian Splatting directly from severely blurred inputs, eliminating the need for sharp reference images that existing methods rely on.

Abstract: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.

</details>


### [57] [Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models](https://arxiv.org/abs/2508.14707)
*Jiabo Huang,Chen Chen,Lingjuan Lyu*

Main category: cs.CV

TL;DR: A model-driven approach that unifies multiple pre-trained teacher models in shared latent space to build vision foundation models without requiring massive labeled data or high-end GPUs.


<details>
  <summary>Details</summary>
Motivation: Overcome the bottleneck of data-centric methods that require vast labeled data and high-end GPUs by leveraging existing open-source domain-specific models as knowledge sources.

Method: Joint knowledge transfer and preservation strategy that unifies multiple teacher models in shared latent space to mitigate distributional gaps, and uses a general-purpose teacher as knowledge base with adapter modules to integrate purpose-specific teachers.

Result: Outperforms existing data-centric models across four fundamental vision tasks: image classification, object detection, semantic and instance segmentation.

Conclusion: Model-driven approach successfully builds powerful vision foundation models by unifying and aggregating existing teacher models, inheriting their expertise without large-scale labeled data requirements.

Abstract: Vision foundation models (VFMs) are predominantly developed using
data-centric methods. These methods require training on vast amounts of data
usually with high-quality labels, which poses a bottleneck for most
institutions that lack both large-scale data and high-end GPUs. On the other
hand, many open-source vision models have been pretrained on domain-specific
data, enabling them to distill and represent core knowledge in a form that is
transferable across diverse applications. Even though these models are highly
valuable assets, they remain largely under-explored in empowering the
development of a general-purpose VFM. In this paper, we presents a new
model-driven approach for training VFMs through joint knowledge transfer and
preservation. Our method unifies multiple pre-trained teacher models in a
shared latent space to mitigate the ``imbalanced transfer'' issue caused by
their distributional gaps. Besides, we introduce a knowledge preservation
strategy to take a general-purpose teacher as a knowledge base for integrating
knowledge from the remaining purpose-specific teachers using an adapter module.
By unifying and aggregating existing models, we build a powerful VFM to inherit
teachers' expertise without needing to train on a large amount of labeled data.
Our model not only provides generalizable visual features, but also inherently
supports multiple downstream tasks. Extensive experiments demonstrate that our
VFM outperforms existing data-centric models across four fundamental vision
tasks, including image classification, object detection, semantic and instance
segmentation.

</details>


### [58] [GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting](https://arxiv.org/abs/2508.14717)
*Jiaxin Wei,Stefan Leutenegger,Simon Schaefer*

Main category: cs.CV

TL;DR: GSFix3D improves 3D Gaussian Splatting by integrating diffusion model knowledge to enhance novel view synthesis from extreme viewpoints and repair missing regions, achieving state-of-the-art performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Address limitations of 3D Gaussian Splatting in handling extreme novel viewpoints and partially observed regions, while overcoming diffusion models' reliance on text prompts and lack of scene awareness for accurate 3D reconstruction.

Method: Introduces GSFixer, a latent diffusion model fine-tuned to leverage both mesh and 3D Gaussians, using a random mask augmentation strategy for plausible inpainting of missing regions and robust novel view repair.

Result: Achieves state-of-the-art performance on challenging benchmarks, requires minimal scene-specific fine-tuning, and demonstrates resilience to potential pose errors in real-world tests.

Conclusion: GSFix3D successfully bridges the gap between 3D Gaussian Splatting and diffusion models, enabling high-quality renderings from extreme viewpoints while preserving scene consistency with minimal computational overhead.

Abstract: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.

</details>


### [59] [Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving](https://arxiv.org/abs/2508.14729)
*Leila Cheshmi,Mennatullah Siam*

Main category: cs.CV

TL;DR: Efficient video transformer for class-agnostic segmentation using motion cues without optical flow, outperforming baselines on multiple datasets while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address safety challenges in autonomous driving by detecting unknown objects and handling unforeseen scenarios, overcoming limitations of traditional methods that rely on known classes and expensive optical flow computations.

Method: Multiscale video transformer with multi-stage query-memory decoding and scale-specific random drop-token. Uses shared learnable memory module to preserve high-resolution spatiotemporal features without feature compression.

Result: Consistently outperforms multiscale baselines on DAVIS'16, KITTI, and Cityscapes datasets while being efficient in GPU memory and run-time.

Conclusion: Demonstrates promising direction for real-time, robust dense prediction in safety-critical robotics applications through efficient memory-centric design.

Abstract: Ensuring safety in autonomous driving is a complex challenge requiring
handling unknown objects and unforeseen driving scenarios. We develop
multiscale video transformers capable of detecting unknown objects using only
motion cues. Video semantic and panoptic segmentation often relies on known
classes seen during training, overlooking novel categories. Recent visual
grounding with large language models is computationally expensive, especially
for pixel-level output. We propose an efficient video transformer trained
end-to-end for class-agnostic segmentation without optical flow. Our method
uses multi-stage multiscale query-memory decoding and a scale-specific random
drop-token to ensure efficiency and accuracy, maintaining detailed
spatiotemporal features with a shared, learnable memory module. Unlike
conventional decoders that compress features, our memory-centric design
preserves high-resolution information at multiple scales. We evaluate on
DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale
baselines while being efficient in GPU memory and run-time, demonstrating a
promising direction for real-time, robust dense prediction in safety-critical
robotics.

</details>


### [60] [Improved Mapping Between Illuminations and Sensors for RAW Images](https://arxiv.org/abs/2508.14730)
*Abhijith Punnappurath,Luxi Zhao,Hoang Le,Abdelrahman Abdelhamed,SaiKiran Kumar Tedla,Michael S. Brown*

Main category: cs.CV

TL;DR: A novel dataset and neural network approach for RAW image illumination and sensor mapping to reduce data capture burden for deep learning methods.


<details>
  <summary>Details</summary>
Motivation: RAW images have sensor- and illumination-specific characteristics that make dataset collection challenging, requiring capture under various illumination conditions for each sensor.

Method: Created a first-of-its-kind dataset using a customized lightbox with tunable illumination spectra to capture scenes under 390 illuminations with 4 cameras and 18 scenes, then developed a lightweight neural network for illumination and sensor mapping.

Result: The proposed neural network approach outperforms competing methods and demonstrates utility in training neural ISP systems.

Conclusion: The introduced dataset and mapping method effectively address the challenges of RAW image processing by enabling illumination augmentation and sensor mapping, reducing the need for extensive data capture.

Abstract: RAW images are unprocessed camera sensor output with sensor-specific RGB
values based on the sensor's color filter spectral sensitivities. RAW images
also incur strong color casts due to the sensor's response to the spectral
properties of scene illumination. The sensor- and illumination-specific nature
of RAW images makes it challenging to capture RAW datasets for deep learning
methods, as scenes need to be captured for each sensor and under a wide range
of illumination. Methods for illumination augmentation for a given sensor and
the ability to map RAW images between sensors are important for reducing the
burden of data capture. To explore this problem, we introduce the
first-of-its-kind dataset comprising carefully captured scenes under a wide
range of illumination. Specifically, we use a customized lightbox with tunable
illumination spectra to capture several scenes with different cameras. Our
illumination and sensor mapping dataset has 390 illuminations, four cameras,
and 18 scenes. Using this dataset, we introduce a lightweight neural network
approach for illumination and sensor mapping that outperforms competing
methods. We demonstrate the utility of our approach on the downstream task of
training a neural ISP. Link to project page:
https://github.com/SamsungLabs/illum-sensor-mapping.

</details>


### [61] [Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels](https://arxiv.org/abs/2508.14767)
*Fabian Holst,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: Novel technique fuses monocular RGB images with AIS data to create 6D pose estimation datasets for marine vessels, achieving high accuracy without manual annotation and releasing a public dataset.


<details>
  <summary>Details</summary>
Motivation: Address limitations of relying purely on AIS data for vessel location due to equipment reliability issues, data manipulation, and transmission delays by combining visual detection with AIS information.

Method: Uses YOLOX-X object detection network to locate vessels in RGB images, then combines with AIS messages to generate 3D bounding boxes. Compares homography and Perspective-n-Point (PnP) transformation methods for data alignment.

Result: PnP method achieves significantly lower projection error than homography. YOLOX-X achieves mAP of 0.80 at IoU threshold 0.5. Created BONK-pose dataset with 3753 images and 3D bounding boxes, plus 1000 images with 2D annotations.

Conclusion: The approach successfully creates accurate 6D pose estimation datasets without manual annotation, providing valuable resources for training and evaluating pose estimation networks in marine environments.

Abstract: The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.

</details>


### [62] [6-DoF Object Tracking with Event-based Optical Flow and Frames](https://arxiv.org/abs/2508.14776)
*Zhichao Li,Arren Glover,Chiara Bartolozzi,Lorenzo Natale*

Main category: cs.CV

TL;DR: Combines event camera optical flow with RGB-based pose estimation for high-speed 6-DoF object tracking, overcoming motion blur limitations of conventional cameras.


<details>
  <summary>Details</summary>
Motivation: Traditional cameras struggle with high-speed object tracking due to frame rate limitations and motion blur. Event cameras offer high temporal resolution but lack rich visual information for single-shot pose estimation.

Method: Uses event-based optical flow for object motion measurement to create a 6-DoF velocity tracker, then integrates this with low-frequency pose estimates from an RGB-based global pose estimator.

Result: Successfully tested on both synthetic and real-world data, demonstrating effective high-speed 6-DoF pose tracking, particularly in high-speed motion scenarios.

Conclusion: The hybrid approach leveraging both event and RGB cameras effectively addresses high-speed object tracking challenges by combining the strengths of both sensor types.

Abstract: Tracking the position and orientation of objects in space (i.e., in 6-DoF) in
real time is a fundamental problem in robotics for environment interaction. It
becomes more challenging when objects move at high-speed due to frame rate
limitations in conventional cameras and motion blur. Event cameras are
characterized by high temporal resolution, low latency and high dynamic range,
that can potentially overcome the impacts of motion blur. Traditional RGB
cameras provide rich visual information that is more suitable for the
challenging task of single-shot object pose estimation. In this work, we
propose using event-based optical flow combined with an RGB based global object
pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the
core advantages of both types of vision sensors. Specifically, we propose an
event-based optical flow algorithm for object motion measurement to implement
an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF
velocity with low frequency estimated pose from the global pose estimator, the
method can track pose when objects move at high-speed. The proposed algorithm
is tested and validated on both synthetic and real world data, demonstrating
its effectiveness, especially in high-speed motion scenarios.

</details>


### [63] [Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification](https://arxiv.org/abs/2508.14779)
*Mengliang Zhang,Jacob M. Luber*

Main category: cs.CV

TL;DR: First systematic study of domain bias in pathology foundation models from hospital variations, proposing lightweight adversarial framework to remove hospital-specific features while maintaining disease classification performance.


<details>
  <summary>Details</summary>
Motivation: Pathology foundation models risk learning hospital-specific features due to hardware/preprocessing variations across hospitals, posing deployment risks that need mitigation.

Method: Lightweight adversarial framework with trainable adapter and domain classifier connected through gradient reversal layer (GRL) to remove latent hospital-specific features from frozen representations.

Result: Substantially reduces domain predictability while maintaining/improving disease classification, especially in out-of-domain scenarios; confirmed by hospital detection and feature space visualization.

Conclusion: Proposed method effectively mitigates hospital bias in pathology foundation models without modifying encoder, enabling more robust clinical deployment across diverse hospital settings.

Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in
whole-slide image (WSI) diagnosis. However, pathology images from different
hospitals often vary due to differences in scanning hardware and preprocessing
styles, which may lead PFMs to inadvertently learn hospital-specific features,
posing risks for clinical deployment. In this work, we present the first
systematic study of domain bias in PFMs arising from hospital source
characteristics. Specifically, we (1) construct a pipeline for quantifying
domain bias in PFMs, (2) evaluate and compare the performance of multiple
models, and (3) propose a lightweight adversarial framework that removes latent
hospital-specific features from frozen representations without modifying the
encoder itself. By introducing a trainable adapter and a domain classifier
connected through a gradient reversal layer (GRL), our method learns
task-discriminative yet domain-invariant representations. Experiments on
multi-center histopathology datasets demonstrate that our approach
substantially reduces domain predictability while maintaining or even improving
disease classification performance, particularly in out-of-domain (unseen
hospital) scenarios. Further analyses, including hospital detection and feature
space visualization, confirm the effectiveness of our method in mitigating
hospital bias. We will provide our code based on acceptance.

</details>


### [64] [MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow](https://arxiv.org/abs/2508.14797)
*Kihyun Na,Junseok Oh,Youngkwan Cho,Bumjin Kim,Sungmin Cho,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: Proposes MF-LPR², a multi-frame license plate restoration framework that aggregates neighboring frames using optical flow and error correction algorithms instead of relying on pretrained priors, achieving state-of-the-art restoration quality and recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: License plate recognition in dash cam images is challenging due to low resolution, motion blur, and glare. Existing generative models based on pretrained priors often introduce artifacts and cannot reliably restore poor-quality license plate images.

Method: Uses optical flow estimator with spatio-temporal consistency algorithms to align and aggregate multiple frames. Includes error detection and correction mechanisms to handle erroneous optical flow estimations. Constructed a new Realistic LPR dataset for evaluation.

Result: Outperformed 8 restoration models in PSNR, SSIM, and LPIPS metrics. Achieved 86.44% recognition accuracy, significantly better than single-frame (14.04%) and multi-frame (82.55%) baselines. Ablation studies confirmed the importance of filtering and refinement algorithms.

Conclusion: MF-LPR² effectively addresses license plate restoration challenges by leveraging multi-frame information and spatio-temporal consistency, providing both improved image quality and recognition accuracy while preserving evidential content.

Abstract: License plate recognition (LPR) is important for traffic law enforcement,
crime investigation, and surveillance. However, license plate areas in dash cam
images often suffer from low resolution, motion blur, and glare, which make
accurate recognition challenging. Existing generative models that rely on
pretrained priors cannot reliably restore such poor-quality images, frequently
introducing severe artifacts and distortions. To address this issue, we propose
a novel multi-frame license plate restoration and recognition framework,
MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and
aggregating neighboring frames instead of relying on pretrained knowledge. To
achieve accurate frame alignment, we employ a state-of-the-art optical flow
estimator in conjunction with carefully designed algorithms that detect and
correct erroneous optical flow estimations by leveraging the spatio-temporal
consistency inherent in license plate image sequences. Our approach enhances
both image quality and recognition accuracy while preserving the evidential
content of the input images. In addition, we constructed a novel Realistic LPR
(RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of
low-quality license plate image sequences and high-quality pseudo ground-truth
images, reflecting the complexities of real-world scenarios. In experiments,
MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM,
and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an
accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and
the multi-frame LPR (82.55%) among the eleven baseline models. The results of
ablation studies confirm that our filtering and refinement algorithms
significantly contribute to these improvements.

</details>


### [65] [DINOv3 with Test-Time Training for Medical Image Registration](https://arxiv.org/abs/2508.14809)
*Shansong Wang,Mojtaba Safari,Mingzhe Hu,Qiang Li,Chih-Wei Chang,Richard LJ Qiu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: Training-free medical image registration using frozen DINOv3 encoder and test-time optimization in feature space, achieving state-of-the-art results without requiring large training datasets.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of learning-based medical image registration methods that require large amounts of training data, which hinders clinical adoption.

Method: Proposes a training-free pipeline that uses a frozen DINOv3 encoder and performs test-time optimization of deformation field in feature space.

Result: Achieved best mean Dice score of 0.790 on Abdomen MR-CT with lowest HD95 of 4.9±5.0 and SDLogJ of 0.08±0.02. On ACDC cardiac MRI, improved mean DSC to 0.769 with SDLogJ of 0.11 and HD95 of 4.8.

Conclusion: Operating in compact foundation feature space at test time provides practical and general solution for clinical registration without additional training requirements.

Abstract: Prior medical image registration approaches, particularly learning-based
methods, often require large amounts of training data, which constrains
clinical adoption. To overcome this limitation, we propose a training-free
pipeline that relies on a frozen DINOv3 encoder and test-time optimization of
the deformation field in feature space. Across two representative benchmarks,
the method is accurate and yields regular deformations. On Abdomen MR-CT, it
attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th
percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard
deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it
improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked
gain over the initial alignment. The results indicate that operating in a
compact foundation feature space at test time offers a practical and general
solution for clinical registration without additional training.

</details>


### [66] [Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization](https://arxiv.org/abs/2508.14811)
*Canyu Zhao,Xiaoman Li,Tianjian Feng,Zhiyue Zhao,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: Tinker is a versatile 3D editing framework that enables high-fidelity edits from just 1-2 images without per-scene finetuning, achieving multi-view consistency through pretrained diffusion models.


<details>
  <summary>Details</summary>
Motivation: Prior 3D editing techniques require extensive per-scene optimization and multiple consistent input views, creating barriers to scalable 3D content creation. Tinker aims to enable robust editing without these constraints.

Method: Repurposes pretrained diffusion models for latent 3D awareness. Includes two novel components: (1) Referring multi-view editor for precise reference-driven edits, and (2) Any-view-to-video synthesizer using spatial-temporal priors for scene completion and novel-view generation from sparse inputs.

Result: Achieves state-of-the-art performance on editing, novel-view synthesis, and rendering enhancement tasks. Significantly reduces barriers to generalizable 3D content creation.

Conclusion: Tinker represents a key step towards truly scalable, zero-shot 3D editing, enabling high-quality multi-view consistent edits without per-scene training.

Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that
operates in both one-shot and few-shot regimes without any per-scene
finetuning. Unlike prior techniques that demand extensive per-scene
optimization to ensure multi-view consistency or to produce dozens of
consistent edited input views, Tinker delivers robust, multi-view consistent
edits from as few as one or two images. This capability stems from repurposing
pretrained diffusion models, which unlocks their latent 3D awareness. To drive
research in this space, we curate the first large-scale multi-view editing
dataset and data pipeline, spanning diverse scenes and styles. Building on this
dataset, we develop our framework capable of generating multi-view consistent
edited views without per-scene training, which consists of two novel
components: (1) Referring multi-view editor: Enables precise, reference-driven
edits that remain coherent across all viewpoints. (2) Any-view-to-video
synthesizer: Leverages spatial-temporal priors from video diffusion to perform
high-quality scene completion and novel-view generation even from sparse
inputs. Through extensive experiments, Tinker significantly reduces the barrier
to generalizable 3D content creation, achieving state-of-the-art performance on
editing, novel-view synthesis, and rendering enhancement tasks. We believe that
Tinker represents a key step towards truly scalable, zero-shot 3D editing.
Project webpage: https://aim-uofa.github.io/Tinker

</details>


### [67] [Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives](https://arxiv.org/abs/2508.14812)
*Haoyu Zhao,Jiaxi Gu,Shicong Wang,Xing Zhang,Hang Xu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: A novel framework for video-language retrieval that uses fine-grained feature learning and an inference pipeline to improve performance without additional training, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of high computational costs in large-scale pre-training for video retrieval and the underexplored fine-grained information in videos and texts.

Method: Coarse-to-fine objectives with contrastive and matching learning, Granularity-Aware Representation module for fine-grained data, and an inference pipeline with voting mechanism and Matching Entropy metric leveraging keyword repetition.

Result: Outperforms previous approaches on four benchmarks, with 2.1% increase in Recall@1 on MSR-VTT and 1.6% increase on DiDeMo dataset.

Conclusion: The proposed framework effectively learns fine-grained features and the inference pipeline significantly improves retrieval performance without requiring additional pre-training, demonstrating the value of leveraging repetition patterns in captions.

Abstract: The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.

</details>


### [68] [TransLight: Image-Guided Customized Lighting Control with Generative Decoupling](https://arxiv.org/abs/2508.14814)
*Zongming Li,Lianghui Zhu,Haocheng Shen,Longjin Ran,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: TransLight enables high-fidelity transfer of complex light effects between images using generative decoupling to separate content from lighting, achieving superior illumination control compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing illumination-editing approaches fail to provide customized light effect control while preserving content integrity, making them ineffective for practical lighting stylization and transferring complex light effects between images.

Method: Uses Generative Decoupling with two fine-tuned diffusion models to separate image content and light effects, creating a million-scale dataset of image-content-light triplets. Employs IC-Light as generative model trained on these triplets with reference lighting as conditioning signal.

Result: TransLight successfully transfers light effects across disparate images, delivering more customized illumination control than existing techniques and enabling natural transfer of diverse light effects.

Conclusion: The method establishes TransLight as the first successful approach for cross-image light effect transfer, charting new directions for illumination harmonization and editing research through thorough disentanglement of light effects.

Abstract: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.

</details>


### [69] [EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention](https://arxiv.org/abs/2508.14856)
*Lakshmi Annamalai,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: EventSSEG: Event-based self-supervised learning for road segmentation using probabilistic attention, achieving SOTA performance with minimal labeled data.


<details>
  <summary>Details</summary>
Motivation: Road segmentation is crucial for autonomous vehicles but challenging with frame-based cameras due to latency and compute requirements. Event cameras offer low-power sensing but face challenges in transferring pretrained weights and lack of labeled event data.

Method: EventSSEG uses event-only computing with a probabilistic attention mechanism and event-based self-supervised learning to eliminate the need for extensive labeled data.

Result: Achieves state-of-the-art performance on DSEC-Semantic and DDD17 datasets with minimal labeled events.

Conclusion: This approach successfully maximizes event cameras' capabilities while addressing the critical lack of labeled event data for road segmentation tasks.

Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low
latency and low compute solutions using frame based cameras remains a
challenge. Event cameras offer a promising alternative. To leverage their low
power sensing, we introduce EventSSEG, a method for road segmentation that uses
event only computing and a probabilistic attention mechanism. Event only
computing poses a challenge in transferring pretrained weights from the
conventional camera domain, requiring abundant labeled data, which is scarce.
To overcome this, EventSSEG employs event-based self supervised learning,
eliminating the need for extensive labeled data. Experiments on DSEC-Semantic
and DDD17 show that EventSSEG achieves state of the art performance with
minimal labeled events. This approach maximizes event cameras capabilities and
addresses the lack of labeled events.

</details>


### [70] [Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on Largescale Clinical Imaging](https://arxiv.org/abs/2508.14878)
*Lucas W. Remedios,Chloe Cho,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Thomas A. Lasko,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: This study analyzes pancreas size and shape changes across lifespan (0-90 years) using AI-based measurements from CT/MRI scans, finding significant morphological differences in type 2 diabetes patients compared to controls.


<details>
  <summary>Details</summary>
Motivation: Understanding pancreatic changes is crucial for detecting deviations in type 2 diabetes and other pancreatic diseases. The research aims to establish normative aging trends and identify reliable imaging modalities for AI-based pancreas measurement.

Method: Analyzed 2533 patients' abdominal CT/MRI scans, resampled to 3mm resolution, used automated segmentation to extract 13 morphological features. Compared CT vs MRI measurements, characterized normative patterns by age/sex, and used GAMLSS regression on 1350 age/sex-matched patients (675 diabetic, 675 controls) to identify diabetes-related deviations.

Result: 10 of 13 morphological features showed significantly different aging trends between diabetic and non-diabetic patients after adjusting for confounders (p<0.05). MRI yielded different measurements than CT. The pancreas was found to be smaller in type 2 diabetes.

Conclusion: The study provides lifespan trends showing altered pancreas size and shape in type 2 diabetes, reinforces that the pancreas is smaller in diabetes, and contributes a reference dataset of normative pancreas morphology from a large clinical cohort.

Abstract: Purpose: Understanding how the pancreas changes is critical for detecting
deviations in type 2 diabetes and other pancreatic disease. We measure pancreas
size and shape using morphological measurements from ages 0 to 90. Our goals
are to 1) identify reliable clinical imaging modalities for AI-based pancreas
measurement, 2) establish normative morphological aging trends, and 3) detect
potential deviations in type 2 diabetes.
  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged
with abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,
segmented the pancreas using automated methods, and extracted 13 morphological
pancreas features across the lifespan. First, we assessed CT and MRI
measurements to determine which modalities provide consistent lifespan trends.
Second, we characterized distributions of normative morphological patterns
stratified by age group and sex. Third, we used GAMLSS regression to model
pancreas morphology trends in 1350 patients matched for age, sex, and type 2
diabetes status to identify any deviations from normative aging associated with
type 2 diabetes.
  Results: When adjusting for confounders, the aging trends for 10 of 13
morphological features were significantly different between patients with type
2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons
corrections). Additionally, MRI appeared to yield different pancreas
measurements than CT using our AI-based method.
  Conclusions: We provide lifespan trends demonstrating that the size and shape
of the pancreas is altered in type 2 diabetes using 675 control patients and
675 diabetes patients. Moreover, our findings reinforce that the pancreas is
smaller in type 2 diabetes. Additionally, we contribute a reference of lifespan
pancreas morphology from a large cohort of non-diabetic control patients in a
clinical setting.

</details>


### [71] [MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition](https://arxiv.org/abs/2508.14889)
*Mert Kiray,Alvaro Ritter,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: MS-CLR is a self-supervised contrastive learning framework that aligns pose representations across multiple skeleton conventions from the same sequence to learn more generalizable features for skeleton-based action recognition.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for skeleton-based action recognition rely on single skeleton conventions, limiting their ability to generalize across datasets with diverse joint structures and anatomical coverage.

Method: Propose Multi-Skeleton Contrastive Learning (MS-CLR) that aligns pose representations across multiple skeleton conventions from the same sequence. Adapt ST-GCN architecture with unified representation scheme to handle varying joint layouts and scales.

Result: MS-CLR consistently improves performance over single-skeleton contrastive learning baselines on NTU RGB+D 60 and 120 datasets. Multi-skeleton ensemble further boosts performance, achieving new state-of-the-art results.

Conclusion: Aligning representations across multiple skeleton conventions enables learning of structural invariances and diverse anatomical cues, resulting in more expressive and generalizable features for skeleton-based action recognition.

Abstract: Contrastive learning has gained significant attention in skeleton-based
action recognition for its ability to learn robust representations from
unlabeled data. However, existing methods rely on a single skeleton convention,
which limits their ability to generalize across datasets with diverse joint
structures and anatomical coverage. We propose Multi-Skeleton Contrastive
Learning (MS-CLR), a general self-supervised framework that aligns pose
representations across multiple skeleton conventions extracted from the same
sequence. This encourages the model to learn structural invariances and capture
diverse anatomical cues, resulting in more expressive and generalizable
features. To support this, we adapt the ST-GCN architecture to handle skeletons
with varying joint layouts and scales through a unified representation scheme.
Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR
consistently improves performance over strong single-skeleton contrastive
learning baselines. A multi-skeleton ensemble further boosts performance,
setting new state-of-the-art results on both datasets.

</details>


### [72] [GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects](https://arxiv.org/abs/2508.14891)
*Licheng Shen,Saining Zhang,Honghan Li,Peilin Yang,Zihao Huang,Zongzheng Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: A unified method using articulated 3D Gaussians to jointly model geometry and motion of complex articulated objects, outperforming previous approaches that struggled with multi-part objects.


<details>
  <summary>Details</summary>
Motivation: Prior methods decouple geometry and motion reconstruction, complicating pipelines and limiting scalability for objects with complex, multi-part articulation.

Method: Introduces a unified representation using articulated 3D Gaussians that jointly models geometry and motion, supporting objects with up to 20 parts.

Result: Achieves superior accuracy in part-level geometry reconstruction and motion estimation across diverse object types, significantly outperforming prior methods.

Conclusion: The unified articulated representation enables scalable physical modeling with applications in robotic simulation and human-scene interaction, demonstrating strong potential for digital twin creation.

Abstract: Reconstructing articulated objects is essential for building digital twins of
interactive environments. However, prior methods typically decouple geometry
and motion by first reconstructing object shape in distinct states and then
estimating articulation through post-hoc alignment. This separation complicates
the reconstruction pipeline and restricts scalability, especially for objects
with complex, multi-part articulation. We introduce a unified representation
that jointly models geometry and motion using articulated 3D Gaussians. This
formulation improves robustness in motion decomposition and supports
articulated objects with up to 20 parts, significantly outperforming prior
approaches that often struggle beyond 2--3 parts due to brittle initialization.
To systematically assess scalability and generalization, we propose MPArt-90, a
new benchmark consisting of 90 articulated objects across 20 categories, each
with diverse part counts and motion configurations. Extensive experiments show
that our method consistently achieves superior accuracy in part-level geometry
reconstruction and motion estimation across a broad range of object types. We
further demonstrate applicability to downstream tasks such as robotic
simulation and human-scene interaction modeling, highlighting the potential of
unified articulated representations in scalable physical modeling.

</details>


### [73] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: Virtual Community is an open-world platform for studying human-robot coexistence using a physics-based simulator with real-world 3D scenes, featuring multi-agent challenges for social intelligence research.


<details>
  <summary>Details</summary>
Motivation: To explore the societal transformation from AI and robotics progress, enabling study of how humans and robots can intelligently coexist, cooperate, and build communities in shared spaces.

Method: Developed an open-source multi-agent physics simulator with real-world aligned community generation, including outdoor/indoor scenes and grounded agents with rich characters. Created two challenges: Community Planning Challenge for multi-agent reasoning and Community Robot Challenge for heterogeneous robot collaboration.

Result: The platform enables evaluation of various baselines, demonstrating challenges in both high-level open-world task planning and low-level cooperation controls for human-robot interaction scenarios.

Conclusion: Virtual Community provides a foundation for further research into human-robot coexistence in open-world environments, addressing both social intelligence and practical cooperation challenges.

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>
