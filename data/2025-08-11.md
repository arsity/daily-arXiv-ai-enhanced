<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: ResPA is a novel adversarial attack method using residual gradients to improve transferability by guiding perturbations toward flat loss regions.


<details>
  <summary>Details</summary>
Motivation: Existing transfer-based attacks overlook perturbation direction, limiting transferability. ResPA addresses this gap.

Method: ResPA uses exponential moving average of gradients to guide perturbations, considering both local and global direction changes.

Result: ResPA outperforms existing transfer-based attacks in transferability and works well with input transformation methods.

Conclusion: ResPA enhances adversarial example transferability by leveraging residual gradients, offering a robust attack method.

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: The paper proposes a Generalized Few-shot OOD Detection (GOOD) framework to improve generalization in few-shot OOD detection by using an auxiliary General Knowledge Model (GKM) and a Knowledge Dynamic Embedding (KDE) mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot OOD detection methods lack generalization capability, leading to overfitting and inconsistent performance across scenarios.

Method: The GOOD framework introduces a GKM to provide general knowledge and a KDE mechanism to dynamically align output distributions based on Generalized Belief (G-Belief).

Result: The framework reduces the generalization error upper bound and shows superior performance on real-world OOD benchmarks.

Conclusion: The GOOD framework effectively balances generality and specificity, enhancing few-shot OOD detection performance.

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide introduces UnGuidance, a dynamic inference mechanism for precise unlearning in diffusion models, outperforming LoRA-based methods.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse of text-to-image models by enabling effective removal of harmful or misleading content without degrading performance.

Method: Uses UnGuidance with Classifier-Free Guidance (CFG) to dynamically control the unlearning process via LoRA, balancing the base model and LoRA adapter.

Result: Achieves controlled concept removal while preserving image fidelity, outperforming existing LoRA-based methods.

Conclusion: UnGuide offers a robust solution for targeted unlearning in diffusion models, maintaining expressive power and content quality.

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [4] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: Proposes a partial-convolution-based style transfer network for applying artistic styles to specific image regions, improving accuracy and blending.


<details>
  <summary>Details</summary>
Motivation: Standard methods apply style transfer to entire images, leading to improper style capture in regions of interest when masked post-stylization.

Method: Uses partial-convolution-based networks and internal blending techniques to target style application to specific regions.

Result: Demonstrates improved stylization accuracy and visual quality on the SA-1B dataset.

Conclusion: The proposed method effectively addresses limitations of post-stylization masking, enhancing precision in artistic style transfer.

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [5] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2 is an accelerated 3D medical image synthesis framework using rectified flow for fast, high-quality generation and a novel contrastive loss for better condition fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for medical image synthesis face issues like slow inference, limited generalizability, and weak alignment with input conditions.

Method: MAISI-v2 integrates rectified flow for speed and introduces a region-specific contrastive loss to improve condition consistency.

Result: Achieves state-of-the-art image quality with 33× acceleration and demonstrates utility in downstream tasks like segmentation.

Conclusion: MAISI-v2 advances medical image synthesis by addressing speed and condition fidelity, with released resources for community use.

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [6] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: A framework for few-shot deployment of pretrained MRI transformers using MAE pretraining achieves high accuracy in classification and segmentation tasks with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of annotated data in medical imaging by leveraging pretrained transformers for few-shot learning.

Method: Utilize Masked Autoencoder (MAE) pretraining on large-scale brain MRI data, combining frozen MAE encoders with lightweight heads for classification and hybrid architectures (MAE-FUnet) for segmentation.

Result: State-of-the-art accuracy in MRI sequence identification and superior performance in skull stripping and multi-class segmentation under data-limited conditions.

Conclusion: The framework is efficient, stable, and scalable, making it suitable for low-resource clinical and neuroimaging applications.

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [7] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: A fast, reconstruction- and optimization-free method for stylizing 3D Gaussian splats using a graph structure and surface-based stylization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D Gaussian splat style transfer require reconstruction, fine-tuning, or optimization, which are computationally expensive.

Method: Generate a graph structure on the splat's implicit surface, apply feed-forward stylization, and interpolate back to splats.

Result: Achieves fast stylization (under 2 minutes on consumer hardware) without additional training or optimization.

Conclusion: The proposed method is efficient, flexible, and produces high-quality results compared to existing approaches.

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [8] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN enhances NeRF for industrial inspection by handling multi-zoom images, improving accuracy and detail capture.


<details>
  <summary>Details</summary>
Motivation: NeRF lacks fine details for industrial tasks like defect detection. Multi-zoom images break NeRF's consistency.

Method: MZEN adds a learnable zoom scalar and a pose strategy for wide-field and zoom-in images.

Result: MZEN outperforms baselines, boosting PSNR by 28%, SSIM by 10%, and reducing LPIPS by 222%.

Conclusion: MZEN extends NeRF to industrial settings, capturing micron-level details while maintaining global accuracy.

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [9] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2 enhances promptable video object segmentation and tracking in surgical videos by addressing motion dynamics and memory redundancy in SAM2, achieving top performance on EndoVis datasets.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models like SAM2 struggle with surgical video analysis due to complex motion and memory redundancy, limiting their effectiveness.

Method: TSMS-SAM2 introduces multi-temporal-scale video sampling augmentation and a memory splitting/pruning mechanism to improve robustness and efficiency.

Result: Achieved mean Dice scores of 95.24 (EndoVis2017) and 86.73 (EndoVis2018), outperforming prior methods.

Conclusion: TSMS-SAM2 is effective for robust, efficient segmentation in surgical scenarios, validated by ablation studies.

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [10] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: Temporal Cluster Assignment (TCA) improves video segmentation by leveraging temporal coherence, reducing computation while retaining details.


<details>
  <summary>Details</summary>
Motivation: Swin Transformer's high computational cost in video segmentation limits real-time applications, and existing token reduction methods fail to exploit temporal redundancy.

Method: TCA refines token clusters using temporal correlations across frames, avoiding indiscriminate token dropping.

Result: TCA enhances accuracy-speed trade-off on multiple datasets, including YouTube-VIS, OVIS, and surgical videos.

Conclusion: TCA effectively generalizes across natural and domain-specific videos, optimizing performance without fine-tuning.

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [11] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: A vision-language framework predicts driver gaze shifts using natural language, outperforming general-purpose models in attention shift detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on static attention estimation, lacking dynamic and explainable predictions. This work aims to model gaze shifts through language for better HCI and autonomous driving.

Method: Uses few-shot/zero-shot learning on RGB images, aligns visual perception with attention-centric understanding via fine-tuned LLaVA, and integrates low-level cues and top-down context.

Result: Fine-tuned model excels in attention shift detection and interpretability, validated by domain-specific metrics.

Conclusion: The framework pioneers language-based gaze prediction, enhancing explainable AI for autonomous driving and enabling downstream applications like behavior forecasting.

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [12] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: A multi-view camera method for gaze target estimation (GTE) improves accuracy by addressing single-view limitations like occlusion and ambiguity, using modules for head information aggregation, gaze selection, and cross-view scene attention.


<details>
  <summary>Details</summary>
Motivation: Existing single-view GTE methods struggle with face occlusion, target ambiguity, and out-of-view targets, limiting accuracy and applicability.

Method: The approach integrates two camera views with modules for Head Information Aggregation (HIA), Uncertainty-based Gaze Selection (UGS), and Epipolar-based Scene Attention (ESA).

Result: The method outperforms single-view baselines, especially when the second camera provides a clear face view, and can estimate gaze targets using only the second view.

Conclusion: The introduced multi-view GTE method and dataset advance the field by addressing single-view limitations and enabling new capabilities.

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [13] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA improves test-time adaptation for VLMs by recursively updating embeddings and adaptively ensembling prompts, outperforming existing methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current cache-based TTA methods limit adaptation by storing only high-confidence samples, ignoring broader test data influence.

Method: ETTA introduces a Recursive Updating module for dynamic embedding refinement and an Adaptive Ensemble module to reduce prompt dependency.

Result: ETTA achieves superior accuracy and computational efficiency on benchmarks, setting a new state-of-the-art.

Conclusion: ETTA offers an effective, efficient solution for test-time adaptation in VLMs, with released code for reproducibility.

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [14] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0 is a vision-language-guided framework for generating and editing 3D scenes from text, supporting diverse styles and interactive feedback.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation requires manual effort and lacks flexibility. HOLODECK 2.0 aims to automate this process with high semantic fidelity and editing support.

Method: Uses vision-language models (VLMs) to parse objects and generate assets, applying spatial constraints for coherent layouts. Supports iterative editing based on human feedback.

Result: Generates high-quality, diverse 3D scenes aligned with text descriptions, outperforming baselines. Enables style-consistent editing and procedural game modeling.

Conclusion: HOLODECK 2.0 advances automated 3D scene generation with flexibility, quality, and practical applications like gaming.

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [15] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch is an unsupervised deep image stitching framework that ensures robustness and naturalness by using a dual-branch architecture and virtual optimal planes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of robustness and naturalness in image stitching, especially in diverse real-world scenes.

Method: Uses a dual-branch model (pretrained and learnable branches) and introduces virtual optimal planes to balance content alignment and structural preservation.

Result: Significantly outperforms existing methods in scene robustness and content naturalness.

Conclusion: RopStitch is a highly effective framework for unsupervised image stitching, with superior performance and generalizability.

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [16] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: Neural field models are used to compactly represent complex geometry and lighting effects in mobile imaging, outperforming state-of-the-art methods without complex pre-processing or labeled data.


<details>
  <summary>Details</summary>
Motivation: The rise of mobile imaging and its diverse technologies, combined with the potential of neural fields, motivates the exploration of efficient scene reconstruction and computational imaging.

Method: Carefully designed neural field models are trained to map spatial inputs to outputs, enabling applications like depth estimation and image stitching directly from smartphone data.

Result: The proposed methods outperform existing approaches by leveraging self-regularized models and stochastic gradient descent, fitting directly to raw smartphone measurements.

Conclusion: Neural field models offer a powerful, efficient solution for mobile computational imaging, eliminating the need for complex pre-processing or labeled data.

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [17] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: The paper evaluates SAM and Mask3D for 3D segmentation in construction sites, highlighting their adaptability and the lack of outdoor benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with complex, dynamic construction environments, necessitating advanced computer-vision solutions.

Method: Comparative analysis of SAM and Mask3D in real-world construction settings, trained on indoor datasets.

Result: Identifies performance gaps due to missing outdoor benchmarks and suggests tailored workflows for actionable insights.

Conclusion: Advances automated monitoring by addressing segmentation challenges in construction, calling for specialized benchmarks.

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [18] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD is a self-supervised framework for normal estimation from a single image, addressing multi-view inconsistency and data dependency via 3D Gaussian splatting and diffusion models.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in normal estimation from single images, such as multi-view normal conflicts and reliance on dense annotations, by integrating physics-driven modeling and differentiable rendering.

Method: Combines light-interaction-driven 3DGS reparameterization, cross-domain feature fusion in a diffusion model, and differentiable 3D reprojection loss for self-supervised optimization.

Result: Outperforms state-of-the-art methods on the Google Scanned Objects dataset.

Conclusion: SINGAD effectively solves multi-view inconsistency and reduces dependency on annotated data, advancing single-image normal estimation.

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [19] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1 integrates pretrained MLLMs and diffusion models using patch-level CLIP embeddings for efficient, high-fidelity image generation without compromising reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable high-fidelity visual synthesis in LLMs without costly training or loss of reasoning capabilities.

Method: Uses patch-level CLIP embeddings as latents, integrates them into diffusion models via ControlNet, and adds a visual generation branch to MLLMs.

Result: Achieves comparable/better performance in fidelity and understanding with lower compute.

Conclusion: Bifrost-1 is an efficient, effective framework for controllable image generation with pretrained models.

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [20] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG bridges the gap between high-level task semantics and low-level geometric features in robotic manipulation by combining automatic primitive extraction, VLM-driven semantic anchoring, and spatial-semantic reasoning.


<details>
  <summary>Details</summary>
Motivation: The challenge of linking task semantics with geometric features in robotic manipulation, and the limitations of current VLMs in capturing dynamic semantic-affordance relationships.

Method: PASG introduces automatic primitive extraction, VLM-driven semantic anchoring, and a spatial-semantic reasoning benchmark with a fine-tuned VLM (Qwen2.5VL-PA).

Result: PASG achieves performance comparable to manual annotations in robotic manipulation tasks, enabling finer-grained semantic-affordance understanding.

Conclusion: PASG provides a unified framework for integrating geometric primitives with task semantics, improving robotic manipulation capabilities.

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [21] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene integrates 4D human animation with 3D scene reconstruction, addressing placement, style alignment, and camera trajectory issues for realistic results.


<details>
  <summary>Details</summary>
Motivation: Seamlessly integrating 4D human animation into 3D scenes is challenging due to placement, lighting/style mismatches, and camera movement needs.

Method: AnimateScene uses a placement module for accurate human positioning, a style alignment method for visual coherence, and joint post-reconstruction for camera trajectories.

Result: The framework produces dynamic scene videos with high geometric detail and spatiotemporal coherence.

Conclusion: AnimateScene effectively solves key challenges in integrating 4D human animation with 3D scenes, enabling realistic and visually appealing results.

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [22] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: Proposes Energy-based Test-time Adaptation (ETA) for adapting pretrained depth completion models to novel environments by minimizing energy scores of predictions.


<details>
  <summary>Details</summary>
Motivation: Addresses covariate shift in depth completion models when applied to new environments without prior target data access.

Method: Uses adversarial perturbations to train an energy model scoring predictions as in- or out-of-distribution, then updates model parameters at test time to minimize energy.

Result: Improves over state-of-the-art by 6.94% (outdoors) and 10.23% (indoors) across six datasets.

Conclusion: ETA effectively adapts depth completion models to novel conditions without prior target data assumptions.

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [23] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: Proposes an efficient video computer vision system by eliminating the image signal processor and using Bayer-format data, along with a fast block matching-based motion estimation algorithm and refinement techniques.


<details>
  <summary>Details</summary>
Motivation: Addresses high temporal redundancy in videos and front-end computation overhead, which existing methods fail to fully reduce.

Method: Removes the image signal processor, uses Bayer-format data, introduces a fast block matching-based motion estimation algorithm with MV refinement, and employs a frame selection strategy.

Result: Achieves significant acceleration with slight performance loss in multiple video computer vision tasks.

Conclusion: The proposed system effectively balances efficiency and accuracy by reducing redundancy and optimizing computation.

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [24] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: A novel multimodal emotion recognition framework is proposed, leveraging pre-trained models and innovative fusion strategies to improve performance on the MER2025-SEMI dataset.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-computer interaction by addressing data scarcity and improving emotion recognition accuracy.

Method: Uses pre-trained models for feature extraction, a dual-branch visual encoder, context-enriched text processing, and a fusion strategy with self-attention and residual connections. Noisy labels are refined via multi-source labeling.

Result: Achieves a weighted F-score of 87.49%, outperforming the baseline of 78.63%.

Conclusion: The framework effectively improves emotion recognition performance, validating its design and methodology.

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [25] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: The paper introduces MakeupQuad, a dataset for facial makeup editing, and EvoMakeup, a training framework that improves makeup fidelity and identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for facial makeup editing produce low-quality results due to lack of structured paired data.

Method: Proposes MakeupQuad dataset and EvoMakeup framework for iterative improvement of data and model quality.

Result: EvoMakeup outperforms prior methods on real-world benchmarks, achieving superior makeup fidelity and identity preservation.

Conclusion: The method effectively balances makeup fidelity and identity preservation, supporting high-fidelity, controllable makeup editing.

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [26] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: MathReal introduces a dataset of 2,000 real-world K-12 math questions with images, evaluating MLLMs' performance under realistic conditions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world educational images, limiting MLLMs' practical applicability.

Method: Curated 2,000 questions with mobile-captured images, classified into 3 main and 14 subcategories, and tested MLLMs in 6 experimental settings.

Result: MLLMs struggle with real-world educational contexts, revealing gaps in recognition, comprehension, and reasoning.

Conclusion: MathReal highlights MLLMs' limitations in realistic settings and suggests future improvements.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [27] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: A 3DGS-based pipeline improves novel view synthesis by generating additional training views and using video diffusion priors, outperforming existing methods in challenging scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3DGS methods struggle with artifacts and missing regions when rendering from viewpoints outside the training trajectory, limiting seamless scene exploration.

Method: The proposed pipeline generates additional training views using an information-gain-driven virtual camera placement strategy and refines results with video diffusion priors. Fine-tuning 3D Gaussians with these views enhances reconstruction.

Result: The method outperforms existing 3DGS-based approaches, enabling high-quality, artifact-free rendering from arbitrary viewpoints, as demonstrated on the Wild-Explore benchmark.

Conclusion: The proposed pipeline significantly improves reconstruction quality and scene exploration, addressing limitations of current 3DGS methods.

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [28] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: A diffusion model is developed to generate synthetic particle images, addressing data imbalance and improving multi-class deep neural network training for sub-visible particle analysis.


<details>
  <summary>Details</summary>
Motivation: The scarcity and imbalance of particle data, especially for rare types like silicone oil and air bubbles, hinder effective multi-class classification.

Method: A state-of-the-art diffusion model generates high-fidelity synthetic images to augment training datasets.

Result: Generated samples closely resemble real images, and experiments show improved classification performance with no significant downsides.

Conclusion: The approach effectively mitigates data imbalance, and the models are released publicly for open research and reproducibility.

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [29] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum is a unified network for detailed human parsing, leveraging a repurposed Image-to-Texture diffusion model to improve alignment with body parts and clothing, outperforming baselines in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human parsing lack fine-grained detail in clothing and body parts, and open-vocabulary segmentation often groups humans into a single category, missing diversity.

Method: Spectrum repurposes an Image-to-Texture diffusion model, fine-tuned on 3D human texture maps, to extract features and generate semantically valid masks for diverse clothing and body parts.

Result: Spectrum consistently outperforms baseline methods in cross-dataset experiments for body parts, clothing, unseen categories, and full-body masks.

Conclusion: Spectrum provides a robust solution for detailed human parsing, addressing limitations of existing methods and achieving superior performance in segmentation tasks.

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [30] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit is a fast text-guided image editing method using RectifiedFlow, PerRFI inversion, and Inversion Latent Injection for coherent edits. It outperforms state-of-the-art methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: To enable fast and high-quality text-guided image editing while preserving critical content and following textual instructions closely.

Method: Leverages RectifiedFlow with PerRFI inversion, Inversion Latent Injection for regeneration, Disentangled Prompt Guidance, and Canny-conditioned ControlNet for structural cues.

Result: Achieves better qualitative and quantitative results on the PIE dataset compared to state-of-the-art few-step editing methods.

Conclusion: InstantEdit is efficient and effective for text-guided image editing, balancing editability and detail preservation.

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [31] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: A robust Mixture of Experts (MoE) framework for semi-supervised emotion recognition, integrating diverse modalities and pseudo-labeling, achieving 2nd place in MER2025-SEMI with an F1-score of 0.8772.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of semi-supervised learning in emotion recognition by leveraging diverse input modalities and unlabeled data effectively.

Method: Proposes a MoE system with diverse experts (e.g., VLMs, AU), consensus-based pseudo-labeling, and a two-stage training paradigm, followed by a multi-expert voting ensemble and rule-based re-ranking.

Result: Achieves an F1-score of 0.8772 on the MER2025-SEMI test set, ranking 2nd.

Conclusion: The framework demonstrates effectiveness in semi-supervised emotion recognition by combining diverse experts and pseudo-labeling, with potential for further refinement.

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [32] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM compresses visual representations in the frequency domain using DCT and FFT, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational overhead and latency caused by the large number of vision tokens in VLMs.

Method: Applies a low-pass filter to vision features using 2D DCT, efficiently computed via FFT.

Result: Reduces inference FLOPs by 83.8% and boosts generation speed by 31.2% while maintaining competitive performance.

Conclusion: Fourier-VLM is an efficient and practical solution for VLMs, balancing performance and computational cost.

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [33] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: The paper proposes Next Editing-token Prediction (NEP) for text-guided image editing, focusing on regenerating only the necessary regions to improve efficiency and edit quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods regenerate entire images, leading to unnecessary costs and compromised edit quality in non-edited regions.

Method: Formulates image editing as NEP using autoregressive generation, pre-trains an any-order autoregressive T2I model, and supports zero-shot editing and test-time scaling.

Result: Achieves state-of-the-art performance on image editing benchmarks and enables zero-shot editing with iterative refinement.

Conclusion: NEP improves efficiency and edit quality by selectively regenerating only the intended regions, supported by a versatile pre-trained model.

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [34] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker is a reasoning-based VQA framework using LMMs and reinforcement learning to improve generalization and explainability in video quality assessment.


<details>
  <summary>Details</summary>
Motivation: Existing VQA models struggle with poor generalization to OOD videos and limited explainability, hindering real-world applicability.

Method: Uses GRPO for rule-guided reinforcement learning with three VQA-specific rewards: bell-shaped regression, pairwise ranking, and temporal consistency.

Result: Achieves state-of-the-art performance on in-domain and OOD benchmarks, excelling in distortion attribution and quality description.

Conclusion: Reinforcement learning with score-level supervision effectively builds generalizable and explainable VQA models.

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [35] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net is a framework for creating individualized 3D LV meshes from brain MRI, improving segmentation and shape analysis for neurological disease biomarkers.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in LV shape analysis due to variability and MRI resolution limitations.

Method: Deforms a joint LV-hippocampus template mesh, incorporating anatomical relationships and vertex classification for better correspondence.

Result: Superior reconstruction accuracy and reliable shape descriptors, even with imperfect segmentations.

Conclusion: LV-Net identifies disease-associated LV subregions in Alzheimer's, offering a robust tool for neurological research.

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [36] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: The paper highlights the underutilization of satellite spectral imagery in AGI research, advocating for a comprehensive benchmark to evaluate Earth Observation models.


<details>
  <summary>Details</summary>
Motivation: Satellite spectral imagery is overlooked in AGI research despite its potential to enhance understanding of the natural world. Existing benchmarks lack the ability to assess generalization in this domain.

Method: The paper reviews current benchmarks, identifies their limitations, and proposes a set of tasks for a new benchmark to evaluate Earth Observation models.

Result: The study underscores the gap in evaluating AGI models for Earth Observation data and suggests a framework for improvement.

Conclusion: A more comprehensive benchmark is needed to advance AGI capabilities in Earth Observation, with proposed tasks to guide future evaluations.

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [37] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet is a lightweight two-stage network for event-based camera demosaicing, outperforming state-of-the-art methods with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Challenges in combining Quad Bayer CFA with event pixels lead to aliasing and artifacts, especially on mobile devices.

Method: TSANet uses a two-stage approach with state space augmented cross-attention and a Cross-Swin State Block for efficient demosaicing.

Result: TSANet achieves better PSNR and SSIM scores than DemosaicFormer, with reduced parameters and computation costs.

Conclusion: TSANet enables efficient demosaicing for mobile devices, offering a lightweight and high-performance solution.

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [38] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: SCJoint is a joint learning scheme for SOD and COD tasks, leveraging task-specific parameters and a saliency-based sampling strategy to improve performance.


<details>
  <summary>Details</summary>
Motivation: Previous works assumed joint learning of SOD and COD would confuse networks, but this paper argues the opposite—proper learning can benefit both tasks.

Method: Proposes SCJoint with task-specific learnable parameters in a shared network and SBSS for balanced, high-quality training.

Result: JoNet, trained with SCJoint and SBSS, achieves competitive performance in both SOD and COD tasks.

Conclusion: Joint learning of SOD and COD is feasible and beneficial with the right approach, as demonstrated by SCJoint and SBSS.

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [39] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena is a visual animation framework for evaluating LLMs and MLLMs, highlighting performance gaps via point-light motion patterns and crowd-sourced human votes.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack intuitive feedback; BioMotion Arena addresses this by visualizing model performance differences through biological motion.

Method: Uses point-light imaging and pairwise comparisons, collecting 45k votes on 90 motion variants for 53 models.

Result: Human and expert votes align well; over 90% of models fail basic motion tasks, revealing significant gaps.

Conclusion: BioMotion Arena is a discriminative, flexible benchmark for visualizing model performance without ground-truth restrictions.

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [40] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: A pipeline for generating super-resolved 3D pseudo-healthy target morphologies from clinical MR scans to improve Trochlear Dysplasia treatment.


<details>
  <summary>Details</summary>
Motivation: Current TD treatments rely on low-resolution MR scans and surgeon intuition, leading to inconsistent outcomes and limited adoption of minimally invasive techniques.

Method: Uses Implicit Neural Representation (INR) for super-resolution, a multi-label network for segmentation, and a Wavelet Diffusion Model (WDM) for generating pseudo-healthy 3D shapes.

Result: Produces sub-millimeter resolved 3D shapes, improving sulcus angle and trochlear groove depth in 25 TD patients.

Conclusion: The pipeline offers a radiation-free, high-resolution solution for preoperative planning, enhancing surgical outcomes.

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [41] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE is a unified model for instruction-based image and video editing, trained in two stages (image first, then video) and using collage-based and generative model-based data synthesis for diverse editing tasks.


<details>
  <summary>Details</summary>
Motivation: Instruction-based editing for videos is limited by scarce training data, hindering practical use. DreamVE aims to overcome this by leveraging scalable image data and unified training.

Method: A two-stage training strategy (image then video) with collage-based and generative model-based data synthesis. An efficient editing framework is built on a state-of-the-art T2V model.

Result: DreamVE achieves strong performance in key editing types, with enhanced generalization and transfer capabilities, though collage-based data lacks some attribute editing cases.

Conclusion: DreamVE effectively unifies image and video editing, leveraging scalable data synthesis and a robust framework, with plans to release codes and models.

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [42] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo is a unified distillation framework combining trajectory-preserving and distribution-matching strategies to accelerate video generation while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing distillation methods for video synthesis suffer from performance breakdown or artifacts under few-step settings.

Method: Introduces continuous-time consistency distillation and dual-perspective alignment (distribution and trajectory alignment).

Result: Outperforms existing methods in few-step video generation on the OpenVid-1M benchmark.

Conclusion: SwiftVideo effectively reduces inference steps without compromising video quality.

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [43] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer is a plug-and-play framework for adaptive vision token pruning in VLMs, reducing CUDA latency by 61.3% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods in VLMs fail to exploit dynamic internal signals during inference, leading to inefficiencies.

Method: Introduces a dynamic text-guided pruning mechanism and a principled pruning schedule based on cross-modal attention shifts.

Result: Reduces CUDA latency by 61.3% with 92.9% accuracy on LLaVA-1.5-7B, surpassing SOTA under the same token budget.

Conclusion: AdaptInfer is effective, lightweight, and generalizable for multi-modal tasks.

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [44] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP is a Vision-Language Model (VLM)-based framework for Video Quality Assessment (VQA) that reduces computational costs and improves sensitivity to quality variations.


<details>
  <summary>Details</summary>
Motivation: Current VQA methods rely on pretraining on large datasets, which is computationally expensive and insufficient for capturing video quality factors like distortion and motion.

Method: Q-CLIP uses a Shared Cross-Modal Adapter (SCMA) with minimal trainable parameters and introduces learnable quality-level prompts to enhance sensitivity. It also explores frame-difference-based sampling.

Result: Q-CLIP achieves excellent performance on multiple VQA datasets.

Conclusion: Q-CLIP offers a computationally efficient and effective solution for VQA by leveraging VLMs and innovative training strategies.

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [45] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: The paper introduces a method to generate diverse human reaction motions driven by emotional cues, addressing the gap in existing frameworks that ignore emotions.


<details>
  <summary>Details</summary>
Motivation: Emotion is crucial in human interactions, but current motion generation frameworks overlook it, reducing naturalness and limiting applications like reaction synthesis.

Method: A semi-supervised emotion prior is integrated into an actor-reactor diffusion model to learn emotion representation and generate reactions considering spatial interaction and emotional response.

Result: The model outperforms existing reaction generation methods, producing realistic reactions under various emotional conditions.

Conclusion: The proposed approach effectively addresses the challenge of emotion-driven motion generation, with potential applications in interactive tasks.

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [46] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: UGD-IML, a novel generative framework using diffusion models, unifies Image Manipulation Localization (IML) and Constrained IML (CIML) tasks, reducing reliance on large labeled datasets and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Advanced image editing tools threaten visual content integrity, but existing IML methods rely on large annotated datasets, which are scarce and lack diversity.

Method: Proposes UGD-IML, a diffusion model-based framework that unifies IML and CIML tasks, leveraging class embedding and parameter-sharing for efficient mode switching.

Result: UGD-IML outperforms SOTA methods by 9.66 and 4.36 F1 metrics for IML and CIML, respectively, and excels in uncertainty estimation and robustness.

Conclusion: UGD-IML offers a scalable, efficient solution for image forgery detection, reducing annotation dependency and improving performance.

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [47] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: The paper proposes MCA, a robust framework for 2D-3D cross-modal retrieval, addressing noisy labels through multimodal joint correction and adaptive alignment.


<details>
  <summary>Details</summary>
Motivation: Imperfect annotations in 2D-3D data pose challenges for cross-modal retrieval, requiring robust solutions to handle noisy labels.

Method: MCA includes a Multimodal Joint label Correction (MJC) mechanism for label refinement and a Multi-level Adaptive Alignment (MAA) strategy for feature enhancement.

Result: MCA achieves state-of-the-art performance on both conventional and noisy 3D benchmarks.

Conclusion: MCA is effective and generalizable for robust 2D-3D cross-modal retrieval under noisy label conditions.

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [48] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: A self-supervised learning framework for handwritten mathematical expression recognition (HMER) using contrastive loss and progressive spatial masking, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: HMER is challenging due to 2D structure, varying symbol scales, and complex spatial relationships; labeled data is expensive.

Method: Combines global/local contrastive loss for encoder pretraining, self-supervised attention with progressive masking, and supervised fine-tuning with a transformer decoder.

Result: Outperforms SSL and fully supervised baselines on CROHME benchmarks.

Conclusion: The progressive attention mechanism enhances HMER performance without requiring labeled data for pretraining.

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [49] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++ is a training framework integrating FMCE-Net for feature convergence, improving model performance without architecture changes.


<details>
  <summary>Details</summary>
Motivation: Address the lack of experimental validation and closed-loop integration in FMCE for DNN interpretability.

Method: Uses a pretrained FMCE-Net as an auxiliary head to generate FMCS predictions, combined with task labels for joint supervision via Representation Auxiliary Loss.

Result: Achieves accuracy gains (e.g., +1.16 pp on ResNet-50/CIFAR-10) across multiple datasets.

Conclusion: FMCE-Net++ effectively enhances model performance by optimizing feature convergence.

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [50] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive introduces a novel end-to-end autonomous driving framework using a gated mamba fusion architecture and geometrically-augmented LiDAR representation, outperforming transformer-based models in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformer-based fusion in diffusion models for autonomous driving faces limitations like quadratic complexity and lack of spatial priors, hindering high-resolution feature use and BEV representation modeling.

Method: GMF-Drive replaces histogram-based LiDAR with a geometrically-augmented pillar format and introduces a hierarchical gated mamba fusion (GM-Fusion) architecture using a state-space model (SSM) for efficient, spatially-aware processing.

Result: GMF-Drive achieves state-of-the-art performance on the NAVSIM benchmark, significantly outperforming DiffusionDrive, with ablation studies confirming component efficacy.

Conclusion: Task-specific SSMs can surpass general-purpose transformers in autonomous driving, offering better performance and efficiency.

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [51] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg introduces Multi-Category Contrastive Learning (MCCL) and Feature Synergy Structure (FSS) to improve weakly-supervised semantic segmentation, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in open-vocabulary semantic segmentation, such as semantic misalignment and poor performance in weakly-supervised settings.

Method: Proposes MCCL for robust intra- and inter-category alignment and FSS for discriminative feature reconstruction.

Result: Achieves higher accuracy than SOTA baselines (e.g., 4.5% on VOC, 8.9% on Context).

Conclusion: SynSeg enhances semantic localization and discrimination under weak supervision, demonstrating superior performance.

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [52] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: The study compared representation learning methods (PCA, CAE, PT) on satellite images for weather event classification, finding CAE superior. Higher-resolution data improved deep learning results, and smaller latent spaces increased false alarms. A physics-informed CAE was suggested for future work.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of different representation learning algorithms (PCA, CAE, PT) for classifying weather events from satellite images.

Method: Applied PCA, CAE, and PT to satellite images, evaluated their latent spaces, and tested classifications of weather events.

Result: CAE outperformed PCA and PT in most tasks, with higher-resolution data improving deep learning results. Smaller latent spaces increased false alarms.

Conclusion: CAE is effective but lacks physical interpretability; a physics-informed CAE is proposed for future research.

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [53] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner is a reinforcement learning framework for self-correcting image caption models, using a novel reward function and refined metrics to improve caption quality.


<details>
  <summary>Details</summary>
Motivation: To enhance image caption models by enabling self-correction and improving evaluation metrics.

Method: Uses scene-graph parsing to decompose captions into sets, calculates set differences for rewards/punishments, and refines CAPTURE metrics.

Result: Outperforms direct preference optimization, generating better captions across scenarios.

Conclusion: SC-Captioner effectively improves caption quality through self-correction and refined evaluation.

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [54] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: VeSCA is a novel method to generate transferable adversarial examples for SAM by identifying shared vulnerabilities through a simplicial complex, improving performance by 12.7% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SAM's vulnerabilities pose risks to downstream applications, but prior attacks lack transferability due to insufficient exploration of shared weaknesses.

Method: VeSCA uses SAM's encoder to characterize shared vulnerabilities via a parametric simplicial complex, refined iteratively, with domain re-adaptation for minimal reference data.

Result: VeSCA outperforms state-of-the-art methods by 12.7% across three downstream model categories and five datasets.

Conclusion: SAM's vulnerabilities threaten downstream models, underscoring the need for more robust foundation models.

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [55] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: A novel 3D gaze redirection framework using explicit 3D eyeball structure and 3D Gaussian Splatting outperforms NeRF-based methods in image quality and gaze accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing gaze redirection methods rely on implicit neural representations (NeRF), lacking explicit modeling of eyeball rotation and translation. This work aims to address this limitation.

Method: Introduces a 3D eyeball structure with 3D Gaussian Splatting (3DGS) for explicit rotation/translation control and an adaptive deformation module for muscle movement replication.

Result: Achieves superior photorealistic image generation and gaze estimation accuracy on the ETH-XGaze dataset compared to state-of-the-art methods.

Conclusion: The framework successfully improves gaze redirection by explicitly modeling eyeball structure and movements, outperforming implicit approaches.

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [56] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: A diffusion-based method combines sparse IMUs and a monocular camera for real-time human motion capture, leveraging sequential visual features and frame-wise IMU data for robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges like occlusions or camera view limitations in human motion capture by fusing sparse IMUs and monocular camera data effectively.

Method: Uses a diffusion model to integrate sequential visual features as a condition embedding and frame-wise IMU measurements with noisy body poses.

Result: Demonstrates robust performance and state-of-the-art accuracy in pose estimation.

Conclusion: The proposed framework effectively combines IMUs and camera data, outperforming previous methods, with code available for research.

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [57] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval is a dynamic safety evaluation framework for MLLMs, using text, image, and text-image dynamics to update benchmarks and address data contamination.


<details>
  <summary>Details</summary>
Motivation: Safety concerns in MLLM outputs and outdated or contaminated datasets necessitate a dynamic evaluation approach.

Method: SDEval employs text, image, and text-image dynamics to generate new samples from benchmarks, testing individual and combined effects on model safety.

Result: SDEval significantly impacts safety evaluation, mitigates data contamination, and reveals MLLM safety limitations across benchmarks.

Conclusion: SDEval provides a flexible, effective solution for dynamic safety evaluation in MLLMs, applicable to both safety and capability benchmarks.

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [58] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO introduces early fusion, order-aligned query selection, and a generative data engine to improve multimodal vision models for open-world segmentation.


<details>
  <summary>Details</summary>
Motivation: Address limitations in late-stage feature fusion, suboptimal query selection, and caption-derived vocabulary constraints in multimodal vision models.

Method: Proposes Prompt-DINO with early fusion, order-aligned query selection, and a generative data engine using the RAP model.

Result: Achieves state-of-the-art performance on open-world detection benchmarks, expands semantic coverage, and reduces label noise by 80.5%.

Conclusion: Establishes a scalable paradigm for multimodal detection and data generation in open-world scenarios.

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [59] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: The paper introduces DSConv, a dynamic kernel-splitting method with attention for pansharpening, improving feature extraction and network performance.


<details>
  <summary>Details</summary>
Motivation: Existing pansharpening methods rely on standard convolutions, neglecting adaptive approaches that leverage inter-pixel correlations in remote sensing images.

Method: Proposes DSConv, which dynamically splits convolution kernels and integrates attention to focus on key positions, enhancing feature extraction.

Result: DSConv achieves state-of-the-art performance, improving generalization, optimization, and feature representation.

Conclusion: DSConv is superior for pansharpening, with rigorous experiments validating its effectiveness and optimal usage conditions.

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [60] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR is a user-centric, multi-dimensional benchmark for text-to-image evaluation, combining deterministic metrics and a novel HWPQ scheme for abstract semantics, achieving high human alignment and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing text-to-image evaluation metrics by incorporating user-centric, multi-dimensional assessment.

Method: Two-tier hybrid paradigm: deterministic metrics for quantifiable attributes and HWPQ for abstract semantics, validated by expert Delphi study and human comparisons.

Result: High human alignment (>75%), HWPQ achieves 85.9% accuracy on abstract semantics, outperforming VQA baselines. No universal champion model identified.

Conclusion: VISTAR provides actionable guidance for domain-specific deployment and fosters reproducible T2I assessment with publicly released resources.

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [61] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: The paper proposes MPF-KANSC, a deep learning framework combining multi-plane fusion and a novel attention mechanism for improved Alzheimer's disease diagnosis using sMRI.


<details>
  <summary>Details</summary>
Motivation: Early and precise AD diagnosis is challenging due to subtle brain changes. Existing methods lack accuracy in capturing complex brain atrophy features.

Method: MPF-KANSC integrates multi-plane fusion (coronal, sagittal, axial) and a Kolmogorov-Arnold Network-guided spatial-channel attention mechanism for feature extraction and disease identification.

Result: MPF-KANSC outperforms existing methods on the ADNI dataset and reveals right-lateralized asymmetry in subcortical changes.

Conclusion: The framework enhances AD diagnosis accuracy and interpretability, offering insights into disease progression.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [62] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff is a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at input and module levels, showing better efficiency-fidelity trade-offs than reducing denoising steps.


<details>
  <summary>Details</summary>
Motivation: High computational demands of diffusion models challenge deployment on resource-limited platforms, prompting investigation into compute-optimal deployment strategies.

Method: Proposes PostDiff, which uses mixed-resolution denoising and hybrid module caching to reduce redundancy without fine-tuning.

Result: PostDiff improves fidelity-efficiency trade-offs; reducing per-step inference cost is more effective than reducing denoising steps.

Conclusion: PostDiff offers a practical solution for efficient diffusion model deployment, prioritizing per-step cost reduction over step reduction.

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [63] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS improves underwater 3D reconstruction by combining 3D Gaussian Splatting with a learnable physics model and adaptive pruning, outperforming existing methods like SeaThru-NeRF.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like NeRF struggle with underwater conditions due to light absorption and scattering, leading to degraded geometry and color fidelity.

Method: UW-3DGS uses a voxel-based regression for underwater image formation and a Physics-Aware Uncertainty Pruning (PAUP) branch to remove noisy Gaussians. It operates in training and rendering stages.

Result: Achieves PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% fewer floating artifacts.

Conclusion: UW-3DGS offers robust underwater reconstruction with improved accuracy and reduced artifacts, leveraging physics-aware learning and adaptive pruning.

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [64] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: A novel framework for automated polyp detection in colonoscopy images uses synthetic data generation and advanced detection/segmentation models, achieving high accuracy and performance metrics.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of cancer-related deaths, necessitating early detection via colonoscopy. Limited dataset sizes and annotation complexities drive the need for automated solutions.

Method: Combines Faster R-CNN for polyp localization and SAM for segmentation, alongside synthetic data generation. Evaluates five segmentation models (U-Net, PSPNet, FPN, LinkNet, MANet) with ResNet34.

Result: Faster R-CNN achieved 93.08% recall, 88.97% precision, and 90.98% F1 score. FPN led in PSNR (7.205893) and SSIM (0.492381), while UNet excelled in recall (84.85%) and LinkNet in IoU (64.20%) and Dice (77.53%).

Conclusion: The framework effectively automates polyp detection, leveraging synthetic data and advanced models, with FPN and LinkNet showing standout performance in segmentation tasks.

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [65] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: A graph-based localization framework using GCNs achieves high accuracy (0.64cm error) and efficiency, solving the kidnapped robot problem without complex filtering.


<details>
  <summary>Details</summary>
Motivation: Traditional localization methods (e.g., Lidar, QR-codes) lack scalability and adaptability in complex environments.

Method: Uses graph-based representations of floor features with Graph Convolutional Networks (GCNs) for localization.

Result: Achieves 0.64cm localization error and handles the kidnapped robot problem per frame.

Conclusion: The framework offers robust and efficient robotic navigation in diverse environments.

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [66] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: MA-CBP is a multi-agent framework for predicting criminal behavior in real-time by analyzing video streams and fusing long- and short-term contexts, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Urbanization increases public security threats; existing methods lack real-time capability and high-level semantic understanding.

Method: Transforms video streams into semantic descriptions, constructs causal summaries, and fuses frames for joint reasoning.

Result: Achieves superior performance on datasets and enables early warning of criminal activity.

Conclusion: MA-CBP offers a promising solution for urban public safety risk warning.

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [67] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: Proposes DBIF-AUNet for pleural effusion CT image segmentation, addressing challenges like similar gray levels and blurred edges with innovative modules, achieving superior performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Enhance clinical diagnosis by improving accuracy in pleural effusion segmentation, tackling issues like semantic gaps and complex edges.

Method: Introduces DBIF-AUNet with Dual-Domain Feature Disentanglement (DDFD) and Branch Interaction Attention Fusion (BIAF) modules for multi-scale feature complementarity and dynamic feature fusion.

Result: Achieved IoU of 80.1% and Dice of 89.0%, outperforming U-Net++ and Swin-UNet by significant margins.

Conclusion: DBIF-AUNet significantly improves segmentation accuracy for complex pleural effusion CT images, demonstrating clinical potential.

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [68] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA introduces an efficient architecture expansion method for CVIT in MLLMs, reducing parameter overhead and improving scalability while mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of catastrophic forgetting and parameter inefficiency in continual visual instruction tuning for MLLMs.

Method: LiLoRA shares LoRA matrix A across tasks, applies low-rank decomposition to matrix B, and uses a cosine-regularized stability loss.

Result: LiLoRA achieves superior performance in sequential task learning with improved parameter efficiency.

Conclusion: LiLoRA is a scalable and efficient solution for CVIT in MLLMs, outperforming existing methods.

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [69] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE is a universal anomaly detection framework using a Mixture-of-Experts architecture to detect diverse anomalies hierarchically, outperforming specialized methods.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods are too specialized, limiting generalizability across domains and anomaly types.

Method: AnomalyMoE decomposes anomaly detection into three semantic hierarchies (local, component, global) with dedicated expert networks, enhanced by EIR and ESB modules for diversity and balance.

Result: Outperforms specialized methods on 8 datasets across various domains, achieving state-of-the-art performance.

Conclusion: AnomalyMoE provides a universal, hierarchical solution for detecting a wide range of anomalies, surpassing domain-specific approaches.

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [70] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: The PA-HOI Motion Capture dataset addresses the gap in existing HOI datasets by focusing on how objects' physical attributes influence human motion dynamics, offering 562 motion sequences for analysis.


<details>
  <summary>Details</summary>
Motivation: Existing HOI datasets overlook the impact of objects' physical properties on human motion, limiting understanding in fields like robotics and virtual reality.

Method: The PA-HOI dataset includes 562 motion sequences of human-object interactions, with varied object sizes, shapes, and weights, to study motion dynamics.

Result: The dataset extends understanding of how physical attributes affect human posture, speed, and interaction strategies, and integrates well with motion generation methods.

Conclusion: The PA-HOI dataset enhances HOI research by providing realistic physical awareness, validated through integration with motion generation techniques.

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [71] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: A two-stage pipeline using dual-hand radiographs for interpretable SvdH score prediction in RA, achieving state-of-the-art accuracy comparable to radiologists.


<details>
  <summary>Details</summary>
Motivation: The complexity of manual SvdH scoring limits its clinical use; this work aims to automate and improve efficiency.

Method: Proposes a pipeline with disease-relevant region extraction and attention-based multiple instance learning for image-level prediction. Two schemes are tested: sampling abnormal tiles and cropping joint patches.

Result: Best model achieved PCC 0.943, RMSE 15.73; ensemble improved to PCC 0.945, RMSE 15.57, matching radiologist performance.

Conclusion: The pipeline is effective, interpretable, and clinically relevant, identifying key anatomical structures for RA progression.

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [72] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: TEFormer, a texture-aware and edge-guided Transformer, improves semantic segmentation of urban remote sensing images by addressing texture differences and edge complexities.


<details>
  <summary>Details</summary>
Motivation: Urban remote sensing images (URSIs) have subtle texture differences and complex edge morphologies, leading to semantic ambiguity and misclassification.

Method: TEFormer integrates a texture-aware module (TaM) for fine-grained texture capture, an edge-guided tri-branch decoder (Eg3Head) for edge preservation, and an edge-guided feature fusion module (EgFFM) for refined segmentation.

Result: TEFormer achieves mIoU scores of 88.57%, 81.46%, and 53.55% on Potsdam, Vaihingen, and LoveDA datasets, demonstrating effectiveness.

Conclusion: TEFormer effectively addresses challenges in URSI semantic segmentation, achieving high performance across datasets.

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [73] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: Depth-Jitter is a depth-based augmentation technique that improves model robustness in depth-sensitive environments by simulating natural depth variations.


<details>
  <summary>Details</summary>
Motivation: Conventional augmentation techniques lack depth-aware transformations, limiting model robustness in real-world depth variations.

Method: Depth-Jitter applies adaptive depth offsetting guided by depth variance thresholds to generate synthetic depth perturbations while preserving structural integrity.

Result: While not always outperforming traditional methods in absolute performance, Depth-Jitter consistently enhances model stability and generalization in depth-sensitive environments.

Conclusion: Depth-Jitter highlights the potential of depth-aware augmentation for real-world applications and provides a foundation for further research in depth-based learning strategies.

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [74] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: The paper introduces an all-in-one image deblurring method using a mixture-of-experts (MoE) decoding module to handle diverse blur types efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing deblurring methods lack generalization, requiring multiple models for different blur types, which is impractical.

Method: Proposes a mixture-of-experts (MoE) decoding module that dynamically routes image features based on recognized blur degradation for precise restoration.

Result: The unified approach matches task-specific models' performance and shows robustness on unseen blur scenarios.

Conclusion: The method offers a practical, efficient solution for diverse image deblurring tasks with strong generalization.

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [75] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: LNCLIP-DF, a parameter-efficient adaptation of CLIP, achieves state-of-the-art deepfake detection by fine-tuning only 0.03% of its parameters and enhancing generalization through hyperspherical feature enforcement.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing deepfake detectors to unseen manipulation techniques without excessive architectural complexity.

Method: Fine-tunes Layer Normalization parameters in CLIP, uses L2 normalization and latent space augmentations for a hyperspherical feature manifold.

Result: Outperforms complex methods in cross-dataset AUROC on 13 benchmarks, showing robust generalization. Key findings highlight the importance of paired real-fake data and dataset diversity.

Conclusion: Targeted, minimal changes to pre-trained models can achieve top performance, offering a computationally efficient and reproducible solution.

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [76] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: FedX reduces communication overhead in federated learning for remote sensing tasks by pruning less important model components, maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning in remote sensing faces high communication costs due to large model updates; FedX aims to minimize this overhead.

Method: FedX uses explanation-guided pruning to identify and remove less relevant model components, reducing the size of transmitted models.

Result: FedX significantly cuts communication costs while improving model generalization, outperforming unpruned and other pruning methods.

Conclusion: FedX effectively balances communication efficiency and performance in federated learning for remote sensing tasks.

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [77] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net, a 2.5D U-Net-based model with cross-slice attention and skip attention gating, improves femur MRI segmentation accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing 2D and 3D deep learning methods for femur MRI segmentation are limited, prompting the need for a more accurate and efficient solution.

Method: Proposes XAG-Net, incorporating pixel-wise cross-slice attention (CSA) and skip attention gating (AG) for better inter-slice and intra-slice feature modeling.

Result: XAG-Net outperforms 2D, 2.5D, and 3D U-Net models in accuracy while remaining computationally efficient.

Conclusion: XAG-Net is a promising framework for efficient and accurate femur MRI segmentation, validated by ablation studies.

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [78] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker is a spatially-aware framework for MLLMs that improves visual tasks by mimicking human perception, using attention correction and spatial cues.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex visual tasks like spatial understanding and fine-grained perception, lacking iterative refinement of focus on relevant regions.

Method: SIFThinker uses depth-enhanced bounding boxes and natural language for attention correction. It includes a reverse-expansion-forward-inference strategy for process-level supervision and GRPO-SIF for reinforced training.

Result: SIFThinker outperforms state-of-the-art methods in spatial understanding and fine-grained perception while maintaining general capabilities.

Conclusion: The framework effectively enhances MLLMs' visual task performance through iterative attention correction and spatial awareness.

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [79] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: The paper introduces URPA, a data-efficient method for cross-domain video temporal grounding without labeled target data, using uncertainty-quantified rollouts for adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods like GRPO require labeled data and are computationally expensive, limiting real-time deployment. URPA addresses these issues by enabling adaptation with minimal unlabeled data.

Method: URPA leverages GRPO rollouts to generate pseudo labels and estimates confidence from rollout variance, weighting training rewards to focus on reliable predictions.

Result: URPA achieves strong generalization across six cross-domain settings using only a few unlabeled target videos.

Conclusion: URPA offers a practical solution for real-time, label-efficient cross-domain video temporal grounding.

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [80] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: GS-MoE, a novel framework for Weakly-Supervised Video Anomaly Detection (WSVAD), uses specialized expert models guided by temporal Gaussian splatting to improve anomaly detection, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current WSVAD models struggle with complex anomalies due to shared model limitations and weak supervision signals lacking temporal precision.

Method: Proposes GS-MoE, a framework with expert models for specific anomaly types, guided by temporal Gaussian splatting loss, integrated via a mixture-of-experts mechanism.

Result: Achieves 91.58% AUC on UCF-Crime and superior performance on XD-Violence and MSAD datasets.

Conclusion: GS-MoE sets a new benchmark for VAD under weak supervision by leveraging category-specific expertise and temporal guidance.

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [81] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: A diffusion model generates synthetic cardiac MR images to address domain shift, improving segmentation performance on unseen domains without needing transfer learning.


<details>
  <summary>Details</summary>
Motivation: Domain shift in MR imaging limits AI model deployment in real-world scenarios due to performance degradation on unseen domains.

Method: Proposes a diffusion model trained on a source domain to generate synthetic cardiac MR images, maintaining structural fidelity. Evaluated using 2D/3D nnU-Net and vanilla U-Net for segmentation.

Result: Significantly improved segmentation performance on unseen domains (Welch's t-test, p < 0.01) compared to real data alone.

Conclusion: The method reduces reliance on transfer learning or online training, offering a solution for domain shift in data-scarce settings.

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [82] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: The paper improves ViPro by enabling unsupervised state inference from observations without initial ground truth, addressing shortcuts in the original model.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of ViPro, which relied on ground truth initial states and failed with noisy observations.

Method: Added improvements to ViPro for unsupervised state inference and extended the Orbits dataset to 3D.

Result: The enhanced model correctly infers states from observations without initial ground truth, validated on a 3D dataset.

Conclusion: The improvements enable robust state inference in noisy scenarios, advancing video frame prediction.

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [83] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: Street view imagery can infer social interaction quality, aligning with urban planning theories, using multimodal models and regression analysis.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on pedestrian volume, not social interaction quality. Street view imagery offers a scalable, global data source to address this gap.

Method: Analyzed 2,998 street view images using a multimodal large language model guided by Mehta's sociability taxonomy, with regression models controlling for variables like weather and pedestrian counts.

Result: Sky view index correlated with all sociability types; green view index predicted enduring sociability. Place attachment linked to fleeting sociability.

Conclusion: Street view imagery shows promise for scalable, privacy-preserving urban sociability research, supporting evidence-based city design.

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [84] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT, a novel MLLM, improves abnormal event detection in videos using spatial and temporal token selection modules (SETS and TETG) and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with anomalies due to spatial and temporal sparsity, leading to suboptimal results.

Method: Proposes VA-GPT with SETS and TETG modules to align visual and language tokens, and introduces a specialized dataset for fine-tuning.

Result: Outperforms state-of-the-art methods on benchmarks, including cross-domain evaluation.

Conclusion: VA-GPT effectively addresses challenges in abnormal event detection by leveraging VLMs and LLMs, achieving superior performance.

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [85] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: Implementation of a two-phase image segmentation algorithm based on Goldstein et al.'s modification of the Chan-Vese model, optimized using the split Bregman method.


<details>
  <summary>Details</summary>
Motivation: To efficiently partition 2D images into foreground and background regions using a modified energy model for smoother boundaries and distinct pixel value averages.

Method: The algorithm uses a modified Chan-Vese energy model, optimized with the split Bregman method for efficient minimization, and tests performance across various parameters.

Result: Successful implementation and performance documentation of the algorithm on multiple images with varying parameters.

Conclusion: The method effectively segments images into two regions with smooth boundaries and distinct averages, validated by empirical testing.

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [86] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: The paper introduces a new method for detecting Out-of-gallery cases in one-to-many facial identification by training a classifier using additional enrolled images, showing effectiveness across various probe conditions and demographics.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on similarity score thresholds for Out-of-gallery detection, which may not be optimal. The paper aims to improve accuracy by leveraging additional enrolled images.

Method: Generates training data from ranks of additional enrolled images, trains a classifier to predict In-gallery/Out-of-gallery status, and validates across datasets and matchers.

Result: The approach works well for mugshot and degraded probe images, with similar accuracy across demographics. It reduces false positives and wrongful arrests.

Conclusion: The method is viable and effective, especially with advanced matchers, offering a practical solution to improve facial identification reliability.

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [87] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT introduces a scalable method for general representation learning across unlimited modalities using only text data, achieving top results without modality-specific labels.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on large labeled datasets or are limited to single modalities, prompting the need for a scalable, generalizable approach.

Method: TaAM-CPT uses modality prompt pools, text construction, and modality-aligned text encoders, with intra- and inter-modal learning objectives for consistency.

Result: TaAM-CPT achieves leading performance on diverse tasks (video, image, audio classification) without modality-specific labeled data.

Conclusion: TaAM-CPT offers a scalable, efficient solution for multimodal learning, leveraging text data and pre-trained models for broad applicability.

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [88] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen enables fast novel view synthesis using Video Diffusion Models (VDMs) in just four steps, improving speed by 90% while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the slow sampling speed of VDMs in sparse-view 3D reconstruction tasks.

Method: Proposes a distillation method using GANs and softened reverse KL-divergence to reduce denoising steps.

Result: Achieves similar or better visual quality with 90% faster sampling than prior methods.

Conclusion: FVGen enhances efficiency for 3D reconstruction with sparse views, making VDMs practical for real-world use.

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [89] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper explores integrating classification objectives into super-resolution (SR) processes to enhance both image quality and downstream classification accuracy, proposing a novel method for synthetic aperture radar imagery.


<details>
  <summary>Details</summary>
Motivation: Low-resolution images limit automated analysis accuracy; traditional SR methods focus on pixel-level metrics without exploring their impact on classification performance.

Method: A novel SR methodology optimizes loss functions for both image quality and classification performance, applied to synthetic aperture radar imagery.

Result: The approach improves image quality and enhances classification accuracy.

Conclusion: Integrating classification objectives into SR processes can simultaneously improve image quality and classification performance.

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [90] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper evaluates oversampling in feature space for SAR ship classification, proposing two novel algorithms (M2m$_f$, M2m$_u$) that outperform baselines, improving F1-scores by 8.82% and 4.44% on two datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance in SAR ship classification due to long-tailed datasets, inspired by oversampling's success in optical data.

Method: Proposed M2m$_f$ and M2m$_u$ algorithms, tested on OpenSARShip and FuSARShip datasets using ViT, VGG16, and ResNet50 as feature extractors. Analyzed oversampling's impact on class sizes.

Result: Novel methods outperformed original M2m and baselines, with F1-score increases of 8.82% (FuSARShip) and 4.44% (OpenSARShip).

Conclusion: The proposed oversampling methods effectively improve SAR ship classification performance, especially for underrepresented classes.

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [91] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: A GAN-based semi-supervised learning framework for medical imaging improves classification with minimal labeled data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning in medical imaging is limited by scarce labeled data. This paper addresses the challenge of achieving robust performance with very few labeled samples.

Method: The framework uses three neural networks (generator, discriminator, classifier) in a three-phase training process, combining supervised and unsupervised learning via image-to-image translation and ensemble-based pseudo-labeling.

Result: The method significantly outperforms six GAN-based semi-supervised methods across eleven MedMNIST datasets, especially in extreme low-data settings (e.g., 5 labeled samples per class).

Conclusion: The proposed framework provides a practical solution for medical imaging with minimal labeled data, maintaining superior performance across all evaluated settings.

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [92] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: The paper enhances SimSwap for high-fidelity face swapping with attention mechanisms, dynamic loss weighting, and learning rate scheduling, achieving better identity preservation and visual quality.


<details>
  <summary>Details</summary>
Motivation: To improve face swapping technology by addressing identity preservation, attribute consistency, and visual quality.

Method: Integrates self and cross-attention mechanisms, dynamic loss weighting, and cosine annealing learning rate scheduling into the SimSwap framework.

Result: Improved identity similarity, lower FID scores, and superior qualitative results compared to the baseline.

Conclusion: Future directions include integrating StyleGAN3, improving lip sync, 3D facial modeling, and temporal consistency for videos.

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [93] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin is a non-contrastive plug-in for CLIP-style models to improve multimodal alignment, tested on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Addresses weak supervision in web-collected datasets and low diversity in medical datasets, hindering robust CLIP training.

Method: Proposes CLIPin, a unified plug-in with shared pre-projectors for contrastive and non-contrastive learning integration.

Result: Enhances alignment robustness and generalizability, validated across downstream tasks.

Conclusion: CLIPin is effective and compatible with various contrastive frameworks, offering plug-and-play utility.

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [94] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST is a novel UDA method using language modality to guide vision model adaptation, leveraging pseudo-labels from captions and uncertainty estimation for robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing complex domain shifts (e.g., geographical) where traditional UDA methods struggle due to significant background and object appearance differences.

Method: Generates pseudo-labels from captions, uses normalized CLIP similarity for uncertainty estimation, and introduces multimodal soft-contrastive learning to align vision and language features.

Result: Outperforms previous methods, achieving state-of-the-art on DomainNet and GeoNet benchmarks.

Conclusion: TRUST effectively leverages language modality for robust domain adaptation, mitigating issues from low-quality captions and complex shifts.

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [95] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: The study integrates large language models (LLMs) with the Swin-UMamba architecture for lesion segmentation on CT scans, achieving high accuracy and outperforming prior models.


<details>
  <summary>Details</summary>
Motivation: To enhance lesion segmentation by combining imaging features with text descriptions from radiology reports.

Method: Integration of LLMs into the Swin-UMamba architecture, tested on the ULS23 DeepLesion dataset with radiology report descriptions.

Result: Achieved a Dice Score of 82% and Hausdorff distance of 6.58 pixels, outperforming prior models like LanGuideMedSeg, xLSTM-UNet, and nnUNet.

Conclusion: The Text-Swin-UMamba model effectively combines text and imaging data for superior lesion segmentation.

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [96] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST is a weakly-supervised deep learning framework for estimating daily 10 m Land Surface Temperature (LST) by fusing Terra MODIS, Landsat 8, and Sentinel-2 data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The need for precise environmental monitoring due to urbanization, climate change, and agricultural stress drives the demand for high-resolution LST estimation, which current remote sensing systems struggle to provide.

Method: WGAST uses a conditional generative adversarial network with four stages: feature extraction, fusion, LST reconstruction, and noise suppression, trained weakly supervised with physical averaging principles.

Result: WGAST reduces RMSE by 17.18% and improves SSIM by 11.00% compared to baselines, showing robustness to clouds and capturing fine-scale thermal patterns.

Conclusion: WGAST is an effective solution for high-resolution daily LST estimation, validated by ground sensors, with code publicly available.

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [97] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: The paper introduces a modular and diversified approach to generating synthetic charts for improving multimodal large language models' (MLLMs) chart understanding, achieving better performance on real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with chart understanding (30%-50% success rate), and synthetic chart datasets lack realism, limiting model training.

Method: A five-step data synthesis pipeline: modular chart generation, diversification, quality filtering, and GPT-4o-generated QA pairs, resulting in the Effective Chart Dataset (ECD).

Result: ECD (10k+ charts, 300k+ QA pairs) improves MLLM performance on real-world and synthetic benchmarks.

Conclusion: Modular and diversified chart generation enhances MLLMs' chart understanding, with ECD providing a scalable solution.

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: LightSwitch is a novel material-relighting diffusion framework that improves 3D relighting by leveraging multi-view and intrinsic properties, outperforming prior methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing 2D relighting priors ignore intrinsic properties and multi-view data, leading to subpar results.

Method: LightSwitch uses a finetuned material-relighting diffusion framework with multi-view and material cues for scalable denoising.

Result: Outperforms state-of-the-art relighting priors and matches diffusion inverse rendering methods in quality and speed (as little as 2 minutes).

Conclusion: LightSwitch efficiently and consistently relights multi-view data, advancing 3D relighting with improved quality and scalability.

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>
