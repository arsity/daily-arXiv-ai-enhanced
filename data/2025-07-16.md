<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet introduces a causal wavelet network for low-light image enhancement, leveraging wavelet transforms and causal reasoning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LLIE methods lack instance-level semantic understanding and feature-specific adjustments, prompting the need for a causal reasoning approach.

Method: CWNet uses causal reasoning with global metric learning and local CLIP semantic loss, combined with a wavelet transform-based backbone for frequency optimization.

Result: CWNet surpasses state-of-the-art methods in diverse datasets, demonstrating robust performance.

Conclusion: CWNet effectively addresses limitations of traditional LLIE by integrating causal reasoning and wavelet transforms, achieving superior enhancement results.

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: A novel framework integrates biological knowledge to improve microscopy image profiling for drug discovery, addressing challenges in de novo cell lines by disentangling perturbation-specific and cell line-specific features.


<details>
  <summary>Details</summary>
Motivation: High-throughput screening for de novo cell lines is hindered by morphological and biological heterogeneity, necessitating improved methods for robust perturbation screening.

Method: The framework uses external biological knowledge (protein interaction data and transcriptomic features) to disentangle perturbation-specific and cell line-specific representations during pretraining.

Result: The method enhances microscopy image profiling for de novo cell lines, validated through experiments on RxRx datasets.

Conclusion: The proposed framework effectively improves phenotype-based drug discovery by leveraging biological knowledge for better generalization.

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: The paper audits FER datasets, revealing biases in posed vs. spontaneous expressions and racial/skin color performance disparities, impacting real-world model accuracy and ethics.


<details>
  <summary>Details</summary>
Motivation: To address performance drops in spontaneous expression detection and racial/skin color biases in FER algorithms, linked to dataset collection practices.

Method: Audit two FER datasets by sampling images to classify as spontaneous or posed, and testing model performance across races/skin tones.

Result: Found posed images in 'in-the-wild' datasets and racial biases in model predictions, with darker-skinned individuals often misclassified as showing negative emotions.

Conclusion: FER datasets and models exhibit biases, risking harm in real-world applications; improved data collection and evaluation are needed.

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: A novel technique eliminates descriptor usage in interest point matching, reducing memory while slightly lowering accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on descriptors for matching, which require computation, storage, and transmission. This work aims to simplify the process by associating interest points during detection.

Method: Interest points are inherently associated during detection, bypassing descriptor computation and matching.

Result: Matching accuracy is marginally lower than conventional methods, but memory usage is drastically reduced.

Conclusion: The method offers a memory-efficient alternative to descriptor-based matching, suitable for localization systems.

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: A new dataset of 64k annotated spacecraft images was created for training segmentation models, addressing the scarcity of such data. YOLOv8 and YOLOv11 models were fine-tuned, achieving high performance metrics under real-world constraints.


<details>
  <summary>Details</summary>
Motivation: The need for reliable, cost-effective autonomous inspection systems for spacecraft due to risks and costs of human or robotic repairs in space.

Method: Creation of a dataset using real spacecraft models and synthetic backgrounds, with added noise/distortion. Fine-tuning YOLOv8 and YOLOv11 models under hardware and time constraints.

Result: Models achieved a Dice score of 0.92, Hausdorff distance of 0.69, and inference time of ~0.5 seconds.

Conclusion: The dataset and models provide a benchmark for real-time spacecraft image segmentation, supporting autonomous inspection in space.

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: A data-efficient LLM agent system is proposed for spatial reasoning in complex indoor warehouse scenarios, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with spatial understanding, and large-scale finetuning is inefficient.

Method: An LLM agent system integrates spatial reasoning tools and API interactions for complex spatial tasks.

Result: Achieves high accuracy in object retrieval, counting, and distance estimation on the AI City Challenge dataset.

Conclusion: The system is effective and efficient for spatial question answering in warehouses.

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT is a nested Vision Transformer that dynamically adjusts computation based on input complexity, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Fixed computational budgets in Vision Transformers lead to inefficiencies, as they allocate the same compute to all inputs regardless of complexity.

Method: ThinkingViT uses progressive thinking stages and Token Recycling to dynamically activate attention heads and terminate early if predictions are certain.

Result: ThinkingViT outperforms nested baselines by up to 2.9 p.p. in accuracy at equal computational cost on ImageNet-1K.

Conclusion: ThinkingViT offers a scalable and efficient solution for Vision Transformers, serving as a plug-in upgrade with improved performance.

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: The paper proposes LAOD, an LLM-guided agentic object detection framework, enabling label-free, zero-shot detection by dynamically generating object names via LLM and using an open-vocabulary detector for localization.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection relies on fixed categories, requiring costly re-training for novel objects. Existing methods like OWOD and OVOD lack semantic labels or depend on user prompts, limiting autonomy.

Method: LAOD uses an LLM to generate scene-specific object names, which are passed to an open-vocabulary detector for localization. Two new metrics, CAAP and SNAP, evaluate localization and naming separately.

Result: Experiments on LVIS, COCO, and COCO-OOD show strong performance in detecting and naming novel objects.

Conclusion: LAOD enhances autonomy and adaptability for open-world understanding by enabling dynamic, label-free detection.

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM improves Grad-CAM by aggregating info across all CNN layers using Winsorization for robust saliency maps, outperforming baselines in interpretability and localization metrics.


<details>
  <summary>Details</summary>
Motivation: Enhancing CNN interpretability for high-stakes domains by addressing Grad-CAM's limitations in layer aggregation and noise handling.

Method: Proposes Winsor-CAM, applying Winsorization to attenuate outliers and allowing human-tunable thresholds for multi-layer saliency maps.

Result: Outperforms Grad-CAM and uniform averaging in interpretability and localization metrics (e.g., IoU, center-of-mass alignment) on PASCAL VOC 2012.

Conclusion: Winsor-CAM advances trustworthy AI with interpretable, human-controllable multi-layer insights.

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: A sparse coding-inspired fine-tuning framework for transformers improves interpretability and performance in downstream tasks like image editing and text-to-image customization.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods lack interpretability in how models adapt to new tasks due to dense parameter updates.

Method: Introduces a sparse coding-based framework where fine-tuned features are sparse combinations of feature dictionary atoms, with coefficients indicating atom importance.

Result: Enhances image editing by improving text alignment and outperforms baselines in text-to-image concept customization.

Conclusion: The sparse fine-tuning framework offers interpretable and efficient adaptation of pre-trained models for downstream tasks.

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: A lightweight framework combining LOF for noise filtering and YOLO-v11n for polyp detection achieves high accuracy and efficiency in real-time colonoscopy.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate polyp detection is vital for colorectal cancer prevention, requiring efficient and robust AI solutions.

Method: LOF filters noisy data; YOLO-v11n processes cleaned data with 5-fold cross-validation and augmentation. Tested on five public datasets.

Result: High performance: precision 95.83%, recall 91.85%, F1-score 93.48%, mAP@0.5 96.48%, mAP@0.5:0.95 77.75%.

Conclusion: The method is effective for real-time clinical use, highlighting the importance of preprocessing and model efficiency in medical AI.

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [12] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super improves centerline tracking in 3D medical images by addressing duplicate branches and premature termination, outperforming SOTA models on new datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate tracking of tubular tree structures (e.g., blood vessels) is vital for medical tasks, but existing models like Trexplorer have limitations.

Method: Trexplorer Super introduces novel advancements to enhance tracking. Three datasets (synthetic and real) are developed for evaluation.

Result: Trexplorer Super outperforms SOTA models on all datasets, but synthetic data performance doesn't guarantee real-data success.

Conclusion: Trexplorer Super advances centerline tracking, with datasets and code made publicly available for further research.

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [13] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: A lightweight CNN-based model, KAI-a, is introduced for global weather forecasting, matching state-of-the-art accuracy with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: AI-based weather models, often Transformer-based, face high complexity and resource demands. This study aims to modernize CNN architectures for efficient, accurate forecasting.

Method: KAI-a uses a scale-invariant architecture with InceptionNeXt blocks, tailored for Earth system data. Trained on ERA5 with 7M parameters, it completes in 12 hours on a single GPU.

Result: KAI-a matches top models in accuracy, excels in extreme events like the 2018 European heatwave, and is computationally efficient.

Conclusion: KAI-a offers a practical, resource-efficient alternative to Transformer-based models for weather forecasting.

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [14] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: The paper introduces two regularization strategies, LVL and LGCL, to address Timescale Dependent Label Inconsistency (TsDLI) in EEG-based emotion recognition, improving model generalization and explainability.


<details>
  <summary>Details</summary>
Motivation: To mitigate TsDLI and enhance model performance and interpretability in EEG emotion recognition.

Method: Proposes LVL and LGCL regularization methods using bounded variation functions and commute-time distances within a graph theoretic framework, along with new evaluation metrics.

Result: The methods outperform baselines on DREAMER and DEAP datasets, with LVL achieving the best aggregate rank.

Conclusion: The proposed framework effectively balances interpretability and predictive power under label inconsistency.

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [15] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill is a weakly supervised framework for cross-view localization using teacher-student learning with FoV-based masking to improve feature learning and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on costly ground-truth pose annotations, prompting the need for a scalable, efficient solution.

Method: Uses teacher-student learning with FoV-based masking; teacher localizes panoramas while student learns from limited FoV images.

Result: Improves localization accuracy and reduces uncertainty, even with limited FoV queries. Introduces a novel orientation estimation network.

Conclusion: GeoDistill offers a scalable, efficient solution for real-world cross-view localization challenges.

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [16] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: The paper proposes GAPL-SCD, a method for Semantic Change Detection (SCD) in remote sensing, addressing multi-task optimization challenges with adaptive weight allocation and gradient rotation. It introduces graph aggregation prototype learning and feature fusion modules, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: SCD provides detailed semantic insights for remote sensing applications but faces challenges like negative transfer due to multi-task optimization conflicts.

Method: GAPL-SCD uses multi-task joint optimization, graph aggregation prototype learning, adaptive weight allocation, and gradient rotation. It includes self-query multi-level feature interaction and bi-temporal feature fusion modules.

Result: The method achieves state-of-the-art performance on SECOND and Landsat-SCD datasets, improving accuracy and robustness.

Conclusion: GAPL-SCD effectively addresses SCD challenges, enhancing multi-task learning and feature representation for better performance in complex scenes.

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [17] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR is a novel ID-specific face restoration framework using diffusion models to address identity uncertainty, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing face restoration methods struggle with identity uncertainty due to obscure inputs and stochastic processes.

Method: RIDFR uses a pre-trained diffusion model with two parallel conditioning modules (Content Injection and Identity Injection) and Alignment Learning to align restoration results.

Result: RIDFR reconstructs high-quality ID-specific results with high identity fidelity and robustness.

Conclusion: RIDFR effectively addresses identity uncertainty in face restoration, achieving superior performance.

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [18] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: A new dataset, WomenSports, is introduced for women sports action classification, addressing the lack of diverse datasets. A CNN with channel attention improves feature extraction, achieving 89.15% accuracy on the dataset.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity in women sports actions, limiting research. This work aims to fill this gap with a comprehensive dataset and improved classification methods.

Method: Proposes a CNN with channel attention for deep feature extraction. Evaluated on WomenSports and other datasets for generalization.

Result: Achieves 89.15% top-1 accuracy on WomenSports using ResNet-50, demonstrating effectiveness.

Conclusion: The WomenSports dataset and proposed CNN method advance sports action classification, especially for women sports, and are publicly available for further research.

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [19] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: The paper proposes a wavelet attention-like backbone and a ray-based encoder for efficient and accurate human-object interaction (HOI) detection, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detectors are inefficient and resource-intensive, struggling with reliable predictions.

Method: Introduces a wavelet backbone for feature aggregation and a ray-based encoder for multi-scale attention, optimizing computational efficiency.

Result: Demonstrates improved performance on benchmark datasets like ImageNet and HICO-DET.

Conclusion: The proposed architecture effectively enhances HOI detection accuracy and efficiency, with code made publicly available.

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [20] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait proposes residual correction for occluded gait recognition, improving performance on occluded sequences without losing accuracy on holistic inputs.


<details>
  <summary>Details</summary>
Motivation: Addressing the practical challenge of occlusions in gait recognition, which current methods either ignore or handle poorly by requiring impractical data or losing holistic performance.

Method: Models occluded gait as a residual deviation from holistic gait, using a network to adaptively integrate this residual for better recognition.

Result: Demonstrates effectiveness on Gait3D, GREW, and BRIAR datasets, showing improved occluded recognition while retaining holistic accuracy.

Conclusion: Residual learning is a viable technique for occluded gait recognition with holistic retention.

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [21] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN is a lightweight CNN architecture that improves spatial and channel-wise processing, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Address the simplicity bias and redundancy in CNNs and transformers, aiming for efficient feature processing.

Method: Uses kernels with varying receptive fields and a wave-based channel aggregation module to enhance feature capture.

Result: Achieves 77.7% accuracy on ImageNet-1k with 3.8M parameters and 50.0% AP on COCO with 21.5M parameters.

Conclusion: SpaRTAN offers an efficient and effective alternative to modern CNNs and transformers.

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [22] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP enhances zero-shot anomaly detection (ZSAD) by combining feature matching and cross-modal alignment with CLIP, using batch-based testing and text filtering for better performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of zero-shot anomaly detection (ZSAD) by leveraging CLIP's capabilities while improving accuracy through feature filtering and local semantic correlation.

Method: FiSeCLIP uses batch-based testing with mutual reference images and text information to filter noisy features. It also restores CLIP's local semantic correlation for fine-grained anomaly detection.

Result: FiSeCLIP outperforms state-of-the-art methods, achieving +4.6%/+5.7% improvement in segmentation metrics (AU-ROC/F1-max) on MVTec-AD.

Conclusion: FiSeCLIP provides a robust baseline for ZSAD, demonstrating superior performance in anomaly classification and segmentation.

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [23] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: SISRNet improves radiology report generation by focusing on medically critical regions, addressing data bias for clinically accurate results.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce fluent but inaccurate reports due to data bias in radiology images, limiting clinical use.

Method: SISRNet identifies salient regions using fine-grained cross-modal semantics and focuses on them during image modeling and report generation.

Result: SISRNet outperforms peers on IU-Xray and MIMIC-CXR datasets, generating more accurate reports.

Conclusion: SISRNet effectively mitigates data bias and improves clinical accuracy in radiology report generation.

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [24] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: A novel CBCT-to-MDCT translation framework using Schrodinger Bridge integrates GAN priors and human-guided diffusion, ensuring anatomical fidelity and controllability with minimal sampling steps.


<details>
  <summary>Details</summary>
Motivation: To improve CBCT-to-MDCT translation by enforcing boundary consistency and incorporating human feedback for clinically preferred outcomes.

Method: Combines GAN-derived priors with human-guided conditional diffusion, using classifier-free guidance and tournament-based preference selection.

Result: Outperforms prior methods in RMSE, SSIM, LPIPS, and Dice metrics, with only 10 sampling steps.

Conclusion: The framework is effective and efficient for real-time, preference-aligned medical image translation.

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [25] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces personalized open-vocabulary semantic segmentation (OVSS) to recognize user-specific text descriptions (e.g., 'my mug cup') and proposes a plug-in method using text prompt tuning and negative mask proposals to reduce false predictions.


<details>
  <summary>Details</summary>
Motivation: Current OVSS fails to segment regions based on personal text descriptions (e.g., 'my mug cup') among similar objects, limiting user-specific applications.

Method: The proposed method uses text prompt tuning and introduces 'negative mask proposals' to reduce false predictions. It also enhances text prompts with visual embeddings of personal concepts.

Result: The method improves personalized OVSS without degrading original OVSS performance, validated on new benchmarks (FSS$^\text{per}$, CUB$^\text{per}$, ADE$^\text{per}$).

Conclusion: The approach successfully addresses the challenge of personalizing OVSS while maintaining general segmentation performance.

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [26] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet is a dual-domain dehazing network combining spatial and frequency domains, using dark channel priors and multi-scale feature fusion for superior performance and real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on costly spatial-domain features or weakly coupled frequency cues, limiting performance under complex haze conditions.

Method: DGFDNet integrates HAFM for adaptive frequency modulation and MGAM for multi-scale feature fusion, with a PCGB for iterative prior refinement.

Result: Achieves state-of-the-art performance on four benchmark datasets with robustness and real-time efficiency.

Conclusion: DGFDNet effectively addresses dehazing challenges by leveraging dual-domain alignment and iterative refinement, outperforming existing methods.

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [27] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D is a multi-view dataset of high-resolution ankle-foot point clouds for gait analysis, addressing occlusion challenges and enabling 3D shape completion benchmarking.


<details>
  <summary>Details</summary>
Motivation: Accurate surface geometry data of the foot-ankle complex during gait is challenging due to occlusions and viewing limitations, necessitating a specialized dataset.

Method: FootGait3D includes 8,403 point cloud frames from 46 subjects, captured using a five-camera system, with complete and partial views for evaluating 3D completion methods.

Result: The dataset supports benchmarking of single- and multi-modal completion networks, aiding in recovering full foot geometry from occluded inputs.

Conclusion: FootGait3D advances biomechanics research and applications like gait analysis, prosthetic design, and robotics by providing detailed 3D foot models during motion.

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [28] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD introduces a transformer-first architecture for satellite object detection, outperforming SOTA by 11.46% on xView.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of object detection in high-resolution satellite imagery, leveraging transformers for better feature extraction.

Method: Uses Swin Transformer for feature extraction, UpConvMixer for upsampling, and Fusion Blocks for multi-scale integration, with CBAM attention and multi-path head design.

Result: Achieves 32.95% on xView, outperforming SOTA by 11.46%.

Conclusion: GLOD is an efficient, transformer-based solution optimized for satellite imagery, excelling in performance and scalability.

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [29] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn introduces a prototype-driven framework to reduce reliance on paired image-text data for medical segmentation, improving performance when text is scarce.


<details>
  <summary>Details</summary>
Motivation: Existing methods depend on paired image-text data, limiting their use in datasets without reports and clinical scenarios where segmentation precedes reporting.

Method: ProLearn uses a Prototype-driven Semantic Approximation (PSA) module to approximate semantic guidance from text, enabling segmentation without paired reports.

Result: ProLearn outperforms state-of-the-art methods on QaTa-COV19, MosMedData+, and Kvasir-SEG datasets when text is limited.

Conclusion: ProLearn effectively reduces textual reliance, enhancing the applicability of language-guided segmentation in real-world clinical settings.

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [30] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP is a novel framework for precise local 3D Gaussian editing, addressing challenges in multi-view segmentation and SDS loss ambiguity with robust 3D masking and regularized SDS loss.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with precise local 3D edits due to inconsistent multi-view segmentations and ambiguous SDS loss, limiting high-quality part-level modifications.

Method: RoMaP introduces 3D-GALP for accurate part segmentation and a regularized SDS loss with L1 anchor loss (via SLaMP) and additional regularizers like Gaussian prior removal.

Result: RoMaP achieves state-of-the-art local 3D editing, enabling precise and flexible part-level modifications while preserving contextual coherence.

Conclusion: RoMaP advances 3D Gaussian editing by improving robustness and flexibility, making it suitable for high-quality part-level modifications.

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [31] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: A novel joint angle-based method improves marker-free human pose estimation by refining keypoint recognition and smoothing trajectories using a bidirectional recurrent network.


<details>
  <summary>Details</summary>
Motivation: Current HPE methods suffer from errors in keypoint recognition and trajectory fluctuations due to inaccurate training datasets with manual annotations.

Method: Proposes joint angle-based modeling, approximating joint angle variations with high-order Fourier series, and using a bidirectional recurrent network to refine HRNet outputs.

Result: The method outperforms state-of-the-art HPE refinement networks, especially in challenging cases like figure skating and breaking.

Conclusion: Joint angle-based refinement (JAR) effectively enhances HPE accuracy and robustness in kinematic pose analysis.

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [32] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: Proposes GKNet, a graph-based keypoints network for monocular pose estimation of non-cooperative spacecraft, addressing symmetry and occlusion issues. Introduces SKD dataset for validation.


<details>
  <summary>Details</summary>
Motivation: High accuracy in pose estimation is crucial for on-orbit service tasks, but current keypoint detectors struggle with symmetry and occlusion.

Method: GKNet leverages geometric constraints of keypoints graph. Validated using the SKD dataset (90,000 simulated images with annotations).

Result: GKNet outperforms state-of-the-art detectors in accuracy and effectiveness.

Conclusion: GKNet and SKD dataset advance monocular pose estimation for non-cooperative spacecraft, with code and data publicly available.

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [33] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: A novel cross-verification strategy using YOLO models and a rigorously validated 3D GPR dataset improves RSD recognition accuracy to over 98.6%, reducing inspection labor by 90%.


<details>
  <summary>Details</summary>
Motivation: Current RSD recognition from GPR images is labor-intensive and expertise-dependent, with deep learning limited by dataset scarcity and network capability.

Method: Constructed a 3D GPR dataset with 2134 samples and proposed a cross-verification strategy using YOLO models trained on different GPR scans.

Result: Achieved over 98.6% recall in field tests and reduced inspection labor by 90%.

Conclusion: The approach significantly enhances RSD recognition accuracy and efficiency, demonstrating practical utility in an online detection system.

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [34] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: The paper introduces Atmos-Bench, a 3D atmospheric benchmark, and FourCastX, a novel network for atmospheric structure recovery, improving accuracy without auxiliary inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for atmospheric structure recovery rely on simplified physics and lack standardized benchmarks, introducing uncertainties.

Method: Developed Atmos-Bench with simulated 3D scattering volumes and FourCastX, a frequency-enhanced spatio-temporal network embedding physical constraints.

Result: FourCastX outperforms state-of-the-art models on Atmos-Bench, achieving consistent improvements across 355 nm and 532 nm bands.

Conclusion: Atmos-Bench sets a new standard for 3D atmospheric recovery, enhancing climate understanding.

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [35] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: A systematic review and taxonomy of interpretability methods for visual recognition models, focusing on human-centered criteria and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the interpretability of visual recognition models for critical applications like autonomous driving and medical diagnostics.

Method: Proposes a taxonomy categorizing interpretable methods by Intent, Object, Presentation, and Methodology, and reviews evaluation metrics and new technologies.

Result: Establishes a coherent grouping of XAI methods and identifies opportunities for future research.

Conclusion: Organizes existing research and inspires further investigation into interpretability for visual recognition models.

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [36] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++ is a multimodal large language model designed for fine-grained keypoint comprehension, achieving state-of-the-art performance through a novel identify-then-detect paradigm and extensive training on 500K diverse samples.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained semantic information like object keypoints, which are crucial for applications such as image analysis and behavior recognition.

Method: KptLLM++ integrates diverse input modalities using a structured chain-of-thought reasoning mechanism, first interpreting keypoint semantics and then localizing their positions.

Result: The model achieves remarkable accuracy and generalization, outperforming benchmarks in keypoint detection.

Conclusion: KptLLM++ serves as a unified solution for fine-grained image understanding, enhancing human-AI collaboration.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [37] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: A deep learning framework for jellyfish species detection achieves 98% accuracy using MobileNetV3 and hybrid classifiers, aiding marine biodiversity monitoring.


<details>
  <summary>Details</summary>
Motivation: Accurate jellyfish species identification is vital for ecological monitoring and management, addressing challenges posed by their rapid proliferation.

Method: The study integrates MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16 with traditional machine learning and neural network classifiers, using softmax for direct species classification.

Result: The MobileNetV3-ANN hybrid model achieved 98% accuracy, outperforming other combinations.

Conclusion: Deep learning and hybrid frameworks effectively address biodiversity challenges, enhancing species detection in marine environments.

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [38] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: The paper introduces HSGL, a multimodal framework for generating and learning hard samples in clothing-changing person Re-ID, improving model robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Hard samples in CC-ReID are ambiguous and lack explicit definitions, limiting learning strategies and model robustness.

Method: HSGL combines Dual-Granularity Hard Sample Generation (DGHSG) for synthesizing hard samples and Hard Sample Adaptive Learning (HSAL) for hardness-aware optimization.

Result: HSGL achieves state-of-the-art performance on PRCC and LTCC datasets, accelerating convergence.

Conclusion: Multimodal-guided hard sample generation and learning enhances CC-ReID robustness and discriminative capability.

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [39] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: MMOne is a framework for multimodal scene representation, addressing modality conflicts (property and granularity disparities) via a modality modeling module and decomposition mechanism.


<details>
  <summary>Details</summary>
Motivation: Humans use multimodal cues to understand the world, but modality conflicts hinder effective scene representation.

Method: Proposes MMOne with a modality modeling module (using modality indicators) and a decomposition mechanism to separate multimodal Gaussians into single-modal ones.

Result: Enhances representation for each modality and scales to additional modalities, as shown in experiments.

Conclusion: MMOne effectively addresses modality conflicts, improving multimodal scene representation.

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [40] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: The paper proposes an end-to-end deep-learning model for automatic landslide observation using remote sensing images, achieving high accuracy in detection and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Landslide disasters are increasing due to extreme weather and human activities, but manual observation is challenging due to large, rugged terrains.

Method: A novel neural network architecture is designed for landslide detection and segmentation using remote sensing images.

Result: High F1 scores (98.23, 93.83) for detection and mIoU scores (63.74, 76.88) for segmentation on benchmark datasets.

Conclusion: The model shows potential for real-life landslide observation systems.

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [41] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: The paper explores color vision in large vision-language models, introduces a testing task, and proposes fine-tuning strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: The color vision abilities of large vision-language models are understudied, prompting the need for a dedicated testing framework.

Method: A color vision testing task is defined, and a dataset with varied difficulty levels is constructed. Error analysis and fine-tuning strategies are proposed.

Result: The study identifies common errors in models and suggests ways to enhance their color vision capabilities.

Conclusion: Fine-tuning strategies can improve the color vision performance of large vision-language models, addressing a critical gap in their capabilities.

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [42] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: A novel self-supervised learning method (CMCRL) for citrus disease detection outperforms existing methods by 4.5%-30.1% accuracy, using unannotated samples and addressing symptom similarity.


<details>
  <summary>Details</summary>
Motivation: Citrus diseases cause significant yield loss; accurate detection is crucial but relies on costly annotated data. AI advancements can help, but current methods need massive labeled datasets.

Method: Proposes CMCRL, combining cluster centroids and multi-layer contrastive training (MCT) for hierarchical feature learning from unannotated samples.

Result: Achieves state-of-the-art performance on CDD dataset, narrowing the gap with fully supervised methods and excelling in F1, precision, and recall.

Conclusion: CMCRL offers a robust, efficient solution for citrus disease detection, reducing reliance on labeled data and handling class imbalance effectively.

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [43] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: Evaluation of general-purpose and medical VLMs shows they perform well on medical tasks but lack reliability for clinical use due to reasoning gaps and benchmark variability.


<details>
  <summary>Details</summary>
Motivation: To assess the competence of Vision-Language Models (VLMs) in medical tasks, given their increasing use in healthcare despite limited exploration.

Method: Comprehensive evaluation of VLMs (3B to 72B parameters) across eight medical benchmarks, analyzing understanding and reasoning separately.

Result: General-purpose VLMs match or exceed medical-specific models in some tasks, but reasoning lags behind understanding, and performance varies by benchmark.

Conclusion: Current VLMs are not clinically reliable; stronger multimodal alignment and finer evaluation protocols are needed.

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [44] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA is a novel framework for efficient training of incomplete multimodal learning models, addressing gradient conflicts in existing methods by decoupling shared and distinct modality information and dynamically adjusting training ratios.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods struggle with incomplete multimodality due to conflicting training gradients from different modality combinations, degrading performance.

Method: MCULoRA uses two modules: MCLA (decouples shared/distinct modality information) and DPFT (dynamically adjusts training ratios based on modality separability).

Result: MCULoRA outperforms previous methods in downstream task accuracy across multiple benchmark datasets.

Conclusion: MCULoRA effectively addresses gradient conflicts in incomplete multimodal learning, improving performance and efficiency.

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [45] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: The paper introduces NarrLV, the first benchmark for evaluating narrative expression in long video generation models, using Temporal Narrative Atoms (TNAs) and a novel MLLM-based metric.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for long video generation lack focus on narrative richness, prompting the need for a specialized evaluation tool.

Method: The authors propose (i) TNAs to measure narrative richness, (ii) an automatic prompt generation pipeline, and (iii) an MLLM-based evaluation metric.

Result: NarrLV aligns with human judgments and reveals capability boundaries of current models in narrative expression.

Conclusion: NarrLV effectively evaluates narrative richness in long videos, providing insights for future model improvements.

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [46] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: Proposes a fairness-based grouping method for continuous sensitive attributes to better identify and address discrimination in datasets and models.


<details>
  <summary>Details</summary>
Motivation: Existing fairness assessments often divide data into predefined groups, which may overlook discrimination in continuous sensitive attributes like skin color.

Method: Introduces a grouping approach that maximizes inter-group variance in discrimination to identify critical subgroups.

Result: Validated on synthetic and real datasets (CelebA, FFHQ), showing nuanced discrimination patterns and stable findings. Also improves fairness in debiasing with minimal accuracy loss.

Conclusion: The method effectively uncovers and mitigates discrimination in continuous sensitive attributes, enabling industrial deployment.

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [47] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: A framework for generating realistic forest fire smoke images using deep learning, improving smoke detection models.


<details>
  <summary>Details</summary>
Motivation: Scarcity of real smoke images hinders forest fire detection; current inpainting models lack quality in synthesizing smoke.

Method: Uses pre-trained segmentation and multimodal models for masks/captions, introduces mask-guided architecture, and a new loss function (mask random difference loss). Generates diverse smoke images with a multimodal LLM for filtering.

Result: Produces realistic, diverse smoke images, enhancing forest fire smoke detection performance.

Conclusion: The proposed framework effectively addresses data scarcity and improves detection models.

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [48] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD improves 3D visual grounding by decomposing complex queries into simpler statements and integrating multi-view textual-scene interactions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with disentangling targets in multi-anchor queries and handling spatial inconsistencies due to perspective variations.

Method: ViewSRD uses Simple Relation Decoupling (SRD) to simplify queries, Multi-view Textual-Scene Interaction (Multi-TSI) with CCVTs for cross-modal consistency, and a reasoning module for unified predictions.

Result: ViewSRD outperforms state-of-the-art methods, especially in complex spatial queries.

Conclusion: The framework effectively addresses challenges in 3D visual grounding by leveraging structured multi-view decomposition and cross-modal consistency.

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [49] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: YOLOatr, a modified YOLOv5s-based model, achieves 99.6% ATR performance in thermal infrared imagery, addressing domain-specific challenges like limited datasets and hardware.


<details>
  <summary>Details</summary>
Motivation: ATD and ATR in thermal infrared imagery face unique challenges (e.g., limited datasets, hardware, and environmental factors), causing SOTA models to underperform.

Method: Proposes YOLOatr, a modified YOLOv5s with optimized detection heads, feature fusion, and custom augmentation for thermal infrared ATR.

Result: Achieves 99.6% ATR performance on the DSIAC MWIR dataset, outperforming existing methods.

Conclusion: YOLOatr sets a new SOTA for ATR in thermal infrared imagery, demonstrating robustness to domain-specific challenges.

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [50] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP is a dataset for Solanum lycopersicum using IoT-based imaging, offering annotated images and AI validation for accurate plant phenotyping.


<details>
  <summary>Details</summary>
Motivation: Traditional plant phenotyping methods suffer from observer bias and inconsistencies, limiting accuracy and reproducibility.

Method: Developed TomatoMAP with 64,464 RGB images, annotated ROIs, and growth stages. Used IoT-based imaging and validated with deep learning models (MobileNetv3, YOLOv11, MaskRCNN).

Result: AI models trained on TomatoMAP achieved accuracy and speed comparable to human experts, confirmed by Cohen's Kappa and inter-rater agreement.

Conclusion: TomatoMAP provides a reliable, automated solution for fine-grained plant phenotyping, overcoming traditional method limitations.

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [51] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: The paper introduces task-oriented human grasp synthesis, using task-aware contact maps to improve grasp quality and task performance.


<details>
  <summary>Details</summary>
Motivation: Traditional grasp synthesis lacks task and context awareness, limiting its effectiveness in real-world applications.

Method: A two-stage pipeline: (1) constructs task-aware contact maps considering scene and task, (2) synthesizes grasps using these maps.

Result: Experiments show significant improvements in grasp quality and task performance over existing methods.

Conclusion: Task-aware contact maps enhance grasp synthesis by incorporating scene and task information, validated by a new dataset and metric.

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [52] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: The paper proposes an AI-based method using YOLOv11 and LiDAR images to automatically detect and quantify fluvial erosion, achieving 70% accuracy, and introduces the EROSCAN web tool for practical use.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for detecting fluvial erosion are manual and require expertise, prompting the need for an automated solution.

Method: Uses YOLOv11, fine-tuned and trained with photos and LiDAR images, segmented and labeled via Roboflow.

Result: Achieves 70% accuracy in detecting erosion patterns and reliably estimates eroded areas in pixels and square meters.

Conclusion: The EROSCAN system provides an efficient, automated tool for erosion detection, aiding risk management and planning.

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [53] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for Gaussian Splatting (GS) that uses multiple types of splatting primitives for improved surface reconstruction, addressing limitations of single-primitive methods.


<details>
  <summary>Details</summary>
Motivation: Existing GS-based methods use only one type of splatting primitive, which is insufficient for high-quality representation of diverse 3D object surfaces.

Method: The proposed framework includes a compositional splatting strategy, mixed-primitive initialization, and vertex pruning to leverage multiple primitive types.

Result: Extensive experiments demonstrate the framework's efficacy and accurate surface reconstruction performance.

Conclusion: The framework successfully enhances GS by incorporating multiple primitives, improving surface representation quality.

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [54] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet integrates monocular depth and feature priors into MVS to improve performance in challenging regions like textureless and reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods struggle with feature matching in difficult regions, while monocular depth estimation is robust there.

Method: Uses monocular features and depth to guide MVS, with attention mechanisms, dynamic depth updates, and a relative consistency loss.

Result: Achieves state-of-the-art performance on DTU and Tanks-and-Temples datasets, ranking first on benchmarks.

Conclusion: MonoMVSNet effectively bridges the gap between monocular depth and MVS, enhancing robustness and accuracy.

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [55] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: The paper introduces UGC-VideoCap, a benchmark and model framework for detailed omnimodal captioning of short-form user-generated videos, addressing the lack of audio-visual integration in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing video captioning benchmarks and models are visual-centric, ignoring audio's role in scene dynamics and narrative context, limiting multimodal video understanding.

Method: The authors propose UGC-VideoCap, featuring 1000 TikTok videos with balanced audio-visual annotations and 4000 QA pairs. They also introduce UGC-VideoCaptioner(3B), a 3B parameter model trained with a two-stage strategy (supervised fine-tuning and GRPO).

Result: The benchmark and model provide a high-quality foundation for omnimodal video captioning, enabling efficient adaptation from limited data while maintaining competitive performance.

Conclusion: UGC-VideoCap and its accompanying model advance omnimodal video captioning in real-world UGC settings by integrating audio and visual modalities effectively.

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [56] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: The paper explores the geometric structure in face recognition embedding spaces and introduces a physics-inspired metric to analyze model invariance to facial and image attributes.


<details>
  <summary>Details</summary>
Motivation: Despite progress in face recognition using deep neural networks, existing contrastive losses focus only on identity labels, ignoring emerging geometric structures influenced by other attributes.

Method: A geometric approach and physics-inspired alignment metric are proposed to evaluate model invariance to attributes, tested on controlled models and fine-tuned FR models with synthetic data.

Result: Models show varying invariance to attributes, revealing strengths and weaknesses and improving interpretability.

Conclusion: The study provides insights into FR model behavior and interpretability, with potential for further refinement.

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [57] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR outperforms DMs in non-DP adaptations but struggles with DP adaptations, highlighting a need for further research in private VAR adaptations.


<details>
  <summary>Details</summary>
Motivation: To explore and benchmark adaptation strategies for VAR, especially in DP settings, where it lags behind DMs.

Method: Implemented and compared various adaptation strategies for VAR and DMs, focusing on downstream tasks like medical data generation.

Result: VAR performs better than DMs in non-DP adaptations but underperforms in DP settings.

Conclusion: Further research is needed to improve DP adaptations for VAR to match its non-DP performance.

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [58] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI introduces a novel framework using Neural Representations for Videos (NeRV) to improve compression of large images, addressing speed and ratio issues of Implicit Neural Representations (INRs).


<details>
  <summary>Details</summary>
Motivation: The need for efficient compression of high-resolution images, as conventional methods lose details and data-driven approaches lack generalizability.

Method: COLI uses a pretraining-finetuning paradigm, mixed-precision training, and parallelizable loss reformulation for speed. It also employs Hyper-Compression to enhance ratios.

Result: COLI achieves better PSNR and SSIM metrics at lower bits per pixel (bpp) and speeds up NeRV training by 4x.

Conclusion: COLI offers a competitive solution for large-image compression with improved speed and efficiency.

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [59] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS is a hierarchical NURBS generative model for vascular geometry synthesis, combining NURBS parameterization with diffusion-based modeling to create realistic aortic geometries.


<details>
  <summary>Details</summary>
Motivation: Traditional SSM methods are limited by linear assumptions, hindering their ability to model complex vascular structures like multi-branch aortas.

Method: HUG-VAS uses a hierarchical architecture with denoising and guided diffusion models to generate centerlines and radial profiles, capturing anatomical variability.

Result: Trained on 21 samples, HUG-VAS produces anatomically accurate aortas with biomarker distributions matching the original dataset.

Conclusion: HUG-VAS bridges image-derived priors with generative shape modeling, enabling applications like segmentation and device optimization.

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [60] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI is a novel algorithm for robust circle detection in degraded images, combining combinatorial sampling and convolution-based density estimation. It outperforms classical methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust circle detection in poor imaging conditions, particularly for applications like medical imaging and industrial inspection.

Method: Combines combinatorial edge pixel sampling with convolution-based density estimation in parameter space.

Result: Achieves state-of-the-art accuracy (Jaccard index 0.896) and real-time performance (40.3 fps), outperforming classical methods. Maintains high accuracy even with outliers and low resolutions.

Conclusion: 3C-FBI is ideal for medical imaging, robotics, and industrial inspection due to its accuracy, speed, and robustness.

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [61] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: The paper introduces COLIBRI, a fuzzy color model aligning computational color representation with human perception, validated by large-scale human experiments.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between computational color models and human visual perception for better applications in design, AI, and HCI.

Method: Three-phase approach: identifying color stimuli, large-scale human categorization survey (n=2496), and fuzzy logic-based modeling.

Result: COLIBRI outperforms traditional models (RGB, HSV, LAB) in aligning with human perception.

Conclusion: The model is significant for fields requiring perceptually accurate color representation, like design and AI.

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [62] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: A novel 5-stage framework decodes visual representations from EEG signals, outperforming SOTA methods in accuracy and image quality.


<details>
  <summary>Details</summary>
Motivation: Decoding visual representations from EEG signals is challenging due to their complexity and noise.

Method: A 5-stage framework involving EEG encoding, cross-modal alignment, caption refinement, weighted interpolation, and image generation.

Result: Outperforms SOTA by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy, and reduces Fréchet Inception Distance by 36.61%.

Conclusion: The method enables high-quality, context-aware EEG-to-image generation with superior semantic alignment.

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [63] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist improves text-to-image generation by ensuring fine-grained consistency in foreground and background details, even with large motion variations, using point-tracking attention and adaptive token merge.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to maintain consistent background details and struggle with identity and clothing inconsistencies during large motion variations.

Method: CharaConsist employs point-tracking attention, adaptive token merge, and decoupled control of foreground and background.

Result: It achieves fine-grained consistency for both foreground and background, supporting continuous and discrete shots, and is tailored for DiT models.

Conclusion: CharaConsist broadens applicability in real-world scenarios by producing high-quality, consistent visual outputs.

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [64] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: A streaming 4D visual geometry transformer is proposed for real-time 4D reconstruction from videos, using causal attention and implicit memory for efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable interactive and real-time 4D geometry perception from videos, addressing the challenge of processing spatial-temporal data efficiently.

Method: Employs a causal transformer architecture with temporal causal attention and historical key-value caching for online processing. Knowledge is distilled from a dense bidirectional transformer (VGGT) for training.

Result: Achieves competitive performance with increased inference speed in online scenarios, enabling scalable and interactive 4D vision systems.

Conclusion: The proposed method effectively balances efficiency and quality for real-time 4D reconstruction, with potential for broader applications in interactive vision systems.

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [65] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: The paper surveys deep learning methods for depth estimation, focusing on the potential of 'depth foundation models' trained on large datasets for robust generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional depth estimation methods (e.g., LiDAR) are costly and limited, while vision-based methods struggle with generalization. The paper explores scalable, robust alternatives.

Method: Reviews deep learning architectures (monocular, stereo, multi-view, video) and training strategies, leveraging large datasets for zero-shot generalization.

Result: Identifies key architectures and datasets, proposing a path toward robust depth foundation models.

Conclusion: Highlights the potential of foundation models for depth estimation, suggesting future research directions and applications.

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [66] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: The paper proposes CLS-DM, a model for 3D CT reconstruction from sparse 2D X-ray images using latent space alignment and contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: CT imaging faces challenges like high time and radiation costs. Sparse-view reconstruction can mitigate these, but existing methods struggle with latent space alignment between 2D X-ray and 3D CT data.

Method: Introduces CLS-DM, which uses cross-modal feature contrastive learning to align latent spaces and extract 3D information from 2D X-ray images.

Result: CLS-DM outperforms classical and state-of-the-art models on LIDC-IDRI and CTSpine1K datasets in voxel-level metrics (PSNR, SSIM).

Conclusion: CLS-DM improves sparse X-ray CT reconstruction and can generalize to other cross-modal tasks. Code is publicly available for further research.

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [67] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: A novel 3D Magnetic Inverse Routine (3D MIR) combines deep learning, spatial-physics constraints, and optimization to accurately recover 3D current flow parameters in semiconductor packaging.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D information recovery is essential for non-destructive testing to localize circuit defects in semiconductor packaging.

Method: 3D MIR uses a CNN to predict wire parameters, spatial-physics constraints for initial estimates, and optimization to refine parameters.

Result: The method achieves high precision in 3D information recovery, setting a new benchmark for magnetic image reconstruction.

Conclusion: Combining deep learning and physics-driven optimization shows great potential for practical applications in semiconductor testing.

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [68] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net, a novel segmentation framework, combines hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, and neural representation for accurate liver and tumor segmentation on CT images, achieving high performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate liver and tumor segmentation is crucial for diagnosis and treatment but is challenging due to anatomical complexity, tumor variability, and limited annotated data.

Method: HANS-Net integrates hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, neural representation, uncertainty-aware Monte Carlo dropout, and temporal attention for robust segmentation.

Result: Achieved mean Dice score of 93.26%, IoU of 88.09%, ASSD of 0.72 mm, and VOE of 11.91% on LiTS dataset, with strong cross-dataset generalization (Dice 87.45% on 3D-IRCADb-01).

Conclusion: HANS-Net is effective, robust, and generalizable for liver and tumor segmentation, providing anatomically consistent and confident results.

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>
