<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 91]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GAITEX: Human motion dataset from impaired gait and rehabilitation exercises of inertial and optical sensor data](https://arxiv.org/abs/2507.21069)
*Andreas Spilz,Heiko Oppel,Jochen Werner,Kathrin Stucke-Straub,Felix Capanni,Michael Munz*

Main category: cs.CV

TL;DR: A multimodal dataset of physiotherapeutic exercises and gait patterns, recorded using IMUs and MoCap, supports machine learning model development for movement analysis.


<details>
  <summary>Details</summary>
Motivation: Large, diverse datasets for sensor-based classification models are costly and time-consuming to collect, hindering robust model development.

Method: Data from 19 participants includes synchronized IMU and MoCap recordings, with raw and processed data, annotations, and tools for analysis.

Result: The dataset enables precise comparison of IMU-derived orientations with MoCap reference values and supports diverse analysis tasks.

Conclusion: This resource accelerates research in machine learning-driven human movement analysis by providing comprehensive data and tools.

Abstract: Wearable inertial measurement units (IMUs) offer a cost-effective and
scalable means to assess human movement quality in clinical and everyday
settings. However, the development of robust sensor-based classification models
for physiotherapeutic exercises and gait analysis requires large, diverse
datasets, which are costly and time-consuming to collect. Here, we present a
multimodal dataset of physiotherapeutic exercises - including correct and
clinically relevant variants - and gait-related exercises - including both
normal and impaired gait patterns - recorded from 19 participants using
synchronized IMUs and marker-based motion capture (MoCap). The dataset includes
raw data from nine IMUs and thirty-five optical markers capturing full-body
kinematics. Each IMU is additionally equipped with four optical markers,
enabling precise comparison between IMU-derived orientation estimates and
reference values from the MoCap system. To support further analysis, we also
provide processed IMU orientations aligned with common segment coordinate
systems, subject-specific OpenSim models, inverse kinematics results, and tools
for visualizing IMU orientations in the musculoskeletal context. Detailed
annotations of movement execution quality and time-stamped segmentations
support diverse analysis goals. This dataset supports the development and
benchmarking of machine learning models for tasks such as automatic exercise
evaluation, gait analysis, temporal activity segmentation, and biomechanical
parameter estimation. To facilitate reproducibility, we provide code for
postprocessing, sensor-to-segment alignment, inverse kinematics computation,
and technical validation. This resource is intended to accelerate research in
machine learning-driven human movement analysis.

</details>


### [2] [Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues](https://arxiv.org/abs/2507.21161)
*Pallavi Zambare,Venkata Nikhil Thanikella,Ying Liu*

Main category: cs.CV

TL;DR: BF-PIP is a zero-shot pedestrian intention prediction method using continuous video clips and metadata, outperforming GPT-4V by 18% without retraining.


<details>
  <summary>Details</summary>
Motivation: Pedestrian intention prediction is critical for autonomous driving, but current methods require extensive retraining for new scenarios.

Method: BF-PIP uses continuous video clips with JAAD metadata, bounding-box annotations, and ego-vehicle speed via multimodal prompts.

Result: Achieves 73% accuracy, 18% higher than GPT-4V, showing improved spatiotemporal perception.

Conclusion: BF-PIP enables agile, retraining-free intent prediction, advancing intelligent transportation systems.

Abstract: Pedestrian intention prediction is essential for autonomous driving in
complex urban environments. Conventional approaches depend on supervised
learning over frame sequences and require extensive retraining to adapt to new
scenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention
Prediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing
intentions directly from short, continuous video clips enriched with structured
JAAD metadata. In contrast to GPT-4V based methods that operate on discrete
frames, BF-PIP processes uninterrupted temporal clips. It also incorporates
bounding-box annotations and ego-vehicle speed via specialized multimodal
prompts. Without any additional training, BF-PIP achieves 73% prediction
accuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate
that combining temporal video inputs with contextual cues enhances
spatiotemporal perception and improves intent inference under ambiguous
conditions. This approach paves the way for agile, retraining-free perception
module in intelligent transportation system.

</details>


### [3] [ChartM$^3$: Benchmarking Chart Editing with Multimodal Instructions](https://arxiv.org/abs/2507.21167)
*Danglu Yang,Liang Zhang,Zihao Yue,Liangyu Chen,Yichen Xu,Wenxuan Wang,Qin Jin*

Main category: cs.CV

TL;DR: The paper introduces ChartM3, a multimodal benchmark for chart editing, combining natural language and visual indicators to improve precision. It highlights limitations in current MLLMs and proposes a training dataset (ChartM3-Train) to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing chart editing methods rely on ambiguous natural language instructions, lacking support for fine-grained edits. A multimodal approach is needed to improve precision and usability.

Method: The authors introduce ChartM3, a benchmark with 1,000 samples of varying difficulty, combining charts, code, and multimodal instructions. They also create ChartM3-Train, a 24,000-sample dataset for fine-tuning MLLMs.

Result: Current MLLMs, including GPT-4o, struggle with visual indicators. Fine-tuning on ChartM3-Train significantly improves performance, demonstrating the value of multimodal supervision.

Conclusion: Multimodal supervision is crucial for practical chart editing systems. ChartM3 and ChartM3-Train provide valuable resources for advancing this field.

Abstract: Charts are a fundamental visualization format widely used in data analysis
across research and industry. While enabling users to edit charts based on
high-level intentions is of great practical value, existing methods primarily
rely on natural language instructions, which are often too ambiguous to support
fine-grained editing. In this work, we introduce a novel paradigm for
multimodal chart editing, where user intent is expressed through a combination
of natural language and visual indicators that explicitly highlight the
elements to be modified. To support this paradigm, we present
Chart$\text{M}^3$, a new benchmark for Multimodal chart editing with
Multi-level complexity and Multi-perspective evaluation. Chart$\text{M}^3$
contains 1,000 samples spanning four levels of editing difficulty. Each sample
includes triplets in the form of (chart, code, multimodal instructions). To
comprehensively evaluate chart editing models, Chart$\text{M}^3$ provides
metrics that assess both visual appearance and code correctness. Our benchmark
reveals significant limitations in current multimodal large language models
(MLLMs), including GPT-4o, particularly in their ability to interpret and act
on visual indicators. To address this, we construct Chart$\text{M}^3$-Train, a
large-scale training set with 24,000 multimodal chart editing samples.
Fine-tuning MLLMs on this dataset leads to substantial improvements,
demonstrating the importance of multimodal supervision in building practical
chart editing systems. Our datasets, codes, and evaluation tools are available
at https://github.com/MLrollIT/ChartM3. %https://github.com/MLrollIT/ChartM3Our
datasets, codes, and evaluation tools are available at
https://github.com/yaolinli/VCE.

</details>


### [4] [PanoGAN A Deep Generative Model for Panoramic Dental Radiographs](https://arxiv.org/abs/2507.21200)
*Soren Pedersen,Sanyam Jain,Mikkel Chavez,Viktor Ladehoff,Bruna Neves de Freitas,Ruben Pauwels*

Main category: cs.CV

TL;DR: A GAN was developed to generate dental panoramic radiographs, addressing data scarcity in dental research. The study used a DCGAN with WGANGP, trained on 2322 radiographs, focusing on dentoalveolar regions. Four models were tested, with trade-offs between detail and clarity noted.


<details>
  <summary>Details</summary>
Motivation: To tackle the lack of data in dental research and education by generating synthetic radiographs.

Method: Trained a DCGAN with WGANGP on 2322 radiographs, preprocessing data and testing four models with varying parameters.

Result: Generated images showed moderate anatomical depiction, with trade-offs: non-denoised data preserved finer details, while denoised data improved clarity.

Conclusion: The study lays groundwork for future GAN applications in dental imaging, highlighting trade-offs in model choices.

Abstract: This paper presents the development of a generative adversarial network (GAN)
for synthesizing dental panoramic radiographs. Although exploratory in nature,
the study aims to address the scarcity of data in dental research and
education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss
with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying
quality. The focus was on the dentoalveolar regions, other anatomical
structures were cropped out. Extensive preprocessing and data cleaning were
performed to standardize the inputs while preserving anatomical variability. We
explored four candidate models by varying critic iterations, feature depth, and
the use of denoising prior to training. A clinical expert evaluated the
generated radiographs based on anatomical visibility and realism, using a
5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical
depiction, although some were degraded by artifacts. A trade-off was observed
the model trained on non-denoised data yielded finer details especially in
structures like the mandibular canal and trabecular bone, while a model trained
on denoised data offered superior overall image clarity and sharpness. These
findings provide a foundation for future work on GAN-based methods in dental
imaging.

</details>


### [5] [On Explaining Visual Captioning with Hybrid Markov Logic Networks](https://arxiv.org/abs/2507.21246)
*Monika Shah,Somdeb Sarkhel,Deepak Venugopal*

Main category: cs.CV

TL;DR: A novel framework using Hybrid Markov Logic Networks (HMLNs) is proposed to explain how DNNs integrate multimodal information for image captioning, offering interpretable insights beyond standard metrics.


<details>
  <summary>Details</summary>
Motivation: Understanding how DNNs combine visual, language, and knowledge representations for captioning remains challenging, as standard metrics lack deep insights.

Method: The framework learns a HMLN distribution over training data and infers shifts when conditioning on generated captions, identifying influential examples.

Result: Experiments on state-of-the-art captioning models demonstrate the framework's interpretability and enable model comparison based on explainability.

Conclusion: The HMLN-based framework provides a transparent and interpretable way to analyze and compare DNN captioning models.

Abstract: Deep Neural Networks (DNNs) have made tremendous progress in multimodal tasks
such as image captioning. However, explaining/interpreting how these models
integrate visual information, language information and knowledge representation
to generate meaningful captions remains a challenging problem. Standard metrics
to measure performance typically rely on comparing generated captions with
human-written ones that may not provide a user with a deep insights into this
integration. In this work, we develop a novel explanation framework that is
easily interpretable based on Hybrid Markov Logic Networks (HMLNs) - a language
that can combine symbolic rules with real-valued functions - where we
hypothesize how relevant examples from the training data could have influenced
the generation of the observed caption. To do this, we learn a HMLN
distribution over the training instances and infer the shift in distributions
over these instances when we condition on the generated sample which allows us
to quantify which examples may have been a source of richer information to
generate the observed caption. Our experiments on captions generated for
several state-of-the-art captioning models using Amazon Mechanical Turk
illustrate the interpretability of our explanations, and allow us to compare
these models along the dimension of explainability.

</details>


### [6] [Dual Guidance Semi-Supervised Action Detection](https://arxiv.org/abs/2507.21247)
*Ankit Singh,Efstratios Gavves,Cees G. M. Snoek,Hilde Kuehne*

Main category: cs.CV

TL;DR: A semi-supervised learning (SSL) approach for spatial-temporal action localization is introduced, using a dual guidance network to improve pseudo-bounding box selection and enhance performance in limited labeled data settings.


<details>
  <summary>Details</summary>
Motivation: SSL is underutilized in spatial-temporal action localization despite its success in image classification. This work aims to bridge this gap.

Method: A dual guidance network combines frame-level classification and bounding-box prediction to ensure action class consistency across frames and boxes.

Result: The framework outperforms image-based SSL baselines on datasets like UCF101-24, J-HMDB-21, and AVA.

Conclusion: The proposed method effectively improves spatial-temporal action localization performance with limited labeled data.

Abstract: Semi-Supervised Learning (SSL) has shown tremendous potential to improve the
predictive performance of deep learning models when annotations are hard to
obtain. However, the application of SSL has so far been mainly studied in the
context of image classification. In this work, we present a semi-supervised
approach for spatial-temporal action localization. We introduce a dual guidance
network to select better pseudo-bounding boxes. It combines a frame-level
classification with a bounding-box prediction to enforce action class
consistency across frames and boxes. Our evaluation across well-known
spatial-temporal action localization datasets, namely UCF101-24 , J-HMDB-21 and
AVA shows that the proposed module considerably enhances the model's
performance in limited labeled data settings. Our framework achieves superior
results compared to extended image-based semi-supervised baselines.

</details>


### [7] [Tracking Moose using Aerial Object Detection](https://arxiv.org/abs/2507.21256)
*Christopher Indris,Raiyan Rahman,Goetz Bramesfeld,Guanghui Wang*

Main category: cs.CV

TL;DR: The paper explores patching augmentation for small object detection in aerial wildlife tracking, comparing three object detectors to optimize accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Aerial wildlife tracking faces challenges like high costs, risks, and computational limits in drones. Small object detection is difficult due to tiny target sizes and efficiency needs.

Method: The study applies patching augmentation to datasets and evaluates three diverse object detectors, varying hyperparameters to assess detection accuracy.

Result: All models achieved ≥93% mAP@IoU=0.5 in at least one configuration. Faster, simpler models performed comparably to more complex ones, supporting UAV deployment.

Conclusion: Patching augmentation and simpler models are effective for small object detection in aerial tracking, balancing accuracy and computational efficiency.

Abstract: Aerial wildlife tracking is critical for conservation efforts and relies on
detecting small objects on the ground below the aircraft. It presents technical
challenges: crewed aircraft are expensive, risky and disruptive; autonomous
drones have limited computational capacity for onboard AI systems. Since the
objects of interest may appear only a few pixels wide, small object detection
is an inherently challenging computer vision subfield compounded by
computational efficiency needs. This paper applies a patching augmentation to
datasets to study model performance under various settings. A comparative study
of three common yet architecturally diverse object detectors is conducted using
the data, varying the patching method's hyperparameters against detection
accuracy. Each model achieved at least 93\% mAP@IoU=0.5 on at least one
patching configuration. Statistical analyses provide an in-depth commentary on
the effects of various factors. Analysis also shows that faster, simpler models
are about as effective as models that require more computational power for this
task and perform well given limited patch scales, encouraging UAV deployment.
Datasets and models will be made available via
https://github.com/chrisindris/Moose.

</details>


### [8] [HDR Environment Map Estimation with Latent Diffusion Models](https://arxiv.org/abs/2507.21261)
*Jack Hilliard,Adrian Hilton,Jean-Yves Guillemaut*

Main category: cs.CV

TL;DR: A novel approach using Latent Diffusion Model (LDM) for HDR environment map estimation from single-view images, addressing ERP distortions and seam artifacts with ERP convolutional padding and a panoramically-adapted Diffusion Transformer (PanoDiT).


<details>
  <summary>Details</summary>
Motivation: To improve HDR environment map estimation by addressing distortions and seam artifacts in ERP representations, enhancing quality and plausibility for mirror-reflective surfaces.

Method: Proposes ERP convolutional padding in the latent autoencoder to remove seam artifacts and introduces PanoDiT, a panoramically-adapted Diffusion Transformer, to reduce ERP distortions.

Result: The models produce high-quality environment maps, competitive with state-of-the-art in image quality and lighting accuracy, though PanoDiT trades off some image quality for reduced distortions.

Conclusion: The approach effectively addresses ERP-related issues, offering competitive performance in environment map estimation, with potential trade-offs between distortion reduction and image quality.

Abstract: We advance the field of HDR environment map estimation from a single-view
image by establishing a novel approach leveraging the Latent Diffusion Model
(LDM) to produce high-quality environment maps that can plausibly light
mirror-reflective surfaces. A common issue when using the ERP representation,
the format used by the vast majority of approaches, is distortions at the poles
and a seam at the sides of the environment map. We remove the border seam
artefact by proposing an ERP convolutional padding in the latent autoencoder.
Additionally, we investigate whether adapting the diffusion network
architecture to the ERP format can improve the quality and accuracy of the
estimated environment map by proposing a panoramically-adapted Diffusion
Transformer architecture. Our proposed PanoDiT network reduces ERP distortions
and artefacts, but at the cost of image quality and plausibility. We evaluate
with standard benchmarks to demonstrate that our models estimate high-quality
environment maps that perform competitively with state-of-the-art approaches in
both image quality and lighting accuracy.

</details>


### [9] [Fairness and Robustness of CLIP-Based Models for Chest X-rays](https://arxiv.org/abs/2507.21291)
*Théo Sourget,David Restrepo,Céline Hudelot,Enzo Ferrante,Stergios Christodoulidis,Maria Vakalopoulou*

Main category: cs.CV

TL;DR: The study evaluates six CLIP-based models for chest X-ray classification, focusing on fairness and robustness. It reveals performance gaps in age groups and reliance on spurious correlations, while embeddings show limitations in detecting sensitive attributes.


<details>
  <summary>Details</summary>
Motivation: To assess the fairness and robustness of CLIP-based models in medical imaging, particularly for chest X-ray classification, given their underexplored performance in clinical tasks.

Method: Evaluated six CLIP-based models on three datasets (MIMIC-CXR, NIH-CXR14, NEATX) for fairness across age, sex, race, and robustness to shortcut learning (pneumothorax cases with/without chest drains). Analyzed embeddings for sensitive attribute classification.

Result: Performance gaps in age groups; equitable results for sex and race. Models performed worse without chest drains, indicating reliance on spurious correlations. Embeddings could classify sensitive attributes but not via PCA.

Conclusion: CLIP-based models show fairness and robustness limitations in medical imaging, highlighting the need for further evaluation and mitigation of biases and spurious correlations.

Abstract: Motivated by the strong performance of CLIP-based models in natural
image-text domains, recent efforts have adapted these architectures to medical
tasks, particularly in radiology, where large paired datasets of images and
reports, such as chest X-rays, are available. While these models have shown
encouraging results in terms of accuracy and discriminative performance, their
fairness and robustness in the different clinical tasks remain largely
underexplored. In this study, we extensively evaluate six widely used
CLIP-based models on chest X-ray classification using three publicly available
datasets: MIMIC-CXR, NIH-CXR14, and NEATX. We assess the models fairness across
six conditions and patient subgroups based on age, sex, and race. Additionally,
we assess the robustness to shortcut learning by evaluating performance on
pneumothorax cases with and without chest drains. Our results indicate
performance gaps between patients of different ages, but more equitable results
for the other attributes. Moreover, all models exhibit lower performance on
images without chest drains, suggesting reliance on spurious correlations. We
further complement the performance analysis with a study of the embeddings
generated by the models. While the sensitive attributes could be classified
from the embeddings, we do not see such patterns using PCA, showing the
limitations of these visualisation techniques when assessing models. Our code
is available at https://github.com/TheoSourget/clip_cxr_fairness

</details>


### [10] [VoluMe -- Authentic 3D Video Calls from Live Gaussian Splat Prediction](https://arxiv.org/abs/2507.21311)
*Martin de La Gorce,Charlie Hewitt,Tibor Takacs,Robert Gerdisch,Zafiirah Hosenie,Givi Meishvili,Marek Kowalski,Thomas J. Cashman,Antonio Criminisi*

Main category: cs.CV

TL;DR: A method for real-time 3D Gaussian reconstructions from a single 2D webcam feed, enhancing virtual meetings with realistic and authentic representations.


<details>
  <summary>Details</summary>
Motivation: Improving remote meetings by overcoming the limitations of existing 3D representation methods, which rely on complex hardware or fixed appearances.

Method: Predicts 3D Gaussian reconstructions in real time from a 2D webcam feed, ensuring authenticity and stability with a novel loss function.

Result: Achieves state-of-the-art accuracy in visual quality and stability, enabling live 3D meetings with standard hardware.

Conclusion: The approach offers a highly accessible, realistic, and authentic solution for 3D videoconferencing.

Abstract: Virtual 3D meetings offer the potential to enhance copresence, increase
engagement and thus improve effectiveness of remote meetings compared to
standard 2D video calls. However, representing people in 3D meetings remains a
challenge; existing solutions achieve high quality by using complex hardware,
making use of fixed appearance via enrolment, or by inverting a pre-trained
generative model. These approaches lead to constraints that are unwelcome and
ill-fitting for videoconferencing applications. We present the first method to
predict 3D Gaussian reconstructions in real time from a single 2D webcam feed,
where the 3D representation is not only live and realistic, but also authentic
to the input video. By conditioning the 3D representation on each video frame
independently, our reconstruction faithfully recreates the input video from the
captured viewpoint (a property we call authenticity), while generalizing
realistically to novel viewpoints. Additionally, we introduce a stability loss
to obtain reconstructions that are temporally stable on video sequences. We
show that our method delivers state-of-the-art accuracy in visual quality and
stability metrics compared to existing methods, and demonstrate our approach in
live one-to-one 3D meetings using only a standard 2D camera and display. This
demonstrates that our approach can allow anyone to communicate volumetrically,
via a method for 3D videoconferencing that is not only highly accessible, but
also realistic and authentic.

</details>


### [11] [GLCP: Global-to-Local Connectivity Preservation for Tubular Structure Segmentation](https://arxiv.org/abs/2507.21328)
*Feixiang Zhou,Zhuangzhi Gao,He Zhao,Jianyang Xie,Yanda Meng,Yitian Zhao,Gregory Y. H. Lip,Yalin Zheng*

Main category: cs.CV

TL;DR: The paper introduces a Global-to-Local Connectivity Preservation (GLCP) framework to improve tubular structure segmentation by addressing both global and local structural challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for tubular structure segmentation often neglect local discontinuity regions, leading to fragmented results. The paper aims to overcome this limitation.

Method: The proposed GLCP framework includes an Interactive Multi-head Segmentation (IMS) module for joint learning of global segmentation, skeleton maps, and local discontinuity maps, and a Dual-Attention-based Refinement (DAR) module for quality improvement.

Result: Experiments on 2D and 3D datasets show GLCP outperforms state-of-the-art methods in accuracy and continuity.

Conclusion: The GLCP framework effectively addresses fragmentation in tubular structure segmentation by integrating global and local features, with promising results.

Abstract: Accurate segmentation of tubular structures, such as vascular networks, plays
a critical role in various medical domains. A remaining significant challenge
in this task is structural fragmentation, which can adversely impact downstream
applications. Existing methods primarily focus on designing various loss
functions to constrain global topological structures. However, they often
overlook local discontinuity regions, leading to suboptimal segmentation
results. To overcome this limitation, we propose a novel Global-to-Local
Connectivity Preservation (GLCP) framework that can simultaneously perceive
global and local structural characteristics of tubular networks. Specifically,
we propose an Interactive Multi-head Segmentation (IMS) module to jointly learn
global segmentation, skeleton maps, and local discontinuity maps, respectively.
This enables our model to explicitly target local discontinuity regions while
maintaining global topological integrity. In addition, we design a lightweight
Dual-Attention-based Refinement (DAR) module to further improve segmentation
quality by refining the resulting segmentation maps. Extensive experiments on
both 2D and 3D datasets demonstrate that our GLCP achieves superior accuracy
and continuity in tubular structure segmentation compared to several
state-of-the-art approaches. The source codes will be available at
https://github.com/FeixiangZhou/GLCP.

</details>


### [12] [Analyzing the Sensitivity of Vision Language Models in Visual Question Answering](https://arxiv.org/abs/2507.21335)
*Monika Shah,Sudarshan Balaji,Somdeb Sarkhel,Sanorita Dey,Deepak Venugopal*

Main category: cs.CV

TL;DR: The paper investigates how Vision Language Models (VLMs) handle violations of Grice's conversational maxims, finding their performance declines with added modifiers.


<details>
  <summary>Details</summary>
Motivation: To understand if VLMs, like humans, can manage violations of Grice's maxims in conversations.

Method: Modifiers were added to human-crafted questions from the VQA v2.0 dataset, and responses of three VLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Flash) were analyzed.

Result: VLMs' performance consistently diminished with modifiers, highlighting limitations.

Conclusion: The approach is promising for uncovering VLM limitations in handling conversational violations.

Abstract: We can think of Visual Question Answering as a (multimodal) conversation
between a human and an AI system. Here, we explore the sensitivity of Vision
Language Models (VLMs) through the lens of cooperative principles of
conversation proposed by Grice. Specifically, even when Grice's maxims of
conversation are flouted, humans typically do not have much difficulty in
understanding the conversation even though it requires more cognitive effort.
Here, we study if VLMs are capable of handling violations to Grice's maxims in
a manner that is similar to humans. Specifically, we add modifiers to
human-crafted questions and analyze the response of VLMs to these modifiers. We
use three state-of-the-art VLMs in our study, namely, GPT-4o, Claude-3.5-Sonnet
and Gemini-1.5-Flash on questions from the VQA v2.0 dataset. Our initial
results seem to indicate that the performance of VLMs consistently diminish
with the addition of modifiers which indicates our approach as a promising
direction to understand the limitations of VLMs.

</details>


### [13] [Enhancing and Accelerating Brain MRI through Deep Learning Reconstruction Using Prior Subject-Specific Imaging](https://arxiv.org/abs/2507.21349)
*Amirmohammad Shamaei,Alexander Stebner,Salome,Bosshart,Johanna Ospel,Gouri Ginde,Mariana Bento,Roberto Souza*

Main category: cs.CV

TL;DR: A deep-learning-based MRI reconstruction framework improves scan quality and reduces time by integrating prior scans and using a transformer-based enhancement network.


<details>
  <summary>Details</summary>
Motivation: Long MRI acquisition times increase costs and reduce patient comfort. Prior scans can enhance reconstruction but require time-consuming registration.

Method: Proposes a framework with an initial reconstruction network, deep registration model, and transformer-based enhancement network, validated on 2,808 T1-weighted MRI scans.

Result: Outperforms existing methods (p < 0.05), improves brain segmentation accuracy, and reduces reconstruction time.

Conclusion: The method is efficient, suitable for clinical use, and publicly available.

Abstract: Magnetic resonance imaging (MRI) is a crucial medical imaging modality.
However, long acquisition times remain a significant challenge, leading to
increased costs, and reduced patient comfort. Recent studies have shown the
potential of using deep learning models that incorporate information from prior
subject-specific MRI scans to improve reconstruction quality of present scans.
Integrating this prior information requires registration of the previous scan
to the current image reconstruction, which can be time-consuming. We propose a
novel deep-learning-based MRI reconstruction framework which consists of an
initial reconstruction network, a deep registration model, and a
transformer-based enhancement network. We validated our method on a
longitudinal dataset of T1-weighted MRI scans with 2,808 images from 18
subjects at four acceleration factors (R5, R10, R15, R20). Quantitative metrics
confirmed our approach's superiority over existing methods (p < 0.05, Wilcoxon
signed-rank test). Furthermore, we analyzed the impact of our MRI
reconstruction method on the downstream task of brain segmentation and observed
improved accuracy and volumetric agreement with reference segmentations. Our
approach also achieved a substantial reduction in total reconstruction time
compared to methods that use traditional registration algorithms, making it
more suitable for real-time clinical applications. The code associated with
this work is publicly available at
https://github.com/amirshamaei/longitudinal-mri-deep-recon.

</details>


### [14] [Group Relative Augmentation for Data Efficient Action Detection](https://arxiv.org/abs/2507.21353)
*Deep Anil Patel,Iain Melvin,Zachary Izzo,Martin Renqiang Min*

Main category: cs.CV

TL;DR: Efficient adaptation of Video-Language Models (VLMs) for action detection using few examples, addressing overfitting and granularity mismatch with parameter-efficient tuning and feature augmentation.


<details>
  <summary>Details</summary>
Motivation: Challenges in adapting VLMs for action detection include overfitting and the mismatch between scene-level pre-training and person-centric understanding.

Method: Combines LoRA for parameter-efficient tuning with learnable internal feature augmentation (FiLM) and a group-weighted loss function.

Result: Achieves strong mAP on AVA and MOMA datasets, demonstrating data efficiency with limited examples.

Conclusion: The proposed method effectively adapts VLMs for action detection, balancing robustness and efficiency.

Abstract: Adapting large Video-Language Models (VLMs) for action detection using only a
few examples poses challenges like overfitting and the granularity mismatch
between scene-level pre-training and required person-centric understanding. We
propose an efficient adaptation strategy combining parameter-efficient tuning
(LoRA) with a novel learnable internal feature augmentation. Applied within the
frozen VLM backbone using FiLM, these augmentations generate diverse feature
variations directly relevant to the task. Additionally, we introduce a
group-weighted loss function that dynamically modulates the training
contribution of each augmented sample based on its prediction divergence
relative to the group average. This promotes robust learning by prioritizing
informative yet reasonable augmentations. We demonstrate our method's
effectiveness on complex multi-label, multi-person action detection datasets
(AVA, MOMA), achieving strong mAP performance and showcasing significant data
efficiency for adapting VLMs from limited examples.

</details>


### [15] [Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy](https://arxiv.org/abs/2507.21358)
*Jicheng Yuan,Manh Nguyen Duc,Qian Liu,Manfred Hauswirth,Danh Le Phuoc*

Main category: cs.CV

TL;DR: CoP introduces a multi-task learning framework for BEV 3D object detection, using spatial occupancy to improve environmental context understanding, outperforming existing methods on nuScenes.


<details>
  <summary>Details</summary>
Motivation: Existing BEV methods neglect environmental contexts like roads, limiting perception. CoP aims to bridge this gap by leveraging spatial occupancy for better feature refinement.

Method: CoP uses LDO for dense occupancy ground truths, VHS for fine-grained feature sampling, and CFF for global-local feature fusion.

Result: Achieves 49.5% mAP and 59.2% NDS on nuScenes, outperforming prior vision-based frameworks.

Conclusion: CoP enhances BEV representations by integrating occupancy prediction, improving 3D object detection performance.

Abstract: Vision-based bird's-eye-view (BEV) 3D object detection has advanced
significantly in autonomous driving by offering cost-effectiveness and rich
contextual information. However, existing methods often construct BEV
representations by collapsing extracted object features, neglecting intrinsic
environmental contexts, such as roads and pavements. This hinders detectors
from comprehensively perceiving the characteristics of the physical world. To
alleviate this, we introduce a multi-task learning framework, Collaborative
Perceiver (CoP), that leverages spatial occupancy as auxiliary information to
mine consistent structural and conceptual similarities shared between 3D object
detection and occupancy prediction tasks, bridging gaps in spatial
representations and feature refinement. To this end, we first propose a
pipeline to generate dense occupancy ground truths incorporating local density
information (LDO) for reconstructing detailed environmental information. Next,
we employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained
local features according to distinct object properties. Furthermore, we develop
a global-local collaborative feature fusion (CFF) module that seamlessly
integrates complementary knowledge between both tasks, thus composing more
robust BEV representations. Extensive experiments on the nuScenes benchmark
demonstrate that CoP outperforms existing vision-based frameworks, achieving
49.5\% mAP and 59.2\% NDS on the test set. Code and supplementary materials are
available at this link https://github.com/jichengyuan/Collaborative-Perceiver.

</details>


### [16] [Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers](https://arxiv.org/abs/2507.21364)
*Lukman Jibril Aliyu,Umar Sani Muhammad,Bilqisu Ismail,Nasiru Muhammad,Almustapha A Wakili,Seid Muhie Yimam,Shamsuddeen Hassan Muhammad,Mustapha Abdullahi*

Main category: cs.CV

TL;DR: The paper compares deep learning models for classifying African wildlife images, finding ViT-H/14 most accurate but computationally costly, while DenseNet-201 offers a balance for practical deployment.


<details>
  <summary>Details</summary>
Motivation: Address the decline of African wildlife by leveraging deep learning for biodiversity monitoring and conservation.

Method: Comparative study of DenseNet-201, ResNet-152, EfficientNet-B4, and ViT-H/14 using transfer learning on a dataset of buffalo, elephant, rhinoceros, and zebra images.

Result: ViT-H/14 achieved 99% accuracy but high computational cost; DenseNet-201 (67% accuracy) was deemed more deployable and integrated into a real-time tool.

Conclusion: The study provides insights for model selection and deployment in conservation, emphasizing trade-offs between accuracy and practicality.

Abstract: Wildlife populations in Africa face severe threats, with vertebrate numbers
declining by over 65% in the past five decades. In response, image
classification using deep learning has emerged as a promising tool for
biodiversity monitoring and conservation. This paper presents a comparative
study of deep learning models for automatically classifying African wildlife
images, focusing on transfer learning with frozen feature extractors. Using a
public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we
evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and
Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among
convolutional networks (67% accuracy), while ViT-H/14 achieved the highest
overall accuracy (99%), but with significantly higher computational cost,
raising deployment concerns. Our experiments highlight the trade-offs between
accuracy, resource requirements, and deployability. The best-performing CNN
(DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time
field use, demonstrating the feasibility of deploying lightweight models in
conservation settings. This work contributes to African-grounded AI research by
offering practical insights into model selection, dataset preparation, and
responsible deployment of deep learning tools for wildlife conservation.

</details>


### [17] [Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation](https://arxiv.org/abs/2507.21367)
*I-Hsiang Chen,Hua-En Chang,Wei-Ting Chen,Jenq-Neng Hwang,Sy-Yen Kuo*

Main category: cs.CV

TL;DR: PDAF introduces a Probabilistic Diffusion Alignment Framework to improve domain generalization in semantic segmentation by modeling latent domain priors and iteratively refining features.


<details>
  <summary>Details</summary>
Motivation: Domain shifts in unseen environments degrade model performance, and existing methods often ignore intrinsic latent domain priors.

Method: PDAF uses a Latent Domain Prior (LDP) to align domains, integrating three modules: Latent Prior Extractor, Domain Compensation Module, and Diffusion Prior Estimator.

Result: PDAF effectively enhances generalization under complex target conditions, validated by experiments in diverse urban scenes.

Conclusion: PDAF successfully addresses domain shifts by leveraging probabilistic diffusion modeling and latent domain priors.

Abstract: Domain Generalized Semantic Segmentation (DGSS) is a critical yet challenging
task, as domain shifts in unseen environments can severely compromise model
performance. While recent studies enhance feature alignment by projecting
features into the source domain, they often neglect intrinsic latent domain
priors, leading to suboptimal results. In this paper, we introduce PDAF, a
Probabilistic Diffusion Alignment Framework that enhances the generalization of
existing segmentation networks through probabilistic diffusion modeling. PDAF
introduces a Latent Domain Prior (LDP) to capture domain shifts and uses this
prior as a conditioning factor to align both source and unseen target domains.
To achieve this, PDAF integrates into a pre-trained segmentation model and
utilizes paired source and pseudo-target images to simulate latent domain
shifts, enabling LDP modeling. The framework comprises three modules: the
Latent Prior Extractor (LPE) predicts the LDP by supervising domain shifts; the
Domain Compensation Module (DCM) adjusts feature representations to mitigate
domain shifts; and the Diffusion Prior Estimator (DPE) leverages a diffusion
process to estimate the LDP without requiring paired samples. This design
enables PDAF to iteratively model domain shifts, progressively refining feature
representations to enhance generalization under complex target conditions.
Extensive experiments validate the effectiveness of PDAF across diverse and
challenging urban scenes.

</details>


### [18] [Top2Pano: Learning to Generate Indoor Panoramas from Top-Down View](https://arxiv.org/abs/2507.21371)
*Zitong Zhang,Suranjan Gautam,Rui Yu*

Main category: cs.CV

TL;DR: Top2Pano is an end-to-end model for generating realistic 360° indoor panoramas from 2D top-down views, outperforming baselines in geometry, occlusions, and spatial arrangements.


<details>
  <summary>Details</summary>
Motivation: The task is challenging due to missing 3D structure and the need for geometric consistency and photorealism in applications like VR, interior design, and robotics.

Method: Top2Pano estimates volumetric occupancy for 3D structure, uses volumetric rendering for coarse panoramas, and refines them with a diffusion-based approach (ControlNet).

Result: Evaluations show Top2Pano outperforms baselines, reconstructing geometry and occlusions effectively, and generalizes well to schematic floorplans.

Conclusion: Top2Pano successfully bridges top-down views with immersive indoor synthesis, demonstrating high-quality results.

Abstract: Generating immersive 360{\deg} indoor panoramas from 2D top-down views has
applications in virtual reality, interior design, real estate, and robotics.
This task is challenging due to the lack of explicit 3D structure and the need
for geometric consistency and photorealism. We propose Top2Pano, an end-to-end
model for synthesizing realistic indoor panoramas from top-down views. Our
method estimates volumetric occupancy to infer 3D structures, then uses
volumetric rendering to generate coarse color and depth panoramas. These guide
a diffusion-based refinement stage using ControlNet, enhancing realism and
structural fidelity. Evaluations on two datasets show Top2Pano outperforms
baselines, effectively reconstructing geometry, occlusions, and spatial
arrangements. It also generalizes well, producing high-quality panoramas from
schematic floorplans. Our results highlight Top2Pano's potential in bridging
top-down views with immersive indoor synthesis.

</details>


### [19] [Multimodal LLMs as Customized Reward Models for Text-to-Image Generation](https://arxiv.org/abs/2507.21391)
*Shijie Zhou,Ruiyi Zhang,Huaisheng Zhu,Branislav Kveton,Yufan Zhou,Jiuxiang Gu,Jian Chen,Changyou Chen*

Main category: cs.CV

TL;DR: LLaVA-Reward is a reward model for evaluating text-to-image generations using pretrained MLLMs, improving efficiency and accuracy with a SkipCA module and diverse preference data.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based methods are time-consuming and hard to train, requiring instruction-following data. LLaVA-Reward aims to simplify and enhance evaluation.

Method: LLaVA-Reward uses hidden states of MLLMs for text-image pairs, introduces SkipCA for better interaction, and supports various preference data types for fine-tuning.

Result: Outperforms conventional and MLLM-based methods in human-aligned scoring and inference-time scaling.

Conclusion: LLaVA-Reward offers an efficient, accurate solution for evaluating text-to-image generations across multiple perspectives.

Abstract: We introduce LLaVA-Reward, an efficient reward model designed to
automatically evaluate text-to-image (T2I) generations across multiple
perspectives, leveraging pretrained multimodal large language models (MLLMs).
Existing MLLM-based approaches require instruction-following data for
supervised fine-tuning and evaluate generation quality on analyzing text
response, which is time-consuming and difficult to train. To address this
problem, we propose LLaVA-Reward, which directly utilizes the hidden states of
MLLMs given text-image pairs. To enhance the bidirectional interaction between
visual and textual representations in decoder-only MLLMs, we further propose
adding a Skip-connection Cross Attention (SkipCA) module. This design enhances
text-image correlation reasoning by connecting early-layer visual features with
later-layer hidden representations.In addition, LLaVA-Reward supports different
types of preference data for efficient fine-tuning, including paired preference
data and unpaired data. We train LLaVA-Reward on four evaluation perspectives:
text-image alignment, fidelity/artifact, safety, and overall ranking. Empirical
results demonstrate that LLaVA-Reward outperforms conventional and MLLM-based
methods in generating human-aligned scores for automatic evaluations and
inference-time scaling in text-to-image generations.

</details>


### [20] [ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs](https://arxiv.org/abs/2507.21420)
*Chaoyu Li,Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: ReGATE is a token pruning method for faster MLLM training, using a teacher-student framework to selectively process tokens, reducing computation by up to 41%.


<details>
  <summary>Details</summary>
Motivation: Training multimodal large language models (MLLMs) is computationally expensive due to token volume. Existing methods focus on inference, not training efficiency.

Method: ReGATE uses a teacher-student framework with a frozen LLM teacher to compute reference losses and adaptive difficulty scores for token pruning.

Result: ReGATE accelerates training (2x faster) with 35% tokens, matches peak accuracy, and surpasses baselines on benchmarks.

Conclusion: ReGATE efficiently reduces training costs while maintaining or improving performance, promising for MLLM scalability.

Abstract: The computational cost of training multimodal large language models (MLLMs)
rapidly increases with the number of tokens involved. Existing efficiency
methods primarily target inference and rely on token reduction or merging,
offering limited benefit during training. In this paper, we propose ReGATE
(Reference$-$Guided Adaptive Token Elision), an adaptive token pruning method
for accelerating MLLM training. Specifically, ReGATE adopts a teacher-student
framework in which the MLLM being trained serves as the student, and a frozen
reference large language model (LLM) acts as the teacher. The teacher computes
per-token reference losses, which are combined with an exponential moving
average (EMA) of the student's own difficulty scores. This adaptive
difficulty-based scoring enables the selective processing of crucial tokens
while bypassing less informative ones in the forward pass, significantly
reducing computational overhead. Experiments demonstrate that ReGATE, when
applied to VideoLLaMA2, matches the peak accuracy of standard training on
MVBench up to 2$\times$ faster, using only 35% of the tokens. With additional
training, it even surpasses the baseline on several multimodal benchmarks, all
while reducing the total token count by over 41%. Code and models will be
released soon.

</details>


### [21] [MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving](https://arxiv.org/abs/2507.21423)
*Thomas Monninger,Zihan Zhang,Zhipeng Mo,Md Zafar Anwar,Steffen Staab,Sihao Ding*

Main category: cs.CV

TL;DR: MapDiffusion introduces a generative diffusion-based method for vectorized map construction, capturing uncertainty and improving accuracy in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Traditional deterministic map models fail to account for uncertainty and ambiguities like occlusions, limiting reliability in real-world environments.

Method: MapDiffusion uses diffusion to iteratively refine randomly initialized queries, conditioned on a BEV latent grid, generating multiple plausible map samples.

Result: Achieves state-of-the-art performance (5% improvement) on nuScenes, with uncertainty estimates correlating with scene ambiguity.

Conclusion: MapDiffusion enhances robustness and reliability in HD map construction, enabling uncertainty-aware decision-making for autonomous vehicles.

Abstract: Autonomous driving requires an understanding of the static environment from
sensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse
multiple inputs, and a vector decoder predicts a vectorized map representation
from the latent BEV grid. However, traditional map construction models provide
deterministic point estimates, failing to capture uncertainty and the inherent
ambiguities of real-world environments, such as occlusions and missing lane
markings. We propose MapDiffusion, a novel generative approach that leverages
the diffusion paradigm to learn the full distribution of possible vectorized
maps. Instead of predicting a single deterministic output from learned queries,
MapDiffusion iteratively refines randomly initialized queries, conditioned on a
BEV latent grid, to generate multiple plausible map samples. This allows
aggregating samples to improve prediction accuracy and deriving uncertainty
estimates that directly correlate with scene ambiguity. Extensive experiments
on the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art
performance in online map construction, surpassing the baseline by 5% in
single-sample performance. We further show that aggregating multiple samples
consistently improves performance along the ROC curve, validating the benefit
of distribution modeling. Additionally, our uncertainty estimates are
significantly higher in occluded areas, reinforcing their value in identifying
regions with ambiguous sensor input. By modeling the full map distribution,
MapDiffusion enhances the robustness and reliability of online vectorized HD
map construction, enabling uncertainty-aware decision-making for autonomous
vehicles in complex environments.

</details>


### [22] [Dual Cross-image Semantic Consistency with Self-aware Pseudo Labeling for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.21440)
*Han Wu,Chong Wang,Zhiming Cui*

Main category: cs.CV

TL;DR: DuCiSC introduces a dual cross-image semantic consistency framework for semi-supervised medical image segmentation, addressing feature discrepancy and improving region-level consistency.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised methods focus on pixel-wise consistency but overlook broader semantic levels and suffer from feature imbalance between labeled and unlabeled data.

Method: DuCiSC enforces region-level consistency across labeled and unlabeled images via prototype alignment and uses a self-aware confidence strategy for reliable pseudo-labeling.

Result: DuCiSC outperforms state-of-the-art methods on four datasets, including binary and multi-class segmentation tasks.

Conclusion: The proposed framework effectively addresses feature discrepancy and improves segmentation accuracy, demonstrating broad applicability in medical imaging.

Abstract: Semi-supervised learning has proven highly effective in tackling the
challenge of limited labeled training data in medical image segmentation. In
general, current approaches, which rely on intra-image pixel-wise consistency
training via pseudo-labeling, overlook the consistency at more comprehensive
semantic levels (e.g., object region) and suffer from severe discrepancy of
extracted features resulting from an imbalanced number of labeled and unlabeled
data. To overcome these limitations, we present a new \underline{Du}al
\underline{C}ross-\underline{i}mage \underline{S}emantic
\underline{C}onsistency (DuCiSC) learning framework, for semi-supervised
medical image segmentation. Concretely, beyond enforcing pixel-wise semantic
consistency, DuCiSC proposes dual paradigms to encourage region-level semantic
consistency across: 1) labeled and unlabeled images; and 2) labeled and fused
images, by explicitly aligning their prototypes. Relying on the dual paradigms,
DuCiSC can effectively establish consistent cross-image semantics via prototype
representations, thereby addressing the feature discrepancy issue. Moreover, we
devise a novel self-aware confidence estimation strategy to accurately select
reliable pseudo labels, allowing for exploiting the training dynamics of
unlabeled data. Our DuCiSC method is extensively validated on four datasets,
including two popular binary benchmarks in segmenting the left atrium and
pancreas, a multi-class Automatic Cardiac Diagnosis Challenge dataset, and a
challenging scenario of segmenting the inferior alveolar nerve that features
complicated anatomical structures, showing superior segmentation results over
previous state-of-the-art approaches. Our code is publicly available at
\href{https://github.com/ShanghaiTech-IMPACT/DuCiSC}{https://github.com/ShanghaiTech-IMPACT/DuCiSC}.

</details>


### [23] [Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation](https://arxiv.org/abs/2507.21450)
*Bolei Chen,Jiaxu Kang,Yifei Wang,Ping Zhong,Qi Wu,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper proposes a navigation policy for Vision Language Navigation (VLN) using Recursive Visual Imagination (RVI) and Adaptive Linguistic Grounding (ALG) to improve scene understanding and command alignment.


<details>
  <summary>Details</summary>
Motivation: Current VLN agents struggle with overly detailed scene representations and poor vision-language alignment, leading to navigation errors.

Method: The method involves RVI to summarize visual perceptions into compact neural grids and ALG to align situational memories with linguistic commands.

Result: The policy outperforms state-of-the-art methods on VLN-CE and ObjectNav tasks.

Conclusion: RVI and ALG enhance VLN by improving scene priors and linguistic grounding, leading to better navigation performance.

Abstract: Vision Language Navigation (VLN) typically requires agents to navigate to
specified objects or remote regions in unknown scenes by obeying linguistic
commands. Such tasks require organizing historical visual observations for
linguistic grounding, which is critical for long-sequence navigational
decisions. However, current agents suffer from overly detailed scene
representation and ambiguous vision-language alignment, which weaken their
comprehension of navigation-friendly high-level scene priors and easily lead to
behaviors that violate linguistic commands. To tackle these issues, we propose
a navigation policy by recursively summarizing along-the-way visual
perceptions, which are adaptively aligned with commands to enhance linguistic
grounding. In particular, by structurally modeling historical trajectories as
compact neural grids, several Recursive Visual Imagination (RVI) techniques are
proposed to motivate agents to focus on the regularity of visual transitions
and semantic scene layouts, instead of dealing with misleading geometric
details. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to
align the learned situational memories with different linguistic components
purposefully. Such fine-grained semantic matching facilitates the accurate
anticipation of navigation actions and progress. Our navigation policy
outperforms the state-of-the-art methods on the challenging VLN-CE and
ObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.

</details>


### [24] [Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation](https://arxiv.org/abs/2507.21455)
*Sheng-Feng Yu,Jia-Jiun Yao,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: The paper introduces Self-Supervised Dataset Distillation, a method to reduce dataset size while preserving performance by distilling images and their self-supervised representations. Novel techniques include low-dimensional bases, predetermined augmentations, and lightweight networks for compact distillation.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of dataset sizes increases training costs. Existing dataset distillation methods focus on supervised datasets, but this work targets self-supervised learning to enhance cross-architecture generalizability.

Method: Proposes Self-Supervised Dataset Distillation with: 1) low-dimensional bases for parameterization, 2) predetermined augmentations to address instability, and 3) a lightweight network to model representation connections.

Result: The method shows superior distillation efficiency, cross-architecture generalization, and transfer learning performance in experiments.

Conclusion: Self-Supervised Dataset Distillation effectively reduces dataset size while maintaining performance, with novel techniques improving fidelity and compactness.

Abstract: Although larger datasets are crucial for training large deep models, the
rapid growth of dataset size has brought a significant challenge in terms of
considerable training costs, which even results in prohibitive computational
expenses. Dataset Distillation becomes a popular technique recently to reduce
the dataset size via learning a highly compact set of representative exemplars,
where the model trained with these exemplars ideally should have comparable
performance with respect to the one trained with the full dataset. While most
of existing works upon dataset distillation focus on supervised datasets, we
instead aim to distill images and their self-supervisedly trained
representations into a distilled set. This procedure, named as Self-Supervised
Dataset Distillation, effectively extracts rich information from real datasets,
yielding the distilled sets with enhanced cross-architecture generalizability.
Particularly, in order to preserve the key characteristics of original dataset
more faithfully and compactly, several novel techniques are proposed: 1) we
introduce an innovative parameterization upon images and representations via
distinct low-dimensional bases, where the base selection for parameterization
is experimentally shown to play a crucial role; 2) we tackle the instability
induced by the randomness of data augmentation -- a key component in
self-supervised learning but being underestimated in the prior work of
self-supervised dataset distillation -- by utilizing predetermined
augmentations; 3) we further leverage a lightweight network to model the
connections among the representations of augmented views from the same image,
leading to more compact pairs of distillation. Extensive experiments conducted
on various datasets validate the superiority of our approach in terms of
distillation efficiency, cross-architecture generalization, and transfer
learning performance.

</details>


### [25] [An Angular-Temporal Interaction Network for Light Field Object Tracking in Low-Light Scenes](https://arxiv.org/abs/2507.21460)
*Mianzhao Wang,Fan Shi,Xu Cheng,Feifei Zhang,Shengyong Chen*

Main category: cs.CV

TL;DR: The paper introduces a novel light field representation (ESI) and an angular-temporal interaction network (ATINet) for improved object tracking in low-light scenes, validated by a new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with reliable angular modeling in low-light scenes, limiting scene perception and moving target identification.

Method: Proposes ESI for geometric structure representation and ATINet for learning angular-aware features, optimized self-supervisedly.

Result: ATINet achieves state-of-the-art performance in single object tracking and shows effectiveness in multiple object tracking.

Conclusion: The proposed ESI and ATINet enhance light field angular-temporal modeling, proving effective for object tracking in challenging conditions.

Abstract: High-quality 4D light field representation with efficient angular feature
modeling is crucial for scene perception, as it can provide discriminative
spatial-angular cues to identify moving targets. However, recent developments
still struggle to deliver reliable angular modeling in the temporal domain,
particularly in complex low-light scenes. In this paper, we propose a novel
light field epipolar-plane structure image (ESI) representation that explicitly
defines the geometric structure within the light field. By capitalizing on the
abrupt changes in the angles of light rays within the epipolar plane, this
representation can enhance visual expression in low-light scenes and reduce
redundancy in high-dimensional light fields. We further propose an
angular-temporal interaction network (ATINet) for light field object tracking
that learns angular-aware representations from the geometric structural cues
and angular-temporal interaction cues of light fields. Furthermore, ATINet can
also be optimized in a self-supervised manner to enhance the geometric feature
interaction across the temporal domain. Finally, we introduce a large-scale
light field low-light dataset for object tracking. Extensive experimentation
demonstrates that ATINet achieves state-of-the-art performance in single object
tracking. Furthermore, we extend the proposed method to multiple object
tracking, which also shows the effectiveness of high-quality light field
angular-temporal modeling.

</details>


### [26] [Describe, Adapt and Combine: Empowering CLIP Encoders for Open-set 3D Object Retrieval](https://arxiv.org/abs/2507.21489)
*Zhichuan Wang,Yang Zhou,Zhe Liu,Rui Yu,Song Bai,Yulong Wang,Xinwei He,Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces DAC, a framework for open-set 3D object retrieval using multi-view images, leveraging CLIP and MLLM for generalized representations, achieving +10.01% mAP improvement.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with generalized representations due to limited 3D training data. CLIP's pre-trained capabilities offer a solution.

Method: DAC combines CLIP with MLLM for adaptation and inference, using AB-LoRA to enhance generalization.

Result: DAC outperforms prior methods by +10.01% mAP on four datasets and shows strong generalization.

Conclusion: DAC is effective for open-set 3DOR, leveraging CLIP and MLLM for superior performance and generalization.

Abstract: Open-set 3D object retrieval (3DOR) is an emerging task aiming to retrieve 3D
objects of unseen categories beyond the training set. Existing methods
typically utilize all modalities (i.e., voxels, point clouds, multi-view
images) and train specific backbones before fusion. However, they still
struggle to produce generalized representations due to insufficient 3D training
data. Being contrastively pre-trained on web-scale image-text pairs, CLIP
inherently produces generalized representations for a wide range of downstream
tasks. Building upon it, we present a simple yet effective framework named
Describe, Adapt and Combine (DAC) by taking only multi-view images for open-set
3DOR. DAC innovatively synergizes a CLIP model with a multi-modal large
language model (MLLM) to learn generalized 3D representations, where the MLLM
is used for dual purposes. First, it describes the seen category information to
align with CLIP's training objective for adaptation during training. Second, it
provides external hints about unknown objects complementary to visual cues
during inference. To improve the synergy, we introduce an Additive-Bias
Low-Rank adaptation (AB-LoRA), which alleviates overfitting and further
enhances the generalization to unseen categories. With only multi-view images,
DAC significantly surpasses prior arts by an average of +10.01\% mAP on four
open-set 3DOR datasets. Moreover, its generalization is also validated on
image-based and cross-dataset setups. Code is available at
https://github.com/wangzhichuan123/DAC.

</details>


### [27] [VAGU & GtS: LLM-Based Benchmark and Framework for Joint Video Anomaly Grounding and Understanding](https://arxiv.org/abs/2507.21507)
*Shibo Gao,Peipei Yang,Yangyang Liu,Yi Chen,Han Zhu,Xuyao Zhang,Linlin Huang*

Main category: cs.CV

TL;DR: The paper introduces VAGU, a benchmark for Video Anomaly Detection (VAD) combining anomaly understanding and grounding, and proposes the GtS framework and JeAUG metric for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing VAD methods lack integration of anomaly understanding and grounding, and no dataset supports both tasks.

Method: Proposes VAGU benchmark with annotations for anomaly category, explanation, temporal grounding, and Video QA. Introduces GtS framework for coarse-to-fine anomaly detection and JeAUG metric for joint evaluation.

Result: Experiments confirm the effectiveness of VAGU, GtS, and JeAUG.

Conclusion: VAGU bridges the gap in VAD by unifying understanding and grounding, with GtS and JeAUG offering robust solutions.

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events in videos and
accurately determine their time intervals. Current VAD methods mainly fall into
two categories: traditional DNN-based approaches that focus on temporal
localization, and LLM-based approaches that emphasize semantic understanding.
Both anomaly understanding and grounding are essential for comprehensive video
anomaly detection and can complement each other. However, no existing model or
dataset supports both tasks simultaneously. To address this, we introduce VAGU
(Video Anomaly Grounding and Understanding), the first benchmark to integrate
both tasks. Each VAGU instance includes annotations for anomaly category,
semantic explanation, precise temporal grounding and Video QA. We also provide
multiple-choice Video QA for objective evaluation. Based on this dataset, we
propose Glance then Scrutinize (GtS), a training-free framework guided by
textual prompts. The framework first enables coarse localization of
high-probability anomalous regions, followed by detailed anomaly interpretation
and temporal boundary refinement. Additionally, we propose the JeAUG metric,
which jointly evaluates semantic interpretability and temporal precision,
overcoming the limitations of traditional metrics. Extensive experiments verify
the effectiveness of our benchmark, framework, and evaluation metric.

</details>


### [28] [Optimizing Active Learning in Vision-Language Models via Parameter-Efficient Uncertainty Calibration](https://arxiv.org/abs/2507.21521)
*Athmanarayanan Lakshmi Narayanan,Amrutha Machireddy,Ranganath Krishnan*

Main category: cs.CV

TL;DR: A novel parameter-efficient AL method with uncertainty calibration loss improves sample selection for vision-language models, outperforming complex techniques while being computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To address challenges in uncertainty estimation and efficient sampling for large-scale vision-language models in AL, minimizing labeling costs.

Method: Introduces a differentiable loss function for uncertainty calibration within AL, comparing Prompt learning and LoRA for sample selection.

Result: Matches/exceeds performance of complex feature-based sampling techniques, computationally efficient.

Conclusion: The proposed method effectively selects informative samples, with detailed analysis of Prompt learning vs. LoRA in AL.

Abstract: Active Learning (AL) has emerged as a powerful approach for minimizing
labeling costs by selectively sampling the most informative data for neural
network model development. Effective AL for large-scale vision-language models
necessitates addressing challenges in uncertainty estimation and efficient
sampling given the vast number of parameters involved. In this work, we
introduce a novel parameter-efficient learning methodology that incorporates
uncertainty calibration loss within the AL framework. We propose a
differentiable loss function that promotes uncertainty calibration for
effectively selecting fewer and most informative data samples for fine-tuning.
Through extensive experiments across several datasets and vision backbones, we
demonstrate that our solution can match and exceed the performance of complex
feature-based sampling techniques while being computationally very efficient.
Additionally, we investigate the efficacy of Prompt learning versus Low-rank
adaptation (LoRA) in sample selection, providing a detailed comparative
analysis of these methods in the context of efficient AL.

</details>


### [29] [Chain-of-Cooking:Cooking Process Visualization via Bidirectional Chain-of-Thought Guidance](https://arxiv.org/abs/2507.21529)
*Mengling Xu,Ming Tao,Bing-Kun Bao*

Main category: cs.CV

TL;DR: The paper introduces Chain-of-Cooking, a model for visualizing cooking processes by generating images for each step, addressing semantic inconsistency and contextual coherence challenges.


<details>
  <summary>Details</summary>
Motivation: Existing works focus on finished food images, lacking solutions for dynamic ingredient appearances and sequential coherence in cooking steps.

Method: Proposes Dynamic Patch Selection, Semantic Evolution Module, and Bidirectional Chain-of-Thought Guidance to ensure correct appearances and coherence.

Result: Outperforms existing methods in generating coherent and semantically consistent cooking process images.

Conclusion: Chain-of-Cooking effectively addresses challenges in cooking process visualization, validated by experiments on the CookViz dataset.

Abstract: Cooking process visualization is a promising task in the intersection of
image generation and food analysis, which aims to generate an image for each
cooking step of a recipe. However, most existing works focus on generating
images of finished foods based on the given recipes, and face two challenges to
visualize the cooking process. First, the appearance of ingredients changes
variously across cooking steps, it is difficult to generate the correct
appearances of foods that match the textual description, leading to semantic
inconsistency. Second, the current step might depend on the operations of
previous step, it is crucial to maintain the contextual coherence of images in
sequential order. In this work, we present a cooking process visualization
model, called Chain-of-Cooking. Specifically, to generate correct appearances
of ingredients, we present a Dynamic Patch Selection Module to retrieve
previously generated image patches as references, which are most related to
current textual contents. Furthermore, to enhance the coherence and keep the
rational order of generated images, we propose a Semantic Evolution Module and
a Bidirectional Chain-of-Thought (CoT) Guidance. To better utilize the
semantics of previous texts, the Semantic Evolution Module establishes the
semantical association between latent prompts and current cooking step, and
merges it with the latent features. Then the CoT Guidance updates the merged
features to guide the current cooking step remain coherent with the previous
step. Moreover, we construct a dataset named CookViz, consisting of
intermediate image-text pairs for the cooking process. Quantitative and
qualitative experiments show that our method outperforms existing methods in
generating coherent and semantic consistent cooking process.

</details>


### [30] [Suppressing Gradient Conflict for Generalizable Deepfake Detection](https://arxiv.org/abs/2507.21530)
*Ming-Hui Liu,Harry Cheng,Xin Luo,Xin-Shun Xu*

Main category: cs.CV

TL;DR: The paper introduces CS-DFD, a framework to mitigate gradient conflicts in deepfake detection, improving generalization and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection models degrade when trained on both original and synthesized data due to gradient conflicts.

Method: Proposes CS-DFD with two modules: UVS for reconciling gradient disparities and CGR for enforcing low-conflict feature embeddings.

Result: CS-DFD achieves state-of-the-art performance in in-domain accuracy and cross-domain generalization.

Conclusion: The framework effectively addresses gradient conflicts, enhancing deepfake detection robustness.

Abstract: Robust deepfake detection models must be capable of generalizing to
ever-evolving manipulation techniques beyond training data. A promising
strategy is to augment the training data with online synthesized fake images
containing broadly generalizable artifacts. However, in the context of deepfake
detection, it is surprising that jointly training on both original and online
synthesized forgeries may result in degraded performance. This contradicts the
common belief that incorporating more source-domain data should enhance
detection accuracy. Through empirical analysis, we trace this degradation to
gradient conflicts during backpropagation which force a trade-off between
source domain accuracy and target domain generalization. To overcome this
issue, we propose a Conflict-Suppressed Deepfake Detection (CS-DFD) framework
that explicitly mitigates the gradient conflict via two synergistic modules.
First, an Update Vector Search (UVS) module searches for an alternative update
vector near the initial gradient vector to reconcile the disparities of the
original and online synthesized forgeries. By further transforming the search
process into an extremum optimization problem, UVS yields the uniquely update
vector, which maximizes the simultaneous loss reductions for each data type.
Second, a Conflict Gradient Reduction (CGR) module enforces a low-conflict
feature embedding space through a novel Conflict Descent Loss. This loss
penalizes misaligned gradient directions and guides the learning of
representations with aligned, non-conflicting gradients. The synergy of UVS and
CGR alleviates gradient interference in both parameter optimization and
representation learning. Experiments on multiple deepfake benchmarks
demonstrate that CS-DFD achieves state-of-the-art performance in both in-domain
detection accuracy and cross-domain generalization.

</details>


### [31] [Sun sensor calibration algorithms: A systematic mapping and survey](https://arxiv.org/abs/2507.21541)
*Michael Herman,Olivia J. Pinon Fischer,Dimitri N. Mavris*

Main category: cs.CV

TL;DR: The paper reviews sun sensor modeling and calibration algorithms, addressing uncertainties and research gaps, and suggests future directions.


<details>
  <summary>Details</summary>
Motivation: Sun sensors are critical for spacecraft attitude determination but face complex uncertainties, necessitating advanced calibration methods.

Method: The paper systematically maps and surveys existing sun sensor modeling and calibration techniques.

Result: It provides a comprehensive analysis of methodologies, identifies research gaps, and offers recommendations.

Conclusion: The review consolidates existing work and guides future advancements in sun sensor calibration.

Abstract: Attitude sensors determine the spacecraft attitude through the sensing of an
astronomical object, field or other phenomena. The Sun and fixed stars are the
two primary astronomical sensing objects. Attitude sensors are critical
components for the survival and knowledge improvement of spacecraft. Of these,
sun sensors are the most common and important sensor for spacecraft attitude
determination. The sun sensor measures the Sun vector in spacecraft
coordinates. The sun sensor calibration process is particularly difficult due
to the complex nature of the uncertainties involved. The uncertainties are
small, difficult to observe, and vary spatio-temporally over the lifecycle of
the sensor. In addition, the sensors are affected by numerous sources of
uncertainties, including manufacturing, electrical, environmental, and
interference sources. This motivates the development of advanced calibration
algorithms to minimize uncertainty over the sensor lifecycle and improve
accuracy. Although modeling and calibration techniques for sun sensors have
been explored extensively in the literature over the past two decades, there is
currently no resource that consolidates and systematically reviews this body of
work. The present review proposes a systematic mapping of sun sensor modeling
and calibration algorithms across a breadth of sensor configurations. It
specifically provides a comprehensive survey of each methodology, along with an
analysis of research gaps and recommendations for future directions in sun
sensor modeling and calibration techniques.

</details>


### [32] [Multi-View Reconstruction with Global Context for 3D Anomaly Detection](https://arxiv.org/abs/2507.21555)
*Yihan Sun,Yuqi Cheng,Yunkang Cao,Yuxin Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: Proposes Multi-View Reconstruction (MVR) for 3D anomaly detection, enhancing global information learning by converting point clouds to multi-view images, achieving high AU-ROC scores.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack sufficient global information for high-precision 3D anomaly detection.

Method: MVR converts high-resolution point clouds into multi-view images and uses a reconstruction-based anomaly detection framework.

Result: Achieves 89.6% object-wise AU-ROC and 95.7% point-wise AU-ROC on Real3D-AD benchmark.

Conclusion: MVR effectively improves 3D anomaly detection by leveraging global information.

Abstract: 3D anomaly detection is critical in industrial quality inspection. While
existing methods achieve notable progress, their performance degrades in
high-precision 3D anomaly detection due to insufficient global information. To
address this, we propose Multi-View Reconstruction (MVR), a method that
losslessly converts high-resolution point clouds into multi-view images and
employs a reconstruction-based anomaly detection framework to enhance global
information learning. Extensive experiments demonstrate the effectiveness of
MVR, achieving 89.6\% object-wise AU-ROC and 95.7\% point-wise AU-ROC on the
Real3D-AD benchmark.

</details>


### [33] [RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors](https://arxiv.org/abs/2507.21567)
*Tianhui Cai,Yun Zhang,Zewei Zhou,Zhiyu Huang,Jiaqi Ma*

Main category: cs.CV

TL;DR: RelMap is a transformer-based framework for online HD map construction that improves accuracy by incorporating spatial relations and semantic priors, achieving top results on nuScenes and Argoverse 2 datasets.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based methods for online HD map construction often ignore spatial and semantic relationships among map elements, limiting accuracy and generalization.

Method: RelMap introduces a Class-aware Spatial Relation Prior to encode positional dependencies and a Mixture-of-Experts-based Semantic Prior for feature routing. It works with single-frame and temporal backbones.

Result: The method achieves state-of-the-art performance on nuScenes and Argoverse 2 datasets.

Conclusion: RelMap effectively addresses limitations of existing approaches by leveraging spatial and semantic relationships, enhancing online HD map construction.

Abstract: Online high-definition (HD) map construction plays an increasingly important
role in scaling autonomous driving systems. Transformer-based methods have
become prevalent in online HD map construction; however, existing approaches
often neglect the inherent spatial and semantic relationships among map
elements, which limits their accuracy and generalization. To address this, we
propose RelMap, an end-to-end framework that enhances online map construction
by incorporating spatial relations and semantic priors. We introduce a
Class-aware Spatial Relation Prior, which explicitly encodes relative
positional dependencies between map elements using a learnable class-aware
relation encoder. Additionally, we propose a Mixture-of-Experts (MoE)-based
Semantic Prior, which routes features to class-specific experts based on
predicted class probabilities, refining instance feature decoding. Our method
is compatible with both single-frame and temporal perception backbones,
achieving state-of-the-art performance on both the nuScenes and Argoverse 2
datasets.

</details>


### [34] [LinDeps: A Fine-tuning Free Post-Pruning Method to Remove Layer-Wise Linear Dependencies with Guaranteed Performance Preservation](https://arxiv.org/abs/2507.21573)
*Maxim Henry,Adrien Deliège,Anthony Cioppa,Marc Van Droogenbroeck*

Main category: cs.CV

TL;DR: LinDeps is a novel post-pruning method that improves CNN compression by analyzing linear dependencies in feature maps, enhancing performance without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The increasing size and complexity of CNNs challenge deployment on resource-constrained platforms, and existing pruning methods often ignore structural dependencies, leading to suboptimal results.

Method: LinDeps uses pivoted QR decomposition to detect and prune linearly dependent filters, followed by a signal recovery mechanism to adjust kernels without fine-tuning.

Result: Experiments on CIFAR-10 and ImageNet show LinDeps improves compression rates and preserves performance, achieving state-of-the-art results. It also excels in low-resource setups.

Conclusion: LinDeps is a versatile and effective add-on for existing or future pruning techniques, enhancing efficiency and performance.

Abstract: Convolutional Neural Networks (CNN) are widely used in many computer vision
tasks. Yet, their increasing size and complexity pose significant challenges
for efficient deployment on resource-constrained platforms. Hence, network
pruning has emerged as an effective way of reducing the size and computational
requirements of neural networks by removing redundant or unimportant
parameters. However, a fundamental challenge with pruning consists in optimally
removing redundancies without degrading performance. Most existing pruning
techniques overlook structural dependencies across feature maps within a layer,
resulting in suboptimal pruning decisions. In this work, we introduce LinDeps,
a novel post-pruning method, i.e., a pruning method that can be applied on top
of any pruning technique, which systematically identifies and removes redundant
filters via linear dependency analysis. Particularly, LinDeps applies pivoted
QR decomposition to feature maps to detect and prune linearly dependent
filters. Then, a novel signal recovery mechanism adjusts the next layer's
kernels to preserve compatibility and performance without requiring any
fine-tuning. Our experiments on CIFAR-10 and ImageNet with VGG and ResNet
backbones demonstrate that LinDeps improves compression rates of existing
pruning techniques while preserving performances, leading to a new state of the
art in CNN pruning. We also benchmark LinDeps in low-resource setups where no
retraining can be performed, which shows significant pruning improvements and
inference speedups over a state-of-the-art method. LinDeps therefore
constitutes an essential add-on for any current or future pruning technique.

</details>


### [35] [TARS: MinMax Token-Adaptive Preference Strategy for Hallucination Reduction in MLLMs](https://arxiv.org/abs/2507.21584)
*Kejia Zhang,Keda Tao,Zhiming Luo,Chang Liu,Jiasheng Tang,Huan Wang*

Main category: cs.CV

TL;DR: TARS, a token-adaptive preference strategy, improves multimodal reasoning by reducing hallucinations through min-max optimization, outperforming standard DPO and matching GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Existing DPO strategies for correcting hallucinations in MLLMs overfit to superficial cues, impairing visual grounding. TARS aims to address this by dynamically adapting preferences.

Method: TARS reformulates DPO as a min-max optimization problem, maximizing token-level distributional shifts under semantic constraints while minimizing preference loss.

Result: TARS reduces hallucination rates from 26.4% to 13.2% and cognition value from 2.5 to 0.4, outperforming standard DPO and matching GPT-4o.

Conclusion: TARS effectively mitigates hallucinations in MLLMs by preserving causal grounding and avoiding overfitting, demonstrating strong performance with minimal data.

Abstract: Multimodal large language models (MLLMs) enable vision-language reasoning,
yet often generate plausible outputs that are factually incorrect or visually
ungrounded, thereby compromising their reliability. Direct preference
optimization (DPO) is a common strategy for correcting hallucinations by
aligning model outputs with human preferences. Existing DPO strategies
typically treat hallucination-related preferences as fixed targets, relying on
static supervision signals during training. This approach tends to overfit to
superficial linguistic cues in preference data, leading to distributional
rigidity and spurious correlations that impair grounding in causally relevant
visual information. To overcome this limitation, we propose TARS, a
token-adaptive preference strategy that reformulates DPO as a min-max
optimization problem. TARS maximizes token-level distributional shifts under
semantic constraints to simulate alignment uncertainty, and simultaneously
minimizes the expected preference loss under these controlled perturbations.
This joint objective preserves causal grounding while mitigating overfitting to
preference patterns, thereby reducing hallucinations in multimodal reasoning.
We evaluate TARS on multiple hallucination benchmarks and find consistently
strong performance. Using only 4.8k preference samples and no expert feedback,
TARS reduces hallucination rates from 26.4% to 13.2% and decreases cognition
value from 2.5 to 0.4. It outperforms standard DPO and matches GPT-4o on
several key metrics.

</details>


### [36] [Emerging Trends in Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation with Image-Level Supervision](https://arxiv.org/abs/2507.21587)
*Zheyuan Zhang,Wang Zhang*

Main category: cs.CV

TL;DR: A review of weakly supervised semantic segmentation (WSSS) with image-level annotations, focusing on recent advancements, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of WSSS methods and the lack of updated surveys necessitate a comprehensive review to capture recent trends and advancements.

Method: The review synthesizes state-of-the-art techniques, categorizes methods by supervision types, and examines challenges in domain-specific applications.

Result: The paper provides an updated overview of WSSS with image-level labels, highlighting current limitations and underexplored areas.

Conclusion: The review serves as a resource for researchers familiar with WSSS, offering insights into advancements, challenges, and future research directions.

Abstract: Unlike fully supervised semantic segmentation, weakly supervised semantic
segmentation (WSSS) relies on weaker forms of supervision to perform dense
prediction tasks. Among the various types of weak supervision, WSSS with image
level annotations is considered both the most challenging and the most
practical, attracting significant research attention. Therefore, in this
review, we focus on WSSS with image level annotations. Additionally, this
review concentrates on mainstream research directions, deliberately omitting
less influential branches.
  Given the rapid development of new methods and the limitations of existing
surveys in capturing recent trends, there is a pressing need for an updated and
comprehensive review. Our goal is to fill this gap by synthesizing the latest
advancements and state-of-the-art techniques in WSSS with image level labels.
  Basically, we provide a comprehensive review of recent advancements in WSSS
with image level labels, categorizing existing methods based on the types and
levels of additional supervision involved. We also examine the challenges of
applying advanced methods to domain specific datasets in WSSS,a topic that
remains underexplored. Finally, we discuss the current challenges, evaluate the
limitations of existing approaches, and outline several promising directions
for future research. This review is intended for researchers who are already
familiar with the fundamental concepts of WSSS and are seeking to deepen their
understanding of current advances and methodological innovations.

</details>


### [37] [Locally Controlled Face Aging with Latent Diffusion Models](https://arxiv.org/abs/2507.21600)
*Lais Isabelle Alves dos Santos,Julien Despois,Thibaut Chauffier,Sileye O. Ba,Giovanni Palma*

Main category: cs.CV

TL;DR: A novel method for face aging using latent diffusion models to selectively age specific facial regions, addressing heterogeneity in aging and improving realism and control.


<details>
  <summary>Details</summary>
Motivation: Current methods treat aging as a global process, ignoring regional heterogeneity due to intrinsic and extrinsic factors.

Method: Leverages latent diffusion models to selectively age facial regions and uses a refiner for seamless blending.

Result: Achieves robust identity preservation, high-fidelity imagery, and natural aging progression.

Conclusion: The approach provides finer-grained control and more realistic, personalized aging compared to existing methods.

Abstract: We present a novel approach to face aging that addresses the limitations of
current methods which treat aging as a global, homogeneous process. Existing
techniques using GANs and diffusion models often condition generation on a
reference image and target age, neglecting that facial regions age
heterogeneously due to both intrinsic chronological factors and extrinsic
elements like sun exposure. Our method leverages latent diffusion models to
selectively age specific facial regions using local aging signs. This approach
provides significantly finer-grained control over the generation process,
enabling more realistic and personalized aging. We employ a latent diffusion
refiner to seamlessly blend these locally aged regions, ensuring a globally
consistent and natural-looking synthesis. Experimental results demonstrate that
our method effectively achieves three key criteria for successful face aging:
robust identity preservation, high-fidelity and realistic imagery, and a
natural, controllable aging progression.

</details>


### [38] [Decoupled Spatio-Temporal Consistency Learning for Self-Supervised Tracking](https://arxiv.org/abs/2507.21606)
*Yaozong Zheng,Bineng Zhong,Qihua Liang,Ning Li,Shuxiang Song*

Main category: cs.CV

TL;DR: A self-supervised tracking framework (SSTrack) eliminates box annotations by using spatio-temporal consistency and instance contrastive loss, outperforming SOTA methods by up to 25.3%.


<details>
  <summary>Details</summary>
Motivation: Manual box annotations are labor-intensive, limiting dataset scale and diversity. SSTrack aims to reduce reliance on such annotations.

Method: Uses decoupled spatio-temporal consistency training and instance contrastive loss for self-supervised learning.

Result: Achieves improvements of 25.3%, 20.4%, and 14.8% in AUC on GOT10K, LaSOT, and TrackingNet datasets.

Conclusion: SSTrack effectively learns tracking representations without box annotations, outperforming existing self-supervised methods.

Abstract: The success of visual tracking has been largely driven by datasets with
manual box annotations. However, these box annotations require tremendous human
effort, limiting the scale and diversity of existing tracking datasets. In this
work, we present a novel Self-Supervised Tracking framework named
\textbf{{\tracker}}, designed to eliminate the need of box annotations.
Specifically, a decoupled spatio-temporal consistency training framework is
proposed to learn rich target information across timestamps through global
spatial localization and local temporal association. This allows for the
simulation of appearance and motion variations of instances in real-world
scenarios. Furthermore, an instance contrastive loss is designed to learn
instance-level correspondences from a multi-view perspective, offering robust
instance supervision without additional labels. This new design paradigm
enables {\tracker} to effectively learn generic tracking representations in a
self-supervised manner, while reducing reliance on extensive box annotations.
Extensive experiments on nine benchmark datasets demonstrate that {\tracker}
surpasses \textit{SOTA} self-supervised tracking methods, achieving an
improvement of more than 25.3\%, 20.4\%, and 14.8\% in AUC (AO) score on the
GOT10K, LaSOT, TrackingNet datasets, respectively. Code:
https://github.com/GXNU-ZhongLab/SSTrack.

</details>


### [39] [Semantic Segmentation of iPS Cells: Case Study on Model Complexity in Biomedical Imaging](https://arxiv.org/abs/2507.21608)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: A DeepLabv3 model outperforms larger models like SAM2 and MedSAM2 in segmenting iPS cell colonies, showing simpler models can excel in specialized biomedical tasks.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption that larger, more generalized models always perform better, especially in tasks with subtle, low-contrast boundaries like medical image segmentation.

Method: Configured DeepLabv3 model for segmenting iPS cell colonies, comparing its performance against SAM2 and MedSAM2 without structural changes.

Result: DeepLabv3 achieved higher performance than SAM2 and MedSAM2, demonstrating that simpler models can be more effective for specialized tasks.

Conclusion: Simpler, domain-specific models like DeepLabv3 can outperform larger models in biomedical applications, offering accuracy and reliability.

Abstract: Medical image segmentation requires not only accuracy but also robustness
under challenging imaging conditions. In this study, we show that a carefully
configured DeepLabv3 model can achieve high performance in segmenting induced
pluripotent stem (iPS) cell colonies, and, under our experimental conditions,
outperforms large-scale foundation models such as SAM2 and its medical variant
MedSAM2 without structural modifications. These results suggest that, for
specialized tasks characterized by subtle, low-contrast boundaries, increased
model complexity does not necessarily translate to better performance. Our work
revisits the assumption that ever-larger and more generalized architectures are
always preferable, and provides evidence that appropriately adapted, simpler
models may offer strong accuracy and practical reliability in domain-specific
biomedical applications. We also offer an open-source implementation that
includes strategies for small datasets and domain-specific encoding, with the
aim of supporting further advances in semantic segmentation for regenerative
medicine and related fields.

</details>


### [40] [Wind Turbine Feature Detection Using Deep Learning and Synthetic Data](https://arxiv.org/abs/2507.21611)
*Arash Shahirpour,Jakob Gebler,Manuel Sanders,Tim Reuscher*

Main category: cs.CV

TL;DR: Proposes synthetic data generation for training drone-based WT blade inspection models, improving diversity and performance.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of manually labeled real-world images by generating diverse synthetic data for better training.

Method: Generate synthetic WT images with controlled variations, train YOLOv11 with modified loss function for feature detection.

Result: Achieves high performance (Pose mAP50-95 of 0.97) on real-world images unseen during training.

Conclusion: Synthetic data enhances model robustness and performance for autonomous drone-based WT inspection.

Abstract: For the autonomous drone-based inspection of wind turbine (WT) blades,
accurate detection of the WT and its key features is essential for safe drone
positioning and collision avoidance. Existing deep learning methods typically
rely on manually labeled real-world images, which limits both the quantity and
the diversity of training datasets in terms of weather conditions, lighting,
turbine types, and image complexity. In this paper, we propose a method to
generate synthetic training data that allows controlled variation of visual and
environmental factors, increasing the diversity and hence creating challenging
learning scenarios. Furthermore, we train a YOLOv11 feature detection network
solely on synthetic WT images with a modified loss function, to detect WTs and
their key features within an image. The resulting network is evaluated both
using synthetic images and a set of real-world WT images and shows promising
performance across both synthetic and real-world data, achieving a Pose
mAP50-95 of 0.97 on real images never seen during training.

</details>


### [41] [EMIT: Enhancing MLLMs for Industrial Anomaly Detection via Difficulty-Aware GRPO](https://arxiv.org/abs/2507.21619)
*Wei Guan,Jun Lan,Jian Cao,Hao Tan,Huijia Zhu,Weiqiang Wang*

Main category: cs.CV

TL;DR: EMIT enhances MLLMs for industrial anomaly detection (IAD) via difficulty-aware GRPO, improving performance by 7.77% over base models.


<details>
  <summary>Details</summary>
Motivation: MLLMs lack domain-specific adaptation for IAD, limiting their effectiveness. EMIT addresses this gap.

Method: EMIT uses GPT-generated text descriptions, soft prompts, heatmap-guided embeddings, and difficulty-aware GRPO for few-shot IAD.

Result: EMIT achieves a 7.77% average improvement over the base model on the MMAD benchmark.

Conclusion: EMIT effectively adapts MLLMs for IAD, demonstrating significant performance gains.

Abstract: Industrial anomaly detection (IAD) plays a crucial role in maintaining the
safety and reliability of manufacturing systems. While multimodal large
language models (MLLMs) show strong vision-language reasoning abilities, their
effectiveness in IAD remains limited without domain-specific adaptation. In
this work, we propose EMIT, a unified framework that enhances MLLMs for IAD via
difficulty-aware group relative policy optimization (GRPO). EMIT constructs a
multi-task IAD dataset and utilizes GPT-generated object text descriptions to
compensate for missing defective images. For few-shot anomaly detection, it
integrates a soft prompt and heatmap-guided contrastive embeddings derived from
patch-level comparisons. To better handle difficult data samples, i.e., cases
where the MLLM struggles to generate correct answers, we propose a
difficulty-aware GRPO that extends the original GRPO by incorporating a
response resampling strategy to ensure the inclusion of correct answers in the
sampled responses, as well as an advantage reweighting mechanism to strengthen
learning from such difficult data samples. Extensive experiments on the MMAD
benchmark demonstrate that EMIT significantly enhances the IAD performance of
MLLMs, achieving an average improvement of 7.77\% over the base model
(InternVL3-8B) across seven tasks.

</details>


### [42] [GuidPaint: Class-Guided Image Inpainting with Diffusion Models](https://arxiv.org/abs/2507.21627)
*Qimin Wang,Xinda Liu,Guohua Geng*

Main category: cs.CV

TL;DR: GuidPaint is a training-free, class-guided image inpainting framework using diffusion models, improving semantic consistency and visual realism without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based inpainting methods lack fine-grained control over masked regions, leading to inconsistent or implausible results.

Method: Incorporates classifier guidance into denoising for precise control, integrates stochastic and deterministic sampling for refinement.

Result: Outperforms existing context-aware inpainting methods in qualitative and quantitative evaluations.

Conclusion: GuidPaint offers a computationally efficient and effective solution for high-quality image inpainting with fine-grained control.

Abstract: In recent years, diffusion models have been widely adopted for image
inpainting tasks due to their powerful generative capabilities, achieving
impressive results. Existing multimodal inpainting methods based on diffusion
models often require architectural modifications and retraining, resulting in
high computational cost. In contrast, context-aware diffusion inpainting
methods leverage the model's inherent priors to adjust intermediate denoising
steps, enabling high-quality inpainting without additional training and
significantly reducing computation. However, these methods lack fine-grained
control over the masked regions, often leading to semantically inconsistent or
visually implausible content. To address this issue, we propose GuidPaint, a
training-free, class-guided image inpainting framework. By incorporating
classifier guidance into the denoising process, GuidPaint enables precise
control over intermediate generations within the masked areas, ensuring both
semantic consistency and visual realism. Furthermore, it integrates stochastic
and deterministic sampling, allowing users to select preferred intermediate
results and deterministically refine them. Experimental results demonstrate
that GuidPaint achieves clear improvements over existing context-aware
inpainting methods in both qualitative and quantitative evaluations.

</details>


### [43] [The Evolution of Video Anomaly Detection: A Unified Framework from DNN to MLLM](https://arxiv.org/abs/2507.21649)
*Shibo Gao,Peipei Yang,Haiyang Guo,Yangyang Liu,Yi Chen,Shuai Li,Han Zhu,Jian Xu,Xu-Yao Zhang,Linlin Huang*

Main category: cs.CV

TL;DR: A survey on video anomaly detection (VAD) leveraging multi-modal large language models (MLLMs) and large language models (LLMs), discussing advancements, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The rapid development of MLLMs and LLMs has transformed VAD, creating a need for a systematic review of these advancements and their impact on the field.

Method: The paper provides a comprehensive survey, analyzing VAD methods based on MLLMs and LLMs, proposing a unified framework, and comparing strengths and weaknesses.

Result: The survey highlights new VAD paradigms, constructs a classification system, and identifies key challenges and future research directions.

Conclusion: The paper offers guidance for the VAD community by summarizing advancements and outlining future opportunities in the era of large models.

Abstract: Video anomaly detection (VAD) aims to identify and ground anomalous behaviors
or events in videos, serving as a core technology in the fields of intelligent
surveillance and public safety. With the advancement of deep learning, the
continuous evolution of deep model architectures has driven innovation in VAD
methodologies, significantly enhancing feature representation and scene
adaptability, thereby improving algorithm generalization and expanding
application boundaries. More importantly, the rapid development of multi-modal
large language (MLLMs) and large language models (LLMs) has introduced new
opportunities and challenges to the VAD field. Under the support of MLLMs and
LLMs, VAD has undergone significant transformations in terms of data
annotation, input modalities, model architectures, and task objectives. The
surge in publications and the evolution of tasks have created an urgent need
for systematic reviews of recent advancements. This paper presents the first
comprehensive survey analyzing VAD methods based on MLLMs and LLMs, providing
an in-depth discussion of the changes occurring in the VAD field in the era of
large models and their underlying causes. Additionally, this paper proposes a
unified framework that encompasses both deep neural network (DNN)-based and
LLM-based VAD methods, offering a thorough analysis of the new VAD paradigms
empowered by LLMs, constructing a classification system, and comparing their
strengths and weaknesses. Building on this foundation, this paper focuses on
current VAD methods based on MLLMs/LLMs. Finally, based on the trajectory of
technological advancements and existing bottlenecks, this paper distills key
challenges and outlines future research directions, offering guidance for the
VAD community.

</details>


### [44] [Automated Detection of Antarctic Benthic Organisms in High-Resolution In Situ Imagery to Aid Biodiversity Monitoring](https://arxiv.org/abs/2507.21665)
*Cameron Trotter,Huw Griffiths,Tasnuva Ming Khan,Rowan Whittle*

Main category: cs.CV

TL;DR: A computer vision framework for monitoring Antarctic benthic biodiversity using high-resolution imagery, addressing challenges like limited annotated data and complex seafloor structures.


<details>
  <summary>Details</summary>
Motivation: Understanding ecological changes in Antarctica due to climate-driven pressures requires efficient biodiversity monitoring, which is hindered by laborious manual annotation of imagery.

Method: A tailored object detection framework combining resolution-preserving patching, spatial data augmentation, fine-tuning, and postprocessing via Slicing Aided Hyper Inference.

Result: Strong performance in detecting medium and large organisms across 25 morphotypes, though detection of small and rare taxa remains challenging.

Conclusion: The framework offers a scalable solution for machine-assisted benthic biodiversity monitoring, with potential for future improvements.

Abstract: Monitoring benthic biodiversity in Antarctica is vital for understanding
ecological change in response to climate-driven pressures. This work is
typically performed using high-resolution imagery captured in situ, though
manual annotation of such data remains laborious and specialised, impeding
large-scale analysis. We present a tailored object detection framework for
identifying and classifying Antarctic benthic organisms in high-resolution
towed camera imagery, alongside the first public computer vision dataset for
benthic biodiversity monitoring in the Weddell Sea. Our approach addresses key
challenges associated with marine ecological imagery, including limited
annotated data, variable object sizes, and complex seafloor structure. The
proposed framework combines resolution-preserving patching, spatial data
augmentation, fine-tuning, and postprocessing via Slicing Aided Hyper
Inference. We benchmark multiple object detection architectures and demonstrate
strong performance in detecting medium and large organisms across 25
fine-grained morphotypes, significantly more than other works in this area.
Detection of small and rare taxa remains a challenge, reflecting limitations in
current detection architectures. Our framework provides a scalable foundation
for future machine-assisted in situ benthic biodiversity monitoring research.

</details>


### [45] [APT: Improving Diffusion Models for High Resolution Image Generation with Adaptive Path Tracing](https://arxiv.org/abs/2507.21690)
*Sangmin Han,Jinho Jeong,Jinwoo Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: APT addresses patch-level issues in LDMs for high-resolution image generation, improving detail and speed.


<details>
  <summary>Details</summary>
Motivation: Fixed-resolution training in LDMs limits high-resolution scalability; patch-based methods introduce distribution shift and monotonicity issues.

Method: APT uses Statistical Matching and Scale-aware Scheduling to resolve patch-level issues and optimize denoising.

Result: APT produces clearer details and faster sampling with minimal quality loss.

Conclusion: APT offers a practical solution for high-resolution image generation with improved efficiency and quality.

Abstract: Latent Diffusion Models (LDMs) are generally trained at fixed resolutions,
limiting their capability when scaling up to high-resolution images. While
training-based approaches address this limitation by training on
high-resolution datasets, they require large amounts of data and considerable
computational resources, making them less practical. Consequently,
training-free methods, particularly patch-based approaches, have become a
popular alternative. These methods divide an image into patches and fuse the
denoising paths of each patch, showing strong performance on high-resolution
generation. However, we observe two critical issues for patch-based approaches,
which we call ``patch-level distribution shift" and ``increased patch
monotonicity." To address these issues, we propose Adaptive Path Tracing (APT),
a framework that combines Statistical Matching to ensure patch distributions
remain consistent in upsampled latents and Scale-aware Scheduling to deal with
the patch monotonicity. As a result, APT produces clearer and more refined
details in high-resolution images. In addition, APT enables a shortcut
denoising process, resulting in faster sampling with minimal quality
degradation. Our experimental results confirm that APT produces more detailed
outputs with improved inference speed, providing a practical approach to
high-resolution image generation.

</details>


### [46] [Semantics versus Identity: A Divide-and-Conquer Approach towards Adjustable Medical Image De-Identification](https://arxiv.org/abs/2507.21703)
*Yuan Tian,Shuo Wang,Rongzhao Zhang,Zijian Chen,Yankai Jiang,Chunyi Li,Xiangyang Zhu,Fang Yan,Qiang Hu,XiaoSong Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: A framework for medical image de-identification balances privacy and medical semantics by blocking identity-related regions and compensating with pre-trained models, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of flexible and semantics-preserving de-identification techniques in medical imaging to mitigate privacy risks.

Method: A two-step framework: Identity-Blocking for adjustable privacy levels and Medical-Semantics-Compensation using pre-trained models, with feature decoupling to remove residual identity information.

Result: Outperforms existing methods across seven datasets and three downstream tasks, achieving state-of-the-art performance.

Conclusion: The proposed framework effectively balances privacy and medical semantics, offering a flexible and superior solution for medical image de-identification.

Abstract: Medical imaging has significantly advanced computer-aided diagnosis, yet its
re-identification (ReID) risks raise critical privacy concerns, calling for
de-identification (DeID) techniques. Unfortunately, existing DeID methods
neither particularly preserve medical semantics, nor are flexibly adjustable
towards different privacy levels. To address these issues, we propose a
divide-and-conquer framework comprising two steps: (1) Identity-Blocking, which
blocks varying proportions of identity-related regions, to achieve different
privacy levels; and (2) Medical-Semantics-Compensation, which leverages
pre-trained Medical Foundation Models (MFMs) to extract medical semantic
features to compensate the blocked regions. Moreover, recognizing that features
from MFMs may still contain residual identity information, we introduce a
Minimum Description Length principle-based feature decoupling strategy, to
effectively decouple and discard such identity components. Extensive
evaluations against existing approaches across seven datasets and three
downstream tasks, demonstrates our state-of-the-art performance.

</details>


### [47] [Impact of Underwater Image Enhancement on Feature Matching](https://arxiv.org/abs/2507.21715)
*Jason M. Summers,Mark W. Jones*

Main category: cs.CV

TL;DR: The paper proposes local matching stability and furthest matchable frame as metrics for evaluating underwater image enhancement, focusing on its impact on frame-matching and SLAM performance.


<details>
  <summary>Details</summary>
Motivation: Underwater image enhancement is crucial for tasks like path detection and autonomous navigation, but existing methods lack robust evaluation in real-world scenarios.

Method: A novel evaluation framework is introduced, incorporating metric-based analysis and a practical matching strategy to assess enhancement techniques.

Result: The framework identifies strengths and limitations of current methods and demonstrates its relevance through improved SLAM performance.

Conclusion: The proposed framework provides a context-aware benchmark for comparing underwater image enhancement methods, enhancing their real-world applicability.

Abstract: We introduce local matching stability and furthest matchable frame as
quantitative measures for evaluating the success of underwater image
enhancement. This enhancement process addresses visual degradation caused by
light absorption, scattering, marine growth, and debris. Enhanced imagery plays
a critical role in downstream tasks such as path detection and autonomous
navigation for underwater vehicles, relying on robust feature extraction and
frame matching. To assess the impact of enhancement techniques on
frame-matching performance, we propose a novel evaluation framework tailored to
underwater environments. Through metric-based analysis, we identify strengths
and limitations of existing approaches and pinpoint gaps in their assessment of
real-world applicability. By incorporating a practical matching strategy, our
framework offers a robust, context-aware benchmark for comparing enhancement
methods. Finally, we demonstrate how visual improvements affect the performance
of a complete real-world algorithm -- Simultaneous Localization and Mapping
(SLAM) -- reinforcing the framework's relevance to operational underwater
scenarios.

</details>


### [48] [Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations](https://arxiv.org/abs/2507.21723)
*Nils Hütten,Florian Hölken,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: The paper investigates the roles of internal components in detection transformer models (DETR, DDETR, DINO) using ablation studies, revealing model-specific resilience patterns and structural redundancies.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and efficiency in Explainable AI by understanding the distinct roles of internal components in detection transformers.

Method: Systematic ablation of key components (query embeddings, MHSA, MHCA) in DETR, DDETR, and DINO, evaluated on COCO dataset using gIoU and F1-score.

Result: DETR is sensitive to encoder MHSA and decoder MHCA ablations, DDETR is robust due to multi-scale deformable attention, and DINO is most resilient due to its update rule. Structural redundancies were found in DDETR and DINO.

Conclusion: The study clarifies component contributions, offering insights for optimizing transparency and efficiency in detection transformers, with potential for model simplification.

Abstract: In recent years, Explainable AI has gained traction as an approach to
enhancing model interpretability and transparency, particularly in complex
models such as detection transformers. Despite rapid advancements, a
substantial research gap remains in understanding the distinct roles of
internal components - knowledge that is essential for improving transparency
and efficiency. Inspired by neuroscientific ablation studies, which investigate
the functions of brain regions through selective impairment, we systematically
analyze the impact of ablating key components in three state-of-the-art
detection transformer models: Detection transformer (DETR), deformable
detection transformer (DDETR), and DETR with improved denoising anchor boxes
(DINO). The ablations target query embeddings, encoder and decoder multi-head
self-attentions (MHSA) as well as decoder multi-head cross-attention (MHCA)
layers. We evaluate the effects of these ablations on the performance metrics
gIoU and F1-score, quantifying effects on both the classification and
regression sub-tasks on the COCO dataset. To facilitate reproducibility and
future research, we publicly release the DeepDissect library. Our findings
reveal model-specific resilience patterns: while DETR is particularly sensitive
to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable
attention enhances robustness, and DINO exhibits the greatest resilience due to
its look-forward twice update rule, which helps distributing knowledge across
blocks. These insights also expose structural redundancies, particularly in
DDETR's and DINO's decoder MHCA layers, highlighting opportunities for model
simplification without sacrificing performance. This study advances XAI for
DETRs by clarifying the contributions of internal components to model
performance, offering insights to optimize and improve transparency and
efficiency in critical applications.

</details>


### [49] [SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking](https://arxiv.org/abs/2507.21732)
*Qianxiong Xu,Lanyun Zhu,Chenxi Liu,Guosheng Lin,Cheng Long,Ziyue Li,Rui Zhao*

Main category: cs.CV

TL;DR: SAMITE improves VOT by addressing occlusion and distraction issues using a Prototypical Memory Bank and Positional Prompt Generator, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing VOT methods lack temporal dependency handling and generalizability, and struggle with occlusions and distractions.

Method: SAMITE enhances SAM2 with a Prototypical Memory Bank to filter inaccurate frames and a Positional Prompt Generator for explicit positional clues.

Result: SAMITE achieves superior performance on six benchmarks.

Conclusion: SAMITE effectively tackles VOT challenges, offering better accuracy and error interception.

Abstract: Visual Object Tracking (VOT) is widely used in applications like autonomous
driving to continuously track targets in videos. Existing methods can be
roughly categorized into template matching and autoregressive methods, where
the former usually neglects the temporal dependencies across frames and the
latter tends to get biased towards the object categories during training,
showing weak generalizability to unseen classes. To address these issues, some
methods propose to adapt the video foundation model SAM2 for VOT, where the
tracking results of each frame would be encoded as memory for conditioning the
rest of frames in an autoregressive manner. Nevertheless, existing methods fail
to overcome the challenges of object occlusions and distractions, and do not
have any measures to intercept the propagation of tracking errors. To tackle
them, we present a SAMITE model, built upon SAM2 with additional modules,
including: (1) Prototypical Memory Bank: We propose to quantify the
feature-wise and position-wise correctness of each frame's tracking results,
and select the best frames to condition subsequent frames. As the features of
occluded and distracting objects are feature-wise and position-wise inaccurate,
their scores would naturally be lower and thus can be filtered to intercept
error propagation; (2) Positional Prompt Generator: To further reduce the
impacts of distractors, we propose to generate positional mask prompts to
provide explicit positional clues for the target, leading to more accurate
tracking. Extensive experiments have been conducted on six benchmarks, showing
the superiority of SAMITE. The code is available at
https://github.com/Sam1224/SAMITE.

</details>


### [50] [MAGE: Multimodal Alignment and Generation Enhancement via Bridging Visual and Semantic Spaces](https://arxiv.org/abs/2507.21741)
*Shaojun E,Yuchen Yang,Jiaheng Wu,Yan Zhang,Tiejun Zhao,Ziyan Chen*

Main category: cs.CV

TL;DR: MAGE is a novel multimodal framework addressing spatial and semantic losses in visual data encoding by aligning vision and text spaces, improving performance in large multimodal models.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from vector gaps and semantic disparities, leading to information loss during propagation in multimodal learning.

Method: MAGE uses an Intelligent Alignment Network (IAN) for dimensional and semantic alignment, combined with a training strategy of cross-entropy and mean squared error. It also employs a fine-tuning dataset for multimodal tool-calling instructions.

Result: MAGE outperforms similar works on benchmarks like MME, MMBench, and SEED.

Conclusion: MAGE effectively bridges semantic gaps in multimodal learning, enhancing model performance and capabilities.

Abstract: In the latest advancements in multimodal learning, effectively addressing the
spatial and semantic losses of visual data after encoding remains a critical
challenge. This is because the performance of large multimodal models is
positively correlated with the coupling between visual encoders and large
language models. Existing approaches often face issues such as vector gaps or
semantic disparities, resulting in information loss during the propagation
process. To address these issues, we propose MAGE (Multimodal Alignment and
Generation Enhancement), a novel framework that bridges the semantic spaces of
vision and text through an innovative alignment mechanism. By introducing the
Intelligent Alignment Network (IAN), MAGE achieves dimensional and semantic
alignment. To reduce the gap between synonymous heterogeneous data, we employ a
training strategy that combines cross-entropy and mean squared error,
significantly enhancing the alignment effect. Moreover, to enhance MAGE's
"Any-to-Any" capability, we developed a fine-tuning dataset for multimodal
tool-calling instructions to expand the model's output capability boundaries.
Finally, our proposed multimodal large model architecture, MAGE, achieved
significantly better performance compared to similar works across various
evaluation benchmarks, including MME, MMBench, and SEED. Complete code and
appendix are available at: https://github.com/GTCOM-NLP/MAGE.

</details>


### [51] [Adversarial Reconstruction Feedback for Robust Fine-grained Generalization](https://arxiv.org/abs/2507.21742)
*Shijie Wang,Jian Shi,Haojie Li*

Main category: cs.CV

TL;DR: AdvRF is a novel adversarial reconstruction feedback framework for fine-grained image retrieval (FGIR) that learns category-agnostic discrepancy representations to improve generalization to unseen categories.


<details>
  <summary>Details</summary>
Motivation: Existing FGIR methods rely on predefined categories, introducing category-specific semantics that hinder generalization. AdvRF addresses this by learning category-agnostic representations.

Method: AdvRF synergizes category-aware discrepancy localization from retrieval models with category-agnostic feature learning from reconstruction models, using adversarial feedback to refine representations.

Result: AdvRF achieves strong performance on fine-grained and coarse-grained datasets, demonstrating improved generalization.

Conclusion: AdvRF effectively learns category-agnostic representations, enhancing FGIR performance and generalization.

Abstract: Existing fine-grained image retrieval (FGIR) methods predominantly rely on
supervision from predefined categories to learn discriminative representations
for retrieving fine-grained objects. However, they inadvertently introduce
category-specific semantics into the retrieval representation, creating
semantic dependencies on predefined classes that critically hinder
generalization to unseen categories. To tackle this, we propose AdvRF, a novel
adversarial reconstruction feedback framework aimed at learning
category-agnostic discrepancy representations. Specifically, AdvRF reformulates
FGIR as a visual discrepancy reconstruction task via synergizing category-aware
discrepancy localization from retrieval models with category-agnostic feature
learning from reconstruction models. The reconstruction model exposes residual
discrepancies overlooked by the retrieval model, forcing it to improve
localization accuracy, while the refined signals from the retrieval model guide
the reconstruction model to improve its reconstruction ability. Consequently,
the retrieval model localizes visual differences, while the reconstruction
model encodes these differences into category-agnostic representations. This
representation is then transferred to the retrieval model through knowledge
distillation for efficient deployment. Quantitative and qualitative evaluations
demonstrate that our AdvRF achieves impressive performance on both widely-used
fine-grained and coarse-grained datasets.

</details>


### [52] [Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards](https://arxiv.org/abs/2507.21745)
*Aybora Koksal,A. Aydin Alatan*

Main category: cs.CV

TL;DR: A few-shot reinforcement learning framework (RLVR) for satellite imagery eliminates the need for caption supervision, using lightweight rewards. It achieves strong performance with minimal data, even matching models trained on thousands of samples.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like remote sensing lack annotated data, making large models impractical. RLVR addresses this by reducing reliance on costly supervision.

Method: Adapts 1-shot RLVR from language to vision-language models, using policy-gradient optimization with rule-based rewards. Evaluated on classification, VQA, and grounding tasks.

Result: Achieves substantial improvements with just one example; scales to 128 examples, matching/exceeding models trained on thousands. Shows robust generalization despite minor overfitting.

Conclusion: RLVR enables cost-effective, data-efficient domain-specialist model development, offering a practical solution for data-scarce fields.

Abstract: Recent advances in large language and vision-language models have enabled
strong reasoning capabilities, yet they remain impractical for specialized
domains like remote sensing, where annotated data is scarce and expensive. We
present the first few-shot reinforcement learning with verifiable reward (RLVR)
framework for satellite imagery that eliminates the need for caption
supervision--relying solely on lightweight, rule-based binary or IoU-based
rewards. Adapting the "1-shot RLVR" paradigm from language models to
vision-language models, we employ policy-gradient optimization with as few as
one curated example to align model outputs for satellite reasoning tasks.
Comprehensive experiments across multiple remote sensing benchmarks--including
classification, visual question answering, and grounding--show that even a
single example yields substantial improvements over the base model. Scaling to
128 examples matches or exceeds models trained on thousands of annotated
samples. While the extreme one-shot setting can induce mild, task-specific
overfitting, our approach consistently demonstrates robust generalization and
efficiency across diverse tasks. Further, we find that prompt design and loss
weighting significantly influence training stability and final accuracy. Our
method enables cost-effective and data-efficient development of
domain-specialist vision-language reasoning models, offering a pragmatic recipe
for data-scarce fields: start from a compact VLM, curate a handful of
reward-checkable cases, and train via RLVR.

</details>


### [53] [LiteFat: Lightweight Spatio-Temporal Graph Learning for Real-Time Driver Fatigue Detection](https://arxiv.org/abs/2507.21756)
*Jing Ren,Suyu Ma,Hong Jia,Xiwei Xu,Ivan Lee,Haytham Fayek,Xiaodong Li,Feng Xia*

Main category: cs.CV

TL;DR: LiteFat is a lightweight spatio-temporal graph learning model for efficient driver fatigue detection, reducing computational demands while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Drowsy driving causes many accidents, but existing deep learning solutions are too resource-intensive for embedded devices.

Method: Converts video data into spatio-temporal graphs using facial landmarks, employs MobileNet for feature extraction, and uses a lightweight graph neural network for fatigue detection.

Result: LiteFat achieves competitive accuracy with lower computational complexity and latency compared to state-of-the-art methods.

Conclusion: Enables real-time, resource-efficient fatigue detection for embedded devices like intelligent vehicles.

Abstract: Detecting driver fatigue is critical for road safety, as drowsy driving
remains a leading cause of traffic accidents. Many existing solutions rely on
computationally demanding deep learning models, which result in high latency
and are unsuitable for embedded robotic devices with limited resources (such as
intelligent vehicles/cars) where rapid detection is necessary to prevent
accidents. This paper introduces LiteFat, a lightweight spatio-temporal graph
learning model designed to detect driver fatigue efficiently while maintaining
high accuracy and low computational demands. LiteFat involves converting
streaming video data into spatio-temporal graphs (STG) using facial landmark
detection, which focuses on key motion patterns and reduces unnecessary data
processing. LiteFat uses MobileNet to extract facial features and create a
feature matrix for the STG. A lightweight spatio-temporal graph neural network
is then employed to identify signs of fatigue with minimal processing and low
latency. Experimental results on benchmark datasets show that LiteFat performs
competitively while significantly decreasing computational complexity and
latency as compared to current state-of-the-art methods. This work enables the
development of real-time, resource-efficient human fatigue detection systems
that can be implemented upon embedded robotic devices.

</details>


### [54] [MOR-VIT: Efficient Vision Transformer with Mixture-of-Recursions](https://arxiv.org/abs/2507.21761)
*YiZhou Li*

Main category: cs.CV

TL;DR: MoR-ViT introduces a token-level dynamic recursion mechanism for efficient vision transformers, reducing parameters by 70% and accelerating inference by 2.5x while maintaining state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard ViTs suffer from parameter redundancy and high computational costs, limiting practical deployment. Existing efficient ViTs lack adaptability in computational depth.

Method: MoR-ViT incorporates a token-level dynamic recursion mechanism (Mixture-of-Recursions) to allow adaptive processing depth for each token.

Result: Achieves state-of-the-art accuracy with 70% parameter reduction and 2.5x faster inference, outperforming DynamicViT and TinyViT.

Conclusion: Dynamic recursion is an effective strategy for efficient ViTs, enabling scalable and deployable models for real-world applications.

Abstract: Vision Transformers (ViTs) have achieved remarkable success in image
recognition, yet standard ViT architectures are hampered by substantial
parameter redundancy and high computational cost, limiting their practical
deployment. While recent efforts on efficient ViTs primarily focus on static
model compression or token-level sparsification, they remain constrained by
fixed computational depth for all tokens. In this work, we present MoR-ViT, a
novel vision transformer framework that, for the first time, incorporates a
token-level dynamic recursion mechanism inspired by the Mixture-of-Recursions
(MoR) paradigm. This approach enables each token to adaptively determine its
processing depth, yielding a flexible and input-dependent allocation of
computational resources. Extensive experiments on ImageNet-1K and transfer
benchmarks demonstrate that MoR-ViT not only achieves state-of-the-art accuracy
with up to 70% parameter reduction and 2.5x inference acceleration, but also
outperforms leading efficient ViT baselines such as DynamicViT and TinyViT
under comparable conditions. These results establish dynamic recursion as an
effective strategy for efficient vision transformers and open new avenues for
scalable and deployable deep learning models in real-world scenarios.

</details>


### [55] [AU-LLM: Micro-Expression Action Unit Detection via Enhanced LLM-Based Feature Fusion](https://arxiv.org/abs/2507.21778)
*Zhishu Liu,Kaishen Yuan,Bo Zhao,Yong Xu,Zitong Yu*

Main category: cs.CV

TL;DR: AU-LLM is a novel framework using LLMs for detecting micro-expression AUs, addressing data scarcity and vision-language gaps with an Enhanced Fusion Projector (EFP). It achieves state-of-the-art results on CASME II and SAMM datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-expression AU detection is challenging due to subtle intensities and data scarcity. LLMs' potential in this domain is unexplored.

Method: AU-LLM integrates a 3D-CNN backbone with an EFP (MLP-based) to fuse mid-level and high-level visual features into a compact token for LLM reasoning.

Result: AU-LLM sets a new state-of-the-art on CASME II and SAMM datasets, demonstrating robustness in LOSO and cross-domain evaluations.

Conclusion: The framework validates LLMs' potential for micro-expression analysis, offering a robust solution for fine-grained AU detection.

Abstract: The detection of micro-expression Action Units (AUs) is a formidable
challenge in affective computing, pivotal for decoding subtle, involuntary
human emotions. While Large Language Models (LLMs) demonstrate profound
reasoning abilities, their application to the fine-grained, low-intensity
domain of micro-expression AU detection remains unexplored. This paper pioneers
this direction by introducing \textbf{AU-LLM}, a novel framework that for the
first time uses LLM to detect AUs in micro-expression datasets with subtle
intensities and the scarcity of data. We specifically address the critical
vision-language semantic gap, the \textbf{Enhanced Fusion Projector (EFP)}. The
EFP employs a Multi-Layer Perceptron (MLP) to intelligently fuse mid-level
(local texture) and high-level (global semantics) visual features from a
specialized 3D-CNN backbone into a single, information-dense token. This
compact representation effectively empowers the LLM to perform nuanced
reasoning over subtle facial muscle movements.Through extensive evaluations on
the benchmark CASME II and SAMM datasets, including stringent
Leave-One-Subject-Out (LOSO) and cross-domain protocols, AU-LLM establishes a
new state-of-the-art, validating the significant potential and robustness of
LLM-based reasoning for micro-expression analysis. The codes are available at
https://github.com/ZS-liu-JLU/AU-LLMs.

</details>


### [56] [MSGCoOp: Multiple Semantic-Guided Context Optimization for Few-Shot Learning](https://arxiv.org/abs/2507.21786)
*Zhaolong Wang,Tongfeng Sun,Mingzheng Du,Yachao Huang*

Main category: cs.CV

TL;DR: MSGCoOp enhances few-shot generalization in VLMs using parallel learnable context vectors and semantic guidance from LLMs, outperforming baselines with improved robustness.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods struggle with generalization to novel classes due to overfitting and computational inefficiency.

Method: MSGCoOp uses an ensemble of parallel learnable context vectors, semantic guidance from LLMs, and a diversity regularization loss.

Result: Achieves 1.10% average harmonic mean improvement over KgCoOp and shows robustness in cross-domain tasks.

Conclusion: MSGCoOp effectively balances generalization and computational efficiency, advancing few-shot learning in VLMs.

Abstract: Vision-language pre-trained models (VLMs) such as CLIP have demonstrated
remarkable zero-shot generalization, and prompt learning has emerged as an
efficient alternative to full fine-tuning. However, existing methods often
struggle with generalization to novel classes, a phenomenon attributed to
overfitting on seen classes and forgetting general knowledge. Furthermore,
recent approaches that improve generalization often introduce complex
architectures or heavy computational overhead. In this paper, we propose a
Multiple Semantic-Guided Context Optimization (MSGCoOp) framework to enhance
few-shot generalization while maintaining computational efficiency. Our
approach leverages an ensemble of parallel learnable context vectors to capture
diverse semantic aspects. To enrich these prompts, we introduce a semantic
guidance mechanism that aligns them with comprehensive class descriptions
automatically generated by a Large Language Model (LLM). Furthermore, a
diversity regularization loss encourages the prompts to learn complementary and
orthogonal features, preventing them from collapsing into redundant
representations. Extensive experiments on 11 benchmark datasets show that
MSGCoOp significantly improves performance on base-to-novel generalization,
achieving an average harmonic mean improvement of 1.10\% over the strong KgCoOp
baseline. Our method also demonstrates enhanced robustness in cross-domain
generalization tasks. Our code is avaliable at:
\href{https://github.com/Rain-Bus/MSGCoOp}{https://github.com/Rain-Bus/MSGCoOp}.

</details>


### [57] [Distribution-Based Masked Medical Vision-Language Model Using Structured Reports](https://arxiv.org/abs/2507.21794)
*Shreyank N Gowda,Ruichi Zhang,Xiao Gu,Ying Weng,Lu Yang*

Main category: cs.CV

TL;DR: An uncertainty-aware medical image-text pre-training model is introduced to improve generalization in medical image analysis by leveraging structured text reports and modeling ambiguity.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with medical data variability and ambiguity, limiting their ability to capture nuanced clinical information.

Method: Uses structured text reports (generated by an LLM) to augment image data, focusing on Chest X-Rays. Models inter- and intra-modal uncertainty.

Result: Achieves state-of-the-art performance on multiple downstream tasks by improving representations.

Conclusion: The framework effectively captures clinical ambiguity, enhancing medical image-text pre-training.

Abstract: Medical image-language pre-training aims to align medical images with
clinically relevant text to improve model performance on various downstream
tasks. However, existing models often struggle with the variability and
ambiguity inherent in medical data, limiting their ability to capture nuanced
clinical information and uncertainty. This work introduces an uncertainty-aware
medical image-text pre-training model that enhances generalization capabilities
in medical image analysis. Building on previous methods and focusing on Chest
X-Rays, our approach utilizes structured text reports generated by a large
language model (LLM) to augment image data with clinically relevant context.
These reports begin with a definition of the disease, followed by the
`appearance' section to highlight critical regions of interest, and finally
`observations' and `verdicts' that ground model predictions in clinical
semantics. By modeling both inter- and intra-modal uncertainty, our framework
captures the inherent ambiguity in medical images and text, yielding improved
representations and performance on downstream tasks. Our model demonstrates
significant advances in medical image-text pre-training, obtaining
state-of-the-art performance on multiple downstream tasks.

</details>


### [58] [HunyuanWorld 1.0: Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels](https://arxiv.org/abs/2507.21809)
*HunyuanWorld Team,Zhenwei Wang,Yuhao Liu,Junta Wu,Zixiao Gu,Haoyuan Wang,Xuhui Zuo,Tianyu Huang,Wenhuan Li,Sheng Zhang,Yihang Lian,Yulin Tsai,Lifu Wang,Sicong Liu,Puhua Jiang,Xianghui Yang,Dongyuan Guo,Yixuan Tang,Xinyue Mao,Jiaao Yu,Junlin Yu,Jihong Zhang,Meng Chen,Liang Dong,Yiwen Jia,Chao Zhang,Yonghao Tan,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Minghui Chen,Zhan Li,Wangchen Qin,Lei Wang,Yifu Sun,Lin Niu,Xiang Yuan,Xiaofeng Yang,Yingping He,Jie Xiao,Yangyu Tao,Jianchen Zhu,Jinbao Xue,Kai Liu,Chongqing Zhao,Xinming Wu,Tian Liu,Peng Chen,Di Wang,Yuhong Liu,Linus,Jie Jiang,Tengfei Wang,Chunchao Guo*

Main category: cs.CV

TL;DR: HunyuanWorld 1.0 is a framework for generating immersive, explorable, and interactive 3D scenes from text or images, combining video-based diversity with 3D consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods either lack 3D consistency (video-based) or struggle with data and memory inefficiency (3D-based). HunyuanWorld aims to bridge this gap.

Method: Uses a semantically layered 3D mesh representation with panoramic world proxies for decomposition and reconstruction. Features include 360° immersion, mesh export, and disentangled object representations.

Result: Achieves state-of-the-art performance in generating coherent, explorable, and interactive 3D worlds.

Conclusion: HunyuanWorld enables versatile applications in VR, simulation, gaming, and content creation.

Abstract: Creating immersive and playable 3D worlds from texts or images remains a
fundamental challenge in computer vision and graphics. Existing world
generation approaches typically fall into two categories: video-based methods
that offer rich diversity but lack 3D consistency and rendering efficiency, and
3D-based methods that provide geometric consistency but struggle with limited
training data and memory-inefficient representations. To address these
limitations, we present HunyuanWorld 1.0, a novel framework that combines the
best of both worlds for generating immersive, explorable, and interactive 3D
scenes from text and image conditions. Our approach features three key
advantages: 1) 360{\deg} immersive experiences via panoramic world proxies; 2)
mesh export capabilities for seamless compatibility with existing computer
graphics pipelines; 3) disentangled object representations for augmented
interactivity. The core of our framework is a semantically layered 3D mesh
representation that leverages panoramic images as 360{\deg} world proxies for
semantic-aware world decomposition and reconstruction, enabling the generation
of diverse 3D worlds. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in generating coherent, explorable, and
interactive 3D worlds while enabling versatile applications in virtual reality,
physical simulation, game development, and interactive content creation.

</details>


### [59] [Anyone Can Jailbreak: Prompt-Based Attacks on LLMs and T2Is](https://arxiv.org/abs/2507.21820)
*Ahmed B Mustafa,Zihan Ye,Yang Lu,Michael P Pound,Shreyank N Gowda*

Main category: cs.CV

TL;DR: The paper investigates how non-experts bypass safety mechanisms in LLMs and T2I models using cleverly worded prompts, proposing a taxonomy of jailbreak strategies and highlighting the need for context-aware defenses.


<details>
  <summary>Details</summary>
Motivation: To understand how everyday users exploit vulnerabilities in LLMs and T2I systems through low-effort, high-impact jailbreak techniques, despite existing safety measures.

Method: A systems-style investigation involving empirical case studies across popular APIs, identifying techniques like multi-turn narrative escalation and lexical camouflage.

Result: Reveals that all stages of moderation pipelines can be bypassed using accessible strategies, emphasizing the vulnerability of current systems.

Conclusion: Urges the development of context-aware defenses to address the reproducibility of jailbreaks in real-world scenarios.

Abstract: Despite significant advancements in alignment and content moderation, large
language models (LLMs) and text-to-image (T2I) systems remain vulnerable to
prompt-based attacks known as jailbreaks. Unlike traditional adversarial
examples requiring expert knowledge, many of today's jailbreaks are low-effort,
high-impact crafted by everyday users with nothing more than cleverly worded
prompts. This paper presents a systems-style investigation into how non-experts
reliably circumvent safety mechanisms through techniques such as multi-turn
narrative escalation, lexical camouflage, implication chaining, fictional
impersonation, and subtle semantic edits. We propose a unified taxonomy of
prompt-level jailbreak strategies spanning both text-output and T2I models,
grounded in empirical case studies across popular APIs. Our analysis reveals
that every stage of the moderation pipeline, from input filtering to output
validation, can be bypassed with accessible strategies. We conclude by
highlighting the urgent need for context-aware defenses that reflect the ease
with which these jailbreaks can be reproduced in real-world settings.

</details>


### [60] [Cross-Architecture Distillation Made Simple with Redundancy Suppression](https://arxiv.org/abs/2507.21844)
*Weijia Zhang,Yuehao Liu,Wu Ran,Chao Ma*

Main category: cs.CV

TL;DR: A simple method for cross-architecture knowledge distillation is proposed, focusing on suppressing redundant information without complex designs.


<details>
  <summary>Details</summary>
Motivation: Existing methods are inefficient due to sophisticated modules and excessive parameters. The goal is to extract architecture-agnostic knowledge by reducing redundant information.

Method: Introduces a redundancy suppression distillation (RSD) loss with cross-architecture invariance maximization and feature decorrelation. Includes a lightweight module to preserve student-specific capabilities.

Result: Outperforms OFA on CIFAR-100 and ImageNet-1k with fewer parameters.

Conclusion: The method is a simple, efficient baseline for cross-architecture distillation.

Abstract: We describe a simple method for cross-architecture knowledge distillation,
where the knowledge transfer is cast into a redundant information suppression
formulation. Existing methods introduce sophisticated modules,
architecture-tailored designs, and excessive parameters, which impair their
efficiency and applicability. We propose to extract the architecture-agnostic
knowledge in heterogeneous representations by reducing the redundant
architecture-exclusive information. To this end, we present a simple redundancy
suppression distillation (RSD) loss, which comprises cross-architecture
invariance maximisation and feature decorrelation objectives. To prevent the
student from entirely losing its architecture-specific capabilities, we further
design a lightweight module that decouples the RSD objective from the student's
internal representations. Our method is devoid of the architecture-specific
designs and complex operations in the pioneering method of OFA. It outperforms
OFA on CIFAR-100 and ImageNet-1k benchmarks with only a fraction of their
parameter overhead, which highlights its potential as a simple and strong
baseline to the cross-architecture distillation community.

</details>


### [61] [Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection](https://arxiv.org/abs/2507.21857)
*Jiahao He,Daerji Suolang,Keren Fu,Qijun Zhao*

Main category: cs.CV

TL;DR: A novel selective cross-modal fusion framework (SMFNet) is proposed for RGB-D VSOD, using pixel-level selective fusion and multi-dimensional attention to enhance feature representation, outperforming 19 state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-D VSOD models treat optical flow and depth equally, limiting their potential. The goal is to optimize their fusion based on actual contributions.

Method: SMFNet introduces a pixel-level selective fusion strategy (PSF) and a multi-dimensional selective attention module (MSAM) to integrate optical flow, depth, and RGB effectively.

Result: SMFNet outperforms 19 state-of-the-art models on RDVS and DVisal datasets and shows efficacy on synthetic depth datasets.

Conclusion: SMFNet advances RGB-D VSOD by selectively leveraging motion and depth, demonstrating superior performance and broad applicability.

Abstract: Applying salient object detection (SOD) to RGB-D videos is an emerging task
called RGB-D VSOD and has recently gained increasing interest, due to
considerable performance gains of incorporating motion and depth and that RGB-D
videos can be easily captured now in daily life. Existing RGB-D VSOD models
have different attempts to derive motion cues, in which extracting motion
information explicitly from optical flow appears to be a more effective and
promising alternative. Despite this, there remains a key issue that how to
effectively utilize optical flow and depth to assist the RGB modality in SOD.
Previous methods always treat optical flow and depth equally with respect to
model designs, without explicitly considering their unequal contributions in
individual scenarios, limiting the potential of motion and depth. To address
this issue and unleash the power of motion and depth, we propose a novel
selective cross-modal fusion framework (SMFNet) for RGB-D VSOD, incorporating a
pixel-level selective fusion strategy (PSF) that achieves optimal fusion of
optical flow and depth based on their actual contributions. Besides, we propose
a multi-dimensional selective attention module (MSAM) to integrate the fused
features derived from PSF with the remaining RGB modality at multiple
dimensions, effectively enhancing feature representation to generate refined
features. We conduct comprehensive evaluation of SMFNet against 19
state-of-the-art models on both RDVS and DVisal datasets, making the evaluation
the most comprehensive RGB-D VSOD benchmark up to date, and it also
demonstrates the superiority of SMFNet over other models. Meanwhile, evaluation
on five video benchmark datasets incorporating synthetic depth validates the
efficacy of SMFNet as well. Our code and benchmark results are made publicly
available at https://github.com/Jia-hao999/SMFNet.

</details>


### [62] [Low-Cost Test-Time Adaptation for Robust Video Editing](https://arxiv.org/abs/2507.21858)
*Jianhui Wang,Yinda Chen,Yangfan He,Xinyuan Song,Yi Xin,Dapeng Zhang,Zhongwei Wan,Bin Li,Rongchao Zhang*

Main category: cs.CV

TL;DR: Vid-TTA is a lightweight test-time adaptation framework for video editing, addressing temporal inconsistencies and prompt overfitting through self-supervised tasks and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing video editing methods struggle with temporal inconsistencies and overfitting to simple prompts due to computational and data limitations.

Method: Vid-TTA uses motion-aware frame reconstruction, prompt perturbation, and meta-learning for dynamic loss balancing to optimize each test video.

Result: The framework improves temporal consistency, reduces prompt overfitting, and maintains low computational overhead.

Conclusion: Vid-TTA offers a plug-and-play performance boost for video editing models without heavy resource demands.

Abstract: Video editing is a critical component of content creation that transforms raw
footage into coherent works aligned with specific visual and narrative
objectives. Existing approaches face two major challenges: temporal
inconsistencies due to failure in capturing complex motion patterns, and
overfitting to simple prompts arising from limitations in UNet backbone
architectures. While learning-based methods can enhance editing quality, they
typically demand substantial computational resources and are constrained by the
scarcity of high-quality annotated data. In this paper, we present Vid-TTA, a
lightweight test-time adaptation framework that personalizes optimization for
each test video during inference through self-supervised auxiliary tasks. Our
approach incorporates a motion-aware frame reconstruction mechanism that
identifies and preserves crucial movement regions, alongside a prompt
perturbation and reconstruction strategy that strengthens model robustness to
diverse textual descriptions. These innovations are orchestrated by a
meta-learning driven dynamic loss balancing mechanism that adaptively adjusts
the optimization process based on video characteristics. Extensive experiments
demonstrate that Vid-TTA significantly improves video temporal consistency and
mitigates prompt overfitting while maintaining low computational overhead,
offering a plug-and-play performance boost for existing video editing models.

</details>


### [63] [CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding](https://arxiv.org/abs/2507.21888)
*Fevziye Irem Eyiokur,Dogucan Yaman,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: A dual-model framework improves embodied reference understanding by leveraging head-to-fingertip and wrist-to-fingertip directions, using Gaussian ray heatmaps and a CLIP-Aware Pointing Ensemble for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multimodal integration (text, pointing, scene context) and oversimplify pointing assumptions, leading to suboptimal referent prediction.

Method: Proposes a dual-model framework with Gaussian ray heatmaps, a CLIP-Aware Pointing Ensemble, and an object center prediction head for auxiliary learning.

Result: Achieves ~4 mAP improvement at 0.25 IoU on the YouRefIt benchmark.

Conclusion: The dual-model approach effectively integrates multimodal cues and improves referent localization.

Abstract: We address the problem of Embodied Reference Understanding, which involves
predicting the object that a person in the scene is referring to through both
pointing gesture and language. Accurately identifying the referent requires
multimodal understanding: integrating textual instructions, visual pointing,
and scene context. However, existing methods often struggle to effectively
leverage visual clues for disambiguation. We also observe that, while the
referent is often aligned with the head-to-fingertip line, it occasionally
aligns more closely with the wrist-to-fingertip line. Therefore, relying on a
single line assumption can be overly simplistic and may lead to suboptimal
performance. To address this, we propose a dual-model framework, where one
model learns from the head-to-fingertip direction and the other from the
wrist-to-fingertip direction. We further introduce a Gaussian ray heatmap
representation of these lines and use them as input to provide a strong
supervisory signal that encourages the model to better attend to pointing cues.
To combine the strengths of both models, we present the CLIP-Aware Pointing
Ensemble module, which performs a hybrid ensemble based on CLIP features.
Additionally, we propose an object center prediction head as an auxiliary task
to further enhance referent localization. We validate our approach through
extensive experiments and analysis on the benchmark YouRefIt dataset, achieving
an improvement of approximately 4 mAP at the 0.25 IoU threshold.

</details>


### [64] [Aether Weaver: Multimodal Affective Narrative Co-Generation with Dynamic Scene Graphs](https://arxiv.org/abs/2507.21893)
*Saeed Ghorbani*

Main category: cs.CV

TL;DR: Aether Weaver is a multimodal narrative co-generation framework that integrates text, scene graphs, visuals, and soundscapes for immersive storytelling, outperforming sequential pipelines.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of sequential text-to-visual pipelines by enabling concurrent, integrated generation of multimodal narrative elements.

Method: Uses a Narrator (LLM) for text and prompts, a Director for scene graphs, a Narrative Arc Controller for story structure, and an Affective Tone Mapper for emotional consistency.

Result: Qualitative evaluations show enhanced narrative depth, visual fidelity, and emotional resonance compared to baselines.

Conclusion: Aether Weaver provides a robust platform for creative prototyping and immersive storytelling.

Abstract: We introduce Aether Weaver, a novel, integrated framework for multimodal
narrative co-generation that overcomes limitations of sequential text-to-visual
pipelines. Our system concurrently synthesizes textual narratives, dynamic
scene graph representations, visual scenes, and affective soundscapes, driven
by a tightly integrated, co-generation mechanism. At its core, the Narrator, a
large language model, generates narrative text and multimodal prompts, while
the Director acts as a dynamic scene graph manager, and analyzes the text to
build and maintain a structured representation of the story's world, ensuring
spatio-temporal and relational consistency for visual rendering and subsequent
narrative generation. Additionally, a Narrative Arc Controller guides the
high-level story structure, influencing multimodal affective consistency,
further complemented by an Affective Tone Mapper that ensures congruent
emotional expression across all modalities. Through qualitative evaluations on
a diverse set of narrative prompts encompassing various genres, we demonstrate
that Aether Weaver significantly enhances narrative depth, visual fidelity, and
emotional resonance compared to cascaded baseline approaches. This integrated
framework provides a robust platform for rapid creative prototyping and
immersive storytelling experiences.

</details>


### [65] [Evaluating Deepfake Detectors in the Wild](https://arxiv.org/abs/2507.21905)
*Viacheslav Pirogov,Maksim Artemev*

Main category: cs.CV

TL;DR: The paper evaluates deepfake detectors' effectiveness in real-world scenarios, finding them largely ineffective, with basic image manipulations further reducing performance.


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose a growing threat to digital media authenticity, but existing detectors lack real-world testing.

Method: A novel testing procedure mimics real-world conditions, using a dataset of 500,000+ high-quality deepfake images.

Result: Fewer than half of detectors achieved an AUC score >60%, with basic manipulations like JPEG compression degrading performance.

Conclusion: Deepfake detection remains challenging, and current detectors struggle in real-world applications.

Abstract: Deepfakes powered by advanced machine learning models present a significant
and evolving threat to identity verification and the authenticity of digital
media. Although numerous detectors have been developed to address this problem,
their effectiveness has yet to be tested when applied to real-world data. In
this work we evaluate modern deepfake detectors, introducing a novel testing
procedure designed to mimic real-world scenarios for deepfake detection. Using
state-of-the-art deepfake generation methods, we create a comprehensive dataset
containing more than 500,000 high-quality deepfake images. Our analysis shows
that detecting deepfakes still remains a challenging task. The evaluation shows
that in fewer than half of the deepfake detectors tested achieved an AUC score
greater than 60%, with the lowest being 50%. We demonstrate that basic image
manipulations, such as JPEG compression or image enhancement, can significantly
reduce model performance. All code and data are publicly available at
https://github.com/messlav/Deepfake-Detectors-in-the-Wild.

</details>


### [66] [Predict Patient Self-reported Race from Skin Histological Images](https://arxiv.org/abs/2507.21912)
*Shengjia Chen,Ruchika Verma,Kevin Clare,Jannes Jegminat,Kuan-lin Huang,Brandon Veremis,Thomas Fuchs,Gabriele Campanella*

Main category: cs.CV

TL;DR: AI in pathology can predict race from slides, revealing unintended biases and the need for better data curation.


<details>
  <summary>Details</summary>
Motivation: To investigate if AI models learn unintended demographic biases in pathology, focusing on race prediction from slides.

Method: Used deep learning with attention mechanisms on a racially diverse dataset, testing three curation strategies.

Result: Models predicted race (AUC: 0.799 for White, 0.762 for Black, 0.663 overall), with epidermis as a key feature.

Conclusion: Highlights risks of bias in AI for pathology, emphasizing careful data curation and mitigation strategies.

Abstract: Artificial Intelligence (AI) has demonstrated success in computational
pathology (CPath) for disease detection, biomarker classification, and
prognosis prediction. However, its potential to learn unintended demographic
biases, particularly those related to social determinants of health, remains
understudied. This study investigates whether deep learning models can predict
self-reported race from digitized dermatopathology slides and identifies
potential morphological shortcuts. Using a multisite dataset with a racially
diverse population, we apply an attention-based mechanism to uncover
race-associated morphological features. After evaluating three dataset curation
strategies to control for confounding factors, the final experiment showed that
White and Black demographic groups retained high prediction performance (AUC:
0.799, 0.762), while overall performance dropped to 0.663. Attention analysis
revealed the epidermis as a key predictive feature, with significant
performance declines when these regions were removed. These findings highlight
the need for careful data curation and bias mitigation to ensure equitable AI
deployment in pathology. Code available at:
https://github.com/sinai-computational-pathology/CPath_SAIF.

</details>


### [67] [ArtSeek: Deep artwork understanding via multimodal in-context reasoning and late interaction retrieval](https://arxiv.org/abs/2507.21917)
*Nicola Fanelli,Gennaro Vessio,Giovanna Castellano*

Main category: cs.CV

TL;DR: ArtSeek is a multimodal framework for art analysis using image input, combining retrieval-augmented generation and multimodal LLMs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Challenges in analyzing digitized artworks require integrating visual interpretation with artistic, contextual, and historical knowledge.

Method: ArtSeek uses a multimodal retrieval module, a contrastive multitask classification network, and agentic reasoning with Qwen2.5-VL, supported by WikiFragments dataset.

Result: Achieves +8.4% F1 in style classification and +7.1 BLEU@1 in captioning, with qualitative success in interpreting obscure works.

Conclusion: ArtSeek generalizes beyond visual arts, supporting scalable multimodal AI research; dataset and code will be public.

Abstract: Analyzing digitized artworks presents unique challenges, requiring not only
visual interpretation but also a deep understanding of rich artistic,
contextual, and historical knowledge. We introduce ArtSeek, a multimodal
framework for art analysis that combines multimodal large language models with
retrieval-augmented generation. Unlike prior work, our pipeline relies only on
image input, enabling applicability to artworks without links to Wikidata or
Wikipedia-common in most digitized collections. ArtSeek integrates three key
components: an intelligent multimodal retrieval module based on late
interaction retrieval, a contrastive multitask classification network for
predicting artist, genre, style, media, and tags, and an agentic reasoning
strategy enabled through in-context examples for complex visual question
answering and artwork explanation via Qwen2.5-VL. Central to this approach is
WikiFragments, a Wikipedia-scale dataset of image-text fragments curated to
support knowledge-grounded multimodal reasoning. Our framework achieves
state-of-the-art results on multiple benchmarks, including a +8.4% F1
improvement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in
captioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret
visual motifs, infer historical context, and retrieve relevant knowledge, even
for obscure works. Though focused on visual arts, our approach generalizes to
other domains requiring external knowledge, supporting scalable multimodal AI
research. Both the dataset and the source code will be made publicly available
at https://github.com/cilabuniba/artseek.

</details>


### [68] [SwinECAT: A Transformer-based fundus disease classification model with Shifted Window Attention and Efficient Channel Attention](https://arxiv.org/abs/2507.21922)
*Peiran Gu,Teng Yao,Mengshen He,Fuhao Duan,Feiyan Liu,RenYuan Peng,Bao Ge*

Main category: cs.CV

TL;DR: The paper introduces SwinECAT, a Transformer-based model combining Swin Attention and ECA Attention for improved fundus image analysis, achieving 88.29% accuracy in 9-category classification.


<details>
  <summary>Details</summary>
Motivation: Fundus image analysis faces challenges like small lesion areas and subtle disease differences, reducing model accuracy. The paper aims to address these issues with a novel model.

Method: Proposes SwinECAT, integrating Swin Attention for spatial structures and ECA Attention for feature channel focus, tested on 16,140 fundus images.

Result: SwinECAT achieves 88.29% accuracy, outperforming baseline models, with weighted F1-score of 0.88 and macro F1-score of 0.90.

Conclusion: SwinECAT sets a new benchmark for 9-category fundus disease classification, demonstrating superior performance over existing models.

Abstract: In recent years, artificial intelligence has been increasingly applied in the
field of medical imaging. Among these applications, fundus image analysis
presents special challenges, including small lesion areas in certain fundus
diseases and subtle inter-disease differences, which can lead to reduced
prediction accuracy and overfitting in the models. To address these challenges,
this paper proposes the Transformer-based model SwinECAT, which combines the
Shifted Window (Swin) Attention with the Efficient Channel Attention (ECA)
Attention. SwinECAT leverages the Swin Attention mechanism in the Swin
Transformer backbone to effectively capture local spatial structures and
long-range dependencies within fundus images. The lightweight ECA mechanism is
incorporated to guide the SwinECAT's attention toward critical feature
channels, enabling more discriminative feature representation. In contrast to
previous studies that typically classify fundus images into 4 to 6 categories,
this work expands fundus disease classification to 9 distinct types, thereby
enhancing the granularity of diagnosis. We evaluate our method on the Eye
Disease Image Dataset (EDID) containing 16,140 fundus images for 9-category
classification. Experimental results demonstrate that SwinECAT achieves 88.29\%
accuracy, with weighted F1-score of 0.88 and macro F1-score of 0.90. The
classification results of our proposed model SwinECAT significantly outperform
the baseline Swin Transformer and multiple compared baseline models. To our
knowledge, this represents the highest reported performance for 9-category
classification on this public dataset.

</details>


### [69] [MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning](https://arxiv.org/abs/2507.21924)
*Tianhong Gao,Yannian Fu,Weiqun Wu,Haixiao Yue,Shanshan Liu,Gang Zhang*

Main category: cs.CV

TL;DR: MMAT-1M is a million-scale multimodal agent tuning dataset designed to enhance Chain-of-Thought (CoT), reflection, and dynamic tool usage in multimodal LLMs, significantly improving performance.


<details>
  <summary>Details</summary>
Motivation: The multimodal domain lacks a large-scale, high-quality agent tuning dataset to unlock the full potential of multimodal LLMs.

Method: A four-stage data engine curates and refines multimodal data, generating rationales, integrating API calls and RAG, and optionally compressing dialogues into a one-turn format.

Result: Fine-tuning on MMAT-1M yields significant gains, e.g., 2.7% average improvement on benchmarks and 8.8% on Dyn-VQA.

Conclusion: MMAT-1M effectively enhances multimodal reasoning and tool-based capabilities, with the dataset publicly available.

Abstract: Large Language Models (LLMs), enhanced through agent tuning, have
demonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool
utilization, significantly surpassing the performance of standalone models.
However, the multimodal domain still lacks a large-scale, high-quality agent
tuning dataset to unlock the full potential of multimodal large language
models. To bridge this gap, we introduce MMAT-1M, the first million-scale
multimodal agent tuning dataset designed to support CoT, reflection, and
dynamic tool usage. Our dataset is constructed through a novel four-stage data
engine: 1) We first curate publicly available multimodal datasets containing
question-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for
the original question-answer pairs and dynamically integrate API calls and
Retrieval Augmented Generation (RAG) information through a multi-turn paradigm;
3) Furthermore, we refine the rationales through reflection to ensure logical
consistency and accuracy, creating a multi-turn dialogue dataset with both
Rationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally
compress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)
format. By fine-tuning open-source multimodal models on the MMAT-1M, we observe
significant performance gains. For instance, the InternVL2.5-8B-RR model
achieves an average improvement of 2.7% across eight public benchmarks and 8.8%
on the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in
enhancing multimodal reasoning and tool-based capabilities. The dataset is
publicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.

</details>


### [70] [Attention-Driven Multimodal Alignment for Long-term Action Quality Assessment](https://arxiv.org/abs/2507.21945)
*Xin Wang,Peng-Jie Li,Yuan-Yuan Shen*

Main category: cs.CV

TL;DR: LMAC-Net improves long-term AQA by aligning multimodal features (visual and audio) with attention consistency, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing AQA methods lack effective multimodal integration and temporal dynamics modeling, limiting performance in artistic sports evaluation.

Method: Proposes LMAC-Net with multimodal attention consistency, local query encoder, and two-level score evaluation for alignment and fusion.

Result: LMAC-Net outperforms existing methods on RG and Fis-V datasets, showing better multimodal integration.

Conclusion: LMAC-Net effectively addresses multimodal and temporal challenges in long-term AQA, enhancing performance assessment.

Abstract: Long-term action quality assessment (AQA) focuses on evaluating the quality
of human activities in videos lasting up to several minutes. This task plays an
important role in the automated evaluation of artistic sports such as rhythmic
gymnastics and figure skating, where both accurate motion execution and
temporal synchronization with background music are essential for performance
assessment. However, existing methods predominantly fall into two categories:
unimodal approaches that rely solely on visual features, which are inadequate
for modeling multimodal cues like music; and multimodal approaches that
typically employ simple feature-level contrastive fusion, overlooking deep
cross-modal collaboration and temporal dynamics. As a result, they struggle to
capture complex interactions between modalities and fail to accurately track
critical performance changes throughout extended sequences. To address these
challenges, we propose the Long-term Multimodal Attention Consistency Network
(LMAC-Net). LMAC-Net introduces a multimodal attention consistency mechanism to
explicitly align multimodal features, enabling stable integration of visual and
audio information and enhancing feature representations. Specifically, we
introduce a multimodal local query encoder module to capture temporal semantics
and cross-modal relations, and use a two-level score evaluation for
interpretable results. In addition, attention-based and regression-based losses
are applied to jointly optimize multimodal alignment and score fusion.
Experiments conducted on the RG and Fis-V datasets demonstrate that LMAC-Net
significantly outperforms existing methods, validating the effectiveness of our
proposed approach.

</details>


### [71] [Enhancing Generalization in Data-free Quantization via Mixup-class Prompting](https://arxiv.org/abs/2507.21947)
*Jiwoong Park,Chaeun Lee,Yongseok Choi,Sein Park,Deokki Hong,Jungwook Choi*

Main category: cs.CV

TL;DR: The paper introduces a mixup-class prompt strategy for data-free quantization (DFQ) to improve generalization and stability in post-training quantization (PTQ), outperforming existing methods like GenQ, especially in low-bit scenarios.


<details>
  <summary>Details</summary>
Motivation: PTQ struggles with limited calibration data under privacy constraints, and existing DFQ methods using single-class prompts suffer from polysemy and performance degradation.

Method: Proposes a mixup-based text prompting strategy (mixup-class prompt) to generate diverse, robust synthetic data by fusing multiple class labels at the prompt level.

Result: Outperforms state-of-the-art DFQ methods like GenQ, achieving new accuracy in challenging 2-bit weight, 4-bit activation (W2A4) quantization.

Conclusion: The mixup-class prompt enhances generalization and optimization stability in PTQ, proving effective for CNNs and ViTs in low-bit scenarios.

Abstract: Post-training quantization (PTQ) improves efficiency but struggles with
limited calibration data, especially under privacy constraints. Data-free
quantization (DFQ) mitigates this by generating synthetic images using
generative models such as generative adversarial networks (GANs) and
text-conditioned latent diffusion models (LDMs), while applying existing PTQ
algorithms. However, the relationship between generated synthetic images and
the generalizability of the quantized model during PTQ remains underexplored.
Without investigating this relationship, synthetic images generated by previous
prompt engineering methods based on single-class prompts suffer from issues
such as polysemy, leading to performance degradation. We propose
\textbf{mixup-class prompt}, a mixup-based text prompting strategy that fuses
multiple class labels at the text prompt level to generate diverse, robust
synthetic data. This approach enhances generalization, and improves
optimization stability in PTQ. We provide quantitative insights through
gradient norm and generalization error analysis. Experiments on convolutional
neural networks (CNNs) and vision transformers (ViTs) show that our method
consistently outperforms state-of-the-art DFQ methods like GenQ. Furthermore,
it pushes the performance boundary in extremely low-bit scenarios, achieving
new state-of-the-art accuracy in challenging 2-bit weight, 4-bit activation
(W2A4) quantization.

</details>


### [72] [Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal](https://arxiv.org/abs/2507.21949)
*Jiyu Wu,Yifan Liu,Jiancheng Huang,Mingfu Yan,Shifeng Chen*

Main category: cs.CV

TL;DR: Proposes AGBA and FCFN for shadow removal without masks, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on shadow masks, which are hard to obtain. Intrinsic cues like contrast are ambiguous in complex scenes.

Method: Uses AGBA to filter contrast cues and FCFN for restoring details via high-frequency and contrast guidance.

Result: Achieves top performance among mask-free methods and competes with mask-based approaches.

Conclusion: AGBA and FCFN effectively address shadow removal without masks, outperforming existing methods.

Abstract: Existing shadow removal methods often rely on shadow masks, which are
challenging to acquire in real-world scenarios. Exploring intrinsic image cues,
such as local contrast information, presents a potential alternative for
guiding shadow removal in the absence of explicit masks. However, the cue's
inherent ambiguity becomes a critical limitation in complex scenes, where it
can fail to distinguish true shadows from low-reflectance objects and intricate
background textures. To address this motivation, we propose the Adaptive Gated
Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs
the contrast prior to effectively disentangle shadow features from confounding
visual elements. Furthermore, to tackle the persistent challenge of restoring
soft shadow boundaries and fine-grained details, we introduce a diffusion-based
Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and
contrast cues to guide the generative process. Extensive experiments
demonstrate that our method achieves state-of-the-art results among mask-free
approaches while maintaining competitive performance relative to mask-based
methods.

</details>


### [73] [Mitigating Spurious Correlations in Weakly Supervised Semantic Segmentation via Cross-architecture Consistency Regularization](https://arxiv.org/abs/2507.21959)
*Zheyuan Zhang,Yen-chia Hsu*

Main category: cs.CV

TL;DR: A novel weakly supervised semantic segmentation (WSSS) framework is proposed to address the co-occurrence problem in industrial smoke detection, using a teacher-student model combining CNNs and ViTs with knowledge transfer loss and post-processing.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labels are scarce, especially in domains like industrial smoke, where expert annotations are hard to obtain. Existing WSSS methods suffer from incomplete coverage and biased predictions due to co-occurring contexts.

Method: A teacher-student framework combining CNNs and ViTs is introduced, with a knowledge transfer loss for cross-architecture consistency and post-processing to refine pseudo masks.

Result: The framework targets co-occurrence bias without external supervision, improving pseudo mask quality and addressing partial coverage.

Conclusion: The proposed method offers a scalable solution to WSSS challenges in industrial smoke detection, reducing reliance on external priors.

Abstract: Scarcity of pixel-level labels is a significant challenge in practical
scenarios. In specific domains like industrial smoke, acquiring such detailed
annotations is particularly difficult and often requires expert knowledge. To
alleviate this, weakly supervised semantic segmentation (WSSS) has emerged as a
promising approach. However, due to the supervision gap and inherent bias in
models trained with only image level labels, existing WSSS methods suffer from
limitations such as incomplete foreground coverage, inaccurate object
boundaries, and spurious correlations, especially in our domain, where
emissions are always spatially coupled with chimneys.
  Previous solutions typically rely on additional priors or external knowledge
to mitigate these issues, but they often lack scalability and fail to address
the model's inherent bias toward co-occurring context. To address this, we
propose a novel WSSS framework that directly targets the co-occurrence problem
without relying on external supervision. Unlike prior methods that adopt a
single network, we employ a teacher-student framework that combines CNNs and
ViTs. We introduce a knowledge transfer loss that enforces cross-architecture
consistency by aligning internal representations. Additionally, we incorporate
post-processing techniques to address partial coverage and further improve
pseudo mask quality.

</details>


### [74] [PanoSplatt3R: Leveraging Perspective Pretraining for Generalized Unposed Wide-Baseline Panorama Reconstruction](https://arxiv.org/abs/2507.21960)
*Jiahui Ren,Mochu Xiang,Jiajun Zhu,Yuchao Dai*

Main category: cs.CV

TL;DR: PanoSplatt3R is a novel unposed wide-baseline panorama reconstruction method that outperforms state-of-the-art methods without requiring accurate pose information, enabling high-quality novel views and accurate depth estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on precise pose information, which is resource-intensive and noise-prone, limiting their practicality.

Method: Extends reconstruction pretrainings from perspective to panoramic domain, introducing RoPE rolling for seamless domain transfer while modeling panorama periodicity.

Result: PanoSplatt3R significantly outperforms current methods in novel view generation and depth estimation without pose data.

Conclusion: PanoSplatt3R demonstrates strong generalization and practical potential, making it a robust solution for wide-baseline panorama reconstruction.

Abstract: Wide-baseline panorama reconstruction has emerged as a highly effective and
pivotal approach for not only achieving geometric reconstruction of the
surrounding 3D environment, but also generating highly realistic and immersive
novel views. Although existing methods have shown remarkable performance across
various benchmarks, they are predominantly reliant on accurate pose
information. In real-world scenarios, the acquisition of precise pose often
requires additional computational resources and is highly susceptible to noise.
These limitations hinder the broad applicability and practicality of such
methods. In this paper, we present PanoSplatt3R, an unposed wide-baseline
panorama reconstruction method. We extend and adapt the foundational
reconstruction pretrainings from the perspective domain to the panoramic
domain, thus enabling powerful generalization capabilities. To ensure a
seamless and efficient domain-transfer process, we introduce RoPE rolling that
spans rolled coordinates in rotary positional embeddings across different
attention heads, maintaining a minimal modification to RoPE's mechanism, while
modeling the horizontal periodicity of panorama images. Comprehensive
experiments demonstrate that PanoSplatt3R, even in the absence of pose
information, significantly outperforms current state-of-the-art methods. This
superiority is evident in both the generation of high-quality novel views and
the accuracy of depth estimation, thereby showcasing its great potential for
practical applications. Project page: https://npucvr.github.io/PanoSplatt3R

</details>


### [75] [A Deep Learning Pipeline Using Synthetic Data to Improve Interpretation of Paper ECG Images](https://arxiv.org/abs/2507.21968)
*Xiaoyu Wang,Ramesh Nadarajah,Zhiqiang Zhang,David Wong*

Main category: cs.CV

TL;DR: A deep learning framework for classifying paper-like ECG images into diagnostic categories, addressing visual noise and fine-detailed waveform detection, achieving high AUROC scores.


<details>
  <summary>Details</summary>
Motivation: Early detection of cardiovascular diseases (CVDs) is crucial, and while ECGs are key, manual interpretation is time-consuming. Most ECG data are stored as images, requiring automated solutions.

Method: Proposes a pre-processing pipeline to reduce visual noise and a two-stage fine-tuning strategy using ConvNeXt architecture, first on synthetic/external datasets, then on the target dataset.

Result: Achieved AUROC scores of 0.9688 (validation) and 0.9677 (test), winning the 2024 British Heart Foundation Open Data Science Challenge.

Conclusion: The framework shows promise for automating ECG interpretation in clinical workflows, bridging the gap between digital signal-based and image-based ECG analysis.

Abstract: Cardiovascular diseases (CVDs) are the leading global cause of death, and
early detection is essential to improve patient outcomes. Electrocardiograms
(ECGs), especially 12-lead ECGs, play a key role in the identification of CVDs.
These are routinely interpreted by human experts, a process that is
time-consuming and requires expert knowledge. Historical research in this area
has focused on automatic ECG interpretation from digital signals, with recent
deep learning approaches achieving strong results. In practice, however, most
ECG data in clinical practice are stored or shared in image form. To bridge
this gap, we propose a deep learning framework designed specifically to
classify paper-like ECG images into five main diagnostic categories. Our method
was the winning entry to the 2024 British Heart Foundation Open Data Science
Challenge. It addresses two main challenges of paper ECG classification: visual
noise (e.g., shadows or creases) and the need to detect fine-detailed waveform
patterns. We propose a pre-processing pipeline that reduces visual noise and a
two-stage fine-tuning strategy: the model is first fine-tuned on synthetic and
external ECG image datasets to learn domain-specific features, and then further
fine-tuned on the target dataset to enhance disease-specific recognition. We
adopt the ConvNeXt architecture as the backbone of our model. Our method
achieved AUROC scores of 0.9688 on the public validation set and 0.9677 on the
private test set of the British Heart Foundation Open Data Science Challenge,
highlighting its potential as a practical tool for automated ECG interpretation
in clinical workflows.

</details>


### [76] [EIFNet: Leveraging Event-Image Fusion for Robust Semantic Segmentation](https://arxiv.org/abs/2507.21971)
*Zhijiang Li,Haoran He*

Main category: cs.CV

TL;DR: EIFNet is a multi-modal fusion network for event-based semantic segmentation, addressing challenges in feature extraction and fusion of sparse event streams with dense image data. It achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Event cameras provide high dynamic range and fine temporal resolution but face challenges in extracting reliable features from sparse, noisy event streams and fusing them with dense image data.

Method: Proposes EIFNet with Adaptive Event Feature Refinement Module (AEFRM), Modality-Adaptive Recalibration Module (MARM), and Multi-Head Attention Gated Fusion Module (MGFM) for feature alignment and fusion.

Result: Achieves state-of-the-art performance on DDD17-Semantic and DSEC-Semantic datasets.

Conclusion: EIFNet effectively addresses the challenges of event-based semantic segmentation, demonstrating robust performance.

Abstract: Event-based semantic segmentation explores the potential of event cameras,
which offer high dynamic range and fine temporal resolution, to achieve robust
scene understanding in challenging environments. Despite these advantages, the
task remains difficult due to two main challenges: extracting reliable features
from sparse and noisy event streams, and effectively fusing them with dense,
semantically rich image data that differ in structure and representation. To
address these issues, we propose EIFNet, a multi-modal fusion network that
combines the strengths of both event and frame-based inputs. The network
includes an Adaptive Event Feature Refinement Module (AEFRM), which improves
event representations through multi-scale activity modeling and spatial
attention. In addition, we introduce a Modality-Adaptive Recalibration Module
(MARM) and a Multi-Head Attention Gated Fusion Module (MGFM), which align and
integrate features across modalities using attention mechanisms and gated
fusion strategies. Experiments on DDD17-Semantic and DSEC-Semantic datasets
show that EIFNet achieves state-of-the-art performance, demonstrating its
effectiveness in event-based semantic segmentation.

</details>


### [77] [Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition](https://arxiv.org/abs/2507.21977)
*Jihao Gu,Kun Li,Fei Wang,Yanyan Wei,Zhiliang Wu,Hehe Fan,Meng Wang*

Main category: cs.CV

TL;DR: The paper introduces a Motion-guided Modulation Network (MMN) to improve Micro-Action Recognition by capturing subtle motion cues, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook subtle changes in Micro-Actions (MAs), limiting recognition accuracy.

Method: Proposes MMN with Motion-guided Skeletal Modulation (MSM) and Temporal Modulation (MTM) modules to enhance spatial-temporal representation learning.

Result: MMN outperforms on Micro-Action 52 and iMiGUE datasets, demonstrating superior performance.

Conclusion: Explicitly modeling subtle motion cues is crucial for accurate micro-action recognition.

Abstract: Micro-Actions (MAs) are an important form of non-verbal communication in
social interactions, with potential applications in human emotional analysis.
However, existing methods in Micro-Action Recognition often overlook the
inherent subtle changes in MAs, which limits the accuracy of distinguishing MAs
with subtle changes. To address this issue, we present a novel Motion-guided
Modulation Network (MMN) that implicitly captures and modulates subtle motion
cues to enhance spatial-temporal representation learning. Specifically, we
introduce a Motion-guided Skeletal Modulation module (MSM) to inject motion
cues at the skeletal level, acting as a control signal to guide spatial
representation modeling. In parallel, we design a Motion-guided Temporal
Modulation module (MTM) to incorporate motion information at the frame level,
facilitating the modeling of holistic motion patterns in micro-actions.
Finally, we propose a motion consistency learning strategy to aggregate the
motion cues from multi-scale features for micro-action classification.
Experimental results on the Micro-Action 52 and iMiGUE datasets demonstrate
that MMN achieves state-of-the-art performance in skeleton-based micro-action
recognition, underscoring the importance of explicitly modeling subtle motion
cues. The code will be available at https://github.com/momiji-bit/MMN.

</details>


### [78] [ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models](https://arxiv.org/abs/2507.21985)
*Hyun Jun Yook,Ga San Jhun,Jae Hyun Cho,Min Jeon,Donghyun Kim,Tae Hyung Kim,Youn Kyu Lee*

Main category: cs.CV

TL;DR: ZIUM is a zero-shot intent-aware adversarial attack method for unlearned models, improving attack customization and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on unlearned models struggle with aligning attacks to intent and high computational costs.

Method: Proposes ZIUM, which customizes target attack images and supports zero-shot attacks without further optimization.

Result: ZIUM achieves higher attack success rates and reduces attack time for previously attacked concepts.

Conclusion: ZIUM effectively addresses intent alignment and computational challenges in adversarial attacks on unlearned models.

Abstract: Machine unlearning (MU) removes specific data points or concepts from deep
learning models to enhance privacy and prevent sensitive content generation.
Adversarial prompts can exploit unlearned models to generate content containing
removed concepts, posing a significant security risk. However, existing
adversarial attack methods still face challenges in generating content that
aligns with an attacker's intent while incurring high computational costs to
identify successful prompts. To address these challenges, we propose ZIUM, a
Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables
the flexible customization of target attack images to reflect an attacker's
intent. Additionally, ZIUM supports zero-shot adversarial attacks without
requiring further optimization for previously attacked unlearned concepts. The
evaluation across various MU scenarios demonstrated ZIUM's effectiveness in
successfully customizing content based on user-intent prompts while achieving a
superior attack success rate compared to existing methods. Moreover, its
zero-shot adversarial attack significantly reduces the attack time for
previously attacked unlearned concepts.

</details>


### [79] [Staining and locking computer vision models without retraining](https://arxiv.org/abs/2507.22000)
*Oliver J. Sutton,Qinghua Zhou,George Leete,Alexander N. Gorban,Ivan Y. Tyukin*

Main category: cs.CV

TL;DR: New methods for staining (watermarking) and locking computer vision models protect intellectual property without fine-tuning, with provable guarantees and minimal performance impact.


<details>
  <summary>Details</summary>
Motivation: To safeguard intellectual property of computer vision models by embedding identifiable behavior (staining) and restricting usage without a secret trigger (locking).

Method: Directly modify a small number of model weights to embed stains or locks, requiring no fine-tuning. Unlocking involves inserting a trigger patch into input images.

Result: Effective staining and locking with minimal performance impact, supported by experimental validation on various models.

Conclusion: The methods offer practical, provable solutions for IP protection in computer vision models without retraining.

Abstract: We introduce new methods of staining and locking computer vision models, to
protect their owners' intellectual property. Staining, also known as
watermarking, embeds secret behaviour into a model which can later be used to
identify it, while locking aims to make a model unusable unless a secret
trigger is inserted into input images. Unlike existing methods, our algorithms
can be used to stain and lock pre-trained models without requiring fine-tuning
or retraining, and come with provable, computable guarantees bounding their
worst-case false positive rates. The stain and lock are implemented by directly
modifying a small number of the model's weights and have minimal impact on the
(unlocked) model's performance. Locked models are unlocked by inserting a small
`trigger patch' into the corner of the input image. We present experimental
results showing the efficacy of our methods and demonstrating their practical
performance on a variety of computer vision models.

</details>


### [80] [Bridging Synthetic and Real-World Domains: A Human-in-the-Loop Weakly-Supervised Framework for Industrial Toxic Emission Segmentation](https://arxiv.org/abs/2507.22002)
*Yida Tao,Yen-Chia Hsu*

Main category: cs.CV

TL;DR: CEDANet, a human-in-the-loop domain adaptation framework, integrates citizen-provided weak labels and adversarial feature alignment to improve industrial smoke segmentation, achieving significant performance gains without costly annotations.


<details>
  <summary>Details</summary>
Motivation: High cost and scarcity of pixel-level annotations for industrial smoke segmentation hinder air-quality monitoring. CEDANet leverages citizen feedback and domain adaptation to address this.

Method: CEDANet refines pseudo-labels using citizen votes and employs class-specific domain discriminators to transfer source-domain knowledge to the industrial domain.

Result: CEDANet achieves a five-fold F1-score increase (0.414 vs. 0.083) and six-fold smoke-class IoU increase (0.261 vs. 0.043) over baseline, matching performance of models trained on fully annotated data.

Conclusion: CEDANet offers a scalable, cost-efficient solution for environmental monitoring by combining citizen science with weakly supervised domain adaptation.

Abstract: Industrial smoke segmentation is critical for air-quality monitoring and
environmental protection but is often hampered by the high cost and scarcity of
pixel-level annotations in real-world settings. We introduce CEDANet, a
human-in-the-loop, class-aware domain adaptation framework that uniquely
integrates weak, citizen-provided video-level labels with adversarial feature
alignment. Specifically, we refine pseudo-labels generated by a source-trained
segmentation model using citizen votes, and employ class-specific domain
discriminators to transfer rich source-domain representations to the industrial
domain. Comprehensive experiments on SMOKE5K and custom IJmond datasets
demonstrate that CEDANet achieves an F1-score of 0.414 and a smoke-class IoU of
0.261 with citizen feedback, vastly outperforming the baseline model, which
scored 0.083 and 0.043 respectively. This represents a five-fold increase in
F1-score and a six-fold increase in smoke-class IoU. Notably, CEDANet with
citizen-constrained pseudo-labels achieves performance comparable to the same
architecture trained on limited 100 fully annotated images with F1-score of
0.418 and IoU of 0.264, demonstrating its ability to reach small-sampled fully
supervised-level accuracy without target-domain annotations. Our research
validates the scalability and cost-efficiency of combining citizen science with
weakly supervised domain adaptation, offering a practical solution for complex,
data-scarce environmental monitoring applications.

</details>


### [81] [See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs](https://arxiv.org/abs/2507.22003)
*Ziyun Dai,Xiaoqiang Li,Shaohua Zhang,Yuanchen Wu,Jide Li*

Main category: cs.CV

TL;DR: ViHallu is a vision-centric framework to reduce hallucinations in LVLMs by improving visual-semantic alignment via visual variation images and tailored instructions.


<details>
  <summary>Details</summary>
Motivation: LVLMs often produce text inconsistent with visual content (hallucinations), and current text-centric methods fail in fine-grained scenarios.

Method: Uses visual variation images and constructed visual instructions to fine-tune LVLMs for better alignment.

Result: ViHallu improves fine-grained visual understanding and reduces hallucinations in benchmarks.

Conclusion: ViHallu effectively mitigates hallucinations and enhances visual-semantic alignment, with a released dataset for further research.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable
capabilities in visual understanding and multimodal reasoning. However, LVLMs
frequently exhibit hallucination phenomena, manifesting as the generated
textual responses that demonstrate inconsistencies with the provided visual
content. Existing hallucination mitigation methods are predominantly
text-centric, the challenges of visual-semantic alignment significantly limit
their effectiveness, especially when confronted with fine-grained visual
understanding scenarios. To this end, this paper presents ViHallu, a
Vision-Centric Hallucination mitigation framework that enhances visual-semantic
alignment through Visual Variation Image Generation and Visual Instruction
Construction. ViHallu introduces \textbf{\textit{visual variation images}} with
controllable visual alterations while maintaining the overall image structure.
These images, combined with carefully constructed visual instructions, enable
LVLMs to better understand fine-grained visual content through fine-tuning,
allowing models to more precisely capture the correspondence between visual
content and text, thereby enhancing visual-semantic alignment. Extensive
experiments on multiple benchmarks show that ViHallu effectively enhances
models' fine-grained visual understanding while significantly reducing
hallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual
instruction dataset specifically designed for hallucination mitigation and
visual-semantic alignment. Code is available at
https://github.com/oliviadzy/ViHallu.

</details>


### [82] [VeS: Teaching Pixels to Listen Without Supervision](https://arxiv.org/abs/2507.22008)
*Sajay Raj*

Main category: cs.CV

TL;DR: Dense audio-visual models perform well in low-resource, multilingual settings, with dense token routing being crucial. A dense max-mean token matcher outperforms global pooling by 59% in retrieval and provides sharp localization heatmaps.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether dense AV models generalize to low-resource, noisy multilingual settings, unlike English-centric, caption-rich environments.

Method: Compared three contrastive objectives: global mean-pooled loss, dense max-mean token matcher, and a hybrid, using a multilingual subset of Project Vaani.

Result: Dense objective improved R@1 by 59% over global pooling, with better retrieval ranks and sharp localization heatmaps, even with a frozen vision backbone.

Conclusion: Dense token routing is critical in low-resource settings, outperforming global pooling, and works without fine-tuning the vision backbone.

Abstract: Recent dense audio-visual (AV) models achieve impressive retrieval and
emergent localization, but almost all evidence comes from English-centric,
caption-rich web video. It is unclear whether these objectives survive in
low-resource, code-switched, and noisy multilingual settings that typify
developing regions. We show they do**-**and that the choice of aggregation
function becomes even more critical. Using a multilingual subset of Project
Vaani spanning dozens of Indian languages and dialectal variants, we compare
three contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)
a dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid
(motivated by frozen-vision alignment strategies). The dense objective delivers
a +59% relative R@1 (Audio Visual) improvement over global pooling and
substantially lower mean/median ranks, while consistently producing sharp
zero-shot localization heatmaps of spoken objects-despite keeping the vision
backbone entirely frozen (no LoRA / partial fine-tuning). Our results
demonstrate that dense token routing is not a luxury of high-resource English
corpora; it is more decisive when annotations and acoustic cleanliness are
scarce. We release the codebase and trained models.

</details>


### [83] [XAI for Point Cloud Data using Perturbations based on Meaningful Segmentation](https://arxiv.org/abs/2507.22020)
*Raju Ningappa Mulawade,Christoph Garth,Alexander Wiebel*

Main category: cs.CV

TL;DR: A novel segmentation-based XAI method for point cloud classification is proposed, featuring a point-shifting mechanism for perturbations, aiming to produce human-interpretable explanations.


<details>
  <summary>Details</summary>
Motivation: Understanding AI decision-making in critical applications is crucial. This work focuses on explaining point cloud classification models with human-friendly explanations.

Method: Uses point cloud segmentation and a novel point-shifting mechanism to introduce perturbations, generating meaningful saliency maps.

Result: Produces more interpretable saliency maps compared to classical clustering methods, validated through example inputs.

Conclusion: The method effectively generates meaningful explanations for point cloud classification, enhancing human understanding of AI decisions.

Abstract: We propose a novel segmentation-based explainable artificial intelligence
(XAI) method for neural networks working on point cloud classification. As one
building block of this method, we propose a novel point-shifting mechanism to
introduce perturbations in point cloud data. Recently, AI has seen an
exponential growth. Hence, it is important to understand the decision-making
process of AI algorithms when they are applied in critical areas. Our work
focuses on explaining AI algorithms that classify point cloud data. An
important aspect of the methods used for explaining AI algorithms is their
ability to produce explanations that are easy for humans to understand. This
allows them to analyze the AI algorithms better and make appropriate decisions
based on that analysis. Therefore, in this work, we intend to generate
meaningful explanations that can be easily interpreted by humans. The point
cloud data we consider represents 3D objects such as cars, guitars, and
laptops. We make use of point cloud segmentation models to generate
explanations for the working of classification models. The segments are used to
introduce perturbations into the input point cloud data and generate saliency
maps. The perturbations are introduced using the novel point-shifting mechanism
proposed in this work which ensures that the shifted points no longer influence
the output of the classification algorithm. In contrast to previous methods,
the segments used by our method are meaningful, i.e. humans can easily
interpret the meaning of the segments. Thus, the benefit of our method over
other methods is its ability to produce more meaningful saliency maps. We
compare our method with the use of classical clustering algorithms to generate
explanations. We also analyze the saliency maps generated for example inputs
using our method to demonstrate the usefulness of the method in generating
meaningful explanations.

</details>


### [84] [From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning](https://arxiv.org/abs/2507.22028)
*Honglin He,Yukai Ma,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Seeing-to-Experiencing (S2E) framework to enhance navigation foundation models by combining offline pre-training with reinforcement learning (RL), improving interactivity and safety in real-world urban navigation.


<details>
  <summary>Details</summary>
Motivation: Current navigation foundation models, trained on offline data, lack reasoning about action consequences and adaptability, limiting real-world performance in interactive scenarios.

Method: S2E integrates pre-training on videos with RL post-training, using Anchor-Guided Distribution Matching and a Residual-Attention Module to stabilize learning and retain pre-trained knowledge.

Result: S2E mitigates diminishing returns of offline data scaling, outperforming supervised fine-tuning, and NavBench-GS benchmarks validate its generalizability and safety.

Conclusion: Integrating interactive online experiences (RL) is crucial for scaling foundation models in robotics, enhancing real-world navigation capabilities.

Abstract: Navigation foundation models trained on massive webscale data enable agents
to generalize across diverse environments and embodiments. However, these
models trained solely on offline data, often lack the capacity to reason about
the consequences of their actions or adapt through counterfactual
understanding. They thus face significant limitations in the real-world urban
navigation where interactive and safe behaviors, such as avoiding obstacles and
moving pedestrians, are critical. To tackle these challenges, we introduce the
Seeing-to-Experiencing framework to scale the capability of navigation
foundation models with reinforcement learning. S2E combines the strengths of
pre-training on videos and post-training through RL. It maintains the
generalizability acquired from large-scale real-world videos while enhancing
its interactivity through RL in simulation environments. Specifically, we
introduce two innovations: an Anchor-Guided Distribution Matching strategy,
which stabilizes learning and models diverse motion patterns through
anchor-based supervision; and a Residual-Attention Module, which obtains
reactive behaviors from simulation environments without erasing the model's
pretrained knowledge. Moreover, we establish a comprehensive end-to-end
evaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions
of real-world scenes that incorporate physical interactions. It can
systematically assess the generalizability and safety of navigation foundation
models. Extensive experiments show that S2E mitigates the diminishing returns
often seen when scaling with offline data alone. We perform a thorough analysis
of the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in
the context of post-training for robot learning. Our findings emphasize the
crucial role of integrating interactive online experiences to effectively scale
foundation models in Robotics.

</details>


### [85] [Shallow Deep Learning Can Still Excel in Fine-Grained Few-Shot Learning](https://arxiv.org/abs/2507.22041)
*Chaofei Qi,Chao Ye,Zhitai Liu,Weiyang Lin,Jianbin Qiu*

Main category: cs.CV

TL;DR: The paper introduces LCN-4, a shallow deep architecture with a location-aware feature clustering module, outperforming ConvNet-4 and matching ResNet12-based methods in FGFSL.


<details>
  <summary>Details</summary>
Motivation: To re-evaluate the role of shallow deep backbones in FGFSL and address their limitations in extracting abstract visual attributes.

Method: Proposes LCN-4 with a location-aware feature clustering module, grid position encoding, and frequency domain location embedding to minimize loss.

Result: LCN-4 outperforms ConvNet-4 and matches ResNet12-based methods on fine-grained few-shot benchmarks.

Conclusion: Shallow architectures like LCN-4 can achieve competitive or superior performance in FGFSL, validating the proposed enhancements.

Abstract: Deep learning has witnessed the extensive utilization across a wide spectrum
of domains, including fine-grained few-shot learning (FGFSL) which heavily
depends on deep backbones. Nonetheless, shallower deep backbones such as
ConvNet-4, are not commonly preferred because they're prone to extract a larger
quantity of non-abstract visual attributes. In this paper, we initially
re-evaluate the relationship between network depth and the ability to fully
encode few-shot instances, and delve into whether shallow deep architecture
could effectuate comparable or superior performance to mainstream deep
backbone. Fueled by the inspiration from vanilla ConvNet-4, we introduce a
location-aware constellation network (LCN-4), equipped with a cutting-edge
location-aware feature clustering module. This module can proficiently encoder
and integrate spatial feature fusion, feature clustering, and recessive feature
location, thereby significantly minimizing the overall loss. Specifically, we
innovatively put forward a general grid position encoding compensation to
effectively address the issue of positional information missing during the
feature extraction process of specific ordinary convolutions. Additionally, we
further propose a general frequency domain location embedding technique to
offset for the location loss in clustering features. We have carried out
validation procedures on three representative fine-grained few-shot benchmarks.
Relevant experiments have established that LCN-4 notably outperforms the
ConvNet-4 based State-of-the-Arts and achieves performance that is on par with
or superior to most ResNet12-based methods, confirming the correctness of our
conjecture.

</details>


### [86] [Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos](https://arxiv.org/abs/2507.22052)
*Ziren Gong,Xiaohan Li,Fabio Tosi,Jiawei Han,Stefano Mattoccia,Jianfei Cai,Matteo Poggi*

Main category: cs.CV

TL;DR: Ov3R is a framework for open-vocabulary 3D reconstruction from RGB videos, integrating CLIP semantics for consistent geometry and fine-grained semantic alignment.


<details>
  <summary>Details</summary>
Motivation: To advance Spatial AI by enabling globally consistent geometry and semantic alignment in 3D reconstruction.

Method: Uses CLIP3R for CLIP-informed 3D reconstruction and 2D-3D OVS for lifting 2D features into 3D with fused descriptors.

Result: Achieves state-of-the-art performance in dense 3D reconstruction and open-vocabulary 3D segmentation.

Conclusion: Ov3R marks progress toward real-time, semantics-aware Spatial AI.

Abstract: We present Ov3R, a novel framework for open-vocabulary semantic 3D
reconstruction from RGB video streams, designed to advance Spatial AI. The
system features two key components: CLIP3R, a CLIP-informed 3D reconstruction
module that predicts dense point maps from overlapping clips while embedding
object-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module
that lifts 2D features into 3D by learning fused descriptors integrating
spatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates
CLIP semantics directly into the reconstruction process, enabling globally
consistent geometry and fine-grained semantic alignment. Our framework achieves
state-of-the-art performance in both dense 3D reconstruction and
open-vocabulary 3D segmentation, marking a step forward toward real-time,
semantics-aware Spatial AI.

</details>


### [87] [MetaLab: Few-Shot Game Changer for Image Recognition](https://arxiv.org/abs/2507.22057)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: Proposed MetaLab method for few-shot image recognition achieves near-human accuracy with one-shot learning by combining CIELab color space transformation and graph-based mutual learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the technical gaps in few-shot image recognition compared to large-scale recognition, aiming for high accuracy and generalization with minimal samples.

Method: MetaLab combines LabNet for CIELab color space transformation and feature extraction, and LabGNN for mutual learning between lightness and color graphs.

Result: Achieves ~99% accuracy on benchmarks, demonstrating robust performance and generalization with one-shot learning.

Conclusion: MetaLab effectively bridges the gap in few-shot recognition, nearing human-level accuracy with minimal visual deviation.

Abstract: Difficult few-shot image recognition has significant application prospects,
yet remaining the substantial technical gaps with the conventional large-scale
image recognition. In this paper, we have proposed an efficient original method
for few-shot image recognition, called CIELab-Guided Coherent Meta-Learning
(MetaLab). Structurally, our MetaLab comprises two collaborative neural
networks: LabNet, which can perform domain transformation for the CIELab color
space and extract rich grouped features, and coherent LabGNN, which can
facilitate mutual learning between lightness graph and color graph. For
sufficient certification, we have implemented extensive comparative studies on
four coarse-grained benchmarks, four fine-grained benchmarks, and four
cross-domain few-shot benchmarks. Specifically, our method can achieve high
accuracy, robust performance, and effective generalization capability with
one-shot sample per class. Overall, all experiments have demonstrated that our
MetaLab can approach 99\% $\uparrow\downarrow$ accuracy, reaching the human
recognition ceiling with little visual deviation.

</details>


### [88] [X-Omni: Reinforcement Learning Makes Discrete Autoregressive Image Generative Models Great Again](https://arxiv.org/abs/2507.22058)
*Zigang Geng,Yibing Wang,Yeyao Ma,Chen Li,Yongming Rao,Shuyang Gu,Zhao Zhong,Qinglin Lu,Han Hu,Xiaosong Zhang,Linus,Di Wang,Jie Jiang*

Main category: cs.CV

TL;DR: The paper introduces X-Omni, a framework combining reinforcement learning with autoregressive modeling to improve image generation quality and integrate it with language generation.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of autoregressive modeling for image generation, such as low fidelity and distortion, by leveraging reinforcement learning.

Method: Uses a semantic image tokenizer, a unified autoregressive model for language and images, and an offline diffusion decoder (X-Omni).

Result: X-Omni achieves state-of-the-art performance in image generation with high aesthetic quality and strong instruction-following capabilities.

Conclusion: Reinforcement learning effectively enhances autoregressive image generation, enabling seamless integration with language tasks.

Abstract: Numerous efforts have been made to extend the ``next token prediction''
paradigm to visual contents, aiming to create a unified approach for both image
generation and understanding. Nevertheless, attempts to generate images through
autoregressive modeling with discrete tokens have been plagued by issues such
as low visual fidelity, distorted outputs, and failure to adhere to complex
instructions when rendering intricate details. These shortcomings are likely
attributed to cumulative errors during autoregressive inference or information
loss incurred during the discretization process. Probably due to this
challenge, recent research has increasingly shifted toward jointly training
image generation with diffusion objectives and language generation with
autoregressive objectives, moving away from unified modeling approaches. In
this work, we demonstrate that reinforcement learning can effectively mitigate
artifacts and largely enhance the generation quality of a discrete
autoregressive modeling method, thereby enabling seamless integration of image
and language generation. Our framework comprises a semantic image tokenizer, a
unified autoregressive model for both language and images, and an offline
diffusion decoder for image generation, termed X-Omni. X-Omni achieves
state-of-the-art performance in image generation tasks using a 7B language
model, producing images with high aesthetic quality while exhibiting strong
capabilities in following instructions and rendering long texts.

</details>


### [89] [StepAL: Step-aware Active Learning for Cataract Surgical Videos](https://arxiv.org/abs/2507.22059)
*Nisarg A. Shah,Bardia Safaei,Shameema Sikder,S. Swaroop Vedula,Vishal M. Patel*

Main category: cs.CV

TL;DR: StepAL is an active learning framework for surgical step recognition that selects full videos for annotation, outperforming traditional methods by leveraging step-aware features and entropy-weighted clustering.


<details>
  <summary>Details</summary>
Motivation: Traditional AL methods are ineffective for surgical videos due to inter-step dependencies and lack of context in frame/clip selection. StepAL addresses this by focusing on full video annotation.

Method: StepAL integrates step-aware feature representation (using pseudo-labels) with entropy-weighted clustering to prioritize uncertain and diverse videos for annotation.

Result: StepAL outperforms existing AL methods on Cataract-1k and Cataract-101 datasets, achieving higher accuracy with fewer labeled videos.

Conclusion: StepAL reduces annotation costs while maintaining performance, offering an efficient solution for surgical video analysis in computer-assisted systems.

Abstract: Active learning (AL) can reduce annotation costs in surgical video analysis
while maintaining model performance. However, traditional AL methods, developed
for images or short video clips, are suboptimal for surgical step recognition
due to inter-step dependencies within long, untrimmed surgical videos. These
methods typically select individual frames or clips for labeling, which is
ineffective for surgical videos where annotators require the context of the
entire video for annotation. To address this, we propose StepAL, an active
learning framework designed for full video selection in surgical step
recognition. StepAL integrates a step-aware feature representation, which
leverages pseudo-labels to capture the distribution of predicted steps within
each video, with an entropy-weighted clustering strategy. This combination
prioritizes videos that are both uncertain and exhibit diverse step
compositions for annotation. Experiments on two cataract surgery datasets
(Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms
existing active learning approaches, achieving higher accuracy in step
recognition with fewer labeled videos. StepAL offers an effective approach for
efficient surgical video analysis, reducing the annotation burden in developing
computer-assisted surgical systems.

</details>


### [90] [MOVE: Motion-Guided Few-Shot Video Object Segmentation](https://arxiv.org/abs/2507.22061)
*Kaining Ying,Hengrui Hu,Henghui Ding*

Main category: cs.CV

TL;DR: The paper introduces MOVE, a dataset for motion-guided few-shot video object segmentation (FSVOS), evaluates existing methods, and proposes a baseline approach (DMA) that outperforms others.


<details>
  <summary>Details</summary>
Motivation: Existing FSVOS datasets and methods focus on static object categories, ignoring temporal dynamics, limiting motion understanding applications.

Method: Introduces MOVE dataset, evaluates 6 state-of-the-art methods, and proposes DMA, a decoupled motion-appearance network.

Result: Current methods struggle with motion-guided FSVOS; DMA achieves superior performance in few-shot motion understanding.

Conclusion: DMA establishes a foundation for future research in motion-guided FSVOS, addressing gaps in existing methods.

Abstract: This work addresses motion-guided few-shot video object segmentation (FSVOS),
which aims to segment dynamic objects in videos based on a few annotated
examples with the same motion patterns. Existing FSVOS datasets and methods
typically focus on object categories, which are static attributes that ignore
the rich temporal dynamics in videos, limiting their application in scenarios
requiring motion understanding. To fill this gap, we introduce MOVE, a
large-scale dataset specifically designed for motion-guided FSVOS. Based on
MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different
related tasks across 2 experimental settings. Our results reveal that current
methods struggle to address motion-guided FSVOS, prompting us to analyze the
associated challenges and propose a baseline method, Decoupled Motion
Appearance Network (DMA). Experiments demonstrate that our approach achieves
superior performance in few shot motion understanding, establishing a solid
foundation for future research in this direction.

</details>


### [91] [MetaCLIP 2: A Worldwide Scaling Recipe](https://arxiv.org/abs/2507.22062)
*Yung-Sung Chuang,Yang Li,Dong Wang,Ching-Feng Yeh,Kehan Lyu,Ramya Raghavendra,James Glass,Lifei Huang,Jason Weston,Luke Zettlemoyer,Xinlei Chen,Zhuang Liu,Saining Xie,Wen-tau Yih,Shang-Wen Li,Hu Xu*

Main category: cs.CV

TL;DR: MetaCLIP 2 improves CLIP training on worldwide web-scale data, outperforming English-only CLIP and mSigLIP in zero-shot tasks and setting new benchmarks in multilingual performance.


<details>
  <summary>Details</summary>
Motivation: Address challenges in scaling CLIP training to non-English data and mitigating the 'curse of multilinguality' seen in LLMs.

Method: Introduces MetaCLIP 2, a recipe for training CLIP from scratch on global web-scale image-text pairs, with minimal changes for mutual benefits from English and non-English data.

Result: Surpasses English-only CLIP by 0.8% and mSigLIP by 0.7% in zero-shot ImageNet classification; sets SOTA on multilingual benchmarks like CVQA (57.4%), Babel-ImageNet (50.2%), and XM3600 (64.3%).

Conclusion: MetaCLIP 2 successfully scales CLIP training globally, improving performance in both English and multilingual tasks without system-level changes.

Abstract: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,
supporting from zero-shot classification, retrieval to encoders for multimodal
large language models (MLLMs). Although CLIP is successfully trained on
billion-scale image-text pairs from the English world, scaling CLIP's training
further to learning from the worldwide web data is still challenging: (1) no
curation method is available to handle data points from non-English world; (2)
the English performance from existing multilingual CLIP is worse than its
English-only counterpart, i.e., "curse of multilinguality" that is common in
LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch
on worldwide web-scale image-text pairs. To generalize our findings, we conduct
rigorous ablations with minimal changes that are necessary to address the above
challenges and present a recipe enabling mutual benefits from English and
non-English world data. In zero-shot ImageNet classification, MetaCLIP 2
ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,
and surprisingly sets new state-of-the-art without system-level confounding
factors (e.g., translation, bespoke architecture changes) on multilingual
benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with
64.3% on image-to-text retrieval.

</details>
