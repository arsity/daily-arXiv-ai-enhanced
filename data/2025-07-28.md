<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [eess.IV](#eess.IV) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis](https://arxiv.org/abs/2507.18645)
*Milan Maksimovic,Anna Bohdanets,Immaculate Motsi-Omoijiade,Guido Governatori,Ivan S. Maksymov*

Main category: cs.CV

TL;DR: The paper explores QT-based neural networks for distinguishing military/civilian vehicles and sentiment analysis, suggesting enhanced AI for battlefield scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve AI's human-like reasoning in battlefield contexts, especially for drone warfare, by leveraging quantum tunnelling (QT) in neural networks.

Method: Novel QT-based neural networks are applied to customized CIFAR-format images and sentiment analysis using military-specific vocabulary.

Result: QT-based models effectively distinguish military/civilian vehicles and analyze sentiment, showing promise for multimodal AI in battlefield applications.

Conclusion: QT-based models can enhance AI's human-like reasoning in drone warfare, offering potential for improved battlefield decision-making.

Abstract: Prior work has demonstrated that incorporating well-known quantum tunnelling
(QT) probability into neural network models effectively captures important
nuances of human perception, particularly in the recognition of ambiguous
objects and sentiment analysis. In this paper, we employ novel QT-based neural
networks and assess their effectiveness in distinguishing customised
CIFAR-format images of military and civilian vehicles, as well as sentiment,
using a proprietary military-specific vocabulary. We suggest that QT-based
models can enhance multimodal AI applications in battlefield scenarios,
particularly within human-operated drone warfare contexts, imbuing AI with
certain traits of human reasoning.

</details>


### [2] [Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching](https://arxiv.org/abs/2507.18649)
*Haiyang Liu,Xiaolin Hong,Xuancheng Yang,Yudi Ruan,Xiang Lian,Michael Lingelbach,Hongwei Yi,Wei Li*

Main category: cs.CV

TL;DR: Livatar is a real-time audio-driven talking head video generation framework that improves lip-sync accuracy and reduces pose drift using flow matching and system optimizations.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limited lip-sync accuracy and long-term pose drift, which Livatar aims to address.

Method: Uses a flow matching framework and system optimizations to enhance performance.

Result: Achieves 8.50 LipSync Confidence on HDTF dataset, 141 FPS throughput, and 0.17s latency on an A10 GPU.

Conclusion: Livatar enables high-fidelity avatars for broader applications, with project and examples available online.

Abstract: We present Livatar, a real-time audio-driven talking heads videos generation
framework. Existing baselines suffer from limited lip-sync accuracy and
long-term pose drift. We address these limitations with a flow matching based
framework. Coupled with system optimizations, Livatar achieves competitive
lip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and
reaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single
A10 GPU. This makes high-fidelity avatars accessible to broader applications.
Our project is available at https://www.hedra.com/ with with examples at
https://h-liu1997.github.io/Livatar-1/

</details>


### [3] [Features extraction for image identification using computer vision](https://arxiv.org/abs/2507.18650)
*Venant Niyonkuru,Sylla Sekou,Jimmy Jackson Sinzinkayo*

Main category: cs.CV

TL;DR: The paper compares feature extraction techniques in computer vision, focusing on Vision Transformers (ViTs) and other methods like GANs and CNNs, highlighting ViTs' superior performance due to their architecture.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of various feature extraction techniques, particularly ViTs, in advancing computer vision.

Method: Analyzes architectures like ViTs (patch embedding, positional encoding, multi-head self-attention) and compares them with GANs, deep feature models, and traditional methods (SIFT, SURF, ORB).

Result: ViTs outperform conventional CNNs, with experimental results showcasing their merits and limitations.

Conclusion: ViTs are a promising advancement in computer vision, though their utility depends on specific applications.

Abstract: This study examines various feature extraction techniques in computer vision,
the primary focus of which is on Vision Transformers (ViTs) and other
approaches such as Generative Adversarial Networks (GANs), deep feature models,
traditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive
feature models. Emphasizing ViTs, the report summarizes their architecture,
including patch embedding, positional encoding, and multi-head self-attention
mechanisms with which they overperform conventional convolutional neural
networks (CNNs). Experimental results determine the merits and limitations of
both methods and their utilitarian applications in advancing computer vision.

</details>


### [4] [Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift](https://arxiv.org/abs/2507.18653)
*Mohammed Abdul Hafeez Khan,Parth Ganeriwala,Sarah M. Lehman,Siddhartha Bhattacharyya,Amy Alvarez,Natasha Neogi*

Main category: cs.CV

TL;DR: The paper addresses catastrophic forgetting in lane detection models due to cross-dataset distribution shifts by proposing a parameter-efficient adaptation framework with dynamic routing.


<details>
  <summary>Details</summary>
Motivation: Lane detection models suffer from severe catastrophic forgetting when fine-tuned across datasets, even within the same domain.

Method: Train a base model on a source distribution, adapt it to target distributions via separate branches, and use supervised contrastive learning for dynamic routing at inference.

Result: The framework achieves near-optimal F1-scores with fewer parameters than training separate models for each distribution.

Conclusion: The proposed method effectively mitigates catastrophic forgetting and enables efficient adaptation to new datasets.

Abstract: Lane detection models are often evaluated in a closed-world setting, where
training and testing occur on the same dataset. We observe that, even within
the same domain, cross-dataset distribution shifts can cause severe
catastrophic forgetting during fine-tuning. To address this, we first train a
base model on a source distribution and then adapt it to each new target
distribution by creating separate branches, fine-tuning only selected
components while keeping the original source branch fixed. Based on a
component-wise analysis, we identify effective fine-tuning strategies for
target distributions that enable parameter-efficient adaptation. At inference
time, we propose using a supervised contrastive learning model to identify the
input distribution and dynamically route it to the corresponding branch. Our
framework achieves near-optimal F1-scores while using significantly fewer
parameters than training separate models for each distribution.

</details>


### [5] [Part Segmentation of Human Meshes via Multi-View Human Parsing](https://arxiv.org/abs/2507.18655)
*James Dickens,Kamyar Hamad*

Main category: cs.CV

TL;DR: The paper bridges point cloud deep learning and human parsing by enabling per-vertex semantic segmentation of human meshes using a novel pseudo-ground truth pipeline and memory-efficient sampling.


<details>
  <summary>Details</summary>
Motivation: To achieve high-accuracy semantic segmentation of human meshes by leveraging raw geometry and avoiding reliance on texture information.

Method: Develops a pseudo-ground truth labeling pipeline for Thuman2.1, aligns meshes, segments from viewpoints, backprojects labels, and introduces windowed iterative FPS with space-filling curve serialization for efficient downsampling. Uses PointTransformer for geometric segmentation.

Result: The approach proves effective and accurate in semantic parsing of human meshes.

Conclusion: The proposed method successfully bridges point cloud and human parsing domains, offering a texture-independent solution for semantic segmentation.

Abstract: Recent advances in point cloud deep learning have led to models that achieve
high per-part labeling accuracy on large-scale point clouds, using only the raw
geometry of unordered point sets. In parallel, the field of human parsing
focuses on predicting body part and clothing/accessory labels from images. This
work aims to bridge these two domains by enabling per-vertex semantic
segmentation of large-scale human meshes. To achieve this, a pseudo-ground
truth labeling pipeline is developed for the Thuman2.1 dataset: meshes are
first aligned to a canonical pose, segmented from multiple viewpoints, and the
resulting point-level labels are then backprojected onto the original mesh to
produce per-point pseudo ground truth annotations. Subsequently, a novel,
memory-efficient sampling strategy is introduced, a windowed iterative farthest
point sampling (FPS) with space-filling curve-based serialization to
effectively downsample the point clouds. This is followed by a purely geometric
segmentation using PointTransformer, enabling semantic parsing of human meshes
without relying on texture information. Experimental results confirm the
effectiveness and accuracy of the proposed approach.

</details>


### [6] [ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems](https://arxiv.org/abs/2507.18656)
*Muhammad Zaeem Shahzad,Muhammad Abdullah Hanif,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: The paper introduces ShrinkBox, a backdoor attack on ML-ADAS object detection, subtly shrinking bounding boxes to disrupt distance estimation without detection.


<details>
  <summary>Details</summary>
Motivation: To address security vulnerabilities in cost-effective ML-ADAS systems, which rely on object detection for collision avoidance, by exposing a novel attack method.

Method: ShrinkBox manipulates ground truth bounding boxes in training data, targeting YOLOv9m with a low poisoning ratio, and evaluates its impact on distance estimation.

Result: ShrinkBox achieves a 96% ASR, increases MAE in distance estimation by 3x, and remains undetected in standard benchmarks.

Conclusion: ShrinkBox highlights critical security flaws in ML-ADAS, necessitating robust defenses against such subtle attacks.

Abstract: Advanced Driver Assistance Systems (ADAS) significantly enhance road safety
by detecting potential collisions and alerting drivers. However, their reliance
on expensive sensor technologies such as LiDAR and radar limits accessibility,
particularly in low- and middle-income countries. Machine learning-based ADAS
(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera
input, offers a cost-effective alternative. Critical to ML-ADAS is the
collision avoidance feature, which requires the ability to detect objects and
estimate their distances accurately. This is achieved with specialized DNNs
like YOLO, which provides real-time object detection, and a lightweight,
detection-wise distance estimation approach that relies on key features
extracted from the detections like bounding box dimensions and size. However,
the robustness of these systems is undermined by security vulnerabilities in
object detectors. In this paper, we introduce ShrinkBox, a novel backdoor
attack targeting object detection in collision avoidance ML-ADAS. Unlike
existing attacks that manipulate object class labels or presence, ShrinkBox
subtly shrinks ground truth bounding boxes. This attack remains undetected in
dataset inspections and standard benchmarks while severely disrupting
downstream distance estimation. We demonstrate that ShrinkBox can be realized
in the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with
only a 4% poisoning ratio in the training instances of the KITTI dataset.
Furthermore, given the low error targets introduced in our relaxed poisoning
strategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in
downstream distance estimation by more than 3x on poisoned samples, potentially
resulting in delays or prevention of collision warnings altogether.

</details>


### [7] [VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions](https://arxiv.org/abs/2507.18657)
*Zehui Zhao,Laith Alzubaidi,Haider A. Alwzwazy,Jinglan Zhang,Yuantong Gu*

Main category: cs.CV

TL;DR: VGS-ATD is a novel distributed learning framework addressing privacy, scalability, and efficiency in medical imaging, outperforming centralized and swarm learning with 92.7% accuracy and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized and decentralized learning methods in medical imaging face privacy risks, inefficiency, and catastrophic forgetting, necessitating a scalable and resilient solution.

Method: VGS-ATD, a distributed learning framework, is proposed to train models on local nodes while mitigating heterogeneity, imbalance, and communication inefficiencies.

Result: VGS-ATD achieved 92.7% accuracy, outperformed centralized (84.9%) and swarm learning (72.99%), reduced computational costs by 50%, and showed minimal accuracy drop (1%) after expansion.

Conclusion: VGS-ATD is a scalable, efficient, and privacy-preserving solution for medical imaging, resilient to catastrophic forgetting and superior to existing methods.

Abstract: In recent years, advanced deep learning architectures have shown strong
performance in medical imaging tasks. However, the traditional centralized
learning paradigm poses serious privacy risks as all data is collected and
trained on a single server. To mitigate this challenge, decentralized
approaches such as federated learning and swarm learning have emerged, allowing
model training on local nodes while sharing only model weights. While these
methods enhance privacy, they struggle with heterogeneous and imbalanced data
and suffer from inefficiencies due to frequent communication and the
aggregation of weights. More critically, the dynamic and complex nature of
clinical environments demands scalable AI systems capable of continuously
learning from diverse modalities and multilabels. Yet, both centralized and
decentralized models are prone to catastrophic forgetting during system
expansion, often requiring full model retraining to incorporate new data. To
address these limitations, we propose VGS-ATD, a novel distributed learning
framework. To validate VGS-ATD, we evaluate it in experiments spanning 30
datasets and 80 independent labels across distributed nodes, VGS-ATD achieved
an overall accuracy of 92.7%, outperforming centralized learning (84.9%) and
swarm learning (72.99%), while federated learning failed under these conditions
due to high requirements on computational resources. VGS-ATD also demonstrated
strong scalability, with only a 1% drop in accuracy on existing nodes after
expansion, compared to a 20% drop in centralized learning, highlighting its
resilience to catastrophic forgetting. Additionally, it reduced computational
costs by up to 50% relative to both centralized and swarm learning, confirming
its superior efficiency and scalability.

</details>


### [8] [Fuzzy Theory in Computer Vision: A Review](https://arxiv.org/abs/2507.18660)
*Adilet Yerkin,Ayan Igali,Elnara Kadyrgali,Maksat Shagyrov,Malika Ziyada,Muragul Muratbekova,Pakizar Shamoi*

Main category: cs.CV

TL;DR: The paper explores fuzzy logic's role in computer vision for handling uncertainty, noise, and imprecision, highlighting its advantages over traditional methods in tasks like object recognition and image segmentation. It discusses key techniques and applications, including integration with deep learning.


<details>
  <summary>Details</summary>
Motivation: To address challenges like uncertainty and noise in image data by leveraging fuzzy logic's ability to model gradual transitions and human-like reasoning.

Method: Discusses fuzzy techniques such as fuzzy clustering, fuzzy inference systems, type-2 fuzzy sets, and fuzzy rule-based decision-making, along with their integration with deep learning models like CNNs.

Result: Fuzzy logic provides adaptable and interpretable solutions for computer vision tasks, enhancing performance in applications like medical imaging and autonomous systems.

Conclusion: Fuzzy logic is a promising approach for computer vision, with potential for further advancements through hybrid models and explainable AI.

Abstract: Computer vision applications are omnipresent nowadays. The current paper
explores the use of fuzzy logic in computer vision, stressing its role in
handling uncertainty, noise, and imprecision in image data. Fuzzy logic is able
to model gradual transitions and human-like reasoning and provides a promising
approach to computer vision. Fuzzy approaches offer a way to improve object
recognition, image segmentation, and feature extraction by providing more
adaptable and interpretable solutions compared to traditional methods. We
discuss key fuzzy techniques, including fuzzy clustering, fuzzy inference
systems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper
also discusses various applications, including medical imaging, autonomous
systems, and industrial inspection. Additionally, we explore the integration of
fuzzy logic with deep learning models such as convolutional neural networks
(CNNs) to enhance performance in complex vision tasks. Finally, we examine
emerging trends such as hybrid fuzzy-deep learning models and explainable AI.

</details>


### [9] [Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back](https://arxiv.org/abs/2507.18661)
*Ruixing Zhang,Yang Zhang,Tongyu Zhu,Leilei Sun,Weifeng Lv*

Main category: cs.CV

TL;DR: The paper introduces a human-like approach for next-location prediction using Vision-Language Models (VLMs), proposing VGLS and VLMLocPredictor, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing models lack human-like reasoning over maps for trajectory prediction. VLMs' visual reasoning capabilities offer a solution.

Method: Proposes VGLS to test VLM reasoning, then VLMLocPredictor with SFT tasks and reinforcement learning for self-improvement.

Result: Achieves SOTA performance and superior cross-city generalization on datasets from four cities.

Conclusion: VLMs enable human-like trajectory prediction, with VLMLocPredictor outperforming other LLM-based methods.

Abstract: Next Location Prediction is a fundamental task in the study of human
mobility, with wide-ranging applications in transportation planning, urban
governance, and epidemic forecasting. In practice, when humans attempt to
predict the next location in a trajectory, they often visualize the trajectory
on a map and reason based on road connectivity and movement trends. However,
the vast majority of existing next-location prediction models do not reason
over maps \textbf{in the way that humans do}. Fortunately, the recent
development of Vision-Language Models (VLMs) has demonstrated strong
capabilities in visual perception and even visual reasoning. This opens up a
new possibility: by rendering both the road network and trajectory onto an
image and leveraging the reasoning abilities of VLMs, we can enable models to
perform trajectory inference in a human-like manner. To explore this idea, we
first propose a method called Vision-Guided Location Search (VGLS), which
evaluates whether a general-purpose VLM is capable of trajectory-based
reasoning without modifying any of its internal parameters. Based on insights
from the VGLS results, we further propose our main approach: VLMLocPredictor,
which is composed of two stages: In the first stage, we design two Supervised
Fine-Tuning (SFT) tasks that help the VLM understand road network and
trajectory structures and acquire basic reasoning ability on such visual
inputs. In the second stage, we introduce Reinforcement Learning from Visual
Map Feedback, enabling the model to self-improve its next-location prediction
ability through interaction with the environment. Experiments conducted on
datasets from four different cities show that our method achieves
state-of-the-art (SOTA) performance and exhibits superior cross-city
generalization compared to other LLM-based approaches.

</details>


### [10] [Gen-AI Police Sketches with Stable Diffusion](https://arxiv.org/abs/2507.18667)
*Nicholas Fidalgo,Aaron Contreras,Katherine Harvey,Johnny Ni*

Main category: cs.CV

TL;DR: Multimodal AI-driven suspect sketching was enhanced using three pipelines, with a baseline Stable Diffusion model outperforming CLIP-integrated and LoRA fine-tuned variants in structural and perceptual metrics.


<details>
  <summary>Details</summary>
Motivation: To automate and improve suspect sketching using multimodal AI approaches.

Method: Developed three pipelines: (1) baseline Stable Diffusion, (2) CLIP-integrated Stable Diffusion, and (3) LoRA fine-tuned CLIP with Stable Diffusion. Evaluated via SSIM, PSNR, and LPIPS.

Result: Model 1 (baseline) achieved highest SSIM (0.72) and PSNR (25 dB). Model 3 improved LPIPS over Model 2 but lagged behind Model 1.

Conclusion: The baseline Stable Diffusion model is robust for suspect sketching, despite simpler integration methods outperforming more complex approaches.

Abstract: This project investigates the use of multimodal AI-driven approaches to
automate and enhance suspect sketching. Three pipelines were developed and
evaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model
integrated with a pre-trained CLIP model for text-image alignment, and (3)
novel approach incorporating LoRA fine-tuning of the CLIP model, applied to
self-attention and cross-attention layers, and integrated with Stable
Diffusion. An ablation study confirmed that fine-tuning both self- and
cross-attention layers yielded the best alignment between text descriptions and
sketches. Performance testing revealed that Model 1 achieved the highest
structural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of
25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced
perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2
but still trailing Model 1. Qualitatively, sketches generated by Model 1
demonstrated the clearest facial features, highlighting its robustness as a
baseline despite its simplicity.

</details>


### [11] [Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks](https://arxiv.org/abs/2507.18675)
*Sanyam Jain,Marsha Mariya Kappan,Vijeta Sharma*

Main category: cs.CV

TL;DR: The paper evaluates CLIP for human action recognition, revealing its limitations under masking strategies and proposes a noise-based enhancement to improve accuracy and reduce bias.


<details>
  <summary>Details</summary>
Motivation: Human action recognition is vital in healthcare, but traditional models struggle with generalization. CLIP offers potential but needs evaluation.

Method: Tested CLIP on UCF-101 with three masking strategies (black, feature-specific, isolation) and proposed class-specific noise to enhance performance.

Result: CLIP showed inconsistent performance under masking. The proposed noise method improved accuracy and reduced bias.

Conclusion: Challenges remain for clinical use, but future work can enhance generalizability in healthcare.

Abstract: Human action recognition plays a critical role in healthcare and medicine,
supporting applications such as patient behavior monitoring, fall detection,
surgical robot supervision, and procedural skill assessment. While traditional
models like CNNs and RNNs have achieved moderate success, they often struggle
to generalize across diverse and complex actions. Recent advancements in
vision-language models, especially the transformer-based CLIP model, offer
promising capabilities for generalizing action recognition from video data. In
this work, we evaluate CLIP on the UCF-101 dataset and systematically analyze
its performance under three masking strategies: (1) percentage-based and
shape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to
suppress bias-inducing elements, and (3) isolation masking that retains only
class-specific regions. Our results reveal that CLIP exhibits inconsistent
behavior and frequent misclassifications, particularly when essential visual
cues are obscured. To overcome these limitations, we propose incorporating
class-specific noise, learned via a custom loss function, to reinforce
attention to class-defining features. This enhancement improves classification
accuracy and model confidence while reducing bias. We conclude with a
discussion on the challenges of applying such models in clinical domains and
outline directions for future work to improve generalizability across
domain-independent healthcare scenarios.

</details>


### [12] [HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States](https://arxiv.org/abs/2507.18677)
*Siyu Mu,Wei Xuan Chan,Choon Hwai Yap*

Main category: cs.CV

TL;DR: HeartUnloadNet is a deep learning framework that predicts the unloaded left ventricular shape from clinical images, outperforming traditional methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: The unloaded cardiac geometry is crucial for biomechanical modeling but is challenging to estimate from clinical images. Traditional methods are computationally expensive.

Method: HeartUnloadNet uses a graph attention architecture and cycle-consistency strategy to predict the unloaded LV shape from an end diastolic mesh, incorporating biophysical priors.

Result: The model achieves sub-millimeter accuracy (DSC 0.986, HD 0.083 cm) and reduces inference time to 0.02 seconds per case, outperforming traditional solvers.

Conclusion: HeartUnloadNet offers a scalable, accurate, and fast alternative to inverse FE solvers, enabling real-time clinical applications.

Abstract: The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal
pressure) serves as a valuable zero-stress and zero-strain reference and is
critical for personalized biomechanical modeling of cardiac function, to
understand both healthy and diseased physiology and to predict the effects of
cardiac interventions. However, estimating the unloaded geometry from clinical
images remains a challenging task. Traditional approaches rely on inverse
finite element (FE) solvers that require iterative optimization and are
computationally expensive. In this work, we introduce HeartUnloadNet, a deep
learning framework that predicts the unloaded left ventricular (LV) shape
directly from the end diastolic (ED) mesh while explicitly incorporating
biophysical priors. The network accepts a mesh of arbitrary size along with
physiological parameters such as ED pressure, myocardial stiffness scale, and
fiber helix orientation, and outputs the corresponding unloaded mesh. It adopts
a graph attention architecture and employs a cycle-consistency strategy to
enable bidirectional (loading and unloading) prediction, allowing for partial
self-supervision that improves accuracy and reduces the need for large training
datasets. Trained and tested on 20,700 FE simulations across diverse LV
geometries and physiological conditions, HeartUnloadNet achieves sub-millimeter
accuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing
inference time to just 0.02 seconds per case, over 10^5 times faster and
significantly more accurate than traditional inverse FE solvers. Ablation
studies confirm the effectiveness of the architecture. Notably, the
cycle-consistent design enables the model to maintain a DSC of 97% even with as
few as 200 training samples. This work thus presents a scalable and accurate
surrogate for inverse FE solvers, supporting real-time clinical applications in
the future.

</details>


### [13] [Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting](https://arxiv.org/abs/2507.18678)
*Xingyu Miao,Haoran Duan,Quanhao Qian,Jiuniu Wang,Yang Long,Ling Shao,Deli Zhao,Ran Xu,Gongjie Zhang*

Main category: cs.CV

TL;DR: A scalable pipeline converts single-view images into realistic 3D representations, addressing the scarcity of 3D datasets by leveraging abundant 2D imagery.


<details>
  <summary>Details</summary>
Motivation: The scarcity of large-scale 3D datasets limits spatial intelligence in AI, while 2D imagery is abundant.

Method: Integrated depth estimation, camera calibration, and scale calibration transform single-view images into 3D representations like point clouds and depth maps.

Result: Generated datasets (COCO-3D, Objects365-v2-3D) improve 3D tasks, reducing data collection costs and advancing spatial intelligence.

Conclusion: The pipeline effectively bridges the gap between 2D imagery and 3D scene understanding, enhancing AI's spatial capabilities.

Abstract: Spatial intelligence is emerging as a transformative frontier in AI, yet it
remains constrained by the scarcity of large-scale 3D datasets. Unlike the
abundant 2D imagery, acquiring 3D data typically requires specialized sensors
and laborious annotation. In this work, we present a scalable pipeline that
converts single-view images into comprehensive, scale- and appearance-realistic
3D representations - including point clouds, camera poses, depth maps, and
pseudo-RGBD - via integrated depth estimation, camera calibration, and scale
calibration. Our method bridges the gap between the vast repository of imagery
and the increasing demand for spatial scene understanding. By automatically
generating authentic, scale-aware 3D data from images, we significantly reduce
data collection costs and open new avenues for advancing spatial intelligence.
We release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,
and demonstrate through extensive experiments that our generated data can
benefit various 3D tasks, ranging from fundamental perception to MLLM-based
reasoning. These results validate our pipeline as an effective solution for
developing AI systems capable of perceiving, understanding, and interacting
with physical environments.

</details>


### [14] [SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time](https://arxiv.org/abs/2507.18713)
*Yun Chen,Matthew Haines,Jingkang Wang,Krzysztof Baron-Lis,Sivabalan Manivasagam,Ze Yang,Raquel Urtasun*

Main category: cs.CV

TL;DR: SaLF is a novel volumetric representation for sensor simulation, combining rasterization and raytracing, offering fast training/rendering and support for non-pinhole cameras and LiDARs.


<details>
  <summary>Details</summary>
Motivation: Current methods (NeRF, 3DGS) are slow or limited in sensor compatibility, hindering scalable autonomy testing.

Method: SaLF uses sparse 3D voxel primitives with local implicit fields, enabling adaptive pruning/densification for large scenes.

Result: Fast training (<30 min), high rendering speeds (50+ FPS camera, 600+ FPS LiDAR), and realism comparable to existing methods.

Conclusion: SaLF improves efficiency and capabilities for scalable sensor simulation, advancing autonomy testing.

Abstract: High-fidelity sensor simulation of light-based sensors such as cameras and
LiDARs is critical for safe and accurate autonomy testing. Neural radiance
field (NeRF)-based methods that reconstruct sensor observations via ray-casting
of implicit representations have demonstrated accurate simulation of driving
scenes, but are slow to train and render, hampering scale. 3D Gaussian
Splatting (3DGS) has demonstrated faster training and rendering times through
rasterization, but is primarily restricted to pinhole camera sensors,
preventing usage for realistic multi-sensor autonomy evaluation. Moreover, both
NeRF and 3DGS couple the representation with the rendering procedure (implicit
networks for ray-based evaluation, particles for rasterization), preventing
interoperability, which is key for general usage. In this work, we present
Sparse Local Fields (SaLF), a novel volumetric representation that supports
rasterization and raytracing. SaLF represents volumes as a sparse set of 3D
voxel primitives, where each voxel is a local implicit field. SaLF has fast
training (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS
LiDAR), has adaptive pruning and densification to easily handle large scenes,
and can support non-pinhole cameras and spinning LiDARs. We demonstrate that
SaLF has similar realism as existing self-driving sensor simulation methods
while improving efficiency and enhancing capabilities, enabling more scalable
simulation. https://waabi.ai/salf/

</details>


### [15] [KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ](https://arxiv.org/abs/2507.18741)
*Tristan Repolusk,Eduardo Veas*

Main category: cs.CV

TL;DR: The paper introduces advancements in Optical Music Recognition (OMR) for historical Chinese notations, reducing error rates and outperforming human transcribers.


<details>
  <summary>Details</summary>
Motivation: To address challenges like class imbalance and limited data in recognizing historical Chinese musical notations (suzipu, l端l端pu).

Method: Developed a character recognition model for imbalanced data, used temperature scaling for calibration, and employed leave-one-edition-out cross-validation.

Result: Reduced CER from 10.4% to 7.1% for suzipu and achieved 0.9% for l端l端pu, outperforming human transcribers (15.9% average CER).

Conclusion: The work advances digitization of historical Chinese music, promoting cultural diversity and expanding OMR's applicability.

Abstract: Optical Music Recognition (OMR) for historical Chinese musical notations,
such as suzipu and l\"ul\"upu, presents unique challenges due to high class
imbalance and limited training data. This paper introduces significant
advancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ
from 1202. In this work, we develop and evaluate a character recognition model
for scarce imbalanced data. We improve upon previous baselines by reducing the
Character Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with
77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for
l\"ul\"upu. Our models outperform human transcribers, with an average human CER
of 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve
a well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.
Using a leave-one-edition-out cross-validation approach, we ensure robust
performance across five historical editions. Additionally, we extend the
KuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing
suzipu, l\"ul\"upu, and jianzipu notations. Our findings advance the
digitization and accessibility of historical Chinese music, promoting cultural
diversity in OMR and expanding its applicability to underrepresented music
traditions.

</details>


### [16] [SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning](https://arxiv.org/abs/2507.18743)
*Xinjun Cheng,Yiguo He,Junjie Zhu,Chunping Qiu,Jun Wang,Qiangjuan Huang,Ke Yang*

Main category: cs.CV

TL;DR: The paper introduces SAR-Text, a large-scale SAR image-text dataset, and the SAR-Narrator framework for generating descriptions. It demonstrates improved performance in vision-language tasks using models like SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, high-quality SAR image-text datasets hinders semantic understanding in remote sensing.

Method: Constructed SAR-Text dataset using SAR-Narrator, a multi-stage transfer learning framework, and validated it on image-text retrieval, captioning, and VQA tasks.

Result: Significant improvements in retrieval (16.43% and 10.54% recall boosts), captioning (8x, 4x, 10x higher scores), and VQA (outperforming baselines).

Conclusion: SAR-Text and SAR-Narrator advance SAR semantic understanding and can be adopted for larger dataset creation.

Abstract: Vision Language Models (VLMs) have achieved remarkable breakthroughs in the
field of remote sensing in recent years. Synthetic Aperture Radar (SAR)
imagery, with its all-weather capability, is essential in remote sensing, yet
the lack of large-scale, high-quality SAR image-text datasets hinders its
semantic understanding. In this paper, we construct SAR-Text, a large-scale and
high-quality dataset consisting of over 130,000 SAR image-text pairs. To
construct the SAR-Text dataset, we design the SAR-Narrator framework, which
generates textual descriptions for SAR images through a multi-stage progressive
transfer learning strategy. To verify the effectiveness of the SAR-TEXT
dataset, we conduct experiments on three typical vision-language tasks:
image-text retrieval, image captioning, and visual question answering (VQA).
Specifically, we construct three representative models on SAR-TEXT:
SAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable
improvements in retrieval performance, boosting average recall by 16.43% and
10.54% on the OSdataset-512 and HRSID test sets, respectively. In the
captioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding
those of the original CoCa model by more than 8x, 4x, and 10x, respectively. In
the VQA task, SAR-GPT outperforms baseline and single-stage models on multiple
SAR-VQA datasets, demonstrating stronger semantic understanding and reasoning
ability, as further confirmed by qualitative results. It is worth noting that,
as a flexible captioning tool, SAR-Narrator can be readily adopted by the
community to construct larger-scale SAR image-text datasets.

</details>


### [17] [Learning Efficient and Generalizable Human Representation with Human Gaussian Model](https://arxiv.org/abs/2507.18758)
*Yifan Liu,Shengjun Zhang,Chensheng Dai,Yang Chen,Hao Liu,Chen Li,Yueqi Duan*

Main category: cs.CV

TL;DR: A novel method, Human Gaussian Graph, is proposed to model animatable human avatars by connecting 3D Gaussians with SMPL mesh vertices, improving frame relations and animation quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for animatable human avatars either require per-instance optimization or fail to capture temporal relations between frames, limiting their effectiveness.

Method: The Human Gaussian Graph uses dual layers (Gaussians and mesh vertices) with intra-node and inter-node operations to aggregate and pass information across frames.

Result: The method shows improved efficiency and generalization in novel view synthesis and pose animation tasks.

Conclusion: The Human Gaussian Graph effectively models animatable avatars by leveraging temporal relations, outperforming previous approaches.

Abstract: Modeling animatable human avatars from videos is a long-standing and
challenging problem. While conventional methods require per-instance
optimization, recent feed-forward methods have been proposed to generate 3D
Gaussians with a learnable network. However, these methods predict Gaussians
for each frame independently, without fully capturing the relations of
Gaussians from different timestamps. To address this, we propose Human Gaussian
Graph to model the connection between predicted Gaussians and human SMPL mesh,
so that we can leverage information from all frames to recover an animatable
human representation. Specifically, the Human Gaussian Graph contains dual
layers where Gaussians are the first layer nodes and mesh vertices serve as the
second layer nodes. Based on this structure, we further propose the intra-node
operation to aggregate various Gaussians connected to one mesh vertex, and
inter-node operation to support message passing among mesh node neighbors.
Experimental results on novel view synthesis and novel pose animation
demonstrate the efficiency and generalization of our method.

</details>


### [18] [Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving](https://arxiv.org/abs/2507.18763)
*Keshav Gupta,Tejas S. Stanley,Pranjal Paul,Arun K. Singh,K. Madhava Krishna*

Main category: cs.CV

TL;DR: The paper introduces ContourDiff, a self-supervised method for predicting drivable free-space corridors using monocular camera input, leveraging ego trajectories and diffusion-based contour modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume BEV-centric representations, which are hard to obtain. The paper aims to predict navigable subsets of road regions using only monocular images.

Method: A self-supervised approach generates free-space samples using ego trajectories and camera images. ContourDiff, a diffusion-based architecture, models segments via contour points instead of binary masks.

Result: The method accurately predicts multimodal navigable corridors in images, validated on nuScenes and CARLA datasets.

Conclusion: ContourDiff provides structured, interpretable free-space predictions, advancing monocular-based drivable corridor estimation.

Abstract: Drivable Free-space prediction is a fundamental and crucial problem in
autonomous driving. Recent works have addressed the problem by representing the
entire non-obstacle road regions as the free-space. In contrast our aim is to
estimate the driving corridors that are a navigable subset of the entire road
region. Unfortunately, existing corridor estimation methods directly assume a
BEV-centric representation, which is hard to obtain. In contrast, we frame
drivable free-space corridor prediction as a pure image perception task, using
only monocular camera input. However such a formulation poses several
challenges as one doesn't have the corresponding data for such free-space
corridor segments in the image. Consequently, we develop a novel
self-supervised approach for free-space sample generation by leveraging future
ego trajectories and front-view camera images, making the process of visual
corridor estimation dependent on the ego trajectory. We then employ a diffusion
process to model the distribution of such segments in the image. However, the
existing binary mask-based representation for a segment poses many limitations.
Therefore, we introduce ContourDiff, a specialized diffusion-based architecture
that denoises over contour points rather than relying on binary mask
representations, enabling structured and interpretable free-space predictions.
We evaluate our approach qualitatively and quantitatively on both nuScenes and
CARLA, demonstrating its effectiveness in accurately predicting safe multimodal
navigable corridors in the image.

</details>


### [19] [Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning](https://arxiv.org/abs/2507.18788)
*Hitesh Kumar Gupta*

Main category: cs.CV

TL;DR: The paper presents an iterative development of image captioning models, from CNN-LSTM to an advanced attention-based system (Nexus), highlighting the importance of attention mechanisms for performance.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and validate architectural enhancements in image captioning, particularly the role of attention mechanisms alongside visual backbone upgrades.

Method: Developed five models, starting with a simple CNN-LSTM and ending with Nexus, which uses EfficientNetV2B3 and dynamic attention. Trained on MS COCO 2017.

Result: Nexus achieved a BLEU-4 score of 31.4, outperforming benchmarks and showing that attention mechanisms are crucial for handling richer visual details.

Conclusion: The iterative approach validates the shift to attention-based architectures, providing a replicable blueprint for vision-language tasks.

Abstract: Image captioning, a task at the confluence of computer vision and natural
language processing, requires a sophisticated understanding of both visual
scenes and linguistic structure. While modern approaches are dominated by
large-scale Transformer architectures, this paper documents a systematic,
iterative development of foundational image captioning models, progressing from
a simple CNN-LSTM encoder-decoder to a competitive attention-based system. We
present a series of five models, beginning with Genesis and concluding with
Nexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic
attention mechanism. Our experiments chart the impact of architectural
enhancements and demonstrate a key finding within the classic CNN-LSTM
paradigm: merely upgrading the visual backbone without a corresponding
attention mechanism can degrade performance, as the single-vector bottleneck
cannot transmit the richer visual detail. This insight validates the
architectural shift to attention. Trained on the MS COCO 2017 dataset, our
final model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several
foundational benchmarks and validating our iterative design process. This work
provides a clear, replicable blueprint for understanding the core architectural
principles that underpin modern vision-language tasks.

</details>


### [20] [Deepfake Detection Via Facial Feature Extraction and Modeling](https://arxiv.org/abs/2507.18815)
*Benjamin Carter,Nathan Dilla,Micheal Callahan,Atuhaire Ambala*

Main category: cs.CV

TL;DR: The paper proposes using facial landmarks for deepfake detection, focusing on inconsistencies in facial movements rather than raw image processing, achieving high accuracy with various neural networks.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology makes it hard to distinguish AI-generated media from genuine content, necessitating new detection methods beyond traditional image processing.

Method: The approach extracts facial landmarks from videos to detect deepfakes, testing the technique on RNN, ANN, and CNN models.

Result: High accuracy was achieved: RNN (96%), ANN (93%), and CNN (78%), showing the method's effectiveness with fewer parameters.

Conclusion: Facial landmarks are a viable alternative to raw image processing for deepfake detection, offering compatibility with multiple neural network models and efficiency.

Abstract: The rise of deepfake technology brings forth new questions about the
authenticity of various forms of media found online today. Videos and images
generated by artificial intelligence (AI) have become increasingly more
difficult to differentiate from genuine media, resulting in the need for new
models to detect artificially-generated media. While many models have attempted
to solve this, most focus on direct image processing, adapting a convolutional
neural network (CNN) or a recurrent neural network (RNN) that directly
interacts with the video image data. This paper introduces an approach of using
solely facial landmarks for deepfake detection. Using a dataset consisting of
both deepfake and genuine videos of human faces, this paper describes an
approach for extracting facial landmarks for deepfake detection, focusing on
identifying subtle inconsistencies in facial movements instead of raw image
processing. Experimental results demonstrated that this feature extraction
technique is effective in various neural network models, with the same facial
landmarks tested on three neural network models, with promising performance
metrics indicating its potential for real-world applications. The findings
discussed in this paper include RNN and artificial neural network (ANN) models
with accuracy between 96% and 93%, respectively, with a CNN model hovering
around 78%. This research challenges the assumption that raw image processing
is necessary to identify deepfake videos by presenting a facial feature
extraction approach compatible with various neural network models while
requiring fewer parameters.

</details>


### [21] [Flow Stochastic Segmentation Networks](https://arxiv.org/abs/2507.18838)
*Fabio De Sousa Ribeiro,Omar Todd,Charles Jones,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: Flow-SSN is a generative segmentation model with autoregressive and flow variants, overcoming low-rank limitations of prior methods and enabling efficient sampling.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of low-rank parameterization in previous segmentation methods and improving efficiency in sampling.

Method: Introduces Flow-SSN, featuring discrete-time autoregressive and continuous-time flow variants, focusing on high-rank pixel-wise covariances without storing parameters.

Result: Achieves state-of-the-art results on medical imaging benchmarks with more efficient sampling.

Conclusion: Flow-SSNs offer a superior approach for generative segmentation, combining expressiveness and efficiency.

Abstract: We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a
generative segmentation model family featuring discrete-time autoregressive and
modern continuous-time flow variants. We prove fundamental limitations of the
low-rank parameterisation of previous methods and show that Flow-SSNs can
estimate arbitrarily high-rank pixel-wise covariances without assuming the rank
or storing the distributional parameters. Flow-SSNs are also more efficient to
sample from than standard diffusion-based segmentation models, thanks to most
of the model capacity being allocated to learning the base distribution of the
flow, constituting an expressive prior. We apply Flow-SSNs to challenging
medical imaging benchmarks and achieve state-of-the-art results. Code
available: https://github.com/biomedia-mira/flow-ssn.

</details>


### [22] [PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2507.18848)
*Beidi Zhao,SangMook Kim,Hao Chen,Chen Zhou,Zu-hua Gao,Gang Wang,Xiaoxiao Li*

Main category: cs.CV

TL;DR: PTCMIL introduces a Prompt Token Clustering-based ViT for MIL aggregation, improving WSI analysis by dynamically aligning clustering with tasks and reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing MIL methods struggle with aggregating diverse patch information in WSIs, and current approaches like ViTs and clustering are computationally heavy or lack task-specific adaptability.

Method: PTCMIL integrates learnable prompt tokens into a ViT backbone, unifying clustering and prediction end-to-end. It uses projection-based clustering and token merging for efficiency.

Result: PTCMIL outperforms state-of-the-art methods in classification and survival analysis across eight datasets, with strong interpretability.

Conclusion: PTCMIL effectively addresses MIL challenges in WSI analysis, offering robust, efficient, and interpretable performance.

Abstract: Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with
the complexity and heterogeneity of WSIs. Existing MIL methods face challenges
in aggregating diverse patch information into robust WSI representations. While
ViTs and clustering-based approaches show promise, they are computationally
intensive and fail to capture task-specific and slide-specific variability. To
address these limitations, we propose PTCMIL, a novel Prompt Token
Clustering-based ViT for MIL aggregation. By introducing learnable prompt
tokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in
an end-to-end manner. It dynamically aligns clustering with downstream tasks,
using projection-based clustering tailored to each WSI, reducing complexity
while preserving patch heterogeneity. Through token merging and prototype-based
pooling, PTCMIL efficiently captures task-relevant patterns. Extensive
experiments on eight datasets demonstrate its superior performance in
classification and survival analysis tasks, outperforming state-of-the-art
methods. Systematic ablation studies confirm its robustness and strong
interpretability. The code is released at https://github.com/ubc-tea/PTCMIL.

</details>


### [23] [Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction](https://arxiv.org/abs/2507.18863)
*Matthew Kit Khinn Teng,Haibo Zhang,Takeshi Saitoh*

Main category: cs.CV

TL;DR: A novel phoneme-based two-stage framework for Visual Automatic Speech Recognition (V-ASR) improves accuracy by combining visual and landmark motion features, followed by an LLM model for word reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing V-ASR methods struggle with viseme ambiguity and high error rates due to lack of auditory cues and similar lip motions for distinct sounds.

Method: Proposes a two-stage approach: Stage 1 predicts phonemes using visual and landmark features; Stage 2 reconstructs words using an LLM (NLLB).

Result: Achieves 17.4% WER on LRS2 and 21.0% WER on LRS3, outperforming existing methods.

Conclusion: The framework effectively addresses viseme ambiguity and reduces error rates, demonstrating superior performance in V-ASR.

Abstract: Visual Automatic Speech Recognition (V-ASR) is a challenging task that
involves interpreting spoken language solely from visual information, such as
lip movements and facial expressions. This task is notably challenging due to
the absence of auditory cues and the visual ambiguity of phonemes that exhibit
similar visemes-distinct sounds that appear identical in lip motions. Existing
methods often aim to predict words or characters directly from visual cues, but
they commonly suffer from high error rates due to viseme ambiguity and require
large amounts of pre-training data. We propose a novel phoneme-based two-stage
framework that fuses visual and landmark motion features, followed by an LLM
model for word reconstruction to address these challenges. Stage 1 consists of
V-ASR, which outputs the predicted phonemes, thereby reducing training
complexity. Meanwhile, the facial landmark features address speaker-specific
facial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,
that reconstructs the output phonemes back to words. Besides using a large
visual dataset for deep learning fine-tuning, our PV-ASR method demonstrates
superior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the
LRS3 dataset.

</details>


### [24] [Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform](https://arxiv.org/abs/2507.18870)
*Keke Tang,Yuze Gao,Weilong Peng,Xiaofei Wang,Meie Fang,Peican Zhu*

Main category: cs.CV

TL;DR: MAT-Adv is a new adversarial attack framework for 3D point clouds, enhancing transferability and undefendability by perturbing medial axis transform (MAT) representations.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on point clouds lack transferability and robustness against defenses, limiting their practical utility.

Method: MAT-Adv uses an autoencoder to project point clouds into MAT representations, perturbs these intrinsic structures, and employs dropout to optimize perturbations.

Result: MAT-Adv outperforms state-of-the-art methods in transferability and undefendability.

Conclusion: MAT-Adv advances adversarial attack robustness for 3D models by leveraging intrinsic geometric perturbations.

Abstract: Studying adversarial attacks on point clouds is essential for evaluating and
improving the robustness of 3D deep learning models. However, most existing
attack methods are developed under ideal white-box settings and often suffer
from limited transferability to unseen models and insufficient robustness
against common defense mechanisms. In this paper, we propose MAT-Adv, a novel
adversarial attack framework that enhances both transferability and
undefendability by explicitly perturbing the medial axis transform (MAT)
representations, in order to induce inherent adversarialness in the resulting
point clouds. Specifically, we employ an autoencoder to project input point
clouds into compact MAT representations that capture the intrinsic geometric
structure of point clouds. By perturbing these intrinsic representations,
MAT-Adv introduces structural-level adversarial characteristics that remain
effective across diverse models and defense strategies. To mitigate overfitting
and prevent perturbation collapse, we incorporate a dropout strategy into the
optimization of MAT perturbations, further improving transferability and
undefendability. Extensive experiments demonstrate that MAT-Adv significantly
outperforms existing state-of-the-art methods in both transferability and
undefendability. Codes will be made public upon paper acceptance.

</details>


### [25] [Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?](https://arxiv.org/abs/2507.18881)
*Bolei Chen,Jiaxu Kang,Haonan Yang,Ping Zhong,Jianxin Wang*

Main category: cs.CV

TL;DR: The paper introduces a 3D geometric prior-enhanced method for 2D floorplan localization (FLoc) to address challenges like visual changes and occlusions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Floorplans are robust but minimalist, causing modal and geometric mismatches with visual perceptions. Existing methods fail to handle visual changes and occlusions.

Method: The method injects 3D geometric priors into FLoc using multi-view constraints and view-scene aligned geometric priors, modeled via self-supervised contrastive learning.

Result: The approach bridges modal gaps, improves localization success, and outperforms state-of-the-art methods without added computational burden.

Conclusion: The 3D priors significantly enhance FLoc accuracy, validated by comparative studies, with data and code to be released post-review.

Abstract: Since a building's floorplans are easily accessible, consistent over time,
and inherently robust to changes in visual appearance, self-localization within
the floorplan has attracted researchers' interest. However, since floorplans
are minimalist representations of a building's structure, modal and geometric
differences between visual perceptions and floorplans pose challenges to this
task. While existing methods cleverly utilize 2D geometric features and pose
filters to achieve promising performance, they fail to address the localization
errors caused by frequent visual changes and view occlusions due to variously
shaped 3D objects. To tackle these issues, this paper views the 2D Floorplan
Localization (FLoc) problem from a higher dimension by injecting 3D geometric
priors into the visual FLoc algorithm. For the 3D geometric prior modeling, we
first model geometrically aware view invariance using multi-view constraints,
i.e., leveraging imaging geometric principles to provide matching constraints
between multiple images that see the same points. Then, we further model the
view-scene aligned geometric priors, enhancing the cross-modal geometry-color
correspondences by associating the scene's surface reconstruction with the RGB
frames of the sequence. Both 3D priors are modeled through self-supervised
contrastive learning, thus no additional geometric or semantic annotations are
required. These 3D priors summarized in extensive realistic scenes bridge the
modal gap while improving localization success without increasing the
computational burden on the FLoc algorithm. Sufficient comparative studies
demonstrate that our method significantly outperforms state-of-the-art methods
and substantially boosts the FLoc accuracy. All data and code will be released
after the anonymous review.

</details>


### [26] [Synthetic-to-Real Camouflaged Object Detection](https://arxiv.org/abs/2507.18911)
*Zhihao Luo,Luojun Lin,Zheng Lin*

Main category: cs.CV

TL;DR: The paper introduces Syn-to-Real Camouflaged Object Detection (S2R-COD) to address data scarcity in COD by leveraging synthetic datasets and unannotated real images, proposing the CSRDA framework for domain adaptation.


<details>
  <summary>Details</summary>
Motivation: Limited datasets for COD, especially in specialized categories, hinder model performance. Synthetic data helps but degrades performance when used directly.

Method: Proposes CSRDA, a student-teacher model using pseudo labeling and consistency regularization to adapt synthetic data to real-world scenarios.

Result: CSRDA effectively bridges the synthetic-real domain gap, improving model performance with limited real data.

Conclusion: The framework mitigates data scarcity and annotation challenges in COD, with code made publicly available.

Abstract: Due to the high cost of collection and labeling, there are relatively few
datasets for camouflaged object detection (COD). In particular, for certain
specialized categories, the available image dataset is insufficiently
populated. Synthetic datasets can be utilized to alleviate the problem of
limited data to some extent. However, directly training with synthetic datasets
compared to real datasets can lead to a degradation in model performance. To
tackle this problem, in this work, we investigate a new task, namely
Syn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the
model performance in real world scenarios, a set of annotated synthetic
camouflaged images and a limited number of unannotated real images must be
utilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework
(CSRDA), a method based on the student-teacher model. Specially, CSRDA
propagates class information from the labeled source domain to the unlabeled
target domain through pseudo labeling combined with consistency regularization.
Considering that narrowing the intra-domain gap can improve the quality of
pseudo labeling, CSRDA utilizes a recurrent learning framework to build an
evolving real domain for bridging the source and target domain. Extensive
experiments demonstrate the effectiveness of our framework, mitigating the
problem of limited data and handcraft annotations in COD. Our code is publicly
available at: https://github.com/Muscape/S2R-COD

</details>


### [27] [HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback](https://arxiv.org/abs/2507.18921)
*Elham Soltani Kazemi,Imad Eddine Toubal,Gani Rahmon,Jaired Collins,K. Palaniappan*

Main category: cs.CV

TL;DR: HQ-SMem improves Video Object Segmentation (VOS) by refining masks, optimizing memory, and updating appearance models, achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing VOS models struggle with precise mask delineation, deformable objects, tracking drift, and long sequences. HQ-SMem addresses these limitations.

Method: Incorporates SAM-HQ for refined masks, dynamic smart memory for efficiency, and dynamic appearance updates to handle complex variations.

Result: Top performance on VOTS and VOTSt 2024 datasets, with new benchmarks on Long Video Dataset and LVOS.

Conclusion: HQ-SMem effectively mitigates VOS limitations, excelling in complex scenarios.

Abstract: Video Object Segmentation (VOS) is foundational to numerous computer vision
applications, including surveillance, autonomous driving, robotics and
generative video editing. However, existing VOS models often struggle with
precise mask delineation, deformable objects, topologically transforming
objects, tracking drift and long video sequences. In this paper, we introduce
HQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a
novel method that enhances the performance of VOS base models by addressing
these limitations. Our approach incorporates three key innovations: (i)
leveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based
candidate-selection to refine coarse segmentation masks, resulting in improved
object boundaries; (ii) implementing a dynamic smart memory mechanism that
selectively stores relevant key frames while discarding redundant ones, thereby
optimizing memory usage and processing efficiency for long-term videos; and
(iii) dynamically updating the appearance model to effectively handle complex
topological object variations and reduce drift throughout the video. These
contributions mitigate several limitations of existing VOS models including,
coarse segmentations that mix-in background pixels, fixed memory update
schedules, brittleness to drift and occlusions, and prompt ambiguity issues
associated with SAM. Extensive experiments conducted on multiple public
datasets and state-of-the-art base trackers demonstrate that our method
consistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,
HQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its
effectiveness in challenging scenarios characterized by complex multi-object
dynamics over extended temporal durations.

</details>


### [28] [Gaussian Set Surface Reconstruction through Per-Gaussian Optimization](https://arxiv.org/abs/2507.18923)
*Zhentao Huang,Di Wu,Zhenbang He,Minglun Gong*

Main category: cs.CV

TL;DR: GSSR improves 3D Gaussian Splatting by optimizing Gaussian placement and alignment for better geometry reconstruction and scene editing.


<details>
  <summary>Details</summary>
Motivation: 3DGS and variants like PGSR lack accurate geometry reconstruction due to uneven Gaussian distribution and misalignment with latent surfaces.

Method: GSSR enforces normal and photometric consistency, uses opacity regularization, and periodic reinitialization for uniform Gaussian distribution.

Result: GSSR achieves improved geometric precision, enabling intuitive scene editing and high-quality rendering.

Conclusion: GSSR enhances geometric accuracy in Gaussian-based 3D environments while maintaining rendering performance.

Abstract: 3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its
flexible representation, yet fails to accurately reconstruct scene geometry.
While modern variants like PGSR introduce additional losses to ensure proper
depth and normal maps through Gaussian fusion, they still neglect individual
placement optimization. This results in unevenly distributed Gaussians that
deviate from the latent surface, complicating both reconstruction refinement
and scene editing. Motivated by pioneering work on Point Set Surfaces, we
propose Gaussian Set Surface Reconstruction (GSSR), a method designed to
distribute Gaussians evenly along the latent surface while aligning their
dominant normals with the surface normal. GSSR enforces fine-grained geometric
alignment through a combination of pixel-level and Gaussian-level single-view
normal consistency and multi-view photometric consistency, optimizing both
local and global perspectives. To further refine the representation, we
introduce an opacity regularization loss to eliminate redundant Gaussians and
apply periodic depth- and normal-guided Gaussian reinitialization for a
cleaner, more uniform spatial distribution. Our reconstruction results
demonstrate significantly improved geometric precision in Gaussian placement,
enabling intuitive scene editing and efficient generation of novel
Gaussian-based 3D environments. Extensive experiments validate GSSR's
effectiveness, showing enhanced geometric accuracy while preserving
high-quality rendering performance.

</details>


### [29] [WiSE-OD: Benchmarking Robustness in Infrared Object Detection](https://arxiv.org/abs/2507.18925)
*Heitor R. Medeiros,Atif Belal,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: The paper introduces WiSE-OD, a weight-space ensembling method, to improve robustness in IR object detection by combining RGB and IR model knowledge, evaluated on new OOD benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address the modality gap and robustness issues in IR object detection caused by reliance on RGB pre-trained models.

Method: Proposes WiSE-OD with two variants (WiSE-OD$_{ZS}$ and WiSE-OD$_{LP}$) to blend RGB and IR model weights, tested on LLVIP-C and FLIR-C benchmarks.

Result: WiSE-OD enhances cross-modality and corruption robustness without extra training or inference costs.

Conclusion: WiSE-OD effectively bridges the RGB-IR gap and improves robustness in IR object detection.

Abstract: Object detection (OD) in infrared (IR) imagery is critical for low-light and
nighttime applications. However, the scarcity of large-scale IR datasets forces
models to rely on weights pre-trained on RGB images. While fine-tuning on IR
improves accuracy, it often compromises robustness under distribution shifts
due to the inherent modality gap between RGB and IR. To address this, we
introduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)
benchmarks built by applying corruption to standard IR datasets. Additionally,
to fully leverage the complementary knowledge from RGB and infrared trained
models, we propose WiSE-OD, a weight-space ensembling method with two variants:
WiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and
WiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across
three RGB-pretrained detectors and two robust baselines, WiSE-OD improves both
cross-modality and corruption robustness without any additional training or
inference cost.

</details>


### [30] [MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition](https://arxiv.org/abs/2507.18929)
*Jian Chen,Yuxuan Hu,Haifeng Lu,Wei Wang,Min Yang,Chengming Li,Xiping Hu*

Main category: cs.CV

TL;DR: The paper introduces MGHFT, a multi-granularity hierarchical fusion transformer, to improve sticker emotion recognition by leveraging multi-view textual context and hierarchical visual-textual fusion.


<details>
  <summary>Details</summary>
Motivation: Sticker emotion understanding is challenging due to its reliance on multi-view information like background knowledge and stylistic cues, which existing visual models struggle with.

Method: Proposes MGHFT, using Multimodal Large Language Models for multi-view textual context and a hierarchical fusion strategy with a pyramid visual transformer. Includes contrastive learning and attention mechanisms for feature fusion.

Result: MGHFT outperforms existing methods, achieving 5.4% higher F1 and 4.0% higher accuracy on public datasets.

Conclusion: MGHFT effectively integrates multi-view textual and visual features, enhancing sticker emotion recognition with significant performance improvements.

Abstract: Although pre-trained visual models with text have demonstrated strong
capabilities in visual feature extraction, sticker emotion understanding
remains challenging due to its reliance on multi-view information, such as
background knowledge and stylistic cues. To address this, we propose a novel
multi-granularity hierarchical fusion transformer (MGHFT), with a multi-view
sticker interpreter based on Multimodal Large Language Models. Specifically,
inspired by the human ability to interpret sticker emotions from multiple
views, we first use Multimodal Large Language Models to interpret stickers by
providing rich textual context via multi-view descriptions. Then, we design a
hierarchical fusion strategy to fuse the textual context into visual
understanding, which builds upon a pyramid visual transformer to extract both
global and local sticker features at multiple stages. Through contrastive
learning and attention mechanisms, textual features are injected at different
stages of the visual backbone, enhancing the fusion of global- and
local-granularity visual semantics with textual guidance. Finally, we introduce
a text-guided fusion attention mechanism to effectively integrate the overall
multimodal features, enhancing semantic understanding. Extensive experiments on
2 public sticker emotion datasets demonstrate that MGHFT significantly
outperforms existing sticker emotion recognition approaches, achieving higher
accuracy and more fine-grained emotion recognition. Compared to the best
pre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%
on F1 and 4.0% on accuracy. The code is released at
https://github.com/cccccj-03/MGHFT_ACMMM2025.

</details>


### [31] [PDT: Point Distribution Transformation with Diffusion Models](https://arxiv.org/abs/2507.18939)
*Jionghao Wang,Cheng Lin,Yuan Liu,Rui Xu,Zhiyang Dou,Xiao-Xiao Long,Hao-Xiang Guo,Taku Komura,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: PDT is a framework using diffusion models to transform unstructured point clouds into semantically meaningful distributions for 3D geometry processing.


<details>
  <summary>Details</summary>
Motivation: Extracting structured information from unstructured point clouds is under-explored, limiting their semantic utility.

Method: PDT employs diffusion models with novel architecture and learning to correlate source and target distributions via denoising.

Result: The method transforms point clouds into structured outputs like keypoints, joints, and feature lines, capturing geometric and semantic features.

Conclusion: PDT offers a powerful tool for tasks requiring structured point distributions, with promising experimental results.

Abstract: Point-based representations have consistently played a vital role in
geometric data structures. Most point cloud learning and processing methods
typically leverage the unordered and unconstrained nature to represent the
underlying geometry of 3D shapes. However, how to extract meaningful structural
information from unstructured point cloud distributions and transform them into
semantically meaningful point distributions remains an under-explored problem.
We present PDT, a novel framework for point distribution transformation with
diffusion models. Given a set of input points, PDT learns to transform the
point set from its original geometric distribution into a target distribution
that is semantically meaningful. Our method utilizes diffusion models with
novel architecture and learning strategy, which effectively correlates the
source and the target distribution through a denoising process. Through
extensive experiments, we show that our method successfully transforms input
point clouds into various forms of structured outputs - ranging from
surface-aligned keypoints, and inner sparse joints to continuous feature lines.
The results showcase our framework's ability to capture both geometric and
semantic features, offering a powerful tool for various 3D geometry processing
tasks where structured point distributions are desired. Code will be available
at this link: https://github.com/shanemankiw/PDT.

</details>


### [32] [Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation](https://arxiv.org/abs/2507.18944)
*Guanyi Qin,Ziyue Wang,Daiyun Shen,Haofeng Liu,Hantao Zhou,Junde Wu,Runze Hu,Yueming Jin*

Main category: cs.CV

TL;DR: OASIS is a novel SVOS method that improves segmentation accuracy and handles occlusion by refining object boundaries and using evidential learning for uncertainty estimation, achieving high performance and real-time speed.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in SVOS, such as occlusion and object interactions, while meeting real-time processing needs for downstream applications.

Method: Proposes OASIS with a lightweight structure refinement module combining Canny filter edge priors and stored object features, plus evidential learning for uncertainty.

Result: Achieves F values of 91.6 (DAVIS-17) and G values of 86.6 (YouTubeVOS 2019) with 48 FPS speed.

Conclusion: OASIS outperforms state-of-the-art methods in accuracy and speed, making it suitable for real-world applications.

Abstract: Given an object mask, Semi-supervised Video Object Segmentation (SVOS)
technique aims to track and segment the object across video frames, serving as
a fundamental task in computer vision. Although recent memory-based methods
demonstrate potential, they often struggle with scenes involving occlusion,
particularly in handling object interactions and high feature similarity. To
address these issues and meet the real-time processing requirements of
downstream applications, in this paper, we propose a novel bOundary Amendment
video object Segmentation method with Inherent Structure refinement, hereby
named OASIS. Specifically, a lightweight structure refinement module is
proposed to enhance segmentation accuracy. With the fusion of rough edge priors
captured by the Canny filter and stored object features, the module can
generate an object-level structure map and refine the representations by
highlighting boundary features. Evidential learning for uncertainty estimation
is introduced to further address challenges in occluded regions. The proposed
method, OASIS, maintains an efficient design, yet extensive experiments on
challenging benchmarks demonstrate its superior performance and competitive
inference speed compared to other state-of-the-art methods, i.e., achieving the
F values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6
(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive
speed of 48 FPS on DAVIS.

</details>


### [33] [PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection](https://arxiv.org/abs/2507.18958)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Chengju Zhou,Minhua Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: The paper introduces 'PerioXrays', a large-scale annotated dataset for apical periodontitis, and proposes 'PerioDet', a detection method with BDA and IDC mechanisms to improve automated diagnosis.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of large-scale datasets for automated apical periodontitis diagnosis, hindering CAD system development.

Method: Proposes PerioDet, combining Background-Denoising Attention (BDA) and IoU-Dynamic Calibration (IDC) to tackle noise and small targets.

Result: PerioDet outperforms on the PerioXrays dataset, and human-computer experiments confirm its clinical utility.

Conclusion: The work advances automated apical periodontitis detection and offers a practical tool for dentists.

Abstract: Apical periodontitis is a prevalent oral pathology that presents significant
public health challenges. Despite advances in automated diagnostic systems
across various medical fields, the development of Computer-Aided Diagnosis
(CAD) applications for apical periodontitis is still constrained by the lack of
a large-scale, high-quality annotated dataset. To address this issue, we
release a large-scale panoramic radiograph benchmark called "PerioXrays",
comprising 3,673 images and 5,662 meticulously annotated instances of apical
periodontitis. To the best of our knowledge, this is the first benchmark
dataset for automated apical periodontitis diagnosis. This paper further
proposes a clinical-oriented apical periodontitis detection (PerioDet)
paradigm, which jointly incorporates Background-Denoising Attention (BDA) and
IoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by
background noise and small targets in automated detection. Extensive
experiments on the PerioXrays dataset demonstrate the superiority of PerioDet
in advancing automated apical periodontitis detection. Additionally, a
well-designed human-computer collaborative experiment underscores the clinical
applicability of our method as an auxiliary diagnostic tool for professional
dentists.

</details>


### [34] [YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study](https://arxiv.org/abs/2507.18966)
*Saraa Al-Saddik,Manna Elizabeth Philip,Ali Haidar*

Main category: cs.CV

TL;DR: The study evaluates deep learning models (YOLO-v11, YOLO-World, YOLO-Classification) for vehicle attribute identification, achieving high accuracy with multi-view inference (MVI) on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate vehicle attribute identification is crucial for law enforcement and intelligence applications.

Method: Three YOLO-based models were tested on a large dataset (100,000+ images) for make, shape, and color prediction, using MVI to enhance performance.

Result: Best accuracies: 93.70% (make), 82.86% (shape), 85.19% (color), and 94.86% (color-binary). YOLO-v11 and YOLO-World outperformed classification-only models.

Conclusion: MVI is essential for usable models in complex datasets. Smaller YOLO variants offer efficiency benefits for real-time predictions, providing a robust baseline for real-world applications.

Abstract: Accurate identification of vehicle attributes such as make, colour, and shape
is critical for law enforcement and intelligence applications. This study
evaluates the effectiveness of three state-of-the-art deep learning approaches
YOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image
dataset. This dataset was collected under challenging and unconstrained
conditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)
approach was deployed to enhance the performance of the models' predictions. To
conduct the analyses, datasets with 100,000 plus images were created for each
of the three metadata prediction tasks, specifically make, shape and colour.
The models were tested on a separate dataset with 29,937 images belonging to
1809 number plates. Different sets of experiments have been investigated by
varying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,
and 94.86% was achieved with the best performing make, shape, colour, and
colour-binary models respectively. It was concluded that there is a need to use
MVI to get usable models within such complex real-world datasets. Our findings
indicated that the object detection models YOLO-v11 and YOLO-World outperformed
classification-only models in make and shape extraction. Moreover, smaller YOLO
variants perform comparably to larger counterparts, offering substantial
efficiency benefits for real-time predictions. This work provides a robust
baseline for extracting vehicle metadata in real-world scenarios. Such models
can be used in filtering and sorting user queries, minimising the time required
to search large vehicle images datasets.

</details>


### [35] [Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN](https://arxiv.org/abs/2507.18967)
*UMMPK Nawarathne,HMNS Kumari,HMLS Kumari*

Main category: cs.CV

TL;DR: YOLOv8 outperforms other object recognition models (YOLOv7, YOLOv9, YOLOv10, Faster R-CNN) in detecting underwater waste, achieving 80.9% mAP.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of underwater waste is critical for environmental monitoring and cleanup efforts.

Method: Five object recognition models (YOLOv7, YOLOv8, YOLOv9, YOLOv10, Faster R-CNN) were trained and tested on a diverse underwater waste dataset.

Result: YOLOv8 achieved the highest mean Average Precision (mAP) of 80.9%, attributed to its advanced architecture.

Conclusion: YOLOv8 is a promising tool for enhancing underwater pollution detection and cleanup scalability.

Abstract: Underwater pollution is one of today's most significant environmental
concerns, with vast volumes of garbage found in seas, rivers, and landscapes
around the world. Accurate detection of these waste materials is crucial for
successful waste management, environmental monitoring, and mitigation
strategies. In this study, we investigated the performance of five cutting-edge
object recognition algorithms, namely YOLO (You Only Look Once) models,
including YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional
Neural Network (R-CNN), to identify which model was most effective at
recognizing materials in underwater situations. The models were thoroughly
trained and tested on a large dataset containing fifteen different classes
under diverse conditions, such as low visibility and variable depths. From the
above-mentioned models, YOLOv8 outperformed the others, with a mean Average
Precision (mAP) of 80.9%, indicating a significant performance. This increased
performance is attributed to YOLOv8's architecture, which incorporates advanced
features such as improved anchor-free mechanisms and self-supervised learning,
allowing for more precise and efficient recognition of items in a variety of
settings. These findings highlight the YOLOv8 model's potential as an effective
tool in the global fight against pollution, improving both the detection
capabilities and scalability of underwater cleanup operations.

</details>


### [36] [AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction](https://arxiv.org/abs/2507.18988)
*Chao Wang,Kejiang Chen,Zijin Yang,Yaofei Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: AEDR is a training-free attribution method for generative models, improving accuracy and efficiency by using double-reconstruction and image homogeneity calibration.


<details>
  <summary>Details</summary>
Motivation: Addressing security concerns from photorealistic image generation by improving attribution accuracy and reducing computational costs.

Method: AEDR performs two consecutive reconstructions using the model's autoencoder, using the loss ratio as the attribution signal, calibrated by image homogeneity.

Result: AEDR achieves 25.5% higher accuracy than existing methods and requires only 1% of the computational time.

Conclusion: AEDR offers a scalable and efficient solution for tracing the origin of images generated by SOTA models.

Abstract: The rapid advancement of image-generation technologies has made it possible
for anyone to create photorealistic images using generative models, raising
significant security concerns. To mitigate malicious use, tracing the origin of
such images is essential. Reconstruction-based attribution methods offer a
promising solution, but they often suffer from reduced accuracy and high
computational costs when applied to state-of-the-art (SOTA) models. To address
these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel
training-free attribution method designed for generative models with continuous
autoencoders. Unlike existing reconstruction-based approaches that rely on the
value of a single reconstruction loss, AEDR performs two consecutive
reconstructions using the model's autoencoder, and adopts the ratio of these
two reconstruction losses as the attribution signal. This signal is further
calibrated using the image homogeneity metric to improve accuracy, which
inherently cancels out absolute biases caused by image complexity, with
autoencoder-based reconstruction ensuring superior computational efficiency.
Experiments on eight top latent diffusion models show that AEDR achieves 25.5%
higher attribution accuracy than existing reconstruction-based methods, while
requiring only 1% of the computational time.

</details>


### [37] [UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis](https://arxiv.org/abs/2507.18997)
*Zixiang Ai,Zhenyu Cui,Yuxin Peng,Jiahuan Zhou*

Main category: cs.CV

TL;DR: A unified point-level prompting method is proposed to address noise and incompleteness in point clouds, enhancing downstream tasks efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods for point cloud enhancement (denoising and completion) are isolated from downstream tasks, limiting their real-world applicability and feature preservation.

Method: Introduces a Rectification Prompter for noise filtering and a Completion Prompter for robustness, unified by a Shape-Aware Unit for downstream analysis.

Result: Superior performance on noisy and incomplete point cloud data across four datasets compared to state-of-the-art methods.

Conclusion: The proposed method effectively unifies point cloud enhancement and downstream analysis, improving robustness and parameter efficiency.

Abstract: Pre-trained point cloud analysis models have shown promising advancements in
various downstream tasks, yet their effectiveness is typically suffering from
low-quality point cloud (i.e., noise and incompleteness), which is a common
issue in real scenarios due to casual object occlusions and unsatisfactory data
collected by 3D sensors. To this end, existing methods focus on enhancing point
cloud quality by developing dedicated denoising and completion models. However,
due to the isolation between the point cloud enhancement and downstream tasks,
these methods fail to work in various real-world domains. In addition, the
conflicting objectives between denoising and completing tasks further limit the
ensemble paradigm to preserve critical geometric features. To tackle the above
challenges, we propose a unified point-level prompting method that reformulates
point cloud denoising and completion as a prompting mechanism, enabling robust
analysis in a parameter-efficient manner. We start by introducing a
Rectification Prompter to adapt to noisy points through the predicted
rectification vector prompts, effectively filtering noise while preserving
intricate geometric features essential for accurate analysis. Sequentially, we
further incorporate a Completion Prompter to generate auxiliary point prompts
based on the rectified point clouds, facilitating their robustness and
adaptability. Finally, a Shape-Aware Unit module is exploited to efficiently
unify and capture the filtered geometric features for the downstream point
cloud analysis.Extensive experiments on four datasets demonstrate the
superiority and robustness of our method when handling noisy and incomplete
point cloud data against existing state-of-the-art methods. Our code is
released at https://github.com/zhoujiahuan1991/ICCV2025-UPP.

</details>


### [38] [GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution](https://arxiv.org/abs/2507.18998)
*Yongsong Huang,Tomo Miyazaki,Xiaofeng Liu,Shinichiro Omachi*

Main category: cs.CV

TL;DR: GPSMamba improves infrared image super-resolution by integrating non-local context and non-causal supervision to overcome 1D causal scanning limitations.


<details>
  <summary>Details</summary>
Motivation: Infrared images suffer from low contrast and sparse textures, requiring robust long-range modeling for global coherence.

Method: Proposes GPSMamba with Adaptive Semantic-Frequency State Space Module (ASF-SSM) and Thermal-Spectral Attention and Phase Consistency Loss for non-causal supervision.

Result: GPSMamba achieves state-of-the-art performance in infrared image restoration.

Conclusion: GPSMamba offers a systematic solution to causal modeling limitations, validated by superior results.

Abstract: Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and
sparse textures of infrared data, requiring robust long-range modeling to
maintain global coherence. While State-Space Models like Mamba offer
proficiency in modeling long-range dependencies for this task, their inherent
1D causal scanning mechanism fragments the global context of 2D images,
hindering fine-detail restoration. To address this, we propose Global Phase and
Spectral Prompt-guided Mamba (GPSMamba), a framework that synergizes
architectural guidance with non-causal supervision. First, our Adaptive
Semantic-Frequency State Space Module (ASF-SSM) injects a fused
semantic-frequency prompt directly into the Mamba block, integrating non-local
context to guide reconstruction. Then, a novel Thermal-Spectral Attention and
Phase Consistency Loss provides explicit, non-causal supervision to enforce
global structural and spectral fidelity. By combining these two innovations,
our work presents a systematic strategy to mitigate the limitations of causal
modeling. Extensive experiments demonstrate that GPSMamba achieves
state-of-the-art performance, validating our approach as a powerful new
paradigm for infrared image restoration. Code is available at
https://github.com/yongsongH/GPSMamba.

</details>


### [39] [Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment](https://arxiv.org/abs/2507.19002)
*Ying Ba,Tianyu Zhang,Yalong Bai,Wenyi Mo,Tao Liang,Bing Su,Ji-Rong Wen*

Main category: cs.CV

TL;DR: The paper introduces ICT and HP scores to better evaluate image generation systems, addressing flaws in current reward models and improving alignment with human aesthetic preferences.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation frameworks for image generation systems fail to align with human aesthetic preferences, often penalizing detailed and high-quality images.

Method: Proposes ICT score for text-image alignment and HP score for aesthetic quality, training models using CLIP and BLIP architectures.

Result: The new evaluation model improves scoring accuracy by over 10% and enhances text-to-image model optimization.

Conclusion: The study advances image generation evaluation toward higher-order human aesthetic preferences, supported by theoretical and empirical evidence.

Abstract: Contemporary image generation systems have achieved high fidelity and
superior aesthetic quality beyond basic text-image alignment. However, existing
evaluation frameworks have failed to evolve in parallel. This study reveals
that human preference reward models fine-tuned based on CLIP and BLIP
architectures have inherent flaws: they inappropriately assign low scores to
images with rich details and high aesthetic value, creating a significant
discrepancy with actual human aesthetic preferences. To address this issue, we
design a novel evaluation score, ICT (Image-Contained-Text) score, that
achieves and surpasses the objectives of text-image alignment by assessing the
degree to which images represent textual content. Building upon this
foundation, we further train an HP (High-Preference) score model using solely
the image modality to enhance image aesthetics and detail quality while
maintaining text-image alignment. Experiments demonstrate that the proposed
evaluation model improves scoring accuracy by over 10\% compared to existing
methods, and achieves significant results in optimizing state-of-the-art
text-to-image models. This research provides theoretical and empirical support
for evolving image generation technology toward higher-order human aesthetic
preferences. Code is available at https://github.com/BarretBa/ICTHP.

</details>


### [40] [MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment](https://arxiv.org/abs/2507.19004)
*Siyi Xun,Yue Sun,Jingkun Chen,Zitong Yu,Tong Tong,Xiaohong Liu,Mingxiang Wu,Tao Tan*

Main category: cs.CV

TL;DR: MedIQA is a foundation model for medical image quality assessment (IQA) that outperforms existing methods by handling diverse modalities and clinical scenarios.


<details>
  <summary>Details</summary>
Motivation: The need for precise and automated IQA in medical imaging to ensure diagnostic accuracy, as existing methods lack generalization across modalities.

Method: Developed a large-scale multi-modality dataset with manual annotations, integrated a salient slice assessment module, and used an automatic prompt strategy for alignment.

Result: MedIQA significantly outperforms baselines in multiple downstream tasks.

Conclusion: MedIQA provides a scalable framework for medical IQA, improving diagnostic workflows and clinical decision-making.

Abstract: Rapid advances in medical imaging technology underscore the critical need for
precise and automated image quality assessment (IQA) to ensure diagnostic
accuracy. Existing medical IQA methods, however, struggle to generalize across
diverse modalities and clinical scenarios. In response, we introduce MedIQA,
the first comprehensive foundation model for medical IQA, designed to handle
variability in image dimensions, modalities, anatomical regions, and types. We
developed a large-scale multi-modality dataset with plentiful manually
annotated quality scores to support this. Our model integrates a salient slice
assessment module to focus on diagnostically relevant regions feature retrieval
and employs an automatic prompt strategy that aligns upstream physical
parameter pre-training with downstream expert annotation fine-tuning. Extensive
experiments demonstrate that MedIQA significantly outperforms baselines in
multiple downstream tasks, establishing a scalable framework for medical IQA
and advancing diagnostic workflows and clinical decision-making.

</details>


### [41] [A Survey of Multimodal Hallucination Evaluation and Detection](https://arxiv.org/abs/2507.19024)
*Zhiyuan Chen,Yuecong Min,Jie Zhang,Bei Yan,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: This survey reviews hallucination in Multi-modal Large Language Models (MLLMs), proposing a taxonomy, evaluating benchmarks, and summarizing detection methods, while identifying limitations and future directions.


<details>
  <summary>Details</summary>
Motivation: MLLMs integrate visual and textual information but suffer from hallucination, producing plausible but incorrect content. This survey aims to address this issue by reviewing evaluation benchmarks and detection methods.

Method: The paper proposes a taxonomy of hallucination (faithfulness and factuality), reviews benchmarks for T2I and I2T tasks, and summarizes detection methods.

Result: The survey highlights limitations in current benchmarks and detection methods, emphasizing the need for improved evaluation and practical detection tools.

Conclusion: Future research should focus on refining benchmarks and detection methods to mitigate hallucination in MLLMs.

Abstract: Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm
for integrating visual and textual information, supporting a wide range of
multi-modal tasks. However, these models often suffer from hallucination,
producing content that appears plausible but contradicts the input content or
established world knowledge. This survey offers an in-depth review of
hallucination evaluation benchmarks and detection methods across Image-to-Text
(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose
a taxonomy of hallucination based on faithfulness and factuality, incorporating
the common types of hallucinations observed in practice. Then we provide an
overview of existing hallucination evaluation benchmarks for both T2I and I2T
tasks, highlighting their construction process, evaluation objectives, and
employed metrics. Furthermore, we summarize recent advances in hallucination
detection methods, which aims to identify hallucinated content at the instance
level and serve as a practical complement of benchmark-based evaluation.
Finally, we highlight key limitations in current benchmarks and detection
methods, and outline potential directions for future research.

</details>


### [42] [A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation](https://arxiv.org/abs/2507.19045)
*Yufei Ma,Hanwen Zhang,Qiya Yang,Guibo Luo,Yuesheng Zhu*

Main category: cs.CV

TL;DR: A modified OSFL framework with FG-RF and DLKD improves training efficiency and privacy in healthcare, outperforming multi-round FL methods by up to 21.73%.


<details>
  <summary>Details</summary>
Motivation: Existing OSFL methods in healthcare suffer from low efficiency, privacy risks, and challenges with non-IID data.

Method: Proposes FG-RF for feature-level image synthesis and DLKD for handling non-IID data during aggregation.

Result: Achieves up to 21.73% improvement over multi-round FL and reduces privacy leakage risks.

Conclusion: The new framework effectively addresses efficiency, privacy, and non-IID challenges in OSFL.

Abstract: In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted
increasing attention due to its low communication overhead, requiring only a
single round of transmission. However, existing generative model-based OSFL
methods suffer from low training efficiency and potential privacy leakage in
the healthcare domain. Additionally, achieving convergence within a single
round of model aggregation is challenging under non-Independent and Identically
Distributed (non-IID) data. To address these challenges, in this paper a
modified OSFL framework is proposed, in which a new Feature-Guided Rectified
Flow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation
method are developed. FG-RF on the client side accelerates generative modeling
in medical imaging scenarios while preserving privacy by synthesizing
feature-level images rather than pixel-level images. To handle non-IID
distributions, DLKD enables the global student model to simultaneously mimic
the output logits and align the intermediate-layer features of client-side
teacher models during aggregation. Experimental results on three non-IID
medical imaging datasets show that our new framework and method outperform
multi-round federated learning approaches, achieving up to 21.73% improvement,
and exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our
experiments demonstrate that feature-level synthetic images significantly
reduce privacy leakage risks compared to pixel-level synthetic images.

</details>


### [43] [Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding](https://arxiv.org/abs/2507.19052)
*Hamid Abdollahi,Amir Hossein Mansouri Majoumerd,Amir Hossein Bagheri Baboukani,Amir Abolfazl Suratgar,Mohammad Bagher Menhaj*

Main category: cs.CV

TL;DR: The paper explores brain encoding models using advanced visual and auditory feature extractors, revealing a trade-off between model complexity and generalization, with simpler models performing better on out-of-distribution data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting brain activity in response to naturalistic, multimodal stimuli and test the generalization of encoding models to novel contexts.

Method: Developed brain encoding models using X-CLIP (visual) and Whisper (auditory) feature extractors, evaluated on in-distribution and out-of-distribution data.

Result: Higher-capacity models excelled on in-distribution data, but simpler linear models were more robust on out-of-distribution data. Linguistic features did not improve accuracy.

Conclusion: Rigorous out-of-distribution testing is crucial for robust neuro-AI models, and model architecture, stimulus characteristics, and sensory hierarchies shape neural encoding.

Abstract: Predicting brain activity in response to naturalistic, multimodal stimuli is
a key challenge in computational neuroscience. While encoding models are
becoming more powerful, their ability to generalize to truly novel contexts
remains a critical, often untested, question. In this work, we developed brain
encoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper)
feature extractors and rigorously evaluated them on both in-distribution (ID)
and diverse out-of-distribution (OOD) data. Our results reveal a fundamental
trade-off between model complexity and generalization: a higher-capacity
attention-based model excelled on ID data, but a simpler linear model was more
robust, outperforming a competitive baseline by 18\% on the OOD set.
Intriguingly, we found that linguistic features did not improve predictive
accuracy, suggesting that for familiar languages, neural encoding may be
dominated by the continuous visual and auditory streams over redundant textual
information. Spatially, our approach showed marked performance gains in the
auditory cortex, underscoring the benefit of high-fidelity speech
representations. Collectively, our findings demonstrate that rigorous OOD
testing is essential for building robust neuro-AI models and provides nuanced
insights into how model architecture, stimulus characteristics, and sensory
hierarchies shape the neural encoding of our rich, multimodal world.

</details>


### [44] [Closing the Modality Gap for Mixed Modality Search](https://arxiv.org/abs/2507.19054)
*Binxu Li,Yuhui Zhang,Xiaohan Wang,Weixin Liang,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: GR-CLIP, a lightweight post-hoc calibration method, addresses the modality gap in CLIP's embedding space, improving mixed modality search performance significantly.


<details>
  <summary>Details</summary>
Motivation: Mixed modality search is important but underexplored, and contrastive vision-language models like CLIP exhibit a modality gap, limiting their effectiveness.

Method: Proposes GR-CLIP, a post-hoc calibration method to remove the modality gap in CLIP's embedding space.

Result: GR-CLIP improves NDCG@10 by up to 26 percentage points over CLIP and outperforms other models with 75x less compute.

Conclusion: GR-CLIP effectively addresses the modality gap, enhancing mixed modality search performance efficiently.

Abstract: Mixed modality search -- retrieving information across a heterogeneous corpus
composed of images, texts, and multimodal documents -- is an important yet
underexplored real-world application. In this work, we investigate how
contrastive vision-language models, such as CLIP, perform on the mixed modality
search task. Our analysis reveals a critical limitation: these models exhibit a
pronounced modality gap in the embedding space, where image and text embeddings
form distinct clusters, leading to intra-modal ranking bias and inter-modal
fusion failure. To address this issue, we propose GR-CLIP, a lightweight
post-hoc calibration method that removes the modality gap in CLIP's embedding
space. Evaluated on MixBench -- the first benchmark specifically designed for
mixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points
over CLIP, surpasses recent vision-language generative embedding models by 4
percentage points, while using 75x less compute.

</details>


### [45] [ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment](https://arxiv.org/abs/2507.19058)
*Chong Xia,Shengjun Zhang,Fangfu Liu,Chang Liu,Khodchaphun Hirunyaratsameewong,Yueqi Duan*

Main category: cs.CV

TL;DR: ScenePainter addresses semantic drift in perpetual 3D scene generation by aligning outpainting with scene comprehension using a hierarchical SceneConceptGraph.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene generation suffer from semantic drift due to accumulated deviations in outpainting, limiting coherence in long-range view sequences.

Method: Proposes ScenePainter, leveraging a hierarchical SceneConceptGraph to align outpainting with scene-specific prior and dynamically refine for consistency and diversity.

Result: Overcomes semantic drift, generating more consistent and immersive 3D view sequences.

Conclusion: ScenePainter improves perpetual 3D scene generation by ensuring semantic consistency and enhancing diversity.

Abstract: Perpetual 3D scene generation aims to produce long-range and coherent 3D view
sequences, which is applicable for long-term video synthesis and 3D scene
reconstruction. Existing methods follow a "navigate-and-imagine" fashion and
rely on outpainting for successive view expansion. However, the generated view
sequences suffer from semantic drift issue derived from the accumulated
deviation of the outpainting module. To tackle this challenge, we propose
ScenePainter, a new framework for semantically consistent 3D scene generation,
which aligns the outpainter's scene-specific prior with the comprehension of
the current scene. To be specific, we introduce a hierarchical graph structure
dubbed SceneConceptGraph to construct relations among multi-level scene
concepts, which directs the outpainter for consistent novel views and can be
dynamically refined to enhance diversity. Extensive experiments demonstrate
that our framework overcomes the semantic drift issue and generates more
consistent and immersive 3D view sequences. Project Page:
https://xiac20.github.io/ScenePainter/.

</details>


### [46] [Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization](https://arxiv.org/abs/2507.19059)
*Xiaocheng Fang,Jieyi Cai,Huanyu Liu,Wenxiu Cai,Yishu Liu,Bingzhi Chen*

Main category: cs.CV

TL;DR: Proposes NRQO, combining NT-FPN and PS-RPN, to improve small object detection by reducing noise in FPN and enhancing query quality.


<details>
  <summary>Details</summary>
Motivation: Transformer-based detectors struggle with noise sensitivity in FPN and poor query quality in label assignment for small object detection.

Method: Introduces NT-FPN to preserve feature integrity and PS-RPN for better anchor-ground truth matching via similarity metrics.

Result: NRQO outperforms state-of-the-art baselines in extensive experiments.

Conclusion: NRQO effectively addresses noise and query quality issues, advancing small object detection.

Abstract: Despite advancements in Transformer-based detectors for small object
detection (SOD), recent studies show that these detectors still face challenges
due to inherent noise sensitivity in feature pyramid networks (FPN) and
diminished query quality in existing label assignment strategies. In this
paper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm,
which innovatively incorporates the Noise-Tolerance Feature Pyramid Network
(NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN).
Specifically, NT-FPN mitigates noise during feature fusion in FPN by preserving
spatial and semantic information integrity. Unlike existing label assignment
strategies, PS-RPN generates a sufficient number of high-quality positive
queries by enhancing anchor-ground truth matching through position and shape
similarities, without the need for additional hyperparameters. Extensive
experiments on multiple benchmarks consistently demonstrate the superiority of
NRQO over state-of-the-art baselines.

</details>


### [47] [Negation-Aware Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.19064)
*Haochen Han,Alex Jinpeng Wang,Fangming Liu*

Main category: cs.CV

TL;DR: The paper addresses the challenge of negation understanding in Vision-Language Models (VLMs), proposing a low-resource method called NEAT to adapt models during inference without extensive data or computational costs.


<details>
  <summary>Details</summary>
Motivation: Real-world applications, like medical imaging, require models to identify false or non-existent conditions, but VLMs struggle with negation due to data scarcity and computational demands.

Method: The proposed NEAT method adjusts distribution-related parameters during inference to handle dual-concept shifts between affirmation and negation distributions.

Result: Experiments on negation tasks show NEAT effectively reduces distribution shifts and improves negation understanding.

Conclusion: NEAT offers a sustainable, low-resource solution for negation understanding in VLMs, validated by extensive experiments.

Abstract: In this paper, we study a practical but less-touched problem in
Vision-Language Models (VLMs), \ie, negation understanding. Specifically, many
real-world applications require models to explicitly identify what is false or
non-existent, \eg, radiologists may search for images that exclude specific
conditions. Despite the impressive transferability of VLMs through large-scale
training, they suffer from a critical limitation that fails to handle negation.
To address this challenge, existing methods attribute its root cause to the
scarcity of negation training data and propose to fine-tune VLMs on massive
data containing explicit negation. Undoubtedly, such data-centric solutions
demand substantial data and computational resources, limiting their sustainable
widespread adoption. To tackle negation in a low-carbon manner, we empirically
observe that the key obstacle lies in the dual-concept shifts between the
affirmation and negation distributions. Therefore, we propose a Negation-Aware
Test-Time Adaptation (NEAT) method to efficiently adjust distribution-related
parameters during inference. In brief, NEAT can reduce distribution shift in
consistent semantics while eliminating false distributional consistency in
unrelated semantics. Extensive experiments on the various negation
understanding tasks verify the effectiveness of the proposed method. The code
is available at https://github.com/hhc1997/NEAT.

</details>


### [48] [Cross-Subject Mind Decoding from Inaccurate Representations](https://arxiv.org/abs/2507.19071)
*Yangyang Xu,Bangzhen Liu,Wenqi Shao,Yong Du,Shengfeng He,Tingting Zhu*

Main category: cs.CV

TL;DR: Proposes a Bidirectional Autoencoder Intertwining framework to improve cross-subject fMRI decoding by addressing sequential errors and enhancing semantic and visual fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail in cross-subject mappings due to cognitive variability and subject-specific differences, leading to error accumulation in reconstructions.

Method: Introduces bidirectional mapping, Subject Bias Modulation Module, Semantic Refinement Module, and Visual Coherence Module, integrated with ControlNet and Stable Diffusion.

Result: Outperforms state-of-the-art methods in qualitative and quantitative evaluations and adapts well to new subjects with minimal training.

Conclusion: The framework effectively improves fMRI decoding accuracy and fidelity, demonstrating strong adaptability.

Abstract: Decoding stimulus images from fMRI signals has advanced with pre-trained
generative models. However, existing methods struggle with cross-subject
mappings due to cognitive variability and subject-specific differences. This
challenge arises from sequential errors, where unidirectional mappings generate
partially inaccurate representations that, when fed into diffusion models,
accumulate errors and degrade reconstruction fidelity. To address this, we
propose the Bidirectional Autoencoder Intertwining framework for accurate
decoded representation prediction. Our approach unifies multiple subjects
through a Subject Bias Modulation Module while leveraging bidirectional mapping
to better capture data distributions for precise representation prediction. To
further enhance fidelity when decoding representations into stimulus images, we
introduce a Semantic Refinement Module to improve semantic representations and
a Visual Coherence Module to mitigate the effects of inaccurate visual
representations. Integrated with ControlNet and Stable Diffusion, our method
outperforms state-of-the-art approaches on benchmark datasets in both
qualitative and quantitative evaluations. Moreover, our framework exhibits
strong adaptability to new subjects with minimal training samples.

</details>


### [49] [SP-Mamba: Spatial-Perception State Space Model for Unsupervised Medical Anomaly Detection](https://arxiv.org/abs/2507.19076)
*Rui Pan,Ruiying Lu*

Main category: cs.CV

TL;DR: SP-Mamba, a spatial-perception Mamba framework, is introduced for unsupervised medical anomaly detection, leveraging structural regularity and linear computational efficiency to outperform CNN- and transformer-based methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CNNs (long-range dependencies) and transformers (quadratic complexity) in medical anomaly detection, while exploiting the consistent structural patterns in radiography.

Method: SP-Mamba combines window-sliding prototype learning and Circular-Hilbert scanning-based Mamba to utilize anatomical patterns and spatial information, along with anomaly map characteristics for improved detection.

Result: Extensive experiments on three benchmarks show SP-Mamba achieves state-of-the-art performance, confirming its efficacy and robustness.

Conclusion: SP-Mamba is a promising alternative for medical anomaly detection, offering superior performance and computational efficiency.

Abstract: Radiography imaging protocols target on specific anatomical regions,
resulting in highly consistent images with recurrent structural patterns across
patients. Recent advances in medical anomaly detection have demonstrated the
effectiveness of CNN- and transformer-based approaches. However, CNNs exhibit
limitations in capturing long-range dependencies, while transformers suffer
from quadratic computational complexity. In contrast, Mamba-based models,
leveraging superior long-range modeling, structural feature extraction, and
linear computational efficiency, have emerged as a promising alternative. To
capitalize on the inherent structural regularity of medical images, this study
introduces SP-Mamba, a spatial-perception Mamba framework for unsupervised
medical anomaly detection. The window-sliding prototype learning and
Circular-Hilbert scanning-based Mamba are introduced to better exploit
consistent anatomical patterns and leverage spatial information for medical
anomaly detection. Furthermore, we excavate the concentration and contrast
characteristics of anomaly maps for improving anomaly detection. Extensive
experiments on three diverse medical anomaly detection benchmarks confirm the
proposed method's state-of-the-art performance, validating its efficacy and
robustness. The code is available at https://github.com/Ray-RuiPan/SP-Mamba.

</details>


### [50] [Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts](https://arxiv.org/abs/2507.19077)
*Yangyang Xu,Xi Ye,Duo Su*

Main category: cs.CV

TL;DR: A novel Fine-Grained Mixture of Experts (FGMoE) architecture improves multi-task learning for dense prediction by combining intra-task, shared, and global experts with fine-tuning, outperforming existing models with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Balancing shared representations and task-specific specialization in multi-task learning (MTL) for dense prediction remains challenging.

Method: Introduces FGMoE with intra-task, shared, and global experts, along with fine-tuning for parameter efficiency.

Result: FGMoE outperforms current MoE-based MTL models on NYUD-v2 and PASCAL-Context datasets with fewer parameters.

Conclusion: FGMoE effectively addresses MTL challenges by enabling fine-grained decomposition and adaptive knowledge transfer.

Abstract: Multi-task learning (MTL) for dense prediction has shown promising results
but still faces challenges in balancing shared representations with
task-specific specialization. In this paper, we introduce a novel Fine-Grained
Mixture of Experts (FGMoE) architecture that explores MoE-based MTL models
through a combination of three key innovations and fine-tuning. First, we
propose intra-task experts that partition along intermediate hidden dimensions
of MLPs, enabling finer decomposition of task information while maintaining
parameter efficiency. Second, we introduce shared experts that consolidate
common information across different contexts of the same task, reducing
redundancy, and allowing routing experts to focus on unique aspects. Third, we
design a global expert that facilitates adaptive knowledge transfer across
tasks based on both input feature and task requirements, promoting beneficial
information sharing while preventing harmful interference. In addition, we use
the fine-tuning approach to improve parameter efficiency only by training the
parameters of the decoder. Extensive experimental results show that the
proposed FGMoE uses fewer parameters and significantly outperforms current
MoE-based competitive MTL models on two dense prediction datasets
(\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.

</details>


### [51] [MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching](https://arxiv.org/abs/2507.19098)
*Francisco Caetano,Lemar Abdi,Christiaan Viviers,Amaan Valiuddin,Fons van der Sommen*

Main category: cs.CV

TL;DR: MedSymmFlow is a hybrid model combining classification, generation, and uncertainty quantification for medical imaging, outperforming baselines in accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate predictions and reliable uncertainty estimates in high-stakes medical image classification.

Method: Uses Symmetrical Flow Matching with latent-space scaling and semantic mask conditioning for diagnostic relevance.

Result: Matches or exceeds baseline performance in classification accuracy and AUC, with validated uncertainty estimates.

Conclusion: MedSymmFlow effectively unifies classification, generation, and uncertainty quantification, proving valuable for medical imaging.

Abstract: Reliable medical image classification requires accurate predictions and
well-calibrated uncertainty estimates, especially in high-stakes clinical
settings. This work presents MedSymmFlow, a generative-discriminative hybrid
model built on Symmetrical Flow Matching, designed to unify classification,
generation, and uncertainty quantification in medical imaging. MedSymmFlow
leverages a latent-space formulation that scales to high-resolution inputs and
introduces a semantic mask conditioning mechanism to enhance diagnostic
relevance. Unlike standard discriminative models, it naturally estimates
uncertainty through its generative sampling process. The model is evaluated on
four MedMNIST datasets, covering a range of modalities and pathologies. The
results show that MedSymmFlow matches or exceeds the performance of established
baselines in classification accuracy and AUC, while also delivering reliable
uncertainty estimates validated by performance improvements under selective
prediction.

</details>


### [52] [LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2507.19110)
*Zhihui Guo,Xin Man,Hui Xu,Jie Shao*

Main category: cs.CV

TL;DR: LISA reduces object hallucinations in MLLMs by hierarchical modulation and multi-layer fusion, improving consistency and performance.


<details>
  <summary>Details</summary>
Motivation: MLLMs often hallucinate objects not in images, degrading reliability. LISA aims to enhance generation consistency by leveraging the functional hierarchy of MLLMs.

Method: LISA uses zone-specific spectral modulation to stabilize attention and token-level logit fusion via anchor-based routing for adaptive integration during decoding.

Result: LISA reduces hallucinations by up to 53.6% and improves POPE F1 by 4.5%, showing strong generalization.

Conclusion: LISA is a plug-and-play solution that effectively mitigates hallucinations in MLLMs while maintaining performance.

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks such
as image captioning but remain prone to object hallucinations, where they
describe objects that do not appear in the image. To mitigate this, we propose
\textbf{LISA}, a \textbf{L}ayer-wise \textbf{I}ntegration and
\textbf{S}uppression \textbf{A}pproach that enhances generation consistency
through hierarchical modulation and multi-layer fusion. LISA leverages the
functional hierarchy within MLLMs, where shallow layers provide visual
grounding, middle layers encode semantics, and deep layers tend to amplify
spurious signals. First, zone-specific spectral modulation stabilizes attention
by suppressing over-amplified activations in deeper layers while preserving
alignment cues in earlier layers. Second, token-level logits from selected
layers are fused via anchor-based routing, with token-wise anchor selection and
soft logit fusion enabling adaptive integration during decoding. LISA is fully
\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,
including Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces
hallucinations by up to 53.6\% in $\mathrm{CHAIR}_I$ and improves POPE F1 by
4.5\%, demonstrating strong generalization across models and tasks.

</details>


### [53] [Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching](https://arxiv.org/abs/2507.19118)
*Abu Sadat Mohammad Salehin Amit,Xiaoli Zhang,Md Masum Billa Shagar,Zhaojun Liu,Xiongfei Li,Fanlong Meng*

Main category: cs.CV

TL;DR: The paper proposes a Cross Spatial Temporal Fusion (CSTF) mechanism for robust cross-modal remote sensing image matching, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively capture cross-modal similarities due to geometric and radiometric differences in multimodal images.

Method: CSTF integrates scale-invariant keypoints and reformulates matching as a classification task using SoftMax and FCN layers.

Result: Achieves 90.99% mAP on HRSC2016 and 90.86% on DOTA, with 12.5 FPS inference speed.

Conclusion: CSTF enhances downstream applications like object detection by improving cross-modal feature matching.

Abstract: Effectively describing features for cross-modal remote sensing image matching
remains a challenging task due to the significant geometric and radiometric
differences between multimodal images. Existing methods primarily extract
features at the fully connected layer but often fail to capture cross-modal
similarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)
mechanism that enhances feature representation by integrating scale-invariant
keypoints detected independently in both reference and query images. Our
approach improves feature matching in two ways: First, by creating
correspondence maps that leverage information from multiple image regions
simultaneously, and second, by reformulating the similarity matching process as
a classification task using SoftMax and Fully Convolutional Network (FCN)
layers. This dual approach enables CSTF to maintain sensitivity to distinctive
local features while incorporating broader contextual information, resulting in
robust matching across diverse remote sensing modalities. To demonstrate the
practical utility of improved feature matching, we evaluate CSTF on object
detection tasks using the HRSC2016 and DOTA benchmark datasets. Our method
achieves state-of-theart performance with an average mAP of 90.99% on HRSC2016
and 90.86% on DOTA, outperforming existing models. The CSTF model maintains
computational efficiency with an inference speed of 12.5 FPS. These results
validate that our approach to crossmodal feature matching directly enhances
downstream remote sensing applications such as object detection.

</details>


### [54] [PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction](https://arxiv.org/abs/2507.19119)
*Yanghong Liu,Xingping Dong,Ming Li,Weixing Zhang,Yidong Lou*

Main category: cs.CV

TL;DR: PatchTraj is a dynamic patch-based framework for pedestrian trajectory prediction, unifying time and frequency domains to improve motion dynamics modeling and long-range dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods inadequately model human motion dynamics and lack interaction between time and frequency domains in trajectory sequences.

Method: Decomposes trajectories into time and frequency components, uses dynamic patch partitioning, adaptive embedding, hierarchical feature aggregation, and cross-modal attention for fusion.

Result: Achieves state-of-the-art performance on ETH-UCY, SDD, NBA, and JRDB datasets.

Conclusion: PatchTraj effectively balances local and long-range dependencies, enhancing trajectory prediction accuracy and efficiency.

Abstract: Pedestrian trajectory prediction is crucial for autonomous driving and
robotics. While existing point-based and grid-based methods expose two key
limitations: insufficiently modeling human motion dynamics, as they fail to
balance local motion details with long-range spatiotemporal dependencies, and
the time representation lacks interaction with the frequency domain in modeling
trajectory sequences. To address these challenges, we propose PatchTraj, a
dynamic patch-based trajectory prediction framework that unifies time-domain
and frequency-domain representations. Specifically, we decompose the trajectory
into raw time sequences and frequency components, employing dynamic patch
partitioning for multi-scale trajectory segmentation to capture hierarchical
motion patterns. Each patch is processed by an adaptive embedding layer with
scale-aware feature extraction, followed by hierarchical feature aggregation to
model both fine-grained and long-range dependencies. The outputs of two
branches interact via cross-modal attention, enabling complementary fusion of
temporal and spectral cues. Finally, a Transformer encoder-decoder integrates
both modalities to autoregressively predict future trajectories. Extensive
experiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method
achieves state-of-the-art performance with high efficiency.

</details>


### [55] [Preserving Topological and Geometric Embeddings for Point Cloud Recovery](https://arxiv.org/abs/2507.19121)
*Kaiyue Zhou,Zelong Tan,Hongxiao Wang,Ya-li Li,Shengjin Wang*

Main category: cs.CV

TL;DR: TopGeoFormer is an end-to-end architecture for point cloud recovery, combining topological and geometric features through InterTwining Attention and optimized losses.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively leverage both topological and geometric attributes in point cloud recovery.

Method: Uses topological embedding, InterTwining Attention, and geometry/topological loss functions for optimization.

Result: Outperforms existing sampling and recovery methods in experiments.

Conclusion: TopGeoFormer effectively integrates topological and geometric features for superior point cloud recovery.

Abstract: Recovering point clouds involves the sequential process of sampling and
restoration, yet existing methods struggle to effectively leverage both
topological and geometric attributes. To address this, we propose an end-to-end
architecture named \textbf{TopGeoFormer}, which maintains these critical
features throughout the sampling and restoration phases. First, we revisit
traditional feature extraction techniques to yield topological embedding using
a continuous mapping of relative relationships between neighboring points, and
integrate it in both phases for preserving the structure of the original space.
Second, we propose the \textbf{InterTwining Attention} to fully merge
topological and geometric embeddings, which queries shape with local awareness
in both phases to form a learnable shape context facilitated with point-wise,
point-shape-wise, and intra-shape features. Third, we introduce a full geometry
loss and a topological constraint loss to optimize the embeddings in both
Euclidean and topological spaces. The geometry loss uses inconsistent matching
between coarse-to-fine generations and targets for reconstructing better
geometric details, and the constraint loss limits embedding variances for
better approximation of the topological space. In experiments, we
comprehensively analyze the circumstances using the conventional and
learning-based sampling/upsampling algorithms. The quantitative and qualitative
results demonstrate that our method significantly outperforms existing sampling
and recovery methods.

</details>


### [56] [MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective](https://arxiv.org/abs/2507.19131)
*Weitian Wang,Rai Shubham,Cecilia De La Parra,Akash Kumar*

Main category: cs.CV

TL;DR: MixA-Q is a mixed-precision activation quantization framework for window-based vision transformers, improving efficiency by leveraging intra-layer activation sparsity.


<details>
  <summary>Details</summary>
Motivation: To enhance the trade-off between model performance and efficiency in quantized vision transformers by exploiting activation sparsity.

Method: MixA-Q separates window computations, assigns lower bit widths to less important windows, and introduces a Two-Branch Swin Block for high- and low-bit precision processing.

Result: Achieves 1.35x speedup without accuracy loss in PTQ, 1.25x speedup in QAT, and 1.53x speedup with minimal mAP drop. Also improves mAP of W4A4 model by 0.7%.

Conclusion: MixA-Q effectively balances efficiency and performance in quantized vision transformers, reducing quantization degradation.

Abstract: In this paper, we propose MixA-Q, a mixed-precision activation quantization
framework that leverages intra-layer activation sparsity (a concept widely
explored in activation pruning methods) for efficient inference of quantized
window-based vision transformers. For a given uniform-bit quantization
configuration, MixA-Q separates the batched window computations within Swin
blocks and assigns a lower bit width to the activations of less important
windows, improving the trade-off between model performance and efficiency. We
introduce a Two-Branch Swin Block that processes activations separately in
high- and low-bit precision, enabling seamless integration of our method with
most quantization-aware training (QAT) and post-training quantization (PTQ)
methods, or with simple modifications. Our experimental evaluations over the
COCO dataset demonstrate that MixA-Q achieves a training-free 1.35x
computational speedup without accuracy loss in PTQ configuration. With QAT,
MixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP
drop by incorporating activation pruning. Notably, by reducing the quantization
error in important regions, our sparsity-aware quantization adaptation improves
the mAP of the quantized W4A4 model (with both weights and activations in 4-bit
precision) by 0.7%, reducing quantization degradation by 24%.

</details>


### [57] [Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation](https://arxiv.org/abs/2507.19140)
*Tianyu Zou,Shengwu Xiong,Ruilin Yao,Yi Rong*

Main category: cs.CV

TL;DR: PAHNet balances conservative prototype learning and aggressive affinity learning for few-shot segmentation, improving accuracy via hybrid modules.


<details>
  <summary>Details</summary>
Motivation: Address the imbalance between conservative predictions of prototype learning and aggressive predictions of affinity learning in few-shot segmentation.

Method: Proposes PAHNet with Prototype-guided Feature Enhancement (PFE) and Attention Score Calibration (ASC) modules to refine affinity learning.

Result: Outperforms recent methods on PASCAL-5$^i$ and COCO-20$^i$ datasets in 1-shot and 5-shot settings.

Conclusion: PAHNet effectively mitigates aggressiveness in affinity learning, enhancing segmentation accuracy.

Abstract: This paper studies the few-shot segmentation (FSS) task, which aims to
segment objects belonging to unseen categories in a query image by learning a
model on a small number of well-annotated support samples. Our analysis of two
mainstream FSS paradigms reveals that the predictions made by prototype
learning methods are usually conservative, while those of affinity learning
methods tend to be more aggressive. This observation motivates us to balance
the conservative and aggressive information captured by these two types of FSS
frameworks so as to improve the segmentation performance. To achieve this, we
propose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which
introduces a Prototype-guided Feature Enhancement (PFE) module and an Attention
Score Calibration (ASC) module in each attention block of an affinity learning
model (called affinity learner). These two modules utilize the predictions
generated by a pre-trained prototype learning model (called prototype
predictor) to enhance the foreground information in support and query image
representations and suppress the mismatched foreground-background (FG-BG)
relationships between them, respectively. In this way, the aggressiveness of
the affinity learner can be effectively mitigated, thereby eventually
increasing the segmentation accuracy of our PAHNet method. Experimental results
show that PAHNet outperforms most recently proposed methods across 1-shot and
5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its
effectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing
Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot
Segmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)

</details>


### [58] [DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2507.19141)
*Jie Chen,Zhangchi Hu,Peixi Wu,Huyue Zhu,Hebei Li,Xiaoyan Sun*

Main category: cs.CV

TL;DR: DASH is a real-time dynamic scene rendering framework using 4D hash encoding and self-supervised decomposition to improve rendering quality and avoid low-rank assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from feature overlap, poor rendering quality, and hash collisions due to low-rank assumptions and redundancy.

Method: DASH uses self-supervised decomposition to separate dynamic/static components, multiresolution 4D hash encoding for dynamics, and spatio-temporal smoothness regularization.

Result: Achieves 264 FPS on a 4090 GPU with enhanced visual quality, outperforming existing methods.

Conclusion: DASH provides a robust solution for real-time dynamic scene rendering with superior performance.

Abstract: Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing
plane-based methods in dynamic Gaussian splatting suffer from an unsuitable
low-rank assumption, causing feature overlap and poor rendering quality.
Although 4D hash encoding provides an explicit representation without low-rank
constraints, directly applying it to the entire dynamic scene leads to
substantial hash collisions and redundancy. To address these challenges, we
present DASH, a real-time dynamic scene rendering framework that employs 4D
hash encoding coupled with self-supervised decomposition. Our approach begins
with a self-supervised decomposition mechanism that separates dynamic and
static components without manual annotations or precomputed masks. Next, we
introduce a multiresolution 4D hash encoder for dynamic elements, providing an
explicit representation that avoids the low-rank assumption. Finally, we
present a spatio-temporal smoothness regularization strategy to mitigate
unstable deformation artifacts. Experiments on real-world datasets demonstrate
that DASH achieves state-of-the-art dynamic rendering performance, exhibiting
enhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.
Code: https://github.com/chenj02/DASH.

</details>


### [59] [Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers](https://arxiv.org/abs/2507.19175)
*Yuki Igaue,Hiroaki Aizawa*

Main category: cs.CV

TL;DR: A patch pruning strategy for vision transformers evaluates patch importance using variance of attention weights across heads, improving efficiency without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computational complexity of multi-head self-attention in vision transformers by identifying and removing redundant patches.

Method: Propose patch pruning based on variance of attention weights across heads, with options for robust statistical measures and overlapping patch embeddings.

Result: Improved throughput while maintaining classification accuracy, especially in fine-tuning scenarios.

Conclusion: The method efficiently reduces computational overhead without compromising performance, offering flexibility with robust measures and overlapping embeddings.

Abstract: Multi-head self-attention is a distinctive feature extraction mechanism of
vision transformers that computes pairwise relationships among all input
patches, contributing significantly to their high performance. However, it is
known to incur a quadratic computational complexity with respect to the number
of patches. One promising approach to address this issue is patch pruning,
which improves computational efficiency by identifying and removing redundant
patches. In this work, we propose a patch pruning strategy that evaluates the
importance of each patch based on the variance of attention weights across
multiple attention heads. This approach is inspired by the design of multi-head
self-attention, which aims to capture diverse attention patterns across
different subspaces of feature representations. The proposed method can be
easily applied during both training and inference, and achieves improved
throughput while maintaining classification accuracy in scenarios such as
fine-tuning with pre-trained models. In addition, we also found that using
robust statistical measures, such as the median absolute deviation in place of
variance, to assess patch importance can similarly lead to strong performance.
Furthermore, by introducing overlapping patch embeddings, our method achieves
better performance with comparable throughput to conventional approaches that
utilize all patches.

</details>


### [60] [Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks](https://arxiv.org/abs/2507.19184)
*Kotha Kartheek,Lingamaneni Gnanesh Chowdary,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: A unified framework for image restoration under various weather conditions (fog, snow, rain) using continual learning, selective kernel fusion, EWC, and cycle-contrastive loss, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single weather conditions, but autonomous driving requires a unified model for diverse weather restoration.

Method: Continual learning framework with selective kernel fusion, EWC for task retention, and cycle-contrastive loss for feature discrimination. Unpaired image restoration reduces data dependency.

Result: Significant improvements in PSNR, SSIM, and perceptual quality on benchmark datasets for dehazing, desnowing, and deraining.

Conclusion: The proposed framework effectively addresses multi-weather image restoration with continual learning, outperforming state-of-the-art methods.

Abstract: Restoration of images contaminated by different adverse weather conditions
such as fog, snow, and rain is a challenging task due to the varying nature of
the weather conditions. Most of the existing methods focus on any one
particular weather conditions. However, for applications such as autonomous
driving, a unified model is necessary to perform restoration of corrupted
images due to different weather conditions. We propose a continual learning
approach to propose a unified framework for image restoration. The proposed
framework integrates three key innovations: (1) Selective Kernel Fusion layers
that dynamically combine global and local features for robust adaptive feature
selection; (2) Elastic Weight Consolidation (EWC) to enable continual learning
and mitigate catastrophic forgetting across multiple restoration tasks; and (3)
a novel Cycle-Contrastive Loss that enhances feature discrimination while
preserving semantic consistency during domain translation. Further, we propose
an unpaired image restoration approach to reduce the dependance of the proposed
approach on the training data. Extensive experiments on standard benchmark
datasets for dehazing, desnowing and deraining tasks demonstrate significant
improvements in PSNR, SSIM, and perceptual quality over the state-of-the-art.

</details>


### [61] [VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions](https://arxiv.org/abs/2507.19188)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Longjun Gao,Yu Xue,Le Wang*

Main category: cs.CV

TL;DR: VisHall3D is a two-stage framework for monocular semantic scene completion, addressing feature entanglement and geometric inconsistency by separating visible reconstruction (vision) and invisible inference (hallucination).


<details>
  <summary>Details</summary>
Motivation: Existing methods suffer from feature entanglement and geometric inconsistency in scene completion tasks.

Method: VisHall3D uses VisFrontierNet for visible region reconstruction and OcclusionMAE for invisible region hallucination with noise injection.

Result: Achieves state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360 benchmarks.

Conclusion: VisHall3D improves reconstruction quality and advances scene understanding for autonomous driving.

Abstract: This paper introduces VisHall3D, a novel two-stage framework for monocular
semantic scene completion that aims to address the issues of feature
entanglement and geometric inconsistency prevalent in existing methods.
VisHall3D decomposes the scene completion task into two stages: reconstructing
the visible regions (vision) and inferring the invisible regions
(hallucination). In the first stage, VisFrontierNet, a visibility-aware
projection module, is introduced to accurately trace the visual frontier while
preserving fine-grained details. In the second stage, OcclusionMAE, a
hallucination network, is employed to generate plausible geometries for the
invisible regions using a noise injection mechanism. By decoupling scene
completion into these two distinct stages, VisHall3D effectively mitigates
feature entanglement and geometric inconsistency, leading to significantly
improved reconstruction quality.
  The effectiveness of VisHall3D is validated through extensive experiments on
two challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D
achieves state-of-the-art performance, outperforming previous methods by a
significant margin and paves the way for more accurate and reliable scene
understanding in autonomous driving and other applications.

</details>


### [62] [Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet](https://arxiv.org/abs/2507.19209)
*Xiaoyu Zhang,Zhifeng Bao,Hai Dong,Ziwei Wang,Jiajun Liu*

Main category: cs.CV

TL;DR: The paper introduces CounterNet, a heatmap-based network for accurate object counting in 3D point cloud data, improving query reliability in autonomous vehicle analytics.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles generate vast point cloud data, but existing methods fail to provide reliable object counts, leading to errors in queries like retrieval, count, and aggregation.

Method: CounterNet uses heatmap-based detection of object centers and a feature map partitioning strategy with overlapping regions. It also employs dynamic model selection per frame.

Result: CounterNet improves counting accuracy by 5% to 20% across object categories in real-world datasets, enhancing query reliability.

Conclusion: CounterNet addresses the challenge of accurate object counting in point cloud data, significantly improving the reliability of query results for autonomous vehicle analytics.

Abstract: Autonomous vehicles generate massive volumes of point cloud data, yet only a
subset is relevant for specific tasks such as collision detection, traffic
analysis, or congestion monitoring. Effectively querying this data is essential
to enable targeted analytics. In this work, we formalize point cloud querying
by defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each
aligned with distinct analytical scenarios. All these queries rely heavily on
accurate object counts to produce meaningful results, making precise object
counting a critical component of query execution. Prior work has focused on
indexing techniques for 2D video data, assuming detection models provide
accurate counting information. However, when applied to 3D point cloud data,
state-of-the-art detection models often fail to generate reliable object
counts, leading to substantial errors in query results. To address this
limitation, we propose CounterNet, a heatmap-based network designed for
accurate object counting in large-scale point cloud data. Rather than focusing
on accurate object localization, CounterNet detects object presence by finding
object centers to improve counting accuracy. We further enhance its performance
with a feature map partitioning strategy using overlapping regions, enabling
better handling of both small and large objects in complex traffic scenes. To
adapt to varying frame characteristics, we introduce a per-frame dynamic model
selection strategy that selects the most effective configuration for each
input. Evaluations on three real-world autonomous vehicle datasets show that
CounterNet improves counting accuracy by 5% to 20% across object categories,
resulting in more reliable query outcomes across all supported query types.

</details>


### [63] [PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction](https://arxiv.org/abs/2507.19213)
*Hanbing Wu,Ping Jiang,Anyang Su,Chenxu Zhao,Tianyu Fu,Minghui Wu,Beiping Tan,Huiying Li*

Main category: cs.CV

TL;DR: The paper introduces SPA-ADV, a dataset for personalized attention in ads, and PRE-MAP, a model using reinforcement learning to predict gaze points, addressing gaps in existing saliency models.


<details>
  <summary>Details</summary>
Motivation: Existing models ignore subjective cognitive diversity in fixation behavior and struggle with precise point predictions due to limitations like hallucinations in MLLMs.

Method: Proposes PRE-MAP, a reinforcement learning-optimized eye-tracking model, and C-GRPO for accurate point predictions, leveraging a large-scale dataset (SPA-ADV).

Result: Experiments show PRE-MAP's effectiveness in predicting personalized attention patterns, outperforming benchmarks.

Conclusion: The work advances personalized attention modeling and offers a robust dataset and method for future research.

Abstract: Visual selective attention, driven by individual preferences, regulates human
prioritization of visual stimuli by bridging subjective cognitive mechanisms
with objective visual elements, thereby steering the semantic interpretation
and hierarchical processing of dynamic visual scenes. However, existing models
and datasets predominantly neglect the influence of subjective cognitive
diversity on fixation behavior. Conventional saliency prediction models,
typically employing segmentation approaches, rely on low-resolution imagery to
generate saliency heatmaps, subsequently upscaled to native resolutions, which
limiting their capacity to capture personalized attention patterns.
Furthermore, MLLMs are constrained by factors such as hallucinations, making it
very costly to strictly adhere to the expected format in tasks involving
multiple point predictions, and achieving precise point positioning is
challenging. To address these limitations, we present Subjective Personalized
Attention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal
dataset capturing gaze behaviors from over 4,500 participants varying in age
and gender with 486 videos. Furthermore, we propose PRE-MAP, a novel
eye-tracking saliency model that characterizes Personalized visual disparities
through Reinforcement learning-optimized Eye-tracking, built upon MLLMs and
guided by Multi-Attribute user profiles to predict Points. To ensure MLLMs
produce prediction points that are both format-correct and spatially accurate,
we introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired
by the variability in eye movement points and Multi-Attribute profiles.
Extensive experiments on SPA-ADV and other benchmarks demonstrate the
effectiveness of our approach. The code and dataset are available at
\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.

</details>


### [64] [Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene](https://arxiv.org/abs/2507.19232)
*Donggeun Lim,Jinseok Bae,Inwoo Hwang,Seungmin Lee,Hwanhee Lee,Young Min Kim*

Main category: cs.CV

TL;DR: A framework for generating lively virtual scenes with multi-human contextual motions using LLMs, addressing human-human and human-scene interactions at scale.


<details>
  <summary>Details</summary>
Motivation: To tackle the complexity of multi-human contextual motion generation, requiring holistic reasoning over dynamic relationships.

Method: Uses an event generator to break scenes into sequential events, synthesizes motions based on spatial guidance, and employs a high-level module for scalable context translation.

Result: Benchmark results and user studies confirm the framework's effectiveness in capturing scene context with high scalability.

Conclusion: The framework successfully generates diverse and scalable multi-human contextual motions, supported by a new benchmark.

Abstract: In this work, we propose a framework that creates a lively virtual dynamic
scene with contextual motions of multiple humans. Generating multi-human
contextual motion requires holistic reasoning over dynamic relationships among
human-human and human-scene interactions. We adapt the power of a large
language model (LLM) to digest the contextual complexity within textual input
and convert the task into tangible subproblems such that we can generate
multi-agent behavior beyond the scale that was not considered before.
Specifically, our event generator formulates the temporal progression of a
dynamic scene into a sequence of small events. Each event calls for a
well-defined motion involving relevant characters and objects. Next, we
synthesize the motions of characters at positions sampled based on spatial
guidance. We employ a high-level module to deliver scalable yet comprehensive
context, translating events into relative descriptions that enable the
retrieval of precise coordinates. As the first to address this problem at scale
and with diversity, we offer a benchmark to assess diverse aspects of
contextual reasoning. Benchmark results and user studies show that our
framework effectively captures scene context with high scalability. The code
and benchmark, along with result videos, are available at our project page:
https://rms0329.github.io/Event-Driven-Storytelling/.

</details>


### [65] [CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception](https://arxiv.org/abs/2507.19239)
*Jiaru Zhong,Jiahao Wang,Jiahui Xu,Xiaofan Li,Zaiqing Nie,Haibao Yu*

Main category: cs.CV

TL;DR: CoopTrack is an end-to-end framework for cooperative 3D multi-object tracking, using instance-level features and adaptive cross-agent association, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-vehicle autonomous systems and unexplored challenges in cooperative sequential perception tasks like 3D tracking.

Method: CoopTrack uses learnable instance association, sparse instance-level feature transmission, Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation.

Result: Achieves 39.0% mAP and 32.8% AMOTA on V2X-Seq, outperforming existing methods.

Conclusion: CoopTrack enhances cooperative perception with efficient feature transmission and adaptive fusion, demonstrating superior performance.

Abstract: Cooperative perception aims to address the inherent limitations of
single-vehicle autonomous driving systems through information exchange among
multiple agents. Previous research has primarily focused on single-frame
perception tasks. However, the more challenging cooperative sequential
perception tasks, such as cooperative 3D multi-object tracking, have not been
thoroughly investigated. Therefore, we propose CoopTrack, a fully
instance-level end-to-end framework for cooperative tracking, featuring
learnable instance association, which fundamentally differs from existing
approaches. CoopTrack transmits sparse instance-level features that
significantly enhance perception capabilities while maintaining low
transmission costs. Furthermore, the framework comprises two key components:
Multi-Dimensional Feature Extraction, and Cross-Agent Association and
Aggregation, which collectively enable comprehensive instance representation
with semantic and motion features, and adaptive cross-agent association and
fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin
datasets demonstrate that CoopTrack achieves excellent performance.
Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP
and 32.8\% AMOTA. The project is available at
https://github.com/zhongjiaru/CoopTrack.

</details>


### [66] [BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection](https://arxiv.org/abs/2507.19253)
*An Xiang,Zixuan Huang,Xitong Gao,Kejiang Ye,Cheng-zhong Xu*

Main category: cs.CV

TL;DR: A novel unified multimodal anomaly detection framework addresses 3D depth anomaly detection by disentangling depth and appearance, enabling richer anomaly generation and outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to represent 3D information in multimodal scenarios due to disparities in modal information and scarcity of abnormal samples.

Method: Extracts visible depth from 3D point clouds, uses 2D RGB for appearance, and employs anomaly generators for RGB and depth. Modules share parameters for unified detection.

Result: Outperforms SOTA on MVTec-3D AD and Eyecandies datasets.

Conclusion: The framework effectively bridges 2D and 3D anomaly detection, leveraging multimodal features without complex fusion.

Abstract: Industrial anomaly detection for 2D objects has gained significant attention
and achieved progress in anomaly detection (AD) methods. However, identifying
3D depth anomalies using only 2D information is insufficient. Despite
explicitly fusing depth information into RGB images or using point cloud
backbone networks to extract depth features, both approaches struggle to
adequately represent 3D information in multimodal scenarios due to the
disparities among different modal information. Additionally, due to the
scarcity of abnormal samples in industrial data, especially in multimodal
scenarios, it is necessary to perform anomaly generation to simulate real-world
abnormal samples. Therefore, we propose a novel unified multimodal anomaly
detection framework to address these issues. Our contributions consist of 3 key
aspects. (1) We extract visible depth information from 3D point cloud data
simply and use 2D RGB images to represent appearance, which disentangles depth
and appearance to support unified anomaly generation. (2) Benefiting from the
flexible input representation, the proposed Multi-Scale Gaussian Anomaly
Generator and Unified Texture Anomaly Generator can generate richer anomalies
in RGB and depth. (3) All modules share parameters for both RGB and depth data,
effectively bridging 2D and 3D anomaly detection. Subsequent modules can
directly leverage features from both modalities without complex fusion.
Experiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD
and Eyecandies datasets. Code available at:
https://github.com/Xantastic/BridgeNet

</details>


### [67] [OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models](https://arxiv.org/abs/2507.19262)
*Monika Wysoczaska,Shyamal Buch,Anurag Arnab,Cordelia Schmid*

Main category: cs.CV

TL;DR: OV-Fact is a novel method for evaluating long caption factuality in VLMs without human annotations, improving agreement with human judgments and enabling factuality-based data filtering.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics for hallucination and factuality are inadequate for long, diverse captions, especially without human-annotated ground truth.

Method: OV-Fact uses open-vocabulary visual grounding and tool-based verification to measure caption descriptiveness and factual precision.

Result: Models trained on OV-Fact-filtered data show improved factuality precision without losing descriptiveness.

Conclusion: OV-Fact provides a reference-free, effective solution for evaluating and improving caption factuality in VLMs.

Abstract: Large vision-language models (VLMs) often struggle to generate long and
factual captions. However, traditional measures for hallucination and
factuality are not well suited for evaluating longer, more diverse captions and
in settings where ground-truth human-annotated captions are unavailable. We
introduce OV-Fact, a novel method for measuring caption factuality of long
captions that leverages open-vocabulary visual grounding and tool-based
verification without depending on human annotations. Our method improves
agreement with human judgments and captures both caption descriptiveness
(recall) and factual precision in the same metric. Furthermore, unlike previous
metrics, our reference-free method design enables new applications towards
factuality-based data filtering. We observe models trained on an
OVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)
pretraining set meaningfully improve factuality precision without sacrificing
caption descriptiveness across a range of downstream long caption benchmarks.

</details>


### [68] [SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality](https://arxiv.org/abs/2507.19264)
*Sijie Li,Chen Chen,Jungong Han*

Main category: cs.CV

TL;DR: SimMLM is a simple yet effective framework for multimodal learning with missing modalities, using a dynamic gating mechanism and a novel ranking loss to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling missing modalities rely on complex architectures or imputation techniques, which may not generalize well. SimMLM aims to provide a simpler, more adaptable solution.

Method: SimMLM employs a Dynamic Mixture of Modality Experts (DMoME) with a learnable gating mechanism and introduces the More vs. Fewer (MoFe) ranking loss to ensure stable or improved accuracy with more modalities.

Result: SimMLM outperforms existing methods on medical image segmentation and multimodal classification tasks, showing better accuracy, robustness, and reliability in both full and missing modality scenarios.

Conclusion: SimMLM offers a generic, effective solution for multimodal learning with missing modalities, validated by superior performance and alignment with intuitive principles.

Abstract: In this paper, we propose SimMLM, a simple yet powerful framework for
multimodal learning with missing modalities. Unlike existing approaches that
rely on sophisticated network architectures or complex data imputation
techniques, SimMLM provides a generic and effective solution that can adapt to
various missing modality scenarios with improved accuracy and robustness.
Specifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts
(DMoME) architecture, featuring a dynamic, learnable gating mechanism that
automatically adjusts each modality's contribution in both full and partial
modality settings. A key innovation of SimMLM is the proposed More vs. Fewer
(MoFe) ranking loss, which ensures that task accuracy improves or remains
stable as more modalities are made available. This aligns the model with an
intuitive principle: removing one or more modalities should not increase
accuracy. We validate SimMLM on multimodal medical image segmentation (BraTS
2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it
consistently surpasses competitive methods, demonstrating superior accuracy,
interpretability, robustness, and reliability across both complete and missing
modality scenarios at test time.

</details>


### [69] [Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception](https://arxiv.org/abs/2507.19272)
*Marcel Simon,Tae-Ho Kim,Seul-Ki Yeom*

Main category: cs.CV

TL;DR: A video-distilled single-image encoder is introduced to predict next-frame representations, improving mIoU on ADE20K without needing optical flow or tracking.


<details>
  <summary>Details</summary>
Motivation: Most SSL methods miss temporal cues in videos, limiting their ability to learn geometry-aware features.

Method: Train a single-image encoder to predict next-frame representations using a simple objective, injecting 3D spatial and temporal priors.

Result: Pre-training on a 2-hour video raises mIoU on ADE20K from 35.0 to 36.4, remaining compatible with image-only pipelines.

Conclusion: Video self-distillation is a lightweight way to achieve geometry-aware perception, useful for Physical AI and world models.

Abstract: Self-supervised image encoders such as DINO have recently gained significant
interest for learning robust visual features without labels. However, most SSL
methods train on static images and miss the temporal cues inherent in videos.
We introduce a video-distilled single-image encoder trained to predict the
next-frame representation from the current frame. This simple objective injects
3D spatial and temporal priors without optical flow or tracking. When
pre-training on a single 2-hour video, our approach raises the mean
Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while
remaining a drop-in replacement for image-only pipelines. Our results highlight
video self-distillation as a lightweight route to geometry-aware perception an
essential ingredient for physically plausible world models and Physical AI.

</details>


### [70] [RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow](https://arxiv.org/abs/2507.19280)
*Liang Yao,Fan Liu,Hongbo Lu,Chuanyi Zhang,Rui Min,Shengxiang Xu,Shimin Di,Pai Peng*

Main category: cs.CV

TL;DR: The paper proposes RemoteReasoner, a flexible remote sensing workflow using a multi-modal LLM and RL for autonomous reasoning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing methods lack autonomy and flexibility in handling complex queries and diverse output formats.

Method: Integrates a multi-modal LLM for instruction interpretation and target localization, trained with RL for autonomous reasoning.

Result: Achieves strong performance in multi-granularity tasks (region/pixel-level) and enables novel capabilities like contour extraction.

Conclusion: RemoteReasoner offers a robust, flexible solution for complex remote sensing reasoning tasks, surpassing traditional supervised methods.

Abstract: Remote sensing imagery presents vast, inherently unstructured spatial data,
demanding sophisticated reasoning to interpret complex user intents and
contextual relationships beyond simple recognition tasks. In this paper, we aim
to construct an Earth observation workflow to handle complex queries by
reasoning about spatial context and user intent. As a reasoning workflow, it
should be somewhat autonomous, where predefined ground-truth reasoning paths do
not constrain the learning process. Furthermore, its architecture ought to be
unified yet flexible, enabling the model to perform diverse reasoning tasks
with distinct output formats through a single forward pass. Existing remote
sensing approaches fail to address these requirements, as they rely on
supervised fine-tuning paradigms that constrain the autonomy of reasoning. To
this end, we propose RemoteReasoner, a flexible and robust workflow for remote
sensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal
large language model (MLLM) for interpreting user instructions and localizing
targets, together with task adaptation strategies that enable multi-granularity
output generation. In contrast to existing methods, our framework is trained
with reinforcement learning (RL) to endow the MLLM sufficient autonomy for
precise reasoning. At the inference stage, our adaptation strategies enable
diverse output formats at inference time without requiring task-specific
decoders or further fine-tuning. Preliminary experiments demonstrated that
RemoteReasoner achieves remarkable performance across multi-granularity
reasoning tasks, including region-level and pixel-level. Additionally, our
framework enables novel capabilities such as the contour extraction task beyond
the reach of existing reasoning pipelines.

</details>


### [71] [PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups](https://arxiv.org/abs/2507.19292)
*Sakuya Ota,Qing Yu,Kent Fujiwara,Satoshi Ikehata,Ikuro Sato*

Main category: cs.CV

TL;DR: PINO is a training-free framework for generating realistic group interactions by decomposing them into pairwise interactions and using physics-based penalties for plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods for group interaction generation rely on shared prompts, limiting control and realism.

Method: PINO decomposes group interactions into pairwise interactions, uses pretrained two-person models, and applies physics-based penalties during noise optimization.

Result: PINO produces realistic, physically coherent, and customizable multi-person interactions.

Conclusion: PINO is effective for diverse applications like animation, gaming, and robotics without requiring additional training.

Abstract: Generating realistic group interactions involving multiple characters remains
challenging due to increasing complexity as group size expands. While existing
conditional diffusion models incrementally generate motions by conditioning on
previously generated characters, they rely on single shared prompts, limiting
nuanced control and leading to overly simplified interactions. In this paper,
we introduce Person-Interaction Noise Optimization (PINO), a novel,
training-free framework designed for generating realistic and customizable
interactions among groups of arbitrary size. PINO decomposes complex group
interactions into semantically relevant pairwise interactions, and leverages
pretrained two-person interaction diffusion models to incrementally compose
group interactions. To ensure physical plausibility and avoid common artifacts
such as overlapping or penetration between characters, PINO employs
physics-based penalties during noise optimization. This approach allows precise
user control over character orientation, speed, and spatial relationships
without additional training. Comprehensive evaluations demonstrate that PINO
generates visually realistic, physically coherent, and adaptable multi-person
interactions suitable for diverse animation, gaming, and robotics applications.

</details>


### [72] [ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX](https://arxiv.org/abs/2507.19296)
*Ahmed Endris Hasen,Yang Shangming,Chiagoziem C. Ukwuoma,Biniyam Gashaw,Abel Zenebe Yutra*

Main category: cs.CV

TL;DR: The paper proposes an automatic blood cell detection method (ABCD) using an improved YOLOX model with CBAM and ASFF modules, achieving higher accuracy and speed than existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual blood cell detection is time-consuming and error-prone, prompting the need for an efficient automated solution using deep learning.

Method: The method integrates CBAM for better feature extraction, ASFF for optimized feature fusion, and replaces IOU with CIOU loss for faster convergence.

Result: ABCD achieved 95.49% mAP@0.5 and 86.89% mAP@0.5-0.9, outperforming baseline methods by 2.8% and 23.41%, with a 2.9% speed increase.

Conclusion: The proposed ABCD method is highly effective for real-time blood cell detection, offering improved accuracy and efficiency.

Abstract: Detection of blood cells in microscopic images has become a major focus of
medical image analysis, playing a crucial role in gaining valuable insights
into a patient's health. Manual blood cell checks for disease detection are
known to be time-consuming, inefficient, and error-prone. To address these
limitations, analyzing blood cells using deep learning-based object detectors
can be regarded as a feasible solution. In this study, we propose automatic
blood cell detection method (ABCD) based on an improved version of YOLOX, an
object detector, for detecting various types of blood cells, including white
blood cells, red blood cells, and platelets. Firstly, we introduce the
Convolutional Block Attention Module (CBAM) into the network's backbone to
enhance the efficiency of feature extraction. Furthermore, we introduce the
Adaptively Spatial Feature Fusion (ASFF) into the network's neck, which
optimizes the fusion of different features extracted from various stages of the
network. Finally, to speed up the model's convergence, we substitute the
Intersection over Union (IOU) loss function with the Complete Intersection over
Union (CIOU) loss function. The experimental results demonstrate that the
proposed method is more effective than other existing methods for BCCD dataset.
Compared to the baseline algorithm, our method ABCD achieved 95.49 % mAP@0.5
and 86.89 % mAP@0.5-0.9, which are 2.8% and 23.41% higher, respectively, and
increased the detection speed by 2.9%, making it highly efficient for real-time
applications.

</details>


### [73] [Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes](https://arxiv.org/abs/2507.19304)
*Muhammad Ibrahim,Naveed Akhtar,Haitian Wang,Saeed Anwar,Ajmal Mian*

Main category: cs.CV

TL;DR: The paper proposes a MultiStream Detection (MuStD) network for fusing LiDAR and RGB data to improve outdoor 3D object detection, achieving state-of-the-art results on the KITTI benchmark.


<details>
  <summary>Details</summary>
Motivation: Outdoor 3D object detection faces challenges in integrating LiDAR and RGB data effectively, which the paper aims to address.

Method: MuStD uses a three-stream structure: LiDAR-PillarNet for sparse 2D pillar features, LiDAR-Height Compression for Bird's-Eye View features, and a 3D Multimodal stream combining RGB and LiDAR data via UV mapping and polar coordinate indexing.

Result: The method achieves state-of-the-art or highly competitive results on the KITTI benchmark while remaining efficient.

Conclusion: The MuStD network effectively fuses LiDAR and RGB data for precise 3D object detection, demonstrating superior performance on benchmarks.

Abstract: Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object
detection accuracy. To address real-world challenges in outdoor 3D object
detection, fusion of LiDAR and RGB input has started gaining traction. However,
effective integration of these modalities for precise object detection task
still remains a largely open problem. To address that, we propose a MultiStream
Detection (MuStD) network, that meticulously extracts task-relevant information
from both data modalities. The network follows a three-stream structure. Its
LiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input
while the LiDAR-Height Compression stream computes Bird's-Eye View features. An
additional 3D Multimodal stream combines RGB and LiDAR features using UV
mapping and polar coordinate indexing. Eventually, the features containing
comprehensive spatial, textural and geometric information are carefully fused
and fed to a detection head for 3D object detection. Our extensive evaluation
on the challenging KITTI Object Detection Benchmark using public testing server
at
https://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0
establishes the efficacy of our method by achieving new state-of-the-art or
highly competitive results in different categories while remaining among the
most efficient methods. Our code will be released through MuStD GitHub
repository at https://github.com/IbrahimUWA/MuStD.git

</details>


### [74] [SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence](https://arxiv.org/abs/2507.19321)
*Viktar Dubovik,ukasz Struski,Jacek Tabor,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: SIDE introduces a method to improve interpretability in prototypical neural networks by enforcing sparsity and using sigmoid activations, reducing explanation size by over 90% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks lack transparency in high-stakes domains like medical imaging and autonomous driving. Prototypical models offer concept-level explanations but are often complex or limited to fine-grained tasks.

Method: SIDE uses a dedicated training and pruning scheme to enforce sparsity and replaces softmax with sigmoid activations, associating each class with fewer prototypes.

Result: SIDE matches existing methods' accuracy while reducing explanation size by over 90%, enhancing understandability.

Conclusion: SIDE significantly improves the interpretability of prototype-based explanations without sacrificing performance.

Abstract: Understanding the decisions made by deep neural networks is essential in
high-stakes domains such as medical imaging and autonomous driving. Yet, these
models often lack transparency, particularly in computer vision.
Prototypical-parts-based neural networks have emerged as a promising solution
by offering concept-level explanations. However, most are limited to
fine-grained classification tasks, with few exceptions such as InfoDisent.
InfoDisent extends prototypical models to large-scale datasets like ImageNet,
but produces complex explanations.
  We introduce Sparse Information Disentanglement for Explainability (SIDE), a
novel method that improves the interpretability of prototypical parts through a
dedicated training and pruning scheme that enforces sparsity. Combined with
sigmoid activations in place of softmax, this approach allows SIDE to associate
each class with only a small set of relevant prototypes. Extensive experiments
show that SIDE matches the accuracy of existing methods while reducing
explanation size by over $90\%$, substantially enhancing the understandability
of prototype-based explanations.

</details>


### [75] [EffiComm: Bandwidth Efficient Multi Agent Communication](https://arxiv.org/abs/2507.19354)
*Melih Yazgan,Allen Xavier Arasan,J. Marius Z旦llner*

Main category: cs.CV

TL;DR: EffiComm is an end-to-end framework for collaborative perception in connected vehicles, reducing data transmission by 40% while maintaining high 3D object detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Overcoming communication overload and latency in V2V networks by reducing transmitted data without compromising perception accuracy.

Method: Uses a two-stage pipeline: Selective Transmission (ST) to prune low-utility regions and Adaptive Grid Reduction (AGR) with a GNN to assign vehicle-specific keep ratios. Features are fused with a soft-gated MoE attention layer.

Result: Achieves 0.84 mAP@0.7 on OPV2V benchmark, transmitting only ~1.5 MB per frame, outperforming prior methods.

Conclusion: EffiComm demonstrates the effectiveness of adaptive, learned communication for scalable V2X perception.

Abstract: Collaborative perception allows connected vehicles to exchange sensor
information and overcome each vehicle's blind spots. Yet transmitting raw point
clouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,
causing latency and scalability problems. We introduce EffiComm, an end-to-end
framework that transmits less than 40% of the data required by prior art while
maintaining state-of-the-art 3D object detection accuracy. EffiComm operates on
Bird's-Eye-View (BEV) feature maps from any modality and applies a two-stage
reduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions
with a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural
Network (GNN) to assign vehicle-specific keep ratios according to role and
network load. The remaining features are fused with a soft-gated
Mixture-of-Experts (MoE) attention layer, offering greater capacity and
specialization for effective feature integration. On the OPV2V benchmark,
EffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately
1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.
These results highlight the value of adaptive, learned communication for
scalable Vehicle-to-Everything (V2X) perception.

</details>


### [76] [SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning](https://arxiv.org/abs/2507.19359)
*Lanmiao Liu,Esam Ghaleb,Asl脹 zy端rek,Zerrin Yumak*

Main category: cs.CV

TL;DR: A novel approach for semantic grounding in co-speech gesture generation integrates fine-grained and global semantic information, outperforming state-of-the-art methods in realism and coherence.


<details>
  <summary>Details</summary>
Motivation: Existing gesture generation research neglects semantic context, focusing mainly on rhythmic beat gestures.

Method: Uses a vector-quantized variational autoencoder for motion prior learning, followed by a module generating gestures from speech, text semantics, and speaker identity, ensuring semantic coherence.

Result: Enhances realism and coherence of semantic gestures, outperforming benchmarks in objective and subjective metrics.

Conclusion: The proposed method advances co-speech gesture generation by integrating semantic grounding effectively.

Abstract: Creating a virtual avatar with semantically coherent gestures that are
aligned with speech is a challenging task. Existing gesture generation research
mainly focused on generating rhythmic beat gestures, neglecting the semantic
context of the gestures. In this paper, we propose a novel approach for
semantic grounding in co-speech gesture generation that integrates semantic
information at both fine-grained and global levels. Our approach starts with
learning the motion prior through a vector-quantized variational autoencoder.
Built on this model, a second-stage module is applied to automatically generate
gestures from speech, text-based semantics and speaker identity that ensures
consistency between the semantic relevance of generated gestures and
co-occurring speech semantics through semantic coherence and relevance modules.
Experimental results demonstrate that our approach enhances the realism and
coherence of semantic gestures. Extensive experiments and user studies show
that our method outperforms state-of-the-art approaches across two benchmarks
in co-speech gesture generation in both objective and subjective metrics. The
qualitative results of our model, code, dataset and pre-trained models can be
viewed at https://semgesture.github.io/.

</details>


### [77] [EA-ViT: Efficient Adaptation for Elastic Vision Transformer](https://arxiv.org/abs/2507.19360)
*Chen Zhu,Wangbo Zhao,Huiwen Zhang,Samir Khaki,Yuhao Zhou,Weidong Tang,Shuo Wang,Zhihang Yuan,Yuzhang Shang,Xiaojiang Peng,Kai Wang,Dawei Yang*

Main category: cs.CV

TL;DR: EA-ViT proposes a flexible adaptation framework for Vision Transformers (ViTs) to generate multiple model sizes efficiently, avoiding retraining for diverse resource constraints.


<details>
  <summary>Details</summary>
Motivation: Deploying ViTs for varying resource constraints typically requires retraining multiple models, which is inefficient. EA-ViT aims to streamline this process.

Method: The framework uses a nested elastic architecture for structural flexibility and a curriculum-based training strategy. A lightweight router selects submodels based on computational budgets.

Result: EA-ViT demonstrates effectiveness and versatility across multiple benchmarks.

Conclusion: EA-ViT offers an efficient solution for adapting ViTs to diverse deployment scenarios, reducing time and energy costs.

Abstract: Vision Transformers (ViTs) have emerged as a foundational model in computer
vision, excelling in generalization and adaptation to downstream tasks.
However, deploying ViTs to support diverse resource constraints typically
requires retraining multiple, size-specific ViTs, which is both time-consuming
and energy-intensive. To address this issue, we propose an efficient ViT
adaptation framework that enables a single adaptation process to generate
multiple models of varying sizes for deployment on platforms with various
resource constraints. Our approach comprises two stages. In the first stage, we
enhance a pre-trained ViT with a nested elastic architecture that enables
structural flexibility across MLP expansion ratio, number of attention heads,
embedding dimension, and network depth. To preserve pre-trained knowledge and
ensure stable adaptation, we adopt a curriculum-based training strategy that
progressively increases elasticity. In the second stage, we design a
lightweight router to select submodels according to computational budgets and
downstream task demands. Initialized with Pareto-optimal configurations derived
via a customized NSGA-II algorithm, the router is then jointly optimized with
the backbone. Extensive experiments on multiple benchmarks demonstrate the
effectiveness and versatility of EA-ViT. The code is available at
https://github.com/zcxcf/EA-ViT.

</details>


### [78] [LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences](https://arxiv.org/abs/2507.19362)
*Yusuke Hirota,Boyi Li,Ryo Hachiuma,Yueh-Hua Wu,Boris Ivanovic,Yuta Nakashima,Marco Pavone,Yejin Choi,Yu-Chiang Frank Wang,Chao-Han Huck Yang*

Main category: cs.CV

TL;DR: LOTUS is a leaderboard for evaluating detailed image captions by LVLMs, addressing gaps like lack of standardized criteria, bias-aware assessments, and user preferences. It reveals no single model excels in all aspects, with trade-offs between detail and bias risks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for detailed captions lack standardized criteria, bias awareness, and user preference considerations, prompting the need for LOTUS.

Method: LOTUS evaluates caption quality (alignment, descriptiveness), risks (hallucination), societal biases (gender bias), and incorporates user preference-oriented criteria.

Result: Analysis shows no LVLM excels across all criteria, with correlations between caption detail and bias risks. User preferences influence optimal model selection.

Conclusion: LOTUS provides a comprehensive, bias-aware, and preference-oriented framework for evaluating detailed captions, highlighting the need for tailored model selection based on user priorities.

Abstract: Large Vision-Language Models (LVLMs) have transformed image captioning,
shifting from concise captions to detailed descriptions. We introduce LOTUS, a
leaderboard for evaluating detailed captions, addressing three main gaps in
existing evaluations: lack of standardized criteria, bias-aware assessments,
and user preference considerations. LOTUS comprehensively evaluates various
aspects, including caption quality (e.g., alignment, descriptiveness), risks
(\eg, hallucination), and societal biases (e.g., gender bias) while enabling
preference-oriented evaluations by tailoring criteria to diverse user
preferences. Our analysis of recent LVLMs reveals no single model excels across
all criteria, while correlations emerge between caption detail and bias risks.
Preference-oriented evaluations demonstrate that optimal model selection
depends on user priorities.

</details>


### [79] [BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving](https://arxiv.org/abs/2507.19370)
*Felix Brandstaetter,Erik Schuetz,Katharina Winter,Fabian Flohr*

Main category: cs.CV

TL;DR: BEV-LLM is a lightweight 3D captioning model for autonomous driving scenes, combining LiDAR and multi-view images with a novel positional encoding. It outperforms state-of-the-art models by 5% in BLEU scores and introduces two new datasets for diverse scenario evaluation.


<details>
  <summary>Details</summary>
Motivation: Enhancing transparency, safety, and human-AI interaction in autonomous driving through interpretable scene captioning.

Method: BEV-LLM uses BEVFusion to integrate 3D LiDAR point clouds and multi-view images with absolute positional encoding for view-specific descriptions.

Result: Achieves competitive performance on nuCaption, surpassing state-of-the-art by 5% in BLEU scores, and introduces two new datasets (nuView and GroundView) for benchmarking.

Conclusion: BEV-LLM advances scene captioning for autonomous driving, offering improved performance and addressing gaps in current benchmarks.

Abstract: Autonomous driving technology has the potential to transform transportation,
but its wide adoption depends on the development of interpretable and
transparent decision-making systems. Scene captioning, which generates natural
language descriptions of the driving environment, plays a crucial role in
enhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,
a lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM
leverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,
incorporating a novel absolute positional encoding for view-specific scene
descriptions. Despite using a small 1B parameter base model, BEV-LLM achieves
competitive performance on the nuCaption dataset, surpassing state-of-the-art
by up to 5\% in BLEU scores. Additionally, we release two new datasets - nuView
(focused on environmental conditions and viewpoints) and GroundView (focused on
object grounding) - to better assess scene captioning across diverse driving
scenarios and address gaps in current benchmarks, along with initial
benchmarking results demonstrating their effectiveness.

</details>


### [80] [CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays](https://arxiv.org/abs/2507.19398)
*Rajesh Madhipati,Sheethal Bhat,Lukas Buess,Andreas Maier*

Main category: cs.CV

TL;DR: The paper addresses the challenge of class imbalance in CXR diagnosis using a class-weighting mechanism and GMM clustering to improve zero-shot classification, especially for rare classes.


<details>
  <summary>Details</summary>
Motivation: Class imbalance in CXR datasets hinders self-supervised models like CLIP, particularly for long-tailed classes.

Method: Uses GMM clustering and Student t-distribution to refine latent space embeddings, combined with a metric loss for stable clustering.

Result: Achieves a 7% average improvement in zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset.

Conclusion: The proposed method enhances classification performance, especially for rare classes, outperforming previous SOTA models.

Abstract: Chest radiography (CXR) plays a crucial role in the diagnosis of various
diseases. However, the inherent class imbalance in the distribution of clinical
findings presents a significant challenge for current self-supervised deep
learning models. These models often fail to accurately classify long-tailed
classes. Current Vision-Language models such as Contrastive Language Image
Pre-training (CLIP) models effectively model the manifold distribution of the
latent space, enabling high zero-shot classification accuracies. Although CLIP
performs well on most of the primary classes in the dataset, our work reveals
that its effectiveness decreases significantly for classes with a long-tailed
distribution. Our approach employs a class-weighting mechanism that directly
aligns with the distribution of classes within the latent space. This method
ensures a substantial improvement in overall classification performance, with
particular emphasis on enhancing the recognition and accuracy of rarely
observed classes. We accomplish this by applying Gaussian Mixture Model (GMM)
clustering to the latent space. The subsequent clusters are further refined by
Student t-distribution, followed by a metric loss that utilizes the altered
embeddings. Our approach facilitates stable and adaptive clustering of the
features. This results in a notable average improvement of 7\% points in
zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from
previous SOTA models.

</details>


### [81] [Modality Agnostic Efficient Long Range Encoder](https://arxiv.org/abs/2507.19409)
*Toufiq Parag,Ahmed Elgammal*

Main category: cs.CV

TL;DR: MAELRE is a unified transformer architecture for efficient long-range encoding across modalities, reducing quadratic complexity while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of existing long-context models on single devices, which rely on suboptimal tradeoffs between accuracy and efficiency.

Method: Integrates token merging with attention approximation, using lightweight attention for long sequences and standard attention for shorter ones.

Result: Achieves superior accuracy with reduced computational cost across text, time series, audio, and vision tasks.

Conclusion: MAELRE offers a scalable and efficient solution for long-context processing on single devices.

Abstract: The long-context capability of recent large transformer models can be
surmised to rely on techniques such as attention/model parallelism, as well as
hardware-level optimizations. While these strategies allow input lengths to
scale to millions of tokens, they do not fundamentally mitigate the quadratic
computational and memory complexity of the core attention mechanism. In this
paper, we address the challenge of long-context processing on a single device
using generic implementations by reducing the quadratic memory footprint and
inference cost. Existing approaches to extend the context length for generic
single device implementations -- such as token merging and modified attentions
-- are often modality specific and attain a suboptimal tradeoff between
accuracy and efficiency. To overcome these limitations, we propose MAELRE
(Modality Agnostic Efficient Long Range Encoder), a unified and efficient
transformer architecture designed for long-range encoding across diverse
modalities. MAELRE integrates token merging with attention approximation,
progressively merging tokens at different stages of internal computational
blocks. It employs a lightweight attention approximation when the number of
tokens is large, and switches to standard dot-product attention as the sequence
becomes shorter through successive aggregation. We demonstrate that MAELRE
achieves superior accuracy while reducing computational cost compared to
existing long-context models on classification tasks spanning multiple
modalities, including text, time series, audio, and vision.

</details>


### [82] [DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment](https://arxiv.org/abs/2507.19418)
*Yiwei Lou,Yuanpeng He,Rongchao Zhang,Yongzhi Cao,Hanpin Wang,Yu Huang*

Main category: cs.CV

TL;DR: DEFNet, a multitask-based Deep Evidential Fusion Network, improves BIQA by integrating scene and distortion classification, robust fusion strategies, and advanced uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Existing BIQA methods lack sufficient integration and flexible uncertainty estimation, leading to suboptimal performance.

Method: DEFNet uses multitask optimization with scene/distortion classification, a trustworthy fusion strategy, and evidential learning for uncertainty estimation.

Result: Experiments show DEFNet's effectiveness, robustness, and strong generalization on synthetic and authentic distortion datasets.

Conclusion: DEFNet outperforms existing methods with enhanced integration, fusion, and uncertainty estimation, proving adaptable to unseen scenarios.

Abstract: Blind image quality assessment (BIQA) methods often incorporate auxiliary
tasks to improve performance. However, existing approaches face limitations due
to insufficient integration and a lack of flexible uncertainty estimation,
leading to suboptimal performance. To address these challenges, we propose a
multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which
performs multitask optimization with the assistance of scene and distortion
type classification tasks. To achieve a more robust and reliable
representation, we design a novel trustworthy information fusion strategy. It
first combines diverse features and patterns across sub-regions to enhance
information richness, and then performs local-global information fusion by
balancing fine-grained details with coarse-grained context. Moreover, DEFNet
exploits advanced uncertainty estimation technique inspired by evidential
learning with the help of normal-inverse gamma distribution mixture. Extensive
experiments on both synthetic and authentic distortion datasets demonstrate the
effectiveness and robustness of the proposed framework. Additional evaluation
and analysis are carried out to highlight its strong generalization capability
and adaptability to previously unseen scenarios.

</details>


### [83] [CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing](https://arxiv.org/abs/2507.19420)
*Yiming Zhang,Chengzhang Yu,Zhuokai Zhao,Kun Wang,Qiankun Li,Zihan Chen,Yang Liu,Zenghui Ding,Yining Sun*

Main category: cs.CV

TL;DR: A circuit-based framework reveals how spatiotemporal semantics are processed in LVLMs, showing localized object tokens and refined concepts in middle-to-late layers.


<details>
  <summary>Details</summary>
Motivation: To understand the internal reasoning mechanisms of LVLMs for spatiotemporal understanding, which remains poorly studied.

Method: Introduces a framework with three circuits: visual auditing, semantic tracing, and attention flow, to analyze spatiotemporal semantics.

Result: Visual semantics are localized to specific object tokens (removal degrades performance by 92.6%), and interpretable concepts emerge in middle-to-late layers.

Conclusion: The study provides mechanistic insights into LVLMs' spatiotemporal semantics, aiding the design of more robust and interpretable models.

Abstract: The processing mechanisms underlying language and image understanding in
large vision-language models (LVLMs) have been extensively studied. However,
the internal reasoning mechanisms of LVLMs for spatiotemporal understanding
remain poorly understood. In this work, we introduce a systematic,
circuit-based framework designed to investigate how spatiotemporal visual
semantics are represented and processed within these LVLMs. Specifically, our
framework comprises three circuits: visual auditing circuit, semantic tracing
circuit, and attention flow circuit. Through the lens of these circuits, we
discover that visual semantics are highly localized to specific object
tokens--removing these tokens can degrade model performance by up to 92.6%.
Furthermore, we identify that interpretable concepts of objects and actions
emerge and become progressively refined in the middle-to-late layers of LVLMs.
In contrary to the current works that solely focus on objects in one image, we
reveal that the middle-to-late layers of LVLMs exhibit specialized functional
localization for spatiotemporal semantics. Our findings offer significant
mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a
foundation for designing more robust and interpretable models.

</details>


### [84] [GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting](https://arxiv.org/abs/2507.19451)
*Baijun Ye,Minghui Qin,Saining Zhang,Moonjun Gong,Shaoting Zhu,Zebang Shen,Luan Zhang,Lu Zhang,Hao Zhao,Hang Zhao*

Main category: cs.CV

TL;DR: GS-Occ3D is a scalable vision-only framework for occupancy reconstruction in autonomous driving, addressing challenges like sparse viewpoints and occlusions with an Octree-based Gaussian Surfel method.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based occupancy methods limit scalability and exclude crowdsourced data. Vision-only approaches face challenges like incomplete geometry and post-processing needs.

Method: GS-Occ3D uses an Octree-based Gaussian Surfel formulation to optimize occupancy representation, decomposing scenes into static background, ground, and dynamic objects for tailored modeling.

Result: Achieves state-of-the-art geometry reconstruction on Waymo, with effective binary occupancy labels for downstream models and superior zero-shot generalization on Occ3D-nuScenes.

Conclusion: GS-Occ3D demonstrates the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception.

Abstract: Occupancy is crucial for autonomous driving, providing essential geometric
priors for perception and planning. However, existing methods predominantly
rely on LiDAR-based occupancy annotations, which limits scalability and
prevents leveraging vast amounts of potential crowdsourced data for
auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only
framework that directly reconstructs occupancy. Vision-only occupancy
reconstruction poses significant challenges due to sparse viewpoints, dynamic
scene elements, severe occlusions, and long-horizon motion. Existing
vision-based methods primarily rely on mesh representation, which suffer from
incomplete geometry and additional post-processing, limiting scalability. To
overcome these issues, GS-Occ3D optimizes an explicit occupancy representation
using an Octree-based Gaussian Surfel formulation, ensuring efficiency and
scalability. Additionally, we decompose scenes into static background, ground,
and dynamic objects, enabling tailored modeling strategies: (1) Ground is
explicitly reconstructed as a dominant structural element, significantly
improving large-area consistency; (2) Dynamic vehicles are separately modeled
to better capture motion-related occupancy patterns. Extensive experiments on
the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry
reconstruction results. By curating vision-only binary occupancy labels from
diverse urban scenes, we show their effectiveness for downstream occupancy
models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.
It highlights the potential of large-scale vision-based occupancy
reconstruction as a new paradigm for autonomous driving perception. Project
Page: https://gs-occ3d.github.io/

</details>


### [85] [Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization](https://arxiv.org/abs/2507.19459)
*Pol Francesch Huc,Emily Bates,Simone D'Amico*

Main category: cs.CV

TL;DR: A CNN-based pipeline for 3D Gaussian Splatting (3DGS) reduces training iterations and pose dependency, enabling high-fidelity 3D models from monocular images for space applications.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NeRF and 3DGS are limited by pose requirements and high computational costs, hindering their use in space applications.

Method: Proposes a CNN-based primitive initializer for 3DGS, a pipeline for noisy/implicit pose training, and analyzes initialization variants to reduce training costs.

Result: Reduces training iterations and input images by at least 10x, achieving high-fidelity 3D models even with imperfect pose estimates.

Conclusion: The pipeline enables efficient novel view synthesis for space applications by addressing pose and computational limitations.

Abstract: The advent of novel view synthesis techniques such as NeRF and 3D Gaussian
Splatting (3DGS) has enabled learning precise 3D models only from posed
monocular images. Although these methods are attractive, they hold two major
limitations that prevent their use in space applications: they require poses
during training, and have high computational cost at training and inference. To
address these limitations, this work contributes: (1) a Convolutional Neural
Network (CNN) based primitive initializer for 3DGS using monocular images; (2)
a pipeline capable of training with noisy or implicit pose estimates; and (3)
and analysis of initialization variants that reduce the training cost of
precise 3D models. A CNN takes a single image as input and outputs a coarse 3D
model represented as an assembly of primitives, along with the target's pose
relative to the camera. This assembly of primitives is then used to initialize
3DGS, significantly reducing the number of training iterations and input images
needed -- by at least an order of magnitude. For additional flexibility, the
CNN component has multiple variants with different pose estimation techniques.
This work performs a comparison between these variants, evaluating their
effectiveness for downstream 3DGS training under noisy or implicit pose
estimates. The results demonstrate that even with imperfect pose supervision,
the pipeline is able to learn high-fidelity 3D representations, opening the
door for the use of novel view synthesis in space applications.

</details>


### [86] [Back to the Features: DINO as a Foundation for Video World Models](https://arxiv.org/abs/2507.19468)
*Federico Baldassarre,Marc Szafraniec,Basile Terver,Vasil Khalidov,Francisco Massa,Yann LeCun,Patrick Labatut,Maximilian Seitzer,Piotr Bojanowski*

Main category: cs.CV

TL;DR: DINO-world is a generalist video world model predicting future frames in DINOv2's latent space, excelling in benchmarks and intuitive physics, with potential for planning via action-conditioned fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile video world model capable of predicting future frames and understanding diverse scenes, leveraging pre-trained encoders and large-scale video data.

Method: Uses a pre-trained DINOv2 image encoder and trains a future predictor on uncurated video data, fine-tunable for action-conditioned planning.

Result: Outperforms prior models in video prediction benchmarks (e.g., segmentation, depth forecasting) and shows strong intuitive physics understanding.

Conclusion: DINO-world is a robust, generalist model for video prediction and planning, with potential applications in diverse scenarios.

Abstract: We present DINO-world, a powerful generalist video world model trained to
predict future frames in the latent space of DINOv2. By leveraging a
pre-trained image encoder and training a future predictor on a large-scale
uncurated video dataset, DINO-world learns the temporal dynamics of diverse
scenes, from driving and indoor scenes to simulated environments. We show that
DINO-world outperforms previous models on a variety of video prediction
benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong
understanding of intuitive physics. Furthermore, we show that it is possible to
fine-tune the predictor on observation-action trajectories. The resulting
action-conditioned world model can be used for planning by simulating candidate
trajectories in latent space.

</details>


### [87] [Efficient Lines Detection for Robot Soccer](https://arxiv.org/abs/2507.19469)
*Jo達o G. Melo,Jo達o P. Mafaldo,Edna Barros*

Main category: cs.CV

TL;DR: A lightweight method using ELSED and PSO for soccer field line detection, achieving high accuracy and speed for real-time robot soccer applications.


<details>
  <summary>Details</summary>
Motivation: Accurate self-localization in robot soccer relies on reliable detection of field lines, necessitating efficient and fast methods.

Method: Uses ELSED algorithm with RGB color transition classification and PSO for threshold calibration, requiring minimal annotated samples.

Result: Matches state-of-the-art deep learning accuracy with higher processing speed, suitable for low-power robots.

Conclusion: The method is efficient, accurate, and ideal for real-time applications in robot soccer.

Abstract: Self-localization is essential in robot soccer, where accurate detection of
visual field features, such as lines and boundaries, is critical for reliable
pose estimation. This paper presents a lightweight and efficient method for
detecting soccer field lines using the ELSED algorithm, extended with a
classification step that analyzes RGB color transitions to identify lines
belonging to the field. We introduce a pipeline based on Particle Swarm
Optimization (PSO) for threshold calibration to optimize detection performance,
requiring only a small number of annotated samples. Our approach achieves
accuracy comparable to a state-of-the-art deep learning model while offering
higher processing speed, making it well-suited for real-time applications on
low-power robotic platforms.

</details>


### [88] [DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations](https://arxiv.org/abs/2507.19474)
*Ziren Gong,Xiaohan Li,Fabio Tosi,Youmin Zhang,Stefano Mattoccia,Jun Wu,Matteo Poggi*

Main category: cs.CV

TL;DR: DINO-SLAM enhances SLAM systems by integrating DINO features into NeRF and 3DGS representations, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: To improve scene representation in SLAM systems by leveraging DINO features for more comprehensive and hierarchical understanding.

Method: Uses a Scene Structure Encoder (SSE) to enrich DINO features into Enhanced DINO (EDINO) and integrates them into NeRF and 3DGS SLAM systems.

Result: Outperforms state-of-the-art methods on Replica, ScanNet, and TUM datasets.

Conclusion: DINO-SLAM effectively enhances SLAM systems with improved scene representations.

Abstract: This paper presents DINO-SLAM, a DINO-informed design strategy to enhance
neural implicit (Neural Radiance Field -- NeRF) and explicit representations
(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive
scene representations. Purposely, we rely on a Scene Structure Encoder (SSE)
that enriches DINO features into Enhanced DINO ones (EDINO) to capture
hierarchical scene elements and their structural relationships. Building upon
it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems
integrating EDINO features. Our DINO-informed pipelines achieve superior
performance on the Replica, ScanNet, and TUM compared to state-of-the-art
methods.

</details>


### [89] [MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents](https://arxiv.org/abs/2507.19478)
*Xuehui Wang,Zhenyu Wu,JingJing Xie,Zichen Ding,Bowen Yang,Zehao Li,Zhaoyang Liu,Qingyun Li,Xuan Dong,Zhe Chen,Weiyun Wang,Xiangyu Zhao,Jixuan Chen,Haodong Duan,Tianbao Xie,Chenyu Yang,Shiqian Su,Yue Yu,Yuan Huang,Yiqian Liu,Xiao Zhang,Yanting Zhang,Xiangyu Yue,Weijie Su,Xizhou Zhu,Wei Shen,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: MMBench-GUI is a hierarchical benchmark for evaluating GUI automation agents across multiple platforms, introducing the EQA metric for efficiency assessment. It highlights the importance of visual grounding, task planning, and efficiency in GUI automation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive benchmarks for GUI automation agents and the underexplored dimension of task efficiency.

Method: Proposes MMBench-GUI with four levels (GUI Content Understanding, Element Grounding, Task Automation, Task Collaboration) and the EQA metric.

Result: Identifies visual grounding as critical for task success and highlights inefficiencies in current models.

Conclusion: Efficient GUI automation requires precise localization, effective planning, and early stopping strategies, with modular frameworks being beneficial.

Abstract: We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI
automation agents across Windows, macOS, Linux, iOS, Android, and Web
platforms. It comprises four levels: GUI Content Understanding, Element
Grounding, Task Automation, and Task Collaboration, covering essential skills
for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)
metric to assess GUI agent execution efficiency in online automation scenarios.
Through MMBench-GUI, we identify accurate visual grounding as a critical
determinant of overall task success, emphasizing the substantial benefits of
modular frameworks that integrate specialized grounding modules. Furthermore,
to achieve reliable GUI automation, an agent requires strong task planning and
cross-platform generalization abilities, with long-context memory, a broad
action space, and long-term reasoning playing a critical role. More important,
task efficiency remains a critically underexplored dimension, and all models
suffer from substantial inefficiencies, with excessive redundant steps even
when tasks are ultimately completed. The integration of precise localization,
effective planning, and early stopping strategies is indispensable to enable
truly efficient and scalable GUI automation. Our benchmark code, evaluation
data, and running environment will be publicly available at
https://github.com/open-compass/MMBench-GUI.

</details>


### [90] [HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars](https://arxiv.org/abs/2507.19481)
*Byungjun Kim,Shunsuke Saito,Giljoo Nam,Tomas Simon,Jason Saragih,Hanbyul Joo,Junxuan Li*

Main category: cs.CV

TL;DR: A prior model for 3D head avatars with explicit face-hair compositionality, enabling flexible swapping and few-shot fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing holistic models struggle with disentangling face and hair representations and lack flexibility for applications like swapping.

Method: Separate latent spaces for face and hair, trained using synthetic hairless data derived from a diffusion prior.

Result: Enables seamless face and hair transfer between avatars and few-shot fine-tuning for high-fidelity avatars.

Conclusion: The model's compositionality supports flexible and expressive 3D avatar generation, enhancing real-world applicability.

Abstract: We present a universal prior model for 3D head avatars with explicit hair
compositionality. Existing approaches to build generalizable priors for 3D head
avatars often adopt a holistic modeling approach, treating the face and hair as
an inseparable entity. This overlooks the inherent compositionality of the
human head, making it difficult for the model to naturally disentangle face and
hair representations, especially when the dataset is limited. Furthermore, such
holistic models struggle to support applications like 3D face and hairstyle
swapping in a flexible and controllable manner. To address these challenges, we
introduce a prior model that explicitly accounts for the compositionality of
face and hair, learning their latent spaces separately. A key enabler of this
approach is our synthetic hairless data creation pipeline, which removes hair
from studio-captured datasets using estimated hairless geometry and texture
derived from a diffusion prior. By leveraging a paired dataset of hair and
hairless captures, we train disentangled prior models for face and hair,
incorporating compositionality as an inductive bias to facilitate effective
separation. Our model's inherent compositionality enables seamless transfer of
face and hair components between avatars while preserving identity.
Additionally, we demonstrate that our model can be fine-tuned in a few-shot
manner using monocular captures to create high-fidelity, hair-compositional 3D
head avatars for unseen subjects. These capabilities highlight the practical
applicability of our approach in real-world scenarios, paving the way for
flexible and expressive 3D avatar generation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [91] [XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays](https://arxiv.org/abs/2507.18647)
*Rayyan Ridwan*

Main category: eess.IV

TL;DR: A deep learning model using ResNet-50 and BayesGrad-CAM for diagnosing pediatric pneumonia achieves high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Pneumonia is a leading cause of child mortality, necessitating fast and accurate diagnostic tools.

Method: Proposes an interpretable ResNet-50 model with BayesGrad-CAM for uncertainty-aware visual explanations.

Result: Achieves 95.94% accuracy, 98.91% AUC-ROC, and 0.913 Cohen's Kappa, with clinically meaningful visualizations.

Conclusion: High performance and interpretability are achievable and essential for clinical AI.

Abstract: Pneumonia remains one of the leading causes of death among children
worldwide, underscoring a critical need for fast and accurate diagnostic tools.
In this paper, we propose an interpretable deep learning model on Residual
Networks (ResNets) for automatically diagnosing paediatric pneumonia on chest
X-rays. We enhance interpretability through Bayesian Gradient-weighted Class
Activation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual
explanations, and which offers spatial locations accountable for the
decision-making process of the model. Our ResNet-50 model, trained on a large
paediatric chest X-rays dataset, achieves high classification accuracy
(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by
clinically meaningful visual explanations. Our findings demonstrate that high
performance and interpretability are not only achievable but critical for
clinical AI deployment.

</details>


### [92] [Learned Single-Pixel Fluorescence Microscopy](https://arxiv.org/abs/2507.18740)
*Serban C. Tudosie,Valerio Gandolfi,Shivaprasad Varakkoth,Andrea Farina,Cosimo D'Andrea,Simon Arridge*

Main category: eess.IV

TL;DR: A self-supervised autoencoder is trained to improve single-pixel imaging in fluorescence microscopy, achieving faster reconstruction, better quality, and enabling multispectral imaging.


<details>
  <summary>Details</summary>
Motivation: To enhance single-pixel imaging by leveraging learned measurement vectors and reconstruction processes, addressing challenges in speed, quality, and multispectral capabilities.

Method: Training an autoencoder through self-supervision to learn an encoder (measurement matrix) and decoder, then testing on physically acquired multispectral and intensity data.

Result: Reduced reconstruction time by two orders of magnitude, superior image quality, and enabled multispectral reconstructions.

Conclusion: The approach advances diagnosis and biological research by providing cost-effective multispectral imaging.

Abstract: Single-pixel imaging has emerged as a key technique in fluorescence
microscopy, where fast acquisition and reconstruction are crucial. In this
context, images are reconstructed from linearly compressed measurements. In
practice, total variation minimisation is still used to reconstruct the image
from noisy measurements of the inner product between orthogonal sampling
pattern vectors and the original image data. However, data can be leveraged to
learn the measurement vectors and the reconstruction process, thereby enhancing
compression, reconstruction quality, and speed. We train an autoencoder through
self-supervision to learn an encoder (or measurement matrix) and a decoder. We
then test it on physically acquired multispectral and intensity data. During
acquisition, the learned encoder becomes part of the physical device. Our
approach can enhance single-pixel imaging in fluorescence microscopy by
reducing reconstruction time by two orders of magnitude, achieving superior
image quality, and enabling multispectral reconstructions. Ultimately, learned
single-pixel fluorescence microscopy could advance diagnosis and biological
research, providing multispectral imaging at a fraction of the cost.

</details>


### [93] [RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.18830)
*Shen Zhu,Yinzhu Jin,Tyler Spears,Ifrah Zawar,P. Thomas Fletcher*

Main category: eess.IV

TL;DR: A diffusion model (RealDeal) enhances realism in brain MRI images by adding details like sharp edges and noise, improving on overly smooth outputs from latent diffusion models.


<details>
  <summary>Details</summary>
Motivation: Latent diffusion models generate overly smooth brain MRIs lacking real-world details like noise and fine anatomical structures, prompting the need for enhanced realism.

Method: Image-to-image diffusion models refine LDM outputs by introducing sharp edges, textures, anatomical features, and noise, evaluated with FID, LPIPS, and new metrics.

Result: RealDeal improves realism in generated brain MRIs, validated by metrics assessing noise distribution, sharpness, and texture.

Conclusion: The proposed method effectively enhances the realism of brain MRI images, addressing limitations of existing latent diffusion models.

Abstract: We propose image-to-image diffusion models that are designed to enhance the
realism and details of generated brain images by introducing sharp edges, fine
textures, subtle anatomical features, and imaging noise. Generative models have
been widely adopted in the biomedical domain, especially in image generation
applications. Latent diffusion models achieve state-of-the-art results in
generating brain MRIs. However, due to latent compression, generated images
from these models are overly smooth, lacking fine anatomical structures and
scan acquisition noise that are typically seen in real images. This work
formulates the realism enhancing and detail adding process as image-to-image
diffusion models, which refines the quality of LDM-generated images. We employ
commonly used metrics like FID and LPIPS for image realism assessment.
Furthermore, we introduce new metrics to demonstrate the realism of images
generated by RealDeal in terms of image noise distribution, sharpness, and
texture.

</details>


### [94] [Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy](https://arxiv.org/abs/2507.18895)
*Vangelis Kostoulas,Arthur Guijt,Ellen M. Kerkhof,Bradley R. Pieters,Peter A. N. Bosman,Tanja Alderliesten*

Main category: eess.IV

TL;DR: The paper proposes adaptations to existing post-processing techniques in brachytherapy needle reconstruction to handle segmentation errors, improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual needle annotation in brachytherapy is time-consuming, and current post-processing methods lack robustness against segmentation errors.

Method: A two-stage pipeline (segmentation + post-processing) is adapted to better manage segmentation errors.

Result: Adapted techniques achieved median errors of 1.07 mm (needle-tip) and 0.43 mm (needle-bottom), with 0 false positives/negatives on 261 test needles.

Conclusion: The proposed adaptations effectively improve needle reconstruction accuracy in brachytherapy.

Abstract: Brachytherapy involves bringing a radioactive source near tumor tissue using
implanted needles. Image-guided brachytherapy planning requires amongst others,
the reconstruction of the needles. Manually annotating these needles on patient
images can be a challenging and time-consuming task for medical professionals.
For automatic needle reconstruction, a two-stage pipeline is commonly adopted,
comprising a segmentation stage followed by a post-processing stage. While deep
learning models are effective for segmentation, their results often contain
errors. No currently existing post-processing technique is robust to all
possible segmentation errors. We therefore propose adaptations to existing
post-processing techniques mainly aimed at dealing with segmentation errors and
thereby improving the reconstruction accuracy. Experiments on a prostate cancer
dataset, based on MRI scans annotated by medical professionals, demonstrate
that our proposed adaptations can help to effectively manage segmentation
errors, with the best adapted post-processing technique achieving median
needle-tip and needle-bottom point localization errors of $1.07$ (IQR $\pm
1.04$) mm and $0.43$ (IQR $\pm 0.46$) mm, respectively, and median shaft error
of $0.75$ (IQR $\pm 0.69$) mm with 0 false positive and 0 false negative
needles on a test set of 261 needles.

</details>


### [95] [Dual Path Learning -- learning from noise and context for medical image denoising](https://arxiv.org/abs/2507.19035)
*Jitindra Fartiyal,Pedro Freire,Yasmeen Whayeb,James S. Wolffsohn,Sergei K. Turitsyn,Sergei G. Sokolov*

Main category: eess.IV

TL;DR: A Dual-Pathway Learning (DPL) model is introduced to denoise medical images by combining noise and contextual information, outperforming UNet by 3.35% PSNR on Gaussian noise across multiple modalities.


<details>
  <summary>Details</summary>
Motivation: Noise in medical imaging degrades quality, risking misdiagnosis. Current methods focus on single modalities or noise types, lacking generalizability.

Method: DPL integrates noise and contextual information via a dual-pathway architecture, evaluated across multiple modalities and noise types.

Result: DPL improves PSNR by 3.35% over UNet on Gaussian noise, showing robustness across modalities.

Conclusion: DPL is a versatile and effective denoising solution for medical imaging, with potential for broad clinical application.

Abstract: Medical imaging plays a critical role in modern healthcare, enabling
clinicians to accurately diagnose diseases and develop effective treatment
plans. However, noise, often introduced by imaging devices, can degrade image
quality, leading to misinterpretation and compromised clinical outcomes.
Existing denoising approaches typically rely either on noise characteristics or
on contextual information from the image. Moreover, they are commonly developed
and evaluated for a single imaging modality and noise type. Motivated by Geng
et.al CNCL, which integrates both noise and context, this study introduces a
Dual-Pathway Learning (DPL) model architecture that effectively denoises
medical images by leveraging both sources of information and fusing them to
generate the final output. DPL is evaluated across multiple imaging modalities
and various types of noise, demonstrating its robustness and generalizability.
DPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on
Gaussian noise and trained across all modalities. The code is available at
10.5281/zenodo.15836053.

</details>


### [96] [A Self-training Framework for Semi-supervised Pulmonary Vessel Segmentation and Its Application in COPD](https://arxiv.org/abs/2507.19074)
*Shuiqing Zhao,Meihuan Wang,Jiaxuan Xu,Jie Feng,Wei Qian,Rongchang Chen,Zhenyu Liang,Shouliang Qi,Yanan Wu*

Main category: eess.IV

TL;DR: A semi-supervised method (Semi2) improves pulmonary vessel segmentation in COPD patients by 2.3%, achieving 90.3% precision.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of pulmonary vessels, especially smaller ones, is crucial for COPD analysis.

Method: A teacher-student model is used: a teacher generates pseudo-labels for unlabeled CT scans, and a student is trained on reliable pseudo-labels iteratively.

Result: The method achieves 90.3% precision in vessel segmentation and provides insights into vessel differences across COPD severity.

Conclusion: Semi2 enhances segmentation performance and is applicable to COPD analysis; code is publicly available.

Abstract: Background: It is fundamental for accurate segmentation and quantification of
the pulmonary vessel, particularly smaller vessels, from computed tomography
(CT) images in chronic obstructive pulmonary disease (COPD) patients.
Objective: The aim of this study was to segment the pulmonary vasculature using
a semi-supervised method. Methods: In this study, a self-training framework is
proposed by leveraging a teacher-student model for the segmentation of
pulmonary vessels. First, the high-quality annotations are acquired in the
in-house data by an interactive way. Then, the model is trained in the
semi-supervised way. A fully supervised model is trained on a small set of
labeled CT images, yielding the teacher model. Following this, the teacher
model is used to generate pseudo-labels for the unlabeled CT images, from which
reliable ones are selected based on a certain strategy. The training of the
student model involves these reliable pseudo-labels. This training process is
iteratively repeated until an optimal performance is achieved. Results:
Extensive experiments are performed on non-enhanced CT scans of 125 COPD
patients. Quantitative and qualitative analyses demonstrate that the proposed
method, Semi2, significantly improves the precision of vessel segmentation by
2.3%, achieving a precision of 90.3%. Further, quantitative analysis is
conducted in the pulmonary vessel of COPD, providing insights into the
differences in the pulmonary vessel across different severity of the disease.
Conclusion: The proposed method can not only improve the performance of
pulmonary vascular segmentation, but can also be applied in COPD analysis. The
code will be made available at
https://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation.

</details>


### [97] [Learned Image Compression with Hierarchical Progressive Context Modeling](https://arxiv.org/abs/2507.19125)
*Yuqi Li,Haotian Zhang,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: The paper introduces HPCM, a hierarchical progressive context model for efficient long-range and diverse context modeling in image compression, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with efficiently exploiting long-range dependency and diverse context information in image compression.

Method: HPCM uses a hierarchical coding schedule for multi-scale latent dependency modeling and a progressive context fusion mechanism to incorporate past context.

Result: HPCM achieves state-of-the-art rate-distortion performance with balanced computational complexity.

Conclusion: HPCM effectively improves context modeling in image compression, offering better performance and efficiency.

Abstract: Context modeling is essential in learned image compression for accurately
estimating the distribution of latents. While recent advanced methods have
expanded context modeling capacity, they still struggle to efficiently exploit
long-range dependency and diverse context information across different coding
steps. In this paper, we introduce a novel Hierarchical Progressive Context
Model (HPCM) for more efficient context information acquisition. Specifically,
HPCM employs a hierarchical coding schedule to sequentially model the
contextual dependencies among latents at multiple scales, which enables more
efficient long-range context modeling. Furthermore, we propose a progressive
context fusion mechanism that incorporates contextual information from previous
coding steps into the current step, effectively exploiting diverse contextual
information. Experimental results demonstrate that our method achieves
state-of-the-art rate-distortion performance and strikes a better balance
between compression performance and computational complexity. The code is
available at https://github.com/lyq133/LIC-HPCM.

</details>


### [98] [Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI](https://arxiv.org/abs/2507.19186)
*Niklas Bubeck,Yundi Zhang,Suprosanna Shit,Daniel Rueckert,Jiazhen Pan*

Main category: eess.IV

TL;DR: The paper analyzes generative models in medical imaging, comparing latent diffusion and autoregressive models for reconstruction and generation tasks, highlighting their trade-offs in perceptual quality and fidelity.


<details>
  <summary>Details</summary>
Motivation: To understand how generative models balance reconstruction (data fidelity) and generation (perceptual quality/diversity) in medical imaging, given their shared architectures but differing priorities.

Method: Introduces a 'generative model zoo' to systematically evaluate latent diffusion and autoregressive models across cardiac imaging tasks, including inpainting with varying masking ratios and unconditional generation.

Result: Diffusion models excel in perceptual quality for generation but hallucinate with high masking ratios; autoregressive models maintain stable perceptual performance but with lower fidelity.

Conclusion: The study reveals trade-offs between diffusion and autoregressive models, suggesting context-dependent choices for medical imaging tasks.

Abstract: In medical imaging, generative models are increasingly relied upon for two
distinct but equally critical tasks: reconstruction, where the goal is to
restore medical imaging (usually inverse problems like inpainting or
superresolution), and generation, where synthetic data is created to augment
datasets or carry out counterfactual analysis. Despite shared architecture and
learning frameworks, they prioritize different goals: generation seeks high
perceptual quality and diversity, while reconstruction focuses on data fidelity
and faithfulness. In this work, we introduce a "generative model zoo" and
systematically analyze how modern latent diffusion models and autoregressive
models navigate the reconstruction-generation spectrum. We benchmark a suite of
generative models across representative cardiac medical imaging tasks, focusing
on image inpainting with varying masking ratios and sampling strategies, as
well as unconditional image generation. Our findings show that diffusion models
offer superior perceptual quality for unconditional generation but tend to
hallucinate as masking ratios increase, whereas autoregressive models maintain
stable perceptual performance across masking levels, albeit with generally
lower fidelity.

</details>


### [99] [Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model](https://arxiv.org/abs/2507.19201)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: eess.IV

TL;DR: GCDM is a novel framework for synthesizing mammogram images and localized lesions, addressing data limitations and improving lesion-specific feature emphasis.


<details>
  <summary>Details</summary>
Motivation: Overcome data scarcity and lack of lesion diversity in mammography for deep-learning applications.

Method: Uses a latent denoising diffusion framework with soft mask embeddings and a gated conditioning branch to guide synthesis.

Result: Achieves precise lesion control and realistic, diverse mammogram synthesis.

Conclusion: GCDM is a promising tool for clinical mammogram synthesis.

Abstract: Mammography is the most commonly used imaging modality for breast cancer
screening, driving an increasing demand for deep-learning techniques to support
large-scale analysis. However, the development of accurate and robust methods
is often limited by insufficient data availability and a lack of diversity in
lesion characteristics. While generative models offer a promising solution for
data synthesis, current approaches often fail to adequately emphasize
lesion-specific features and their relationships with surrounding tissues. In
this paper, we propose Gated Conditional Diffusion Model (GCDM), a novel
framework designed to jointly synthesize holistic mammogram images and
localized lesions. GCDM is built upon a latent denoising diffusion framework,
where the noised latent image is concatenated with a soft mask embedding that
represents breast, lesion, and their transitional regions, ensuring anatomical
coherence between them during the denoising process. To further emphasize
lesion-specific features, GCDM incorporates a gated conditioning branch that
guides the denoising process by dynamically selecting and fusing the most
relevant radiomic and geometric properties of lesions, effectively capturing
their interplay. Experimental results demonstrate that GCDM achieves precise
control over small lesion areas while enhancing the realism and diversity of
synthesized mammograms. These advancements position GCDM as a promising tool
for clinical applications in mammogram synthesis. Our code is available at
https://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/

</details>


### [100] [Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis](https://arxiv.org/abs/2507.19230)
*Niels Rocholl,Ewoud Smit,Mathias Prokop,Alessa Hering*

Main category: eess.IV

TL;DR: The paper evaluates the ULS23 model for longitudinal lesion segmentation, revealing its failure due to registration errors and lesion displacement, advocating for end-to-end temporal models.


<details>
  <summary>Details</summary>
Motivation: Automated tools for longitudinal lesion analysis lack temporal consistency, and single-timepoint models like ULS23 are inadequate for tracking lesions over time.

Method: The study used a public clinical dataset of CT scans to test ULS23's performance in follow-up cases, artificially displacing lesions to probe vulnerabilities.

Result: The model's segmentation accuracy collapses with displaced lesions, highlighting its reliance on centered lesions and failure in longitudinal contexts.

Conclusion: Robust oncological tracking requires integrated, end-to-end models designed for temporal analysis, not cascading single-purpose tools.

Abstract: Longitudinal lesion analysis is crucial for oncological care, yet automated
tools often struggle with temporal consistency. While universal lesion
segmentation models have advanced, they are typically designed for single time
points. This paper investigates the performance of the ULS23 segmentation model
in a longitudinal context. Using a public clinical dataset of baseline and
follow-up CT scans, we evaluated the model's ability to segment and track
lesions over time. We identified two critical, interconnected failure modes: a
sharp degradation in segmentation quality in follow-up cases due to inter-scan
registration errors, and a subsequent breakdown of the lesion correspondence
process. To systematically probe this vulnerability, we conducted a controlled
experiment where we artificially displaced the input volume relative to the
true lesion center. Our results demonstrate that the model's performance is
highly dependent on its assumption of a centered lesion; segmentation accuracy
collapses when the lesion is sufficiently displaced. These findings reveal a
fundamental limitation of applying single-timepoint models to longitudinal
data. We conclude that robust oncological tracking requires a paradigm shift
away from cascading single-purpose tools towards integrated, end-to-end models
inherently designed for temporal analysis.

</details>


### [101] [NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography](https://arxiv.org/abs/2507.19328)
*Kirsten W. H. Maas,Danny Ruijters,Nicola Pezzotti,Anna Vilanova*

Main category: eess.IV

TL;DR: NerT-CA, a hybrid method combining neural and tensorial representations, accelerates 4D coronary artery reconstruction from sparse X-ray angiograms, outperforming prior works in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving 3D/4D coronary artery reconstruction from sparse X-ray angiography by addressing challenges like sparsity, poor contrast, and motion, while reducing reliance on manual or error-prone methods.

Method: Hybrid approach using low-rank tensorial fields for static reconstruction and neural fields for dynamic reconstruction, building on NeRF-based techniques.

Result: Achieves faster training and higher accuracy, enabling reasonable reconstructions from as few as three angiogram views.

Conclusion: NerT-CA offers a promising solution for efficient and accurate 4D coronary artery reconstruction in clinical settings.

Abstract: Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary
arteries from X-ray coronary angiography (CA) has the potential to improve
clinical procedures. However, there are multiple challenges to be addressed,
most notably, blood-vessel structure sparsity, poor background and blood vessel
distinction, sparse-views, and intra-scan motion. State-of-the-art
reconstruction approaches rely on time-consuming manual or error-prone
automatic segmentations, limiting clinical usability. Recently, approaches
based on Neural Radiance Fields (NeRF) have shown promise for automatic
reconstructions in the sparse-view setting. However, they suffer from long
training times due to their dependence on MLP-based representations. We propose
NerT-CA, a hybrid approach of Neural and Tensorial representations for
accelerated 4D reconstructions with sparse-view CA. Building on top of the
previous NeRF-based work, we model the CA scene as a decomposition of low-rank
and sparse components, utilizing fast tensorial fields for low-rank static
reconstruction and neural fields for dynamic sparse reconstruction. Our
approach outperforms previous works in both training time and reconstruction
accuracy, yielding reasonable reconstructions from as few as three angiogram
views. We validate our approach quantitatively and qualitatively on
representative 4D phantom datasets.

</details>
