<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: A survey on open-vocabulary object detection (OVOD) in UAV aerial scenes, highlighting its potential to overcome limitations of traditional methods by leveraging cross-modal text-image alignment like CLIP.


<details>
  <summary>Details</summary>
Motivation: Traditional UAV aerial object detection is limited to predefined categories, restricting applicability. OVOD, enabled by cross-modal alignment, allows detection of unseen objects via natural language, enhancing UAV intelligence.

Method: The paper constructs a systematic taxonomy of OVOD methods for aerial imagery, reviews relevant datasets, and analyzes challenges and open problems.

Result: A comprehensive survey providing a structured overview of OVOD in UAV contexts, identifying key challenges and future directions.

Conclusion: The survey serves as a roadmap for researchers, fostering innovation in OVOD for UAV applications, with ongoing updates tracked on GitHub.

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [2] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: EDNIG is a deep learning framework for low-light image enhancement, using illumination guidance and SPP for better performance, optimized with GAN and achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Enhancing low-light images is challenging; EDNIG aims to improve performance by focusing on underexposed regions and handling diverse lighting conditions.

Method: Uses U-Net with BCP-derived illumination guidance, SPP for multi-scale features, Swish activation, and GAN optimization with composite loss (adversarial, MSE, perceptual).

Result: Competitive performance in metrics and visual quality, with lower model complexity.

Conclusion: EDNIG is effective for real-world low-light image enhancement, with code available for reproducibility.

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [3] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: VLMs struggle with nonlocal visual reasoning tasks like comparative perception, saccadic search, and smooth visual search, despite excelling in complex tasks like VQA.


<details>
  <summary>Details</summary>
Motivation: To evaluate VLMs' capacity for nonlocal visual reasoning, isolating three distinct forms: comparative perception, saccadic search, and smooth visual search.

Method: Presented an evaluation suite testing VLMs on nonlocal reasoning tasks, comparing performance of flagship models (e.g., Gemini 2.5 Pro, Claude Vision 3.7, GPT-o4-mini).

Result: Flagship models failed the tests, barely exceeding random accuracy, despite performing well on primitive-vision benchmarks.

Conclusion: Current VLMs lack core visual reasoning capabilities despite gains in raw visual acuity.

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [4] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: The study explores spatial reasoning in vision-language models (VLMs) using Chain-of-Thought (CoT) prompting and reinforcement learning, finding structured SceneGraph CoT and GRPO fine-tuning improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance spatial reasoning in VLMs by evaluating prompting strategies and reinforcement learning methods.

Method: Evaluated CoT prompting strategies and fine-tuned models using GRPO on the SAT dataset, testing on CVBench.

Result: SceneGraph CoT improves accuracy; GRPO outperforms SFT in accuracy and robustness, especially under OOD conditions.

Conclusion: Structured prompting and GRPO enhance spatial reasoning and generalization in VLMs, with open-source code provided.

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [5] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: The paper introduces a method for open-vocabulary 3D object detection using 2D vision-language models, avoiding costly 3D annotations. It leverages 2D proposals, SAM segmentation, and geometric inflation for 3D bounding boxes, tested on a fog-augmented dataset.


<details>
  <summary>Details</summary>
Motivation: Current 3D object detection datasets are limited by narrow taxonomies and expensive annotations, hindering scalability. The work aims to utilize 2D foundation models for open-world 3D detection without human-labeled 3D data.

Method: The pipeline uses a 2D vision-language detector for text-conditioned proposals, segments them with SAM, and back-projects into 3D using camera geometry and pseudo-depth. Geometric inflation (DBSCAN and Rotating Calipers) infers 3D boxes without training.

Result: The method achieves competitive localization performance in LiDAR-based and RGB-D settings, remaining training-free and open-vocabulary.

Conclusion: The work demonstrates the potential of 2D foundation models for scalable 3D perception, with code and resources made publicly available.

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [6] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: A novel multimodal multitask network with a shared transformer architecture and cross-attention mechanisms, handling 12 modalities and achieving state-of-the-art results across 25 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of processing and integrating diverse data modalities (e.g., image, text, audio) in a unified framework for multitask learning.

Method: Uses modality-specific tokenizers, a shared transformer, and cross-attention for embedding. Introduces iterative modality switching for pretraining and a training algorithm balancing joint and pairwise modality training.

Result: Achieves state-of-the-art performance across 25 datasets from 12 modalities.

Conclusion: The proposed architecture, pretraining strategy, and training algorithm effectively handle multimodal multitask scenarios.

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [7] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: An end-to-end deep learning framework using Transformer-based models enhances medical rehabilitation by addressing noise and missing data in motion capture, detecting anomalies in real time.


<details>
  <summary>Details</summary>
Motivation: To improve medical rehabilitation by tackling data noise, occlusion issues, and ensuring patient safety through real-time anomaly detection.

Method: Integrates optical motion capture with a Transformer-based model for temporal sequence modeling to denoise and complete data.

Result: Superior performance in data reconstruction and anomaly detection on stroke and orthopedic rehabilitation datasets.

Conclusion: Provides a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [8] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: A novel framework combining Vision Transformers (ViT) and Graph Neural Networks (GNN) improves breast cancer detection, achieving 84.2% accuracy on the CBIS-DDSM dataset.


<details>
  <summary>Details</summary>
Motivation: Early detection of breast cancer is crucial for survival, but current methods lack accuracy and interpretability.

Method: Integrates ViT for global image features and GNN for structural relationships, tested on the CBIS-DDSM dataset.

Result: Achieves 84.2% accuracy, surpassing traditional methods, with interpretable attention heatmaps.

Conclusion: The framework enhances detection accuracy and provides interpretability, aiding radiologists in clinical diagnosis.

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [9] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: Butter is a novel object detection framework for autonomous driving, enhancing hierarchical feature representations with FAFCE and PHFFNet, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing architectures like YOLO and DETR struggle with feature consistency and computational efficiency in multi-scale object detection for autonomous driving.

Method: Butter introduces FAFCE for feature consistency via adaptive frequency filtering and PHFFNet for progressive hierarchical feature fusion.

Result: Experiments on BDD100K, KITTI, and Cityscapes show improved detection accuracy and reduced complexity.

Conclusion: Butter balances accuracy, deployability, and efficiency, advancing real-time object detection for autonomous driving.

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [10] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute is an LLM-based system for multimodal video retrieval, optimizing modality selection to reduce computational costs by 41% while maintaining competitive performance (60.9% Recall@5).


<details>
  <summary>Details</summary>
Motivation: Existing methods like dense text captions are costly and miss visual details, prompting the need for a smarter routing system.

Method: Uses GPT-4.1 to analyze query intent and route queries across ASR, OCR, and visual indices, averaging 1.78 modalities per query.

Result: Achieves 60.9% Recall@5 with 41% less computational overhead compared to exhaustive search.

Conclusion: ModaRoute offers a scalable, cost-effective solution for real-world multimodal retrieval.

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [11] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: The paper surveys industrial defect detection, highlighting the shift from closed-set to open-set frameworks and the role of computer vision and deep learning in advancing the field.


<details>
  <summary>Details</summary>
Motivation: Conventional inspection methods are inadequate for modern manufacturing demands, prompting the need for advanced defect detection techniques.

Method: The survey analyzes closed-set and open-set defect detection strategies in 2D and 3D modalities, tracking their evolution.

Result: Open-set techniques are gaining prominence, reducing reliance on extensive defect annotations and enabling novel anomaly recognition.

Conclusion: The paper provides a comprehensive overview of industrial defect detection, identifying challenges and emerging trends in this rapidly evolving field.

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [12] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: The paper explores the impact of integrating additional geospatial data layers with optical satellite imagery in machine learning models, finding significant performance improvements, especially in data-limited and out-of-sample scenarios.


<details>
  <summary>Details</summary>
Motivation: To assess the value of combining non-optical geospatial data (e.g., elevation, temperature) with optical satellite imagery in supervised learning tasks for better model performance.

Method: Augmented benchmark tasks by appending extra geographic data layers to datasets for classification, regression, and segmentation, then evaluated model performance with fused inputs.

Result: Fusing additional geographic inputs with optical imagery significantly improves model performance, particularly in data-limited and out-of-sample settings. Hard-coded fusion strategies outperformed learned ones.

Conclusion: Multi-modal inputs enhance data efficiency and out-of-sample performance in SatML models, with hard-coded fusion being surprisingly effective, suggesting directions for future research.

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [13] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: A minimalist method for concept erasure in generative models avoids excessive modifications, maintaining model utility while addressing safety and copyright concerns.


<details>
  <summary>Details</summary>
Motivation: Address safety and copyright issues in generative models by minimizing unwanted concept retention without degrading overall performance.

Method: Formulates a concept erasure objective based on distributional distance of outputs, uses differentiable optimization with backpropagation, and incorporates neuron masking.

Result: Empirical evaluations show robust concept erasure without performance degradation in state-of-the-art models.

Conclusion: The method enables safer and more responsible generative models by effectively erasing unwanted concepts while preserving utility.

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [14] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: The paper proposes a framework using binary occupancy data to improve 3D semantic occupancy prediction, outperforming existing methods in pre-training and auto-labeling.


<details>
  <summary>Details</summary>
Motivation: High cost of annotated LiDAR data for semantic occupancy prediction motivates leveraging cheaper binary occupancy data.

Method: Decomposes prediction into binary and semantic occupancy modules to utilize binary data effectively.

Result: Outperforms existing methods in pre-training and auto-labeling tasks.

Conclusion: The framework effectively enhances 3D semantic occupancy prediction using binary data.

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [15] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: InSyn, a Transformer-based model, improves pedestrian trajectory prediction by capturing diverse interaction patterns and using the SSOS training strategy to reduce initial-step errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook specific pedestrian interaction patterns, limiting accuracy in crowded scenarios.

Method: Proposes InSyn, a Transformer-based model, and the SSOS training strategy to address interaction modeling and initial-step divergence.

Result: Outperforms baselines on ETH and UCY datasets, especially in high-density scenarios, with a 6.58% reduction in initial-step error.

Conclusion: InSyn and SSOS effectively improve trajectory prediction by modeling interactions and mitigating initial-step errors.

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [16] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: MADI enhances diffusion models for structured, controllable generation and editing via Masking-Augmented gaussian Diffusion (MAgD) and inference-time scaling with Pause Tokens.


<details>
  <summary>Details</summary>
Motivation: Improving diffusion models' editability, compositionality, and controllability for grounded visual editing and compositional control.

Method: Introduces MAgD for training with dual corruption (denoising and masked reconstruction) and Pause Tokens for inference-time capacity scaling.

Result: Enhanced editability and performance, especially with dense prompts, enabling localized and structure-aware editing.

Conclusion: MADI advances diffusion models toward general-purpose, in-context generative architectures.

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [17] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: A multimodal dataset for driver drowsiness detection, combining facial, behavioral, and biometric signals, collected over 40-minute sessions from 19 subjects in alert and drowsy states.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset capturing gradual changes in driver drowsiness using diverse signals, addressing limitations of existing datasets with discrete labels.

Method: Data collection included 3D facial video, IR footage, biometric signals (heart rate, EDA, SpO2, skin temperature, accelerometer), grip sensor data, and telemetry from a truck simulator. Drowsiness levels were self-reported using KSS every 4 minutes.

Result: A dataset with 1,400 minutes of continuous, multimodal recordings from 19 subjects, offering detailed insights into drowsiness transitions.

Conclusion: This dataset enhances drowsiness detection research by providing rich, continuous multimodal data, available upon request.

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [18] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff is a diffusion-based framework for generating smooth 3D aortic surfaces from CT/MRI volumes, addressing limitations of existing methods by requiring minimal labeled data and producing CFD-compatible meshes.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D aortic construction is essential for clinical and CFD applications, but current methods rely on large datasets and manual effort, often yielding inconsistent results.

Method: AortaDiff uses a volume-guided conditional diffusion model to generate aortic centerlines, extracts vessel contours, and fits them into smooth 3D surfaces.

Result: AortaDiff works effectively with limited data, constructing high-fidelity meshes for normal and pathological aortas, suitable for CFD analysis.

Conclusion: AortaDiff is a practical, end-to-end solution for cardiovascular research, offering high-quality visualizations and CFD compatibility.

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [19] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: The paper introduces COREVQA, a benchmark for evaluating VLMs on visual entailment tasks using crowded images, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on VQA but lack evaluation of visual entailment, especially in crowded scenes.

Method: Proposes COREVQA, a benchmark with 5608 image and synthetic true/false statement pairs derived from CrowdHuman dataset.

Result: Top VLMs achieve <80% accuracy; others range 39.98%-69.95%, highlighting limitations in crowded scene reasoning.

Conclusion: COREVQA exposes VLMs' weaknesses in visual entailment for crowded images, urging further model improvements.

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [20] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: IConMark is a novel semantic watermarking method for AI-generated images, embedding interpretable concepts to ensure robustness and human-readability, outperforming traditional techniques.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI and synthetic media necessitates reliable methods to distinguish AI-generated images from real ones, combating misinformation and ensuring authenticity. Traditional watermarking is vulnerable to attacks.

Method: IConMark embeds meaningful semantic attributes into AI-generated images, making watermarks interpretable and resilient. It is combined with StegaStamp (IConMark+SS) and TrustMark (IConMark+TM) for enhanced robustness.

Result: IConMark and its variants achieve 10.8%, 14.5%, and 15.9% higher AUROC scores for watermark detection compared to baselines, demonstrating superior accuracy and image quality retention.

Conclusion: IConMark offers a robust, interpretable solution for watermarking AI-generated images, with hybrid variants further enhancing resilience against manipulations.

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [21] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: AI-driven ensemble model achieves high accuracy (95.5%) in detecting shoulder fractures from X-rays, aiding early diagnosis in clinical settings.


<details>
  <summary>Details</summary>
Motivation: Address underdiagnosis of shoulder fractures in emergency settings by leveraging AI for scalable early detection.

Method: Developed a multi-model deep learning system (Faster R-CNN, EfficientDet, RF-DETR) using 10,000 annotated X-rays, enhanced with ensemble techniques like Soft-NMS, WBF, and NMW fusion.

Result: NMW ensemble achieved 95.5% accuracy and F1-score of 0.9610, outperforming individual models in recall and localization precision.

Conclusion: Ensemble-based AI reliably detects shoulder fractures, suitable for real-time diagnostic workflows, though limited to binary detection for rapid screening.

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [22] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: Upgrading a deep learning model with CORONA satellite imagery improved archaeological site detection in transformed landscapes, achieving high accuracy and identifying new sites.


<details>
  <summary>Details</summary>
Motivation: To enhance AI's ability to identify archaeological sites in landscapes altered or destroyed over decades.

Method: Retrained a Bing-based convolutional network with CORONA imagery for Abu Ghraib, focusing on segmentation and detection.

Result: Achieved 85% IoU and 90% accuracy, identifying four new archaeological sites.

Conclusion: AI and CORONA imagery are effective for discovering vanished archaeological sites, offering breakthroughs in landscape studies.

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [23] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: CaSTFormer, a Causal Spatio-Temporal Transformer, improves driving intention prediction by modeling causal interactions and eliminating spurious correlations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to accurately model complex spatio-temporal dependencies and variability in human driving behavior, limiting safety and efficiency in human-machine co-driving systems.

Method: CaSTFormer uses Reciprocal Shift Fusion (RSF) for temporal alignment, Causal Pattern Extraction (CPE) to remove spurious correlations, and a Feature Synthesis Network (FSN) to synthesize purified representations.

Result: CaSTFormer outperforms existing methods on the Brain4Cars dataset, capturing complex dependencies and improving prediction accuracy and transparency.

Conclusion: CaSTFormer advances driving intention prediction by explicitly modeling causal interactions, enhancing both performance and interpretability.

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [24] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: PhyWorldBench is a benchmark for evaluating video generation models on their adherence to physics, covering fundamental to complex scenarios, including an "Anti-Physics" category. It assesses 12 models via human and MLLM-based evaluation, identifying challenges and offering prompt-crafting recommendations.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with accurately simulating physical phenomena, necessitating a benchmark to evaluate and improve their physics realism.

Method: PhyWorldBench evaluates models using 1,050 curated prompts across fundamental, composite, and anti-physics scenarios, combining human and MLLM-based zero-shot evaluation.

Result: The study identifies key challenges in models' adherence to physics, providing detailed comparisons of 12 state-of-the-art models and their performance across diverse physical phenomena.

Conclusion: The benchmark highlights the need for improved physics realism in video generation models and offers targeted recommendations for enhancing fidelity through prompt design.

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [25] [Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation](https://arxiv.org/abs/2507.13486)
*Debao Huang,Rongjun Qin*

Main category: cs.CV

TL;DR: The paper presents a framework for quantifying uncertainty in photogrammetric point clouds, addressing gaps in Multi-view Stereo (MVS) uncertainty estimation by using a self-calibrating method with reliable n-view points.


<details>
  <summary>Details</summary>
Motivation: Photogrammetric point clouds lack standardized uncertainty quantification in the MVS stage, unlike LiDAR, due to its non-differentiable and multi-modal nature.

Method: Proposes a self-calibrating method using reliable n-view points (n>=6) to regress disparity uncertainty with relevant cues from MVS, ensuring self-supervised and robust error propagation.

Result: Outperforms existing methods by achieving high bounding rates without overestimating uncertainty, validated on airborne and UAV datasets.

Conclusion: The framework provides a robust and certifiable uncertainty quantification for photogrammetric point clouds, closing the gap in MVS uncertainty estimation.

Abstract: Uncertainty quantification of the photogrammetry process is essential for
providing per-point accuracy credentials of the point clouds. Unlike airborne
LiDAR, which typically delivers consistent accuracy across various scenes, the
accuracy of photogrammetric point clouds is highly scene-dependent, since it
relies on algorithm-generated measurements (i.e., stereo or multi-view stereo).
Generally, errors of the photogrammetric point clouds propagate through a
two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),
followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM
stage has been well studied using the first-order statistics of the
reprojection error function, that in the MVS stage remains largely unsolved and
non-standardized, primarily due to its non-differentiable and multi-modal
nature (i.e., from pixel values to geometry). In this paper, we present an
uncertainty quantification framework closing this gap by associating an error
covariance matrix per point accounting for this two-step photogrammetry
process. Specifically, to estimate the uncertainty in the MVS stage, we propose
a novel, self-calibrating method by taking reliable n-view points (n>=6)
per-view to regress the disparity uncertainty using highly relevant cues (such
as matching cost values) from the MVS stage. Compared to existing approaches,
our method uses self-contained, reliable 3D points extracted directly from the
MVS process, with the benefit of being self-supervised and naturally adhering
to error propagation path of the photogrammetry process, thereby providing a
robust and certifiable uncertainty quantification across diverse scenes. We
evaluate the framework using a variety of publicly available airborne and UAV
imagery datasets. Results demonstrate that our method outperforms existing
approaches by achieving high bounding rates without overestimating uncertainty.

</details>


### [26] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorov*

Main category: cs.CV

TL;DR: A 3D convolutional autoencoder with temporal encodings is used for unsupervised stress detection in sugar-beet fields using Sentinel-2 SITS data.


<details>
  <summary>Details</summary>
Motivation: To address stress detection in sugar-beet fields without labeled data, leveraging the spectral and temporal richness of SITS.

Method: A 3D convolutional autoencoder extracts features from Sentinel-2 sequences, enhanced with temporal encodings for growth dynamics. Downstream clustering separates stressed from healthy fields.

Result: The system effectively detects stress in sugar-beet fields and is applicable across different years.

Conclusion: The proposed unsupervised approach offers a practical tool for agricultural stress detection using SITS data.

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [27] [SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM](https://arxiv.org/abs/2507.13527)
*Levi Harris,Md Jayed Hossain,Mufan Qiu,Ruichen Zhang,Pingchuan Ma,Tianlong Chen,Jiaqi Gu,Seth Ariel Tongay,Umberto Celano*

Main category: cs.CV

TL;DR: SparseC-AFM, a deep learning model, accelerates conductivity mapping of 2D materials like MoS$_2$ from sparse C-AFM scans, reducing acquisition time by 11x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for faster and robust electrical characterization of 2D materials in nanoelectronics, overcoming the slow data acquisition of traditional C-AFM.

Method: Introduces SparseC-AFM, a deep learning model that reconstructs conductivity maps from sparse C-AFM scans, validated across various conditions.

Result: Achieves 11x faster acquisition time, accurately extracts material parameters, and matches electrical properties of full-resolution C-AFM data.

Conclusion: SparseC-AFM bridges AI-assisted 2D material characterization from lab research to industrial applications, with open-source code available.

Abstract: The increasing use of two-dimensional (2D) materials in nanoelectronics
demands robust metrology techniques for electrical characterization, especially
for large-scale production. While atomic force microscopy (AFM) techniques like
conductive AFM (C-AFM) offer high accuracy, they suffer from slow data
acquisition speeds due to the raster scanning process. To address this, we
introduce SparseC-AFM, a deep learning model that rapidly and accurately
reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM
scans. Our approach is robust across various scanning modes, substrates, and
experimental conditions. We report a comparison between (a) classic flow
implementation, where a high pixel density C-AFM image (e.g., 15 minutes to
collect) is manually parsed to extract relevant material parameters, and (b)
our SparseC-AFM method, which achieves the same operation using data that
requires substantially less acquisition time (e.g., under 5 minutes).
SparseC-AFM enables efficient extraction of critical material parameters in
MoS$_2$, including film coverage, defect density, and identification of
crystalline island boundaries, edges, and cracks. We achieve over 11x reduction
in acquisition time compared to manual extraction from a full-resolution C-AFM
image. Moreover, we demonstrate that our model-predicted samples exhibit
remarkably similar electrical properties to full-resolution data gathered using
classic-flow scanning. This work represents a significant step toward
translating AI-assisted 2D material characterization from laboratory research
to industrial fabrication. Code and model weights are available at
github.com/UNITES-Lab/sparse-cafm.

</details>


### [28] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgrtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Wei*

Main category: cs.CV

TL;DR: A novel formulation for second-order total generalized variation (TGV) of normal vectors on 3D meshes is proposed, extending discrete TGV models to manifold-valued functions.


<details>
  <summary>Details</summary>
Motivation: To generalize discrete TGV models for scalar data to manifold-valued functions, specifically normal vectors on 3D meshes.

Method: Constructs a tangential Raviart-Thomas type finite element space for the manifold setting and applies it to mesh denoising.

Result: The new regularizer is evaluated and compared to existing methods in mesh denoising experiments.

Conclusion: The proposed formulation successfully extends TGV to manifold-valued functions, demonstrating effectiveness in mesh denoising.

Abstract: We propose a novel formulation for the second-order total generalized
variation (TGV) of the normal vector on an oriented, triangular mesh embedded
in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued
function, taking values on the unit sphere. Our formulation extends previous
discrete TGV models for piecewise constant scalar data that utilize a
Raviart-Thomas function space. To exctend this formulation to the manifold
setting, a tailor-made tangential Raviart-Thomas type finite element space is
constructed in this work. The new regularizer is compared to existing methods
in mesh denoising experiments.

</details>


### [29] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA introduces a Neighborhood Adaptive Block-Level Attention mechanism to reduce computational overhead in video diffusion transformers while maintaining generative quality.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of full attention in transformers is a bottleneck for high-resolution and long-duration video generation.

Method: Proposes NABLA, a block-wise attention mechanism with adaptive sparsity-driven thresholds, compatible with PyTorch's Flex Attention.

Result: Achieves up to 2.7x faster training and inference with minimal quality loss (CLIP, VBench, human evaluation scores).

Conclusion: NABLA efficiently addresses computational challenges in video generation without compromising quality.

Abstract: Recent progress in transformer-based architectures has demonstrated
remarkable success in video generation tasks. However, the quadratic complexity
of full attention mechanisms remains a critical bottleneck, particularly for
high-resolution and long-duration video sequences. In this paper, we propose
NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that
dynamically adapts to sparsity patterns in video diffusion transformers (DiTs).
By leveraging block-wise attention with adaptive sparsity-driven threshold,
NABLA reduces computational overhead while preserving generative quality. Our
method does not require custom low-level operator design and can be seamlessly
integrated with PyTorch's Flex Attention operator. Experiments demonstrate that
NABLA achieves up to 2.7x faster training and inference compared to baseline
almost without compromising quantitative metrics (CLIP score, VBench score,
human evaluation score) and visual quality drop. The code and model weights are
available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [30] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: The paper proposes a LoRA-enhanced synthetic-replay framework for continual learning in vision-language models, improving replay fidelity by adapting Stable Diffusion with task-specific low-rank adapters and confidence-based sample selection.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic-replay methods generate misaligned samples due to domain-specific nuances, undermining knowledge retention. The goal is to enhance replay fidelity by adapting the generator to capture task-specific patterns.

Method: A LoRA-enhanced framework injects task-specific low-rank adapters into Stable Diffusion. It uses a two-stage, confidence-based sample selection: first ranking real task data by VLM confidence, then generating and selecting synthetic samples similarly.

Result: The method outperforms previous synthetic-replay techniques on the MTIL benchmark, balancing plasticity, stability, and zero-shot capability.

Conclusion: Generator adaptation via LoRA effectively improves continual learning in VLMs, demonstrating robust performance in retaining prior knowledge while learning new tasks.

Abstract: Continual learning for vision-language models has achieved remarkable
performance through synthetic replay, where samples are generated using Stable
Diffusion to regularize during finetuning and retain knowledge. However,
real-world downstream applications often exhibit domain-specific nuances and
fine-grained semantics not captured by generators, causing synthetic-replay
methods to produce misaligned samples that misguide finetuning and undermine
retention of prior knowledge. In this work, we propose a LoRA-enhanced
synthetic-replay framework that injects task-specific low-rank adapters into a
frozen Stable Diffusion model, efficiently capturing each new task's unique
visual and semantic patterns. Specifically, we introduce a two-stage,
confidence-based sample selection: we first rank real task data by
post-finetuning VLM confidence to focus LoRA finetuning on the most
representative examples, then generate synthetic samples and again select them
by confidence for distillation. Our approach integrates seamlessly with
existing replay pipelines-simply swap in the adapted generator to boost replay
fidelity. Extensive experiments on the Multi-domain Task Incremental Learning
(MTIL) benchmark show that our method outperforms previous synthetic-replay
techniques, achieving an optimal balance among plasticity, stability, and
zero-shot capability. These results demonstrate the effectiveness of generator
adaptation via LoRA for robust continual learning in VLMs.

</details>


### [31] [NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision](https://arxiv.org/abs/2507.13595)
*Tengkai Wang,Weihao Li,Ruikai Cui,Shi Qiu,Nick Barnes*

Main category: cs.CV

TL;DR: NoiseSDF2NoiseSDF extends the Noise2Noise paradigm to 3D neural fields, enabling clean neural SDF learning from noisy point clouds by minimizing MSE loss between noisy SDF representations.


<details>
  <summary>Details</summary>
Motivation: Accurate implicit surface reconstruction from noisy point clouds is challenging, especially with low-quality scans.

Method: Extends Noise2Noise to 3D, using noisy supervision to learn clean neural SDFs by minimizing MSE loss between noisy SDFs.

Result: Significantly improves surface reconstruction quality on benchmarks like ShapeNet, ABC, Famous, and Real datasets.

Conclusion: NoiseSDF2NoiseSDF effectively denoises and refines surface estimations from noisy inputs.

Abstract: Reconstructing accurate implicit surface representations from point clouds
remains a challenging task, particularly when data is captured using
low-quality scanning devices. These point clouds often contain substantial
noise, leading to inaccurate surface reconstructions. Inspired by the
Noise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel
method designed to extend this concept to 3D neural fields. Our approach
enables learning clean neural SDFs directly from noisy point clouds through
noisy supervision by minimizing the MSE loss between noisy SDF representations,
allowing the network to implicitly denoise and refine surface estimations. We
evaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the
ShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that
our framework significantly improves surface reconstruction quality from noisy
inputs.

</details>


### [32] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel diffusion model (DM)-based framework, dubbed \ours, is proposed for unsupervised image deblurring by learning spatially varying texture priors from unpaired data. It outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Learning blind image deblurring from unpaired data is practical but challenging due to the complexity of real-world blur patterns. Existing adversarial learning methods are insufficient.

Method: The framework uses a Texture Prior Encoder (TPE) with a memory mechanism for DM training and a Texture Transfer Transformer layer (TTformer) with Filter-Modulated Multi-head Self-Attention (FM-MSA) for blur removal. A wavelet-based adversarial loss preserves texture details.

Result: Extensive evaluations show \ours outperforms SOTA methods in benchmarks.

Conclusion: \ours provides a promising unsupervised deblurring solution by leveraging texture priors and adaptive filtering.

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is
difficult and expensive, learning blind image deblurring from unpaired data is
a more practical and promising solution. Unfortunately, dominant approaches
rely heavily on adversarial learning to bridge the gap from blurry domains to
sharp domains, ignoring the complex and unpredictable nature of real-world blur
patterns. In this paper, we propose a novel diffusion model (DM)-based
framework, dubbed \ours, for image deblurring by learning spatially varying
texture prior from unpaired data. In particular, \ours performs DM to generate
the prior knowledge that aids in recovering the textures of blurry images. To
implement this, we propose a Texture Prior Encoder (TPE) that introduces a
memory mechanism to represent the image textures and provides supervision for
DM training. To fully exploit the generated texture priors, we present the
Texture Transfer Transformer layer (TTformer), in which a novel
Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes
spatially varying blurring through adaptive filtering. Furthermore, we
implement a wavelet-based adversarial loss to preserve high-frequency texture
details. Extensive evaluations show that \ours provides a promising
unsupervised deblurring solution and outperforms SOTA methods in widely-used
benchmarks.

</details>


### [33] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: The paper proposes a diffusion model for burst Super Resolution (SR) to address blurry outputs from deterministic methods, improving efficiency with a stochastic sampler and one-step diffusion.


<details>
  <summary>Details</summary>
Motivation: Prior burst SR methods produce blurry images, which degrade perceptual quality. The goal is to reconstruct sharp, high-fidelity SR images.

Method: Uses a diffusion model with a stochastic sampler (high-order ODE) and one-step diffusion via knowledge distillation.

Result: Reduces runtime to 1.6% of baseline while maintaining SR quality in distortion and perceptual metrics.

Conclusion: The method efficiently enhances burst SR by combining diffusion models with optimized sampling, balancing speed and quality.

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super
Resolution (SR) image compared to a single LR image, prior burst SR methods are
trained in a deterministic manner, which produces a blurry SR image. Since such
blurry images are perceptually degraded, we aim to reconstruct sharp and
high-fidelity SR images by a diffusion model. Our method improves the
efficiency of the diffusion model with a stochastic sampler with a high-order
ODE as well as one-step diffusion using knowledge distillation. Our
experimental results demonstrate that our method can reduce the runtime to 1.6
% of its baseline while maintaining the SR quality measured based on image
distortion and perceptual quality.

</details>


### [34] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: CoTasks introduces a framework for enhancing VideoLLMs with chain-of-thought reasoning by decomposing complex video questions into four foundational tasks, improving performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing VideoLLMs lack structured annotations for fine-grained, object-level reasoning, limiting their ability for compositional, step-by-step video understanding.

Method: CoTasks decomposes video questions into frame localization, entity tracking, and spatial/temporal relation extraction, embedding these as intermediate reasoning steps.

Result: CoTasks boosts performance: LLaVA-video-7B improves by +3.3 GPT-4 score, Qwen2.5-VL-3B by +17.4, with significant gains in causal, temporal, and descriptive reasoning.

Conclusion: CoTasks effectively enhances VideoLLMs' compositional reasoning through structured, object-centric supervision.

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [35] [Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation](https://arxiv.org/abs/2507.13628)
*Masahiro Ogawa,Qi An,Atsushi Yamashita*

Main category: cs.CV

TL;DR: FoELS integrates optical flow and texture to separate moving objects in complex scenes with camera motion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing optical flow-based methods struggle with detecting moving objects in structured scenes involving camera motion.

Method: FoELS combines optical flow (FoE computation) and texture information, fusing motion likelihood with segmentation for moving probability estimation.

Result: Effective in complex scenes, rotational camera motion, and parallel motion; achieves state-of-the-art performance on DAVIS 2016 and real-world traffic videos.

Conclusion: FoELS is a robust solution for moving object detection in dynamic scenes with camera motion.

Abstract: Separating moving and static objects from a moving camera viewpoint is
essential for 3D reconstruction, autonomous navigation, and scene understanding
in robotics. Existing approaches often rely primarily on optical flow, which
struggles to detect moving objects in complex, structured scenes involving
camera motion. To address this limitation, we propose Focus of Expansion
Likelihood and Segmentation (FoELS), a method based on the core idea of
integrating both optical flow and texture information. FoELS computes the focus
of expansion (FoE) from optical flow and derives an initial motion likelihood
from the outliers of the FoE computation. This likelihood is then fused with a
segmentation-based prior to estimate the final moving probability. The method
effectively handles challenges including complex structured scenes, rotational
camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016
dataset and real-world traffic videos demonstrate its effectiveness and
state-of-the-art performance.

</details>


### [36] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: EPSilon introduces efficient point sampling strategies (ERO and EIO) to speed up hybrid NeRF-SMPL avatar generation by omitting empty points, achieving 20x faster inference and 4x faster training without quality loss.


<details>
  <summary>Details</summary>
Motivation: Hybrid NeRF-SMPL models for animatable avatars are slow due to unnecessary deformation computations on empty points.

Method: Proposes EPSilon with two sampling strategies: Empty Ray Omission (ERO) and Empty Interval Omission (EIO) to skip empty points.

Result: EPSilon uses only 3.9% of sampled points, achieves 20x faster inference, and 4x faster training while maintaining quality.

Conclusion: EPSilon's efficient sampling optimizes hybrid avatar generation, balancing speed and quality.

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [37] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces EvReID, a large-scale RGB-event person ReID dataset, and proposes TriPro-ReID, an attribute-guided contrastive learning framework to enhance feature learning.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in event camera-based person ReID by providing a large-scale dataset and improving feature learning methods.

Method: Creation of the EvReID dataset and development of the TriPro-ReID framework, which uses pedestrian attributes for contrastive learning.

Result: The EvReID dataset contains 118,988 image pairs across 1200 identities. TriPro-ReID shows effectiveness on EvReID and MARS datasets.

Conclusion: The work provides a benchmark dataset and a novel framework, advancing event-based person ReID research.

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [38] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: PW-FNet, a novel image restoration method, uses pyramid Wavelet-Fourier processing to improve efficiency and quality, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Adverse weather degrades image quality, hindering downstream tasks. Existing transformer-based methods are complex and inefficient for real-time use.

Method: PW-FNet integrates pyramid wavelet-based multi-scale decomposition and Fourier transforms to replace self-attention, reducing complexity.

Result: PW-FNet excels in tasks like deraining, dehazing, and super-resolution, offering better quality and efficiency than current methods.

Conclusion: PW-FNet is a highly efficient and effective baseline for image restoration, balancing performance and computational cost.

Abstract: Natural image quality is often degraded by adverse weather conditions,
significantly impairing the performance of downstream tasks. Image restoration
has emerged as a core solution to this challenge and has been widely discussed
in the literature. Although recent transformer-based approaches have made
remarkable progress in image restoration, their increasing system complexity
poses significant challenges for real-time processing, particularly in
real-world deployment scenarios. To this end, most existing methods attempt to
simplify the self-attention mechanism, such as by channel self-attention or
state space model. However, these methods primarily focus on network
architecture while neglecting the inherent characteristics of image restoration
itself. In this context, we explore a pyramid Wavelet-Fourier iterative
pipeline to demonstrate the potential of Wavelet-Fourier processing for image
restoration. Inspired by the above findings, we propose a novel and efficient
restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).
Specifically, PW-FNet features two key design principles: 1) at the inter-block
level, integrates a pyramid wavelet-based multi-input multi-output structure to
achieve multi-scale and multi-frequency bands decomposition; and 2) at the
intra-block level, incorporates Fourier transforms as an efficient alternative
to self-attention mechanisms, effectively reducing computational complexity
while preserving global modeling capability. Extensive experiments on tasks
such as image deraining, raindrop removal, image super-resolution, motion
deblurring, image dehazing, image desnowing and underwater/low-light
enhancement demonstrate that PW-FNet not only surpasses state-of-the-art
methods in restoration quality but also achieves superior efficiency, with
significantly reduced parameter size, computational cost and inference time.

</details>


### [39] [MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training](https://arxiv.org/abs/2507.13673)
*Yuechen Xie,Haobo Jiang,Jian Yang,Yigong Zhang,Jin Xie*

Main category: cs.CV

TL;DR: MaskHOI is a novel MAE-driven pretraining framework for 3D hand-object interaction pose estimation, addressing challenges like geometric ambiguity and occlusions through region-specific masking and SDF-driven learning.


<details>
  <summary>Details</summary>
Motivation: Precise joint pose estimation in 3D hand-object interactions from monocular RGB is challenging due to geometric ambiguity and occlusions.

Method: Proposes MaskHOI with region-specific masking ratios and skeleton-driven hand masking, plus a masked SDF-driven multimodal learning mechanism.

Result: Outperforms state-of-the-art methods in experiments.

Conclusion: MaskHOI effectively improves geometric-aware and occlusion-robust representation learning for HOI tasks.

Abstract: In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of
hands and objects from monocular RGB input remains highly challenging due to
the inherent geometric ambiguity of RGB images and the severe mutual occlusions
that occur during interaction.To address these challenges, we propose MaskHOI,
a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI
pose estimation. Our core idea is to leverage the masking-then-reconstruction
strategy of MAE to encourage the feature encoder to infer missing spatial and
structural information, thereby facilitating geometric-aware and
occlusion-robust representation learning. Specifically, based on our
observation that human hands exhibit far greater geometric complexity than
rigid objects, conventional uniform masking fails to effectively guide the
reconstruction of fine-grained hand structures. To overcome this limitation, we
introduce a Region-specific Mask Ratio Allocation, primarily comprising the
region-specific masking assignment and the skeleton-driven hand masking
guidance. The former adaptively assigns lower masking ratios to hand regions
than to rigid objects, balancing their feature learning difficulty, while the
latter prioritizes masking critical hand parts (e.g., fingertips or entire
fingers) to realistically simulate occlusion patterns in real-world
interactions. Furthermore, to enhance the geometric awareness of the pretrained
encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven
multimodal learning mechanism. Through the self-masking 3D SDF prediction, the
learned encoder is able to perceive the global geometric structure of hands and
objects beyond the 2D image plane, overcoming the inherent limitations of
monocular input and alleviating self-occlusion issues. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches.

</details>


### [40] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse is a unified framework for cooperative perception in heterogeneous V2X systems, using adaptive feature fusion and learning to outperform baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in feature fusion and perception reliability due to heterogeneous sensor configurations in V2X systems.

Method: Hierarchical fusion with channel-wise and spatial attention, adaptive spatial resolution adjustment, and dynamic cooperative learning.

Result: Achieves 43.22% 3D mAP (LC+LC) and 43.38% (L+LC), outperforming CoopDet3D by 1.17%, with robust performance across nine configurations.

Conclusion: HeCoFuse is state-of-the-art for heterogeneous V2X cooperative perception, validated by CVPR 2025 DriveX challenge results.

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [41] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: A novel Gaussian kernel-based method for high-precision, sub-pixel motion measurement in structural health monitoring, eliminating manual tuning needs.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of current vision-based methods in achieving high accuracy for sub-pixel motion measurement without extensive manual parameter tuning.

Method: Developed a Gaussian kernel-based motion measurement method with motion consistency and super-resolution constraints to enhance accuracy and robustness.

Result: Achieves consistent high accuracy in numerical and experimental validations without requiring customized parameter setups.

Conclusion: The proposed method offers a reliable, low-cost solution for precise structural health monitoring.

Abstract: The growing demand for structural health monitoring has driven increasing
interest in high-precision motion measurement, as structural information
derived from extracted motions can effectively reflect the current condition of
the structure. Among various motion measurement techniques, vision-based
methods stand out due to their low cost, easy installation, and large-scale
measurement. However, when it comes to sub-pixel-level motion measurement,
current vision-based methods either lack sufficient accuracy or require
extensive manual parameter tuning (e.g., pyramid layers, target pixels, and
filter parameters) to reach good precision. To address this issue, we developed
a novel Gaussian kernel-based motion measurement method, which can extract the
motion between different frames via tracking the location of Gaussian kernels.
The motion consistency, which fits practical structural conditions, and a
super-resolution constraint, are introduced to increase accuracy and robustness
of our method. Numerical and experimental validations show that it can
consistently reach high accuracy without customized parameter setup for
different test samples.

</details>


### [42] [GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms](https://arxiv.org/abs/2507.13706)
*ngel F. Garca-Fernndez,Jinhao Gu,Lennart Svensson,Yuxuan Xia,Jan Krej,Oliver Kost,Ondej Straka*

Main category: cs.CV

TL;DR: The paper introduces two quasi-metrics for evaluating multi-object tracking (MOT) algorithms, extending GOSPA and T-GOSPA metrics with flexible cost penalties.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing MOT performance metrics by allowing asymmetric costs for missed/false objects and non-symmetric localization errors.

Method: Extends GOSPA and T-GOSPA metrics into quasi-metrics with customizable penalties. Evaluates Bayesian MOT algorithms using simulations.

Result: The proposed quasi-metrics offer flexibility in penalizing errors, useful for specific MOT applications.

Conclusion: The quasi-metrics enhance MOT evaluation by accommodating asymmetric costs, demonstrated via simulations.

Abstract: This paper introduces two quasi-metrics for performance assessment of
multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an
extension of the generalised optimal subpattern assignment (GOSPA) metric and
measures the discrepancy between sets of objects. The other quasi-metric is an
extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy
between sets of trajectories. Similar to the GOSPA-based metrics, these
quasi-metrics include costs for localisation error for properly detected
objects, the number of false objects and the number of missed objects. The
T-GOSPA quasi-metric also includes a track switching cost. Differently from the
GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of
penalising missed and false objects with different costs, and the localisation
costs are not required to be symmetric. These properties can be useful in MOT
evaluation in certain applications. The performance of several Bayesian MOT
algorithms is assessed with the T-GOSPA quasi-metric via simulations.

</details>


### [43] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: The paper introduces PoemTale Diffusion, a training-free method to improve image generation from poetic texts by refining prompts and modifying self-attention mechanisms in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with creative expressions like poetry, which often involve abstract and layered meanings.

Method: Proposes a multi-stage prompt refinement loop and modifies self-attention mechanisms in diffusion models to generate consistent images.

Result: Validated by human and quantitative evaluations, the method enhances poem-to-image generation and introduces the P4I dataset.

Conclusion: PoemTale Diffusion offers a novel approach to better capture poetic meanings in generated images, advancing research in creative text-to-image generation.

Abstract: Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.

</details>


### [44] [Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction](https://arxiv.org/abs/2507.13719)
*Daniele Pannone,Alessia Castronovo,Maurizio Mancini,Gian Luca Foresti,Claudio Piciarelli,Rossana Gabrieli,Muhammad Yasir Bilal,Danilo Avola*

Main category: cs.CV

TL;DR: An AR pipeline for museums uses two depth estimation models (GLPN and Depth-Anything) to create accurate 3D models from single images, improving reconstruction accuracy and visitor engagement.


<details>
  <summary>Details</summary>
Motivation: To enhance museum visitor engagement by providing immersive AR experiences through accurate 3D reconstructions of artworks from single images.

Method: Integrates GLPN for global scene structure and Depth-Anything for local details, converting depth maps into point clouds and meshes using advanced neural networks and computer vision.

Result: Significant improvements in reconstruction accuracy and visual realism, making the system robust for museum applications.

Conclusion: The proposed pipeline is effective for creating immersive AR experiences in museums, overcoming challenges in artwork reconstruction.

Abstract: This paper presents an innovative augmented reality pipeline tailored for
museum environments, aimed at recognizing artworks and generating accurate 3D
models from single images. By integrating two complementary pre-trained depth
estimation models, i.e., GLPN for capturing global scene structure and
Depth-Anything for detailed local reconstruction, the proposed approach
produces optimized depth maps that effectively represent complex artistic
features. These maps are then converted into high-quality point clouds and
meshes, enabling the creation of immersive AR experiences. The methodology
leverages state-of-the-art neural network architectures and advanced computer
vision techniques to overcome challenges posed by irregular contours and
variable textures in artworks. Experimental results demonstrate significant
improvements in reconstruction accuracy and visual realism, making the system a
highly robust tool for museums seeking to enhance visitor engagement through
interactive digital content.

</details>


### [45] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: The paper analyzes StyleGAN's generator, revealing weight pruning reduces computational needs without major output loss, and examines latent vector manipulation for precise facial feature control, raising ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To understand StyleGAN's inner workings and its potential for misuse in generating realistic synthetic faces.

Method: Analyzes StyleGAN's architecture, trains a model using PyTorch, prunes weights, and examines latent vector manipulation.

Result: Weight pruning reduces computational demands; latent vector changes allow precise facial feature control.

Conclusion: The study highlights StyleGAN's capabilities and ethical risks, such as misuse for creating fake identities.

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [46] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL leverages a frozen text-to-image diffusion model for few-shot class-incremental learning, outperforming state-of-the-art methods by utilizing multi-scale features and minimal trainable components.


<details>
  <summary>Details</summary>
Motivation: Few-shot class-incremental learning (FSCIL) is challenging due to limited data and catastrophic forgetting. The paper aims to address this by leveraging a pre-trained generative model's capabilities.

Method: The proposed Diffusion-FSCIL uses a frozen diffusion model backbone, extracts complementary diffusion features for latent replay, and employs feature distillation to mitigate biases. It emphasizes efficiency with minimal trainable components.

Result: Experiments on CUB-200, miniImageNet, and CIFAR-100 show Diffusion-FSCIL outperforms existing methods, maintaining performance on old classes and adapting well to new ones.

Conclusion: Diffusion-FSCIL effectively tackles FSCIL challenges by combining generative model strengths with efficient feature extraction and distillation.

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [47] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: EVS combines text-to-image (T2I) and text-to-video (T2V) models to improve video quality and motion smoothness without additional training, achieving faster inference.


<details>
  <summary>Details</summary>
Motivation: Existing T2V models struggle with high imaging quality and motion consistency, often causing flickering and artifacts.

Method: EVS uses a diffusion-based T2I model to refine frames and T2V backbones for motion consistency, encapsulating their strengths.

Result: EVS enhances video quality and motion smoothness, with a 1.6x-4.5x speedup in inference time.

Conclusion: EVS effectively leverages T2I and T2V models to outperform previous approaches in video synthesis.

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [48] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: The paper proposes a Spectral Diffusion Prior (SDP) and Spectral Prior Injector Module (SPIM) to enhance HSI reconstruction by capturing high-frequency details, outperforming existing methods by 0.5 dB.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based HSI reconstruction methods struggle to accurately capture high-frequency details.

Method: The paper introduces SDP, learned via a diffusion model, and SPIM to dynamically guide HSI detail recovery.

Result: The method outperforms existing networks by ~0.5 dB on MST and BISRNet benchmarks.

Conclusion: The proposed SDP and SPIM significantly improve HSI reconstruction performance.

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [49] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: The paper proposes a novel feature extraction method for image classification using Permutation Entropy (PE), combined with HOG and LBP, achieving competitive results without deep learning.


<details>
  <summary>Details</summary>
Motivation: To address the need for interpretable and computationally efficient alternatives to deep learning models in image classification.

Method: Extends PE to 2D images, integrates HOG and LBP for feature extraction, and trains SVM classifiers with a 780-dimensional feature set.

Result: Competitive performance on benchmark datasets (Fashion-MNIST, KMNIST, EMNIST, CIFAR-10) without deep architectures.

Conclusion: The fusion of PE, HOG, and LBP offers a lightweight, interpretable, and effective solution for image classification, showcasing the potential of entropy-based descriptors.

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [50] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: The paper introduces ClearVQA, a benchmark for assessing VLMs' ability to resolve ambiguities in VQA through interactive clarification, addressing gaps in existing research.


<details>
  <summary>Details</summary>
Motivation: Existing VQA research focuses on rephrasing ambiguous questions but ignores interactive clarification, which is more natural for user-VLM interactions.

Method: The authors propose ClearVQA, a benchmark targeting three common ambiguity categories in VQA, designed to evaluate VLMs' interactive clarification capabilities.

Result: ClearVQA provides a framework to assess and improve VLMs' ability to handle ambiguities through interaction, filling the gap in current benchmarks.

Conclusion: ClearVQA addresses the lack of benchmarks and VLMs' reluctance to seek clarification, advancing research in interactive VQA systems.

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [51] [SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering](https://arxiv.org/abs/2507.13779)
*Durgesh Singh,Ahcne Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper introduces a differentiable clustering module for SSL and UDA, leveraging supervised data to compute centroids, improving performance in low supervision settings.


<details>
  <summary>Details</summary>
Motivation: To enhance SSL and UDA by explicitly enforcing the clustering assumption, which is typically implicit in existing methods.

Method: Proposes an end-to-end training strategy with a differentiable clustering module, using supervised data for centroid computation.

Result: Demonstrates effectiveness in SSL and UDA, especially in low supervision regimes, as both a standalone model and a regularizer.

Conclusion: The explicit clustering approach improves performance and can complement existing methods.

Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)
enhance the model performance by exploiting information from labeled and
unlabeled data. The clustering assumption has proven advantageous for learning
with limited supervision and states that data points belonging to the same
cluster in a high-dimensional space should be assigned to the same category.
Recent works have utilized different training mechanisms to implicitly enforce
this assumption for the SSL and UDA. In this work, we take a different approach
by explicitly involving a differentiable clustering module which is extended to
leverage the supervised data to compute its centroids. We demonstrate the
effectiveness of our straightforward end-to-end training strategy for SSL and
UDA over extensive experiments and highlight its benefits, especially in low
supervision regimes, both as a standalone model and as a regularizer for
existing approaches.

</details>


### [52] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: LoFNO enhances hemodynamic analysis by improving spatiotemporal resolution and predicting WSS directly from clinical imaging data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Low spatiotemporal resolution and signal-to-noise ratio in magnetic resonance flow imaging limit its diagnostic utility for aneurysm rupture prediction.

Method: Proposes LoFNO, a 3D architecture integrating Laplacian eigenvectors for geometric priors and EDSR for upsampling, to de-noise and upsample flow data.

Result: LoFNO achieves superior velocity and WSS predictions compared to interpolation and other deep learning methods.

Conclusion: LoFNO enables more precise cerebrovascular diagnostics by enhancing resolution and accuracy in hemodynamic analysis.

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [53] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: DynFaceRestore is a novel blind face restoration method that dynamically adjusts diffusion sampling timesteps and guidance scales to balance fidelity and quality, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing methods assume uniform degradation and use fixed diffusion parameters, leading to imbalances between fidelity and quality in restored facial images.

Method: DynFaceRestore maps degraded inputs to Gaussian blurry images, dynamically selects timesteps, and uses closed-form guidance with local dynamic scaling for detail enhancement.

Result: The method achieves state-of-the-art performance in quantitative and qualitative evaluations, demonstrating robustness.

Conclusion: DynFaceRestore effectively balances fidelity and quality in blind face restoration, offering a superior solution to existing limitations.

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial
images from unknown degraded inputs, presenting significant challenges in
preserving both identity and detail. Pre-trained diffusion models have been
increasingly used as image priors to generate fine details. Still, existing
methods often use fixed diffusion sampling timesteps and a global guidance
scale, assuming uniform degradation. This limitation and potentially imperfect
degradation kernel estimation frequently lead to under- or over-diffusion,
resulting in an imbalance between fidelity and quality. We propose
DynFaceRestore, a novel blind face restoration approach that learns to map any
blindly degraded input to Gaussian blurry images. By leveraging these blurry
images and their respective Gaussian kernels, we dynamically select the
starting timesteps for each blurry image and apply closed-form guidance during
the diffusion sampling process to maintain fidelity. Additionally, we introduce
a dynamic guidance scaling adjuster that modulates the guidance strength across
local regions, enhancing detail generation in complex areas while preserving
structural fidelity in contours. This strategy effectively balances the
trade-off between fidelity and quality. DynFaceRestore achieves
state-of-the-art performance in both quantitative and qualitative evaluations,
demonstrating robustness and effectiveness in blind face restoration.

</details>


### [54] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: CF-SSC is a temporal SSC framework using pseudo-future frame prediction to enhance 3D scene completion by modeling spatial-temporal relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing monocular SSC methods struggle with occlusions and limited field of view in real-world traffic scenarios.

Method: Combines poses and depths for accurate 3D correspondences, fusing past, present, and predicted future frames in 3D space with a 3D-aware architecture.

Result: Achieves state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360 benchmarks, improving occlusion reasoning and accuracy.

Conclusion: CF-SSC effectively addresses occlusion challenges and enhances 3D scene completion by leveraging temporal data.

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [55] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: GRAM-MAMBA is a framework for efficient and robust multimodal fusion in IoT, addressing high complexity, unidirectional alignment, and missing data issues.


<details>
  <summary>Details</summary>
Motivation: Existing IoT multimodal systems struggle with complexity, poor alignment, and missing data, limiting real-world deployment.

Method: Uses Mamba for efficient time-series processing, GRAM matrix for pairwise alignment, and LoRA-inspired adaptive layers for missing data.

Result: Outperforms baselines in indoor positioning and activity recognition, with significant performance boosts and minimal parameter updates.

Conclusion: GRAM-MAMBA is effective for resource-constrained IoT, offering efficiency and robustness in multimodal perception.

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely
deployed in smart homes, intelligent transport, industrial automation, and
healthcare. However, existing systems often face challenges: high model
complexity hinders deployment in resource-constrained environments,
unidirectional modal alignment neglects inter-modal relationships, and
robustness suffers when sensor data is missing. These issues impede efficient
and robust multimodal perception in real-world IoT settings. To overcome these
limitations, we propose GRAM-MAMBA. This framework utilizes the
linear-complexity Mamba model for efficient sensor time-series processing,
combined with an optimized GRAM matrix strategy for pairwise alignment among
modalities, addressing the shortcomings of traditional single-modality
alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive
low-rank layer compensation strategy to handle missing modalities
post-training. This strategy freezes the pre-trained model core and irrelevant
adaptive layers, fine-tuning only those related to available modalities and the
fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On
the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower
error than baselines; adapting to missing modalities yields a 24.5% performance
boost by training less than 0.2% of parameters. On the USC-HAD human activity
recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),
outperforming prior work; the update strategy increases F1 by 23% while
training less than 0.3% of parameters. These results highlight GRAM-MAMBA's
potential for achieving efficient and robust multimodal perception in
resource-constrained environments.

</details>


### [56] [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/abs/2507.13812)
*Yingying Zhang,Lixiang Ru,Kang Wu,Lei Yu,Lei Liang,Yansheng Li,Jingdong Chen*

Main category: cs.CV

TL;DR: SkySense V2 is a unified multi-modal remote sensing foundation model using a single transformer backbone, tailored SSL for RS data, and innovative modules like adaptive patch merging and MoE, outperforming its predecessor by 1.8 points on average.


<details>
  <summary>Details</summary>
Motivation: Existing MM-RSFMs require separate backbones per modality, leading to redundancy, and SSL methods don't fully address RS image characteristics like complex semantics.

Method: Uses a single transformer backbone with tailored SSL, adaptive patch merging, learnable modality prompts, and MoE for enhanced performance.

Result: Outperforms SkySense by 1.8 points on average across 16 datasets and 7 tasks.

Conclusion: SkySense V2 effectively addresses modality redundancy and RS-specific challenges, demonstrating strong generalization.

Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly
advanced various Earth observation tasks, such as urban planning, environmental
monitoring, and natural disaster management. However, most existing approaches
generally require the training of separate backbone networks for each data
modality, leading to redundancy and inefficient parameter utilization.
Moreover, prevalent pre-training methods typically apply self-supervised
learning (SSL) techniques from natural images without adequately accommodating
the characteristics of remote sensing (RS) images, such as the complicated
semantic distribution within a single RS image. In this work, we present
SkySense V2, a unified MM-RSFM that employs a single transformer backbone to
handle multiple modalities. This backbone is pre-trained with a novel SSL
strategy tailored to the distinct traits of RS data. In particular, SkySense V2
incorporates an innovative adaptive patch merging module and learnable modality
prompt tokens to address challenges related to varying resolutions and limited
feature diversity across modalities. In additional, we incorporate the mixture
of experts (MoE) module to further enhance the performance of the foundation
model. SkySense V2 demonstrates impressive generalization abilities through an
extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense
by an average of 1.8 points.

</details>


### [57] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: A novel framework for open-ended video QA improves reasoning depth and robustness by integrating multiple VLMs via structured chains of thought, outperforming baselines on CVRR-ES.


<details>
  <summary>Details</summary>
Motivation: Existing Video-LMMs lack contextual understanding, temporal modeling, and generalization to complex queries.

Method: Uses a prompting-and-response integration mechanism with multiple VLMs and an LLM evaluator/integrator.

Result: Significantly outperforms baselines in all metrics, showing superior generalization and robustness.

Conclusion: Offers a lightweight, extensible strategy for advancing multimodal reasoning without retraining, setting a foundation for future Video-LMMs.

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [58] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: The paper explores Quanvolutional pre-processing with Attention U-Net for building segmentation in dense urban areas, using SAR imagery of Tunis. It shows comparable accuracy to standard methods with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Accurate building segmentation in dense urban areas is challenging due to large, high-resolution satellite images. The study aims to enhance segmentation using quantum-assisted methods.

Method: Quanvolutional pre-processing is applied to extract informative features from SAR imagery, integrated with Attention U-Net for segmentation.

Result: The method achieves comparable accuracy to standard Attention U-Net while reducing network parameters, improving computational efficiency.

Conclusion: Quantum-assisted Deep Learning shows promise for efficient, large-scale building segmentation in urban environments.

Abstract: Building segmentation in urban areas is essential in fields such as urban
planning, disaster response, and population mapping. Yet accurately segmenting
buildings in dense urban regions presents challenges due to the large size and
high resolution of satellite images. This study investigates the use of a
Quanvolutional pre-processing to enhance the capability of the Attention U-Net
model in the building segmentation. Specifically, this paper focuses on the
urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)
imagery. In this work, Quanvolution was used to extract more informative
feature maps that capture essential structural details in radar imagery,
proving beneficial for accurate building segmentation. Preliminary results
indicate that proposed methodology achieves comparable test accuracy to the
standard Attention U-Net model while significantly reducing network parameters.
This result aligns with findings from previous works, confirming that
Quanvolution not only maintains model accuracy but also increases computational
efficiency. These promising outcomes highlight the potential of
quantum-assisted Deep Learning frameworks for large-scale building segmentation
in urban environments.

</details>


### [59] [Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2507.13857)
*Max van den Hoven,Kishaan Jeeveswaran,Pieter Piscaer,Thijs Wensveen,Elahe Arani,Bahram Zonooz*

Main category: cs.CV

TL;DR: Depth3DLane is a dual-pathway framework for monocular 3D lane detection, integrating self-supervised depth estimation to avoid reliance on expensive sensors or ground-truth depth data. It also predicts camera parameters per-frame, enhancing applicability in uncalibrated scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive sensors or ground-truth depth data, and assume known camera parameters, limiting scalability and applicability. Depth3DLane addresses these issues.

Method: Uses a dual-pathway framework: a bird's-eye view pathway for spatial info and a front view pathway for semantic info. Integrates self-supervised depth estimation and predicts camera parameters per-frame.

Result: Achieves competitive performance on OpenLane benchmark and works in scenarios without camera calibration.

Conclusion: Depth3DLane offers a scalable, practical solution for 3D lane detection without costly sensors or calibration, expanding its applicability.

Abstract: Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.

</details>


### [60] [PositionIC: Unified Position and Identity Consistency for Image Customization](https://arxiv.org/abs/2507.13861)
*Junjie Hu,Tianyang Han,Kai Ma,Jialin Gao,Hao Dou,Song Yang,Xianhua He,Jianhui Zhang,Junfeng Luo,Xiaoming Wei,Wenqiang Zhang*

Main category: cs.CV

TL;DR: PositionIC is a framework for precise spatial control in multi-subject image customization, addressing the lack of scalable datasets for identity and positional cues.


<details>
  <summary>Details</summary>
Motivation: Current image customization lacks fine-grained spatial control due to missing datasets linking identity with positional cues.

Method: PositionIC uses a scalable synthesis pipeline with bidirectional generation and a positional modulation layer to decouple spatial embeddings.

Result: The approach achieves precise spatial control and high consistency in image customization.

Conclusion: PositionIC enables controllable, high-fidelity customization in open-world scenarios and will be released for further research.

Abstract: Recent subject-driven image customization has achieved significant
advancements in fidelity, yet fine-grained entity-level spatial control remains
elusive, hindering the broader real-world application. This limitation is
mainly attributed to scalable datasets that bind identity with precise
positional cues are absent. To this end, we introduce PositionIC, a unified
framework that enforces position and identity consistency for multi-subject
customization. We construct a scalable synthesis pipeline that employs a
bidirectional generation paradigm to eliminate subject drift and maintain
semantic coherence. On top of these data, we design a lightweight positional
modulation layer that decouples spatial embeddings among subjects, enabling
independent, accurate placement while preserving visual fidelity. Extensive
experiments demonstrate that our approach can achieve precise spatial control
while maintaining high consistency in image customization task. PositionIC
paves the way for controllable, high-fidelity image customization in
open-world, multi-entity scenarios and will be released to foster further
research.

</details>


### [61] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: The paper investigates how vision-language models (VLMs) resolve conflicts between internal knowledge and external visual inputs, identifying specific model heads that control these conflicts and demonstrating their role in steering model behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding how VLMs handle conflicts between internal knowledge and external inputs to mitigate hallucinations and unreliable responses.

Method: Analyzing VLMs using a dataset of multimodal counterfactual queries, localizing conflict-controlling heads via logit inspection, and modifying these heads to steer model behavior.

Result: Identified specific heads that control knowledge conflicts; modifying them allows steering the model toward internal knowledge or visual inputs. Attention from these heads outperforms gradient-based attribution in precision.

Conclusion: The study provides insights into conflict resolution mechanisms in VLMs, offering a method to control model behavior by targeting specific heads.

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [62] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: A novel method for marine vision enhancement by fusing real-time visual data with nautical charts, using a transformer-based neural network for accurate buoy detection and matching.


<details>
  <summary>Details</summary>
Motivation: To improve object localization and association accuracy in dynamic maritime environments by integrating live video with chart data.

Method: A transformer-based end-to-end neural network predicts bounding boxes and confidence scores for buoy queries, enabling direct matching of image detections with chart markers. Compared to baseline methods like ray-casting and YOLOv7 with distance estimation.

Result: Significant improvement in object localization and association accuracy in real-world maritime scenes.

Conclusion: The proposed method outperforms baseline approaches, offering robust and accurate marine vision enhancement.

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [63] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: PCR-GS improves 3D Gaussian Splatting by co-regularizing camera poses, addressing challenges in scenes with complex trajectories.


<details>
  <summary>Details</summary>
Motivation: Current COLMAP-free 3D-GS struggles with complex camera trajectories, leading to degraded pose estimation and optimization issues.

Method: PCR-GS uses feature reprojection regularization (aligning DINO features) and wavelet-based frequency regularization (optimizing rotation matrices).

Result: PCR-GS outperforms in pose-free 3D-GS scene modeling, especially under dramatic camera trajectory changes.

Conclusion: PCR-GS offers a robust solution for high-quality 3D scene reconstruction without relying on COLMAP, even in challenging scenarios.

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [64] [Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection](https://arxiv.org/abs/2507.13899)
*Yujian Mo,Yan Wu,Junqiao Zhao,Jijun Wang,Yinghao Hu,Jun Yan*

Main category: cs.CV

TL;DR: The paper enhances LiDAR-based 3D object detection by integrating depth priors from DepthAnything, improving point feature expressiveness and detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limited expressiveness of raw LiDAR point features, especially weak reflectance attributes, by leveraging dense depth priors from monocular RGB images.

Method: Fuses DepthAnything depth priors with LiDAR attributes, introduces a point-wise feature extraction module, and uses a Dual-Path RoI framework with a bidirectional gated fusion module.

Result: Experiments on KITTI show consistent improvement in detection accuracy.

Conclusion: Incorporating visual foundation model priors enhances LiDAR-based 3D object detection.

Abstract: Recent advances in foundation models have opened up new possibilities for
enhancing 3D perception. In particular, DepthAnything offers dense and reliable
geometric priors from monocular RGB images, which can complement sparse LiDAR
data in autonomous driving scenarios. However, such priors remain underutilized
in LiDAR-based 3D object detection. In this paper, we address the limited
expressiveness of raw LiDAR point features, especially the weak discriminative
capability of the reflectance attribute, by introducing depth priors predicted
by DepthAnything. These priors are fused with the original LiDAR attributes to
enrich each point's representation. To leverage the enhanced point features, we
propose a point-wise feature extraction module. Then, a Dual-Path RoI feature
extraction framework is employed, comprising a voxel-based branch for global
semantic context and a point-based branch for fine-grained structural details.
To effectively integrate the complementary RoI features, we introduce a
bidirectional gated RoI feature fusion module that balances global and local
cues. Extensive experiments on the KITTI benchmark show that our method
consistently improves detection accuracy, demonstrating the value of
incorporating visual foundation model priors into LiDAR-based 3D object
detection.

</details>


### [65] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF is a neural rendering method for synthesizing novel views at arbitrary times and viewpoints with few input views, leveraging multi-view stereo, NeRF, and disentanglement strategies.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of collecting multiple views and re-optimizing for unseen scenes, and meeting the demand for immersive 3D environments that transition between day and night.

Method: Combines multi-view stereo, neural radiance fields (NeRF), and disentanglement strategies to enable generalizability in few-shot settings and temporal scene modeling.

Result: TimeNeRF renders novel views without per-scene optimization and smoothly transitions between times, capturing natural scene changes.

Conclusion: TimeNeRF advances temporal 3D scene modeling, offering efficient and realistic novel view synthesis for immersive applications.

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [66] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD is a novel video diffusion framework for disentangling static appearance and dynamic motion in videos, outperforming existing methods by reducing leakage and improving fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing VAE- and GAN-based approaches struggle with information leakage and blurry reconstructions in disentangling static and dynamic content in videos.

Method: DiViD uses a sequence encoder to extract static and dynamic tokens, a conditional DDPM decoder with shared-noise schedules, time-varying KL bottlenecks, and cross-attention, plus an orthogonality regularizer.

Result: DiViD achieves the highest swap-based joint accuracy, better static fidelity, improved dynamic transfer, and reduced cross-leakage compared to state-of-the-art methods.

Conclusion: DiViD effectively disentangles static and dynamic video content, setting a new benchmark for sequential disentanglement.

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


### [67] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vlez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,Joo Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: The paper explores the link between a vision model's perceptual ability and its forecasting performance, introducing a generalist framework using latent diffusion models for feature forecasting.


<details>
  <summary>Details</summary>
Motivation: Understanding the correlation between perceptual ability and forecasting performance to improve temporally grounded video understanding.

Method: A novel generalist forecasting framework using latent diffusion models on frozen vision backbones, with task-specific readouts and distributional metrics for evaluation.

Result: Strong correlation found between perceptual ability and forecasting performance across diverse models and tasks.

Conclusion: Bridging representation learning and generative modeling enhances video understanding, with implications for general-purpose systems.

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [68] [Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset](https://arxiv.org/abs/2507.13981)
*Sara Abdulaziz,Giacomo D'Amicantonio,Egor Bondarev*

Main category: cs.CV

TL;DR: A framework for evaluating visual privacy-protection methods is introduced, along with the HR-VISPR dataset, to assess privacy, utility, and practicality.


<details>
  <summary>Details</summary>
Motivation: Addressing concerns over AI-powered surveillance by developing objective techniques for evaluating privacy protection.

Method: Proposes a three-dimensional framework (privacy, utility, practicality) and introduces HR-VISPR, a human-centric dataset, to evaluate 11 privacy protection methods.

Result: The framework aligns privacy levels with human perception and highlights trade-offs between privacy, utility, and practicality.

Conclusion: The study provides a structured evaluation tool and dataset applicable in diverse contexts for privacy protection.

Abstract: Recent advances in AI-powered surveillance have intensified concerns over the
collection and processing of sensitive personal data. In response, research has
increasingly focused on privacy-by-design solutions, raising the need for
objective techniques to evaluate privacy protection. This paper presents a
comprehensive framework for evaluating visual privacy-protection methods across
three dimensions: privacy, utility, and practicality. In addition, it
introduces HR-VISPR, a publicly available human-centric dataset with biometric,
soft-biometric, and non-biometric labels to train an interpretable privacy
metric. We evaluate 11 privacy protection methods, ranging from conventional
techniques to advanced deep-learning methods, through the proposed framework.
The framework differentiates privacy levels in alignment with human visual
perception, while highlighting trade-offs between privacy, utility, and
practicality. This study, along with the HR-VISPR dataset, serves as an
insightful tool and offers a structured evaluation framework applicable across
diverse contexts.

</details>


### [69] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: The paper introduces CSD-VAR, a method for content-style decomposition using Visual Autoregressive Modeling (VAR), outperforming prior approaches with innovations like scale-aware optimization, SVD-based rectification, and Augmented Key-Value memory.


<details>
  <summary>Details</summary>
Motivation: To leverage VAR's scale-wise generation for improved disentanglement of content and style, addressing limitations in existing methods tailored for diffusion models.

Method: Proposes CSD-VAR with three innovations: scale-aware alternating optimization, SVD-based rectification, and Augmented Key-Value memory, evaluated on the new CSD-100 dataset.

Result: CSD-VAR achieves superior content preservation and stylization fidelity compared to prior methods.

Conclusion: VAR is a viable framework for CSD, with CSD-VAR's innovations enhancing disentanglement and performance.

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [70] [DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation](https://arxiv.org/abs/2507.13985)
*Haoran Li,Yuli Tian,Kun Lan,Yong Liao,Lin Wang,Pan Hui,Peng Yuan Zhou*

Main category: cs.CV

TL;DR: DreamScene is an end-to-end framework for generating high-quality, editable 3D scenes from text or dialogue, addressing automation, consistency, and control challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene generation lack automation, 3D consistency, and fine-grained control, limiting their practical use in gaming, film, and design.

Method: DreamScene uses a scene planning module with GPT-4 for object semantics and spatial constraints, a graph-based placement algorithm, Formation Pattern Sampling (FPS) for geometry, and progressive camera sampling for consistency.

Result: DreamScene outperforms prior methods in quality, consistency, and flexibility, enabling open-domain 3D content creation.

Conclusion: DreamScene offers a practical solution for generating and editing 3D scenes from natural language, with applications in various industries.

Abstract: Generating 3D scenes from natural language holds great promise for
applications in gaming, film, and design. However, existing methods struggle
with automation, 3D consistency, and fine-grained control. We present
DreamScene, an end-to-end framework for high-quality and editable 3D scene
generation from text or dialogue. DreamScene begins with a scene planning
module, where a GPT-4 agent infers object semantics and spatial constraints to
construct a hybrid graph. A graph-based placement algorithm then produces a
structured, collision-free layout. Based on this layout, Formation Pattern
Sampling (FPS) generates object geometry using multi-timestep sampling and
reconstructive optimization, enabling fast and realistic synthesis. To ensure
global consistent, DreamScene employs a progressive camera sampling strategy
tailored to both indoor and outdoor settings. Finally, the system supports
fine-grained scene editing, including object movement, appearance changes, and
4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior
methods in quality, consistency, and flexibility, offering a practical solution
for open-domain 3D content creation. Code and demos are available at
https://dreamscene-project.github.io.

</details>


### [71] [Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations](https://arxiv.org/abs/2507.14010)
*Yong Feng,Xiaolei Zhang,Shijin Feng,Yong Zhao,Yihan Chen*

Main category: cs.CV

TL;DR: A two-step deep learning method for classifying and segmenting tunnel cracks, combining DenseNet-169 for classification and DeepLabV3+ for segmentation, achieves high accuracy and efficiency, validated by superior experimental results.


<details>
  <summary>Details</summary>
Motivation: Tunnel lining cracks are critical for safety assessment, but existing methods lack accuracy and efficiency. This study aims to improve both using deep learning.

Method: Proposes a two-step approach: (1) DenseNet-169 for classifying tunnel images, and (2) DeepLabV3+ for segmenting cracks, with visual explanations for model transparency.

Result: Classification accuracy: 92.23%, FPS: 39.80; Segmentation IoU: 57.01%, F1 score: 67.44%, outperforming other models.

Conclusion: The method enables fast, accurate tunnel health assessment and enhances understanding of deep learning models via visual explanations.

Abstract: Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming
to classify and segment tunnel cracks with enhanced accuracy and efficiency,
this study proposes a two-step deep learning-based method. An automatic tunnel
image classification model is developed using the DenseNet-169 in the first
step. The proposed crack segmentation model in the second step is based on the
DeepLabV3+, whose internal logic is evaluated via a score-weighted visual
explanation technique. Proposed method combines tunnel image classification and
segmentation together, so that the selected images containing cracks from the
first step are segmented in the second step to improve the detection accuracy
and efficiency. The superior performances of the two-step method are validated
by experiments. The results show that the accuracy and frames per second (FPS)
of the tunnel crack classification model are 92.23% and 39.80, respectively,
which are higher than other convolutional neural networks (CNN) based and
Transformer based models. Also, the intersection over union (IoU) and F1 score
of the tunnel crack segmentation model are 57.01% and 67.44%, respectively,
outperforming other state-of-the-art models. Moreover, the provided visual
explanations in this study are conducive to understanding the "black box" of
deep learning-based models. The developed two-stage deep learning-based method
integrating visual explanations provides a basis for fast and accurate
quantitative assessment of tunnel health status.

</details>


### [72] [Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model](https://arxiv.org/abs/2507.14013)
*Ji-Yan Wu,Zheng Yong Poh,Anoop C. Patil,Bongsoo Park,Giovanni Volpe,Daisuke Urano*

Main category: cs.CV

TL;DR: A deep learning framework using multispectral imaging and an enhanced YOLOv5 model with a transformer-based attention head improves nutrient deficiency detection in plant leaves, outperforming the baseline by 12% in Dice score and IoU.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of nutrient deficiency in plant leaves is crucial for precision agriculture, aiding in early intervention for fertilization, disease, and stress management.

Method: The study employs a deep learning framework with multispectral imaging and an enhanced YOLOv5 model featuring a transformer-based attention head, tailored for nine-channel multispectral input.

Result: The proposed model outperforms the baseline YOLOv5 by about 12% in Dice score and IoU, excelling in detecting symptoms like chlorosis and pigment accumulation.

Conclusion: Combining multispectral imaging with spectral-spatial feature learning shows promise for advancing plant phenotyping and precision agriculture.

Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for
precision agriculture, enabling early intervention in fertilization, disease,
and stress management. This study presents a deep learning framework for leaf
anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model
with a transformer-based attention head. The model is tailored for processing
nine-channel multispectral input and uses self-attention mechanisms to better
capture subtle, spatially-distributed symptoms. The plants in the experiments
were grown under controlled nutrient stress conditions for evaluation. We carry
out extensive experiments to benchmark the proposed model against the baseline
YOLOv5. Extensive experiments show that the proposed model significantly
outperforms the baseline YOLOv5, with an average Dice score and IoU
(Intersection over Union) improvement of about 12%. In particular, this model
is effective in detecting challenging symptoms like chlorosis and pigment
accumulation. These results highlight the promise of combining multi-spectral
imaging with spectral-spatial feature learning for advancing plant phenotyping
and precision agriculture.

</details>


### [73] [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](https://arxiv.org/abs/2507.14024)
*Jiarong Ye,Sharon X. Huang*

Main category: cs.CV

TL;DR: The paper introduces MoodArchive, MoodifyCLIP, and Moodifier to bridge emotions and visual content for precise image editing, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Emotion-driven image editing is challenging due to emotions' abstract nature and varied manifestations.

Method: Three components: MoodArchive dataset, MoodifyCLIP model, and Moodifier editing system.

Result: Moodifier excels in emotional accuracy and content preservation across diverse domains.

Conclusion: The solution enables new possibilities for emotional content creation in real-world applications.

Abstract: Bridging emotions and visual content for emotion-driven image editing holds
great potential in creative industries, yet precise manipulation remains
challenging due to the abstract nature of emotions and their varied
manifestations across different contexts. We tackle this challenge with an
integrated approach consisting of three complementary components. First, we
introduce MoodArchive, an 8M+ image dataset with detailed hierarchical
emotional annotations generated by LLaVA and partially validated by human
evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned
on MoodArchive to translate abstract emotions into specific visual attributes.
Third, we propose Moodifier, a training-free editing model leveraging
MoodifyCLIP and multimodal large language models (MLLMs) to enable precise
emotional transformations while preserving content integrity. Our system works
across diverse domains such as character expressions, fashion design, jewelry,
and home d\'ecor, enabling creators to quickly visualize emotional variations
while preserving identity and structure. Extensive experimental evaluations
show that Moodifier outperforms existing methods in both emotional accuracy and
content preservation, providing contextually appropriate edits. By linking
abstract emotions to concrete visual changes, our solution unlocks new
possibilities for emotional content creation in real-world applications. We
will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier
code and demo publicly available upon acceptance.

</details>


### [74] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: QuantEIT is a lightweight, quantum-assisted framework for EIT image reconstruction, using minimal parameters and no training data, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: EIT's ill-posed inverse problem and inefficiency of deep learning methods motivate the need for a simpler, scalable solution.

Method: QuantEIT uses a QA-Net with parallel 2-qubit quantum circuits for latent representations and a linear layer for reconstruction, operating unsupervised.

Result: QuantEIT achieves superior accuracy with 0.2% of parameters, outperforming conventional methods and showing noise robustness.

Conclusion: QuantEIT offers a scalable, efficient, and accurate solution for EIT image reconstruction with quantum-assisted inference.

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [75] [Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)
*Qiankun Ma,Ziyao Zhang,Chi Su,Jie Chen,Zhen Song,Hairong Zheng,Wen Gao*

Main category: cs.CV

TL;DR: Vision Mamba's token reduction is explored, proposing MTR, a training-free framework for efficient token reduction without performance loss.


<details>
  <summary>Details</summary>
Motivation: Token reduction in Vision Mamba is underexplored, and existing ViT techniques degrade performance due to Mamba's sequence model nature.

Method: Proposes MTR, a Mamba structure-aware importance score for token reduction, requiring no training or extra parameters.

Result: MTR reduces FLOPs by ~40% on Vim-B with only a 1.6% drop in ImageNet performance.

Conclusion: MTR is an efficient, plug-and-play solution for token reduction in Vision Mamba, maintaining performance while reducing computation.

Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)
due to its ability to efficiently capture long-range dependencies with linear
computational complexity. While token reduction, an effective compression
technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision
Mamba's efficiency is essential for enabling broader applications. However, we
find that directly applying existing token reduction techniques for ViTs to
Vision Mamba leads to significant performance degradation. This is primarily
because Mamba is a sequence model without attention mechanisms, whereas most
token reduction techniques for ViTs rely on attention mechanisms for importance
measurement and overlook the order of compressed tokens. In this paper, we
investigate a Mamba structure-aware importance score to evaluate token
importance in a simple and effective manner. Building on this score, we further
propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction
framework. Without the need for training or additional tuning parameters, our
method can be seamlessly integrated as a plug-and-play component across various
Mamba models. Extensive experiments demonstrate that our approach significantly
reduces computational workload while minimizing performance impact across
various tasks and multiple backbones. Notably, MTR reduces FLOPs by
approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet
performance without retraining.

</details>


### [76] [Foundation Models as Class-Incremental Learners for Dermatological Image Classification](https://arxiv.org/abs/2507.14050)
*Mohamed Elkhayat,Mohamed Mahmoud,Jamil Fayyad,Nourhan Bayasi*

Main category: cs.CV

TL;DR: The paper evaluates frozen foundation models (FMs) for Class-Incremental Learning (CIL) in dermatology, proposing a lightweight MLP approach that outperforms existing methods and exploring zero-training prototype-based variants.


<details>
  <summary>Details</summary>
Motivation: To leverage pretrained FMs for CIL in dermatology, addressing the gap in their application for incremental learning in medical domains.

Method: Uses frozen FMs with a lightweight MLP trained incrementally for each task and explores zero-training prototype-based classification.

Result: Achieves state-of-the-art performance without forgetting and competitive results with zero-training prototypes.

Conclusion: Frozen FMs are highly effective for CIL in dermatology, supporting their real-world medical adoption.

Abstract: Class-Incremental Learning (CIL) aims to learn new classes over time without
forgetting previously acquired knowledge. The emergence of foundation models
(FM) pretrained on large datasets presents new opportunities for CIL by
offering rich, transferable representations. However, their potential for
enabling incremental learning in dermatology remains largely unexplored. In
this paper, we systematically evaluate frozen FMs pretrained on large-scale
skin lesion datasets for CIL in dermatological disease classification. We
propose a simple yet effective approach where the backbone remains frozen, and
a lightweight MLP is trained incrementally for each task. This setup achieves
state-of-the-art performance without forgetting, outperforming regularization,
replay, and architecture based methods. To further explore the capabilities of
frozen FMs, we examine zero training scenarios using nearest mean classifiers
with prototypes derived from their embeddings. Through extensive ablation
studies, we demonstrate that this prototype based variant can also achieve
competitive results. Our findings highlight the strength of frozen FMs for
continual learning in dermatology and support their broader adoption in real
world medical applications. Our code and datasets are available here.

</details>


### [77] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: VLA-Mark is a vision-aligned watermarking framework for vision-language models that preserves semantic fidelity and multimodal coherence, outperforming existing methods in detection and resilience.


<details>
  <summary>Details</summary>
Motivation: Existing text watermarking methods disrupt visual-textual alignment and compromise semantic-critical concepts, necessitating a solution that protects intellectual property without harming multimodal coherence.

Method: VLA-Mark integrates multiscale visual-textual alignment metrics (patch affinity, global coherence, contextual attention) to guide watermark injection dynamically, balancing strength and semantic preservation.

Result: Achieves 7.4% lower PPL, 26.6% higher BLEU, 98.8% AUC detection, and 96.1% attack resilience while maintaining text-visual consistency.

Conclusion: VLA-Mark sets new standards for quality-preserving multimodal watermarking by ensuring visual-textual alignment and robustness against attacks.

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [78] [Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection](https://arxiv.org/abs/2507.14083)
*Sara Abdulaziz,Egor Bondarev*

Main category: cs.CV

TL;DR: The paper evaluates how four human anonymization techniques (blurring, masking, encryption, avatar replacement) impact anomaly detection performance on the UCF-Crime dataset, revealing algorithm-specific sensitivities and trade-offs between privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in anomaly detection for surveillance videos by analyzing the impact of anonymization techniques on detection performance.

Method: Test four anomaly detection methods (MGFN, UR-DMU, BN-WVAD, PEL4VAD) on the UCF-Crime dataset anonymized with four techniques. Compare performance and algorithm responsiveness.

Result: Anomaly detection remains viable under anonymization, with some methods performing better under specific techniques (e.g., encryption, masking). Trade-offs between privacy and utility are algorithm-dependent.

Conclusion: The study benchmarks privacy-utility trade-offs in anomaly detection, highlighting the need for algorithm-specific anonymization strategies and emerging privacy-by-design solutions.

Abstract: Advancements in deep learning have improved anomaly detection in surveillance
videos, yet they raise urgent privacy concerns due to the collection of
sensitive human data. In this paper, we present a comprehensive analysis of
anomaly detection performance under four human anonymization techniques,
including blurring, masking, encryption, and avatar replacement, applied to the
UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,
BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method
responds to different obfuscation techniques. Experimental results demonstrate
that anomaly detection remains viable under anonymized data and is dependent on
the algorithmic design and the learning strategy. For instance, under certain
anonymization patterns, such as encryption and masking, some models
inadvertently achieve higher AUC performance compared to raw data, due to the
strong responsiveness of their algorithmic components to these noise patterns.
These results highlight the algorithm-specific sensitivities to anonymization
and emphasize the trade-off between preserving privacy and maintaining
detection utility. Furthermore, we compare these conventional anonymization
techniques with the emerging privacy-by-design solutions, highlighting an often
overlooked trade-off between robust privacy protection and utility flexibility.
Through comprehensive experiments and analyses, this study provides a
compelling benchmark and insights into balancing human privacy with the demands
of anomaly detection.

</details>


### [79] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*imon Kubov,Simon Klnk,Jakub Dandr,Zdenk Straka,Karolna Kvakov,Daniel Kvak*

Main category: cs.CV

TL;DR: An automated deep learning tool for Cobb angle measurement in scoliosis shows strong agreement with radiologists, suggesting clinical utility.


<details>
  <summary>Details</summary>
Motivation: Manual Cobb angle measurement is time-consuming and variable; automation could improve efficiency and consistency.

Method: Retrospective evaluation of AI software on 103 radiographs, compared to two radiologists' measurements using statistical metrics.

Result: AI achieved MAE ~3.9 degrees, strong correlation (r ~0.9), and moderate kappa (0.51-0.64) for severity grading.

Conclusion: The AI tool matches expert-level accuracy, supporting its use for clinical scoliosis assessment.

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [80] [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected -Overlap Graphs](https://arxiv.org/abs/2507.14095)
*Yung-Hong Sun,Ting-Hung Lin,Jiangang Chen,Hongrui Jiang,Yu Hen Hu*

Main category: cs.CV

TL;DR: C-DOG is a training-free framework for robust multi-view multi-object association, combining graph modeling and epipolar geometry to handle noise and indistinguishable objects without relying on visual features.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail when objects are visually indistinguishable or observations are noisy. C-DOG addresses this by not relying on visual features.

Method: Uses connected delta-overlap graph modeling with epipolar geometry, IQR filtering, and 3D back-projection error for robust association.

Result: Outperforms geometry-based baselines in synthetic benchmarks, even under high object density and limited camera overlap.

Conclusion: C-DOG is effective for scalable 3D reconstruction in challenging real-world scenarios.

Abstract: Multi-view multi-object association is a fundamental step in 3D
reconstruction pipelines, enabling consistent grouping of object instances
across multiple camera views. Existing methods often rely on appearance
features or geometric constraints such as epipolar consistency. However, these
approaches can fail when objects are visually indistinguishable or observations
are corrupted by noise. We propose C-DOG, a training-free framework that serves
as an intermediate module bridging object detection (or pose estimation) and 3D
reconstruction, without relying on visual features. It combines connected
delta-overlap graph modeling with epipolar geometry to robustly associate
detections across views. Each 2D observation is represented as a graph node,
with edges weighted by epipolar consistency. A delta-neighbor-overlap
clustering step identifies strongly consistent groups while tolerating noise
and partial connectivity. To further improve robustness, we incorporate
Interquartile Range (IQR)-based filtering and a 3D back-projection error
criterion to eliminate inconsistent observations. Extensive experiments on
synthetic benchmarks demonstrate that C-DOG outperforms geometry-based
baselines and remains robust under challenging conditions, including high
object density, without visual features, and limited camera overlap, making it
well-suited for scalable 3D reconstruction in real-world scenarios.

</details>


### [81] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: An automated pipeline generates high-quality image-text triplets for training image editing assistants, using public models and no human intervention, and releases a large open dataset.


<details>
  <summary>Details</summary>
Motivation: The need for high-quality training data for image editing assistants without manual annotation, addressing challenges in edit quality and scalability.

Method: An automated pipeline leveraging public generative models and a Gemini validator for scoring, with inversion and compositional bootstrapping to expand the dataset.

Result: Creation of NHR-Edit, a 358k triplet dataset, and Bagel-NHR-Edit, a fine-tuned model achieving state-of-the-art performance.

Conclusion: The approach enables scalable, high-fidelity training data generation and democratizes research in image editing.

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [82] [Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning](https://arxiv.org/abs/2507.14137)
*Shashanka Venkataramanan,Valentinos Pariza,Mohammadreza Salehi,Lukas Knobel,Spyros Gidaris,Elias Ramzi,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: Franca is the first fully open-source vision foundation model, outperforming proprietary models like DINOv2 and CLIP. It uses transparent training with public data and introduces multi-head clustering and positional disentanglement for better performance.


<details>
  <summary>Details</summary>
Motivation: To create a transparent, high-performance vision model that surpasses proprietary models and addresses limitations in SSL clustering methods.

Method: Uses a transparent training pipeline with public data (ImageNet-21K, ReLAION-2B). Introduces multi-head clustering with nested Matryoshka representations and positional disentanglement.

Result: Matches or surpasses proprietary models, improves feature space clarity, and achieves consistent gains on downstream benchmarks.

Conclusion: Franca sets a new standard for open-source vision models, promoting reproducibility and generalizability in AI.

Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source
(data, code, weights) vision foundation model that matches and in many cases
surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,
CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training
pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and
a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in
SSL clustering methods. While modern models rely on assigning image features to
large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to
account for the inherent ambiguity in clustering semantics. To address this, we
introduce a parameter-efficient, multi-head clustering projector based on
nested Matryoshka representations. This design progressively refines features
into increasingly fine-grained clusters without increasing the model size,
enabling both performance and memory efficiency. Additionally, we propose a
novel positional disentanglement strategy that explicitly removes positional
biases from dense representations, thereby improving the encoding of semantic
content. This leads to consistent gains on several downstream benchmarks,
demonstrating the utility of cleaner feature spaces. Our contributions
establish a new standard for transparent, high-performance vision models and
open a path toward more reproducible and generalizable foundation models for
the broader AI community. The code and model checkpoints are available at
https://github.com/valeoai/Franca.

</details>
