<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: CHAIR-DPO method reduces hallucinations in Multimodal LLMs by using CHAIR metric to identify non-hallucinated samples and fine-tuning with Direct Preference Optimization.


<details>
  <summary>Details</summary>
Motivation: Address the persistent problem of hallucinations in MLLMs where models generate answers not reflected in visual inputs, treating it as an alignment problem.

Method: Leverage CHAIR metric to distinguish hallucinated vs non-hallucinated samples, then fine-tune off-the-shelf MLLMs using Direct Preference Optimization (DPO) with this preference data.

Result: Effectively diminishes hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of CHAIR-based reward fine-tuning.

Conclusion: CHAIR-DPO provides an effective approach to reduce hallucinations in MLLMs without requiring complex synthetic data pipelines or proprietary models, using publicly available CHAIR metric and DPO.

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: Integrates Stable Diffusion 3's multimodal capabilities for image forgery localization by treating forgery residuals as an explicit modality, achieving 12% performance improvement without requiring annotated data.


<details>
  <summary>Details</summary>
Motivation: Existing image forgery localization methods struggle to keep pace with advanced manipulation technologies and rely on costly annotated data. Multi-modal large models like Stable Diffusion offer new opportunities for forensic analysis.

Method: Leverages SD3's multimodal framework by treating image forgery residuals (high-frequency signals from highpass filters) as an explicit modality fused into latent space during training, while preserving original semantic features.

Result: Achieves up to 12% performance improvement on benchmark datasets compared to state-of-the-art methods. Shows strong generalization to unseen real-world document forgery and natural scene forging images.

Conclusion: The integration of SD3's multimodal capabilities provides an efficient and accurate approach to image forgery localization that doesn't require annotated data and generalizes well to real-world scenarios.

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: Combining multimodal LLMs with quantitative lesion attributes improves interpretability of skin cancer diagnosis AI models by grounding predictions in measurable clinical concepts.


<details>
  <summary>Details</summary>
Motivation: AI models show promise in skin disease diagnosis but lack interpretability needed for clinical use. MLLMs offer natural language reasoning while quantitative attributes provide measurable grounding for predictions.

Method: Fine-tune multimodal LLMs to predict quantitative lesion attributes (e.g., lesion area) from images and evaluate grounding through attribute-specific content-based image retrieval on SLICE-3D dataset.

Result: Evidence shows MLLM embedding spaces can be effectively grounded in quantitative lesion attributes through fine-tuning to predict attribute values from images.

Conclusion: Grounding MLLMs in quantitative clinical attributes provides a promising approach to improve interpretability of AI diagnostic models for practical clinical application.

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: A unified Vision Transformer framework for automatic modulation recognition that combines supervised, self-supervised, and reconstruction objectives to achieve strong performance with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Existing AMR solutions require large labeled datasets or complex multi-stage training pipelines, limiting scalability and generalization in practical applications.

Method: Uses a ViT encoder with lightweight convolutional decoder and linear classifier; reconstruction branch maps augmented signals back to originals to preserve fine-grained I/Q structure; combines supervised, self-supervised, and reconstruction objectives.

Result: Outperforms supervised CNN and ViT baselines in low-label regimes on RML2018.01A dataset; approaches ResNet-level accuracy with only 15-20% labeled data; maintains strong performance across varying SNR levels.

Conclusion: Provides a simple, generalizable, and label-efficient solution for automatic modulation recognition that addresses the limitations of existing approaches.

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityHuman is a coarse-to-fine framework for generating high-resolution, long-duration audio-driven human animation videos with stable appearance and natural hand motions, addressing issues of identity drift and error accumulation in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing audio-driven human animation methods suffer from error accumulation leading to identity drift, color shifts, scene instability, and poorly modeled hand movements that result in distortions and audio misalignment.

Method: A coarse-to-fine framework that first generates audio-synchronized representations, then refines them using a pose-guided refiner with stable poses and initial frame as visual anchor. Includes hand-specific reward mechanism trained with high-quality hand motion data.

Result: Achieves state-of-the-art performance on EMTD and HDTF datasets in video quality, identity preservation, hand accuracy, and lip-sync. Ablation studies confirm effectiveness of each module.

Conclusion: InfinityHuman successfully addresses key challenges in audio-driven human animation by leveraging pose-guided refinement and hand-specific rewards, producing high-quality, stable long-duration videos with accurate lip synchronization and natural hand motions.

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: This paper introduces a new dataset YT360-EyeTracking and two novel saliency prediction models (SalViT360 and SalViT360-AV) for 360-degree videos, demonstrating that incorporating spatial audio cues significantly improves visual saliency prediction in omnidirectional VR environments.


<details>
  <summary>Details</summary>
Motivation: The study addresses the lack of comprehensive datasets for 360-degree audio-visual saliency prediction and the complexities of spherical distortion and spatial audio integration in omnidirectional videos, which are transforming VR experiences.

Method: The authors curated YT360-EyeTracking dataset with 81 ODVs under varying audio-visual conditions, and proposed two models: SalViT360 (vision-transformer with spherical geometry-aware attention) and SalViT360-AV (incorporates transformer adapters conditioned on audio input).

Result: Both SalViT360 and SalViT360-AV significantly outperform existing methods on benchmark datasets including their new YT360-EyeTracking dataset, showing improved viewer attention prediction in 360-degree scenes.

Conclusion: Integrating spatial audio cues in model architecture is crucial for accurate saliency prediction in omnidirectional videos, and the proposed audio-visual approach effectively addresses the challenges of 360-degree saliency prediction.

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: A pipeline using Vision-Language Models to explain vision models at both sample and dataset levels, enabling discovery of failure cases and model insights with minimal effort.


<details>
  <summary>Details</summary>
Motivation: Current vision model development focuses on performance metrics like accuracy while neglecting explainability. Existing xAI methods mainly explain models sample-by-sample, lacking methods to understand general model behavior across large datasets, which is crucial to prevent biased judgments and identify model patterns.

Method: Proposes a pipeline leveraging Vision-Language Models to provide explanations for vision models at both sample level (individual explanations) and dataset level (general behavior patterns across large datasets).

Result: The pipeline enables discovery of failure cases and provides insights into vision models' behavior, facilitating integration of xAI analysis with vision model development.

Conclusion: The proposed approach advances image analysis by making vision model explanations more comprehensive and accessible, helping developers understand model behavior patterns and potential biases across entire datasets rather than just individual samples.

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: ATMS-KD framework combines adaptive temperature scheduling with mixed-sample augmentation to create lightweight CNN models for agricultural applications, achieving 97.11% accuracy with compact models while reducing inference latency.


<details>
  <summary>Details</summary>
Motivation: To develop efficient CNN models suitable for resource-constrained agricultural environments where computational resources are limited but accurate plant maturity classification is needed.

Method: Combines adaptive temperature scheduling with mixed-sample augmentation for knowledge distillation from MobileNetV3 Large teacher to lightweight residual CNN students with three configurations (1.3M, 2.4M, 3.8M parameters).

Result: All student models achieved >96.7% validation accuracy (vs 95-96% with direct training), with compact model reaching 97.11% accuracy and 72.19ms inference latency. Outperformed 11 established KD methods with >99% knowledge retention.

Conclusion: ATMS-KD effectively transfers knowledge to lightweight models, making it suitable for agricultural computer vision applications where computational efficiency and accuracy are both critical.

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: Novel framework using hybrid vision-language representations to enable zero-shot classification of microstructures by integrating visual data with expert textual assessments, validated on additive manufacturing samples.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck in industrial manufacturing qualification of advanced materials, particularly for heterogeneous structures from non-conventional additive manufacturing processes.

Method: Integrates deep semantic segmentation with pre-trained multi-modal models (CLIP and FLAVA) to encode visual microstructural data and textual expert assessments into shared representations. Uses customized similarity-based representation with positive/negative references and net similarity scoring for zero-shot classification.

Result: Successfully distinguishes between acceptable and defective samples on additively manufactured metal matrix composite dataset. FLAVA offers higher visual sensitivity, CLIP provides consistent textual alignment. Z-score normalization enables effective classification.

Conclusion: Enhances traceability and interpretability in qualification pipelines, enables human-in-the-loop decision-making without retraining, and advances semantic interoperability for scalable domain-adaptable qualification strategies.

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: MedNeXt-L-k5, a Transformer-inspired 3D CNN, was adapted for automated perivascular spaces (PVS) segmentation in MRI. It achieved state-of-the-art performance on T2w images (Dice=0.88) but performed worse on T1w images and in cross-site validation, while not outperforming nnU-Net.


<details>
  <summary>Details</summary>
Motivation: Manual PVS segmentation is time-consuming with moderate inter-rater reliability, while existing automated deep learning models have moderate performance and poor generalization across diverse MRI datasets.

Method: Adapted MedNeXt-L-k5 (Transformer-inspired 3D encoder-decoder CNN) for PVS segmentation. Trained two models: one on homogeneous T2w MRI from HCP-Aging (200 scans), another on heterogeneous T1w MRI from seven studies across six scanners (40 volumes). Evaluated using 5-fold cross-validation and leave-one-site-out cross-validation.

Result: T2w model achieved voxel-level Dice=0.88±0.06 (WM), comparable to inter-rater reliability. T1w model achieved Dice=0.58±0.09 (WM). LOSOCV showed Dice=0.38±0.16 (WM) and 0.35±0.12 (BG) at voxel level, and 0.61±0.19 (WM) and 0.62±0.21 (BG) at cluster level.

Conclusion: MedNeXt-L-k5 provides efficient automated PVS segmentation across diverse T1w/T2w MRI datasets but did not outperform nnU-Net, suggesting attention-based mechanisms for global context are not required for high accuracy in PVS segmentation.

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: Training-free feedback framework that adapts output-based patch correspondences back to intermediate attention to improve CLIP's spatial coherence for open-vocabulary segmentation.


<details>
  <summary>Details</summary>
Motivation: CLIP has strong visual-textual alignment but poor localization for segmentation. Prior methods modify intermediate attention but coherence isn't consistently propagated to final output due to subsequent operations and lack of direct text interaction.

Method: Feedback-driven self-adaptive framework that leverages output predictions as spatial coherence prior. Includes attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble modules.

Result: Consistently improves performance across eight benchmarks when integrated into four state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H) and multiple attention types.

Conclusion: The proposed plug-in module effectively enhances semantic consistency between internal representations and final predictions by using output coherence cues as feedback.

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: A probing framework to analyze how Multimodal Large Language Models process visual and textual inputs across layers, revealing consistent stage-wise processing structure.


<details>
  <summary>Details</summary>
Motivation: MLLMs show strong performance but their internal processing dynamics remain underexplored, particularly how they integrate visual and textual information across different layers.

Method: Train linear classifiers to predict fine-grained visual categories from token embeddings at each layer using standardized anchor questions, with controlled prompt variations (lexical, semantic negation, output format). Applied to LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL.

Result: Identified consistent stage-wise structure: early layers perform visual grounding, middle layers support lexical integration and semantic reasoning, final layers prepare task-specific outputs. Structure remains stable across variations but layer allocation shifts with base LLM architecture changes.

Conclusion: Provides unified perspective on MLLM layer-wise organization and offers lightweight, model-agnostic approach for analyzing multimodal representation dynamics.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: SLiCS disentangles vision-language embeddings into concept-specific subspaces using supervised dictionary learning for improved concept-filtered image retrieval.


<details>
  <summary>Details</summary>
Motivation: To separate complex scene information in vision-language embedding spaces by decomposing embeddings into concept-specific component vectors in different subspaces.

Method: Supervised dictionary learning with sparse non-negative combinations of grouped vectors, optimized through novel alternating optimization with convergence guarantees.

Result: SLiCS enables more precise concept-filtered image retrieval and conditional generation across CLIP, TiTok, and DINOv2 embeddings with improved quantitative and qualitative performance.

Conclusion: The proposed sparse linear concept subspaces effectively disentangle embedding spaces for enhanced concept-specific applications in retrieval and generation tasks.

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: MedFoundationHub is a GUI toolkit that enables secure deployment of medical vision-language models for clinical applications while addressing privacy concerns and making advanced AI accessible to physicians without programming expertise.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs offer great potential for clinical applications but introduce serious security risks including PHI exposure, data leakage, and cyberthreats. Healthcare organizations need secure, accessible tools that protect patient privacy while enabling medical AI adoption.

Method: Developed MedFoundationHub - a graphical user interface toolkit that allows manual model selection without programming, supports plug-and-play deployment with Hugging Face integration, and ensures privacy-preserving inference through Docker-orchestrated deployment on local workstations with single GPU requirements.

Result: Evaluated five state-of-the-art VLMs (Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, LLaVA-1.5-7B/13B) with board-certified pathologists across colon and renal cases, generating 1015 clinician-model scoring events. Identified recurring limitations including off-target answers, vague reasoning, and inconsistent pathology terminology.

Conclusion: MedFoundationHub provides a secure, accessible solution for deploying medical VLMs while addressing critical privacy concerns. The evaluation reveals current VLMs still have significant limitations in clinical accuracy and terminology consistency, highlighting the need for continued improvement and expert validation in medical AI applications.

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: Bidirectional Interaction Mamba (BIM) uses novel scanning mechanisms to achieve efficient cross-task interaction for multi-task dense prediction without computational trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing methods face a trade-off between interaction completeness and computational efficiency in multi-task dense prediction. Sufficient cross-task interaction is crucial but often results in high computational complexity.

Method: Proposes BIM with two novel mechanisms: 1) Bidirectional Interaction Scan (BI-Scan) that constructs task-specific representations as bidirectional sequences with task-first and position-first scanning modes, 2) Multi-Scale Scan (MS-Scan) for multi-granularity scene modeling to meet diverse task requirements.

Result: Extensive experiments on NYUD-V2 and PASCAL-Context benchmarks show BIM outperforms state-of-the-art competitors.

Conclusion: BIM successfully addresses the computational efficiency vs interaction completeness trade-off in multi-task dense prediction through innovative bidirectional scanning mechanisms with linear complexity.

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: A novel audio-guided visual editing framework that uses both text and audio prompts for complex editing tasks without requiring additional training, leveraging pre-trained multi-modal encoders and addressing audio-text space discrepancies.


<details>
  <summary>Details</summary>
Motivation: Existing visual editing methods struggle with complex scenarios where textual guidance alone is insufficient, and current audio-guided approaches require specific training that limits real-world generalization.

Method: Uses pre-trained multi-modal encoder with zero-shot capabilities, integrates diverse audio into visual editing by aligning audio encoder space with diffusion model's prompt encoder space, and employs separate noise branching with adaptive patch selection for multi-modal prompts.

Result: The framework successfully handles complex editing scenarios by incorporating rich audio information where text-only approaches fail, as demonstrated through comprehensive experiments on diverse editing tasks.

Conclusion: The proposed audio-guided visual editing framework provides an effective solution for complex multi-modal editing tasks without requiring additional training, overcoming limitations of text-only approaches and enabling better handling of real-world editing scenarios.

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: Proposes AEVLP framework with GPR Loss and DAMP technique for single positive multi-label learning, achieving state-of-the-art results by effectively handling noisy pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: Fully annotating large-scale multi-label datasets is costly and impractical. Traditional SPML methods treating missing labels as unknown/negative cause inaccuracies and false negatives, while pseudo-labeling introduces noise.

Method: Introduces Generalized Pseudo-Label Robust Loss (GPR Loss) to learn from diverse pseudo-labels while mitigating noise, and Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique.

Result: Extensive experiments on four benchmark datasets demonstrate significant advancements in multi-label classification, achieving state-of-the-art results.

Conclusion: The AEVLP framework effectively addresses challenges in single positive multi-label learning by combining robust loss functions with dynamic pseudo-labeling techniques.

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: Proposes delay-spike approach and temporal-dependent Integrate-and-Fire (tdIF) neuron for SNNs to improve visual detection tasks with ultra-low latency (5 time-steps), achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current ANN-SNN conversion methods perform well in classification tasks but remain suboptimal in visual detection tasks due to residual membrane potential issues from heterogeneous spiking patterns.

Method: Introduces delay-spike approach to mitigate residual membrane potential, and proposes tdIF neuron that dynamically adjusts accumulation and firing behaviors based on temporal order of time-steps, enabling distinct temporal properties beyond frequency-based representations.

Result: Achieves more precise feature representation with lower time-steps, enabling high performance and ultra-low latency in visual detection tasks. Surpasses current ANN-SNN conversion approaches with state-of-the-art performance within 5 time-steps.

Conclusion: The proposed tdIF neuron maintains energy consumption comparable to traditional IF neurons while significantly improving performance in object detection and lane line detection tasks with ultra-low latency.

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: Proposes DUP-MCRNet for salient object detection, addressing detail loss and edge blurring through dynamic uncertainty propagation and multimodal fusion.


<details>
  <summary>Details</summary>
Motivation: Existing SOD methods suffer from detail loss, edge blurring, and insufficient multimodal fusion in complex scenes, leading to poor performance with small structures and background interference.

Method: Uses dynamic uncertainty graph convolution (DUGC) for uncertainty propagation, multimodal collaborative fusion (MCF) with learnable gating weights for RGB/depth/edge features, and multi-scale loss optimization with BCE, IoU, and consistency constraints.

Result: Outperforms various SOD methods on benchmark datasets, particularly in edge clarity and robustness to complex backgrounds.

Conclusion: DUP-MCRNet effectively improves detection accuracy for small structures and edge regions while enhancing multimodal information fusion and robustness in complex scenes.

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: MSMVD improves multi-view pedestrian detection by generating multi-scale BEV features from multi-scale image features, addressing scale variation issues across different camera views.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end MVPD methods struggle with pedestrians having consistently small/large scales within views or vastly different scales between views, as they don't exploit multi-scale image features for BEV generation.

Method: Proposes MSMVD which generates multi-scale BEV features by projecting multi-scale image features into BEV space scale-by-scale, then processes them with a feature pyramid network to combine information across different scales and views.

Result: Extensive experiments show MSMVD outperforms previous methods by 4.5 MODA points on GMVD dataset, demonstrating significant improvement in detection performance.

Conclusion: Exploiting multi-scale image features through multi-scale BEV features greatly enhances pedestrian detection performance in multi-view scenarios, effectively addressing scale variation challenges.

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: Lightweight real-time deepfake detection network that combines spatial and frequency features with efficient multi-scale fusion for practical deployment.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detectors achieve high accuracy but are computationally expensive, hindering real-time deployment in applications like video conferencing and social media.

Method: Proposed SFMFNet with spatial-frequency hybrid aware module using gated mechanism, token-selective cross attention for multi-level feature interaction, and residual-enhanced blur pooling for downsampling.

Result: Achieves favorable balance between accuracy and efficiency on benchmark datasets with strong generalization capabilities.

Conclusion: SFMFNet provides practical value for real-time deepfake detection applications with lightweight yet effective architecture.

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: A lightweight medical image classification method using dual-model weight selection and self-knowledge distillation to achieve comparable performance to large models with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Address computational constraints in real-world medical settings where deploying large-scale models is impractical, requiring lightweight alternatives that maintain performance.

Method: Uses dual-model weight selection from large pretrained models followed by self-knowledge distillation (SKD) to transfer knowledge to lightweight models without excessive computational overhead.

Result: Superior performance and robustness demonstrated on multiple medical imaging datasets including chest X-rays, lung CT scans, and brain MRI scans compared to existing methods.

Conclusion: The combined approach of dual-model weight selection with SKD effectively overcomes limitations of conventional methods in retaining critical information in compact medical image classification models.

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: A novel LiDAR point cloud compression method that uses geometry re-densification and cross-scale feature propagation to achieve efficient predictive coding with state-of-the-art compression ratios and real-time performance.


<details>
  <summary>Details</summary>
Motivation: High-precision LiDAR scans require substantial storage and transmission overhead. Existing methods struggle with efficient context modeling due to extreme sparsity of geometric details, limiting compression performance and speed.

Method: Proposes two lightweight modules: 1) Geometry Re-Densification Module that re-densifies sparse geometry, extracts features at denser scale, then re-sparsifies for predictive coding; 2) Cross-scale Feature Propagation Module that leverages multi-resolution occupancy cues to guide hierarchical feature propagation and enable cross-scale information sharing.

Result: Achieves state-of-the-art compression ratios on KITTI dataset with real-time performance (26 FPS for both encoding and decoding at 12-bit quantization).

Conclusion: The proposed framework generates compact feature representations that enable efficient context modeling and accelerate the coding process while maintaining high compression performance.

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: Droplet3D leverages video data to overcome 3D data scarcity, using video priors for spatial consistency and semantic fidelity in 3D generation, achieving state-of-the-art results with open-sourced resources.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity in 3D domain by utilizing abundant video data that contains spatial consistency priors and rich semantic information to improve 3D generation quality and generalization.

Method: Introduces Droplet3D-4M dataset with multi-view annotations and trains Droplet3D generative model supporting both image and dense text input, leveraging video modality for 3D asset generation.

Result: Extensive experiments show the approach produces spatially consistent and semantically plausible 3D content, with potential for scene-level applications, outperforming prevailing 3D solutions.

Conclusion: Video commonsense priors significantly facilitate 3D creation, providing an effective alternative to limited native 3D data, with open-sourced dataset, code, and models for community use.

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor is a framework for photorealistic object editing in driving videos using 3D Gaussian representation and hierarchical features for precise pose control and visual quality.


<details>
  <summary>Details</summary>
Motivation: Collecting corner cases for autonomous driving validation is costly and hazardous. Existing editing methods suffer from limited visual fidelity or imprecise pose control.

Method: Uses 3D Gaussian representation as dense prior in denoising process, scene-level 3D bounding box layout for occlusion handling, and hierarchical fine-grained features for appearance guidance.

Result: Outperforms existing methods in pose controllability and visual quality on Waymo Open Dataset, supporting object repositioning, insertion, and deletion.

Conclusion: G^2Editor provides a unified framework for photorealistic and precise object editing in driving scenarios, benefiting downstream data-driven tasks.

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: A pathology-informed domain randomization strategy that simulates CCD brain alterations from healthy data alone, enabling robust fetal brain segmentation without requiring pathological annotations and improving biomarker estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate fetal brain segmentation is crucial for biomarker extraction and neurodevelopment assessment, but the rarity of corpus callosum dysgenesis (CCD) severely limits annotated data, hindering deep learning model generalization.

Method: Proposed a pathology-informed domain randomization strategy that embeds prior knowledge of CCD manifestations into a synthetic data generation pipeline, simulating diverse brain alterations from healthy data alone.

Result: Validated on 248 healthy fetuses, 26 with CCD, and 47 with other pathologies. Reduced LCC estimation error from 1.89mm to 0.80mm in healthy cases and from 10.9mm to 0.7mm in CCD cases. Achieved substantial improvements on CCD cases while maintaining performance on other cases.

Conclusion: Incorporating domain-specific anatomical priors into synthetic data pipelines can effectively mitigate data scarcity and enhance analysis of rare but clinically significant malformations, yielding segmentations with improved topological consistency for reliable shape-based analyses.

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: First unified framework combining sign language, lip movements, and audio for spoken-language text generation, achieving state-of-the-art performance across multiple tasks while revealing lip movements' importance for sign language comprehension.


<details>
  <summary>Details</summary>
Motivation: Current ASR systems are inaccessible to deaf/hard-of-hearing individuals, and visual alternatives (sign language, lip reading) have been studied in isolation without integration into a unified framework.

Method: Designed a unified modality-agnostic architecture capable of processing heterogeneous inputs (sign language, lip movements, audio) for spoken-language text generation, with explicit modeling of lip movements as a separate modality.

Result: Achieved performance on par with or better than task-specific state-of-the-art models across SLT, VSR, ASR, and AVSR tasks. Explicit lip movement modeling significantly improved SLT performance.

Conclusion: The unified framework successfully integrates multiple modalities, demonstrates strong synergy between modalities (particularly lip movements' role in sign language comprehension), and provides accessible communication technology for diverse user needs.

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: Video-MTR is a reinforced multi-turn reasoning framework that iteratively selects key video segments and comprehends questions, outperforming existing methods in long-form video understanding.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding faces challenges with long-range temporal dependencies and multiple events. Existing methods rely on static reasoning or external VLMs, leading to complexity and sub-optimal performance due to lack of end-to-end training.

Method: A reinforced multi-turn reasoning framework that performs iterative key video segment selection and question comprehension. Uses a novel gated bi-level reward system combining trajectory-level rewards (answer correctness) and turn-level rewards (frame-query relevance) for end-to-end optimization.

Result: Extensive experiments on VideoMME, MLVU, and EgoSchema benchmarks show Video-MTR outperforms existing methods in both accuracy and efficiency.

Conclusion: Video-MTR advances state-of-the-art in long video understanding by enabling iterative reasoning without external VLMs through end-to-end training with a novel reward system.

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: DUO is a test-time adaptation framework that jointly minimizes semantic and geometric uncertainties in monocular 3D object detection through convex optimization and semantic-aware constraints.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods fail to address the dual uncertainty (semantic and geometric) inherent in monocular 3D object detection, which deteriorates under real-world domain shifts in safety-critical applications like autonomous driving.

Method: Proposes Dual Uncertainty Optimization (DUO) with convex optimization of focal loss, unsupervised uncertainty weighting, and semantic-aware normal field constraints to preserve geometric coherence and form a complementary dual-branch mechanism.

Result: Extensive experiments show DUO's superiority over existing methods across various datasets and domain shift types, demonstrating robust performance in monocular 3D object detection.

Conclusion: DUO effectively bridges the gap in addressing dual uncertainties for robust monocular 3D object detection, providing a comprehensive solution that enhances both spatial perception and semantic classification through complementary optimization.

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: CaddieSet dataset with joint and ball trajectory data enables interpretable golf swing analysis and ball trajectory prediction using computer vision and domain knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning studies haven't quantitatively established the relationship between swing posture and ball trajectory, limiting practical swing improvement insights for golfers.

Method: Created CaddieSet dataset with joint information from swing videos segmented into 8 phases using computer vision, plus 15 key metrics based on expert golf knowledge for interpretable swing analysis.

Result: Demonstrated feasibility of predicting ball trajectories using various benchmarks, with interpretable models showing swing feedback quantitatively consistent with established domain knowledge.

Conclusion: CaddieSet provides new insights for golf swing analysis in both academia and sports industry by enabling interpretable relationship between swing posture and ball trajectory.

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: IAENet is a 3D surface anomaly detection framework that combines 2D and 3D experts using importance-aware fusion to achieve state-of-the-art performance with lower false positives.


<details>
  <summary>Details</summary>
Motivation: 3D point cloud-based anomaly detection is underexplored compared to 2D methods, lacking powerful pretrained backbones. Existing fusion strategies suffer from poor modality performance degradation.

Method: Proposes Importance-Aware Ensemble Network (IAENet) with novel Importance-Aware Fusion module that dynamically weights anomaly scores from 2D and 3D experts, guided by specialized loss functions.

Result: Extensive experiments on MVTec 3D-AD show IAENet achieves new state-of-the-art with significantly lower false positive rate.

Conclusion: The framework effectively bridges the 2D-3D gap for industrial surface anomaly detection, demonstrating practical value for manufacturing quality control.

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit reframes image editing as reference-image-based text-to-image generation using a Cross-Attentive UNet to inject reference features, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Address reconstruction errors in inversion-based methods and limited dataset quality/scale issues in instruction-based models for semantic image editing.

Method: Propose DescriptiveEdit framework using Cross-Attentive UNet with attention bridges to inject reference image features into prompt-to-edit-image generation process.

Result: Experiments on Emu Edit benchmark show improved editing accuracy and consistency compared to existing methods.

Conclusion: DescriptiveEdit preserves generative power of text-to-image models without architectural changes, integrates well with extensions like ControlNet and IP-Adapter, and offers better scalability.

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: DCFS is a novel continual test-time adaptation framework that uses dual-path feature consistency and confidence-aware learning to address error accumulation and pseudo-label noise in target domain adaptation without source data.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods rely on pseudo-labels from model predictions, which suffer from quality issues and error accumulation. The model needs to better capture target domain features without source data access while reducing noise in learning.

Method: Proposes dual classifiers to disentangle semantic-related and domain-related features from target data. Uses feature consistency between sub-features and whole features, plus confidence-aware sample learning with adaptive thresholds to weight self-supervised loss.

Result: Extensive experiments on CIFAR10-C, CIFAR100-C, and ImageNet-C datasets demonstrate consistent performance improvements in continual test-time adaptation scenarios.

Conclusion: DCFS effectively reduces pseudo-label noise and error accumulation by comprehensively capturing target domain features through dual-path consistency and confidence-aware learning, showing strong performance across multiple datasets.

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: Using 3D Gaussian Splatting (3DGS) to fine-tune camera calibration through backpropagation of novel view color loss, achieving 0.4 dB PSNR improvement on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Camera calibration quality significantly impacts novel view synthesis results, with even 1-pixel errors affecting reconstruction quality. Since real scenes lack ground truth calibration, novel view synthesis quality becomes the primary evaluation metric.

Method: Proposes using a 3DGS model to fine-tune camera calibration by backpropagating novel view color loss with respect to camera parameters.

Result: Achieves average improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS, demonstrating significant calibration enhancement.

Conclusion: While the fine-tuning process may be time-consuming, the method is particularly valuable for reference scenes like Mip-NeRF 360 where novel view quality is paramount, making the calibration improvement worthwhile despite computational costs.

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: Proposes an active sequential domain adaptation framework for multi-modal medical image segmentation that dynamically selects the most informative samples to reduce annotation costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Medical image labeling is time-consuming and labor-intensive. Existing active domain adaptation methods suffer from negative transfer and limited source data access, with no effective query strategies for multi-modal medical data.

Method: Develops an active and sequential domain adaptation framework with a query strategy that prioritizes samples based on both informativeness and representativeness for dynamic multi-modal sample selection.

Result: Achieves favorable segmentation performance on diverse gross tumor volume segmentation tasks, significantly outperforming state-of-the-art ADA methods.

Conclusion: The proposed framework effectively reduces annotation costs while maintaining high segmentation accuracy, addressing limitations of previous ADA methods in multi-modal medical imaging.

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: A novel data-level fusion framework that integrates RGB images and LiDAR data early in the process, using vision foundation models for instance segmentation and depth estimation, with bi-directional fusion and filtering methods to improve pseudo-box quality for unsupervised 3D object detection.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based 3D object detectors require manual annotations which are time-consuming and labor-intensive. Current unsupervised methods simply fuse pseudo-boxes from LiDAR and RGB at label level, overlooking the complementary nature of these modalities and providing limited improvements.

Method: Proposes data-level fusion integrating RGB and LiDAR early. Uses vision foundation models for instance segmentation and depth estimation. Implements bi-directional fusion: real points get category labels from 2D space, 2D pixels are projected to 3D to enhance point density. Includes local radius filtering for depth errors and global statistical filtering for segmentation outliers. Features data-level fusion based dynamic self-evolution strategy for iterative pseudo-box refinement.

Result: Achieves 28.4% mAP on nuScenes validation benchmark, significantly outperforming previous state-of-the-art unsupervised 3D object detection methods.

Conclusion: The proposed data-level fusion framework effectively leverages complementary RGB and LiDAR information, overcoming limitations of label-level fusion approaches and demonstrating superior performance in unsupervised 3D object detection.

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: Deep learning method for BMI estimation from smartphone images using large proprietary dataset (84,963 images) with automatic filtering, achieving state-of-the-art results (7.9% MAPE) and robust generalization to unseen data.


<details>
  <summary>Details</summary>
Motivation: Enable rapid weight assessment via camera images when traditional BMI measurement methods are unavailable or impractical, particularly for telehealth and emergency scenarios.

Method: Deep learning-based approach trained on WayBED dataset (84,963 images) with automatic filtering using posture clustering and person detection to remove low-quality images. Deployed on Android via CLAID framework.

Result: Achieved 7.9% MAPE on hold-out test set (lowest published), 13% MAPE on unseen VisualBodyToBMI dataset (comparable to SOTA), and 8.56% MAPE after fine-tuning (lowest reported on VisualBodyToBMI).

Conclusion: The method provides accurate BMI estimation from smartphone images with strong generalization capabilities, and the complete pipeline is released as open-source for mobile deployment.

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: Comprehensive evaluation of 7 domain adaptation techniques across 13 datasets shows DSAN algorithm excels in medical image classification, achieving 91.2% accuracy on COVID-19 data and +6.7% improvement in dynamic data streams.


<details>
  <summary>Details</summary>
Motivation: To better understand domain adaptation benefits for both natural and medical images, addressing performance bias in mainstream datasets and challenges with medical data.

Method: 557 simulation studies using 7 widely-used DA techniques for image classification across 5 natural and 8 medical datasets, covering various scenarios including out-of-distribution, dynamic data streams, and limited training samples.

Result: Deep Subdomain Adaptation Network (DSAN) showed outstanding performance with 91.2% accuracy on COVID-19 dataset using Resnet50, +6.7% improvement in dynamic data streams, and remarkable explainability on medical datasets.

Conclusion: DSAN demonstrates superior performance and explainability for medical image adaptation, providing valuable insights for effective model adaptation to medical data and contributing to understanding of DA techniques.

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: CLAB method uses contrastive learning with auxiliary branch and dynamic loss weighting to improve video object detection without increasing computational cost during inference.


<details>
  <summary>Details</summary>
Motivation: Video object detection faces challenges from image deterioration like motion blur and occlusion. Existing methods improve performance but add computational complexity. The goal is to enhance robustness to degradation without extra inference cost.

Method: Contrastive Learning through Auxiliary Branch (CLAB) with contrastive loss to enhance backbone feature representation, plus dynamic loss weighting that prioritizes auxiliary learning early then shifts to detection task.

Result: Achieves 84.0% mAP with ResNet-101 and 85.2% mAP with ResNeXt-101 on ImageNet VID dataset, setting state-of-the-art for CNN-based models without additional post-processing.

Conclusion: CLAB provides an effective and computationally efficient approach to improve video object detection performance through contrastive learning and adaptive training strategy.

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: Analysis of CLIP vision encoders under typographic attacks reveals specialized attention heads that transmit text information. A training-free defense method selectively ablates these heads, improving robustness by up to 19.6% on typographic attacks with minimal standard accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Typographic attacks exploit multi-modal systems by injecting text into images, causing targeted misclassifications, malicious content generation, and VLM jailbreaks. Understanding and defending against these attacks is crucial for safety-critical applications.

Method: Analyze CLIP vision encoder behavior under typographic attacks to locate specialized attention heads. Introduce a training-free defense method that selectively ablates the identified typographic circuit (attention heads) without requiring finetuning.

Result: Method improves performance by up to 19.6% on typographic variant of ImageNet-100 while reducing standard ImageNet-100 accuracy by less than 1%. Remains competitive with state-of-the-art finetuning-based defenses. Releases family of dyslexic CLIP models as drop-in replacements.

Conclusion: The training-free selective ablation approach effectively defends CLIP models against typographic attacks while maintaining standard performance. The released dyslexic CLIP models provide robust alternatives for safety-critical applications where text-based manipulation risks outweigh text recognition utility.

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: GLaRE: Graph-based Landmark Region Embedding network for facial expression recognition using hierarchical coarsening of facial landmarks to achieve state-of-the-art performance on AffectNet and FERG datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional FER systems face challenges with occlusion, expression variability, and lack of interpretability. GNNs offer structured and interpretable learning by modeling relational dependencies between facial landmarks.

Method: Extract facial landmarks using 3D facial alignment, construct quotient graph via hierarchical coarsening to preserve spatial structure while reducing complexity, and use region-level embeddings for emotion recognition.

Result: Achieves 64.89% accuracy on AffectNet and 94.24% on FERG, outperforming existing baselines. Ablation studies show region-level embeddings from quotient graphs improve prediction performance.

Conclusion: GLaRE demonstrates that graph-based approaches with hierarchical coarsening and region-level embeddings effectively address FER challenges and achieve superior performance with improved interpretability.

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: FastFit is a high-speed multi-reference virtual try-on framework that addresses efficiency bottlenecks in current methods by using cacheable diffusion architecture, achieving 3.5x speedup while maintaining superior fidelity.


<details>
  <summary>Details</summary>
Motivation: Current virtual try-on methods cannot support multi-reference outfit compositions (garments and accessories) and suffer from significant inefficiency due to redundant re-computation of reference features in each denoising step.

Method: Proposes FastFit with Semi-Attention mechanism and class embeddings instead of timestep embeddings for reference items, fully decoupling reference feature encoding from denoising process. Also introduces DressCode-MR dataset with 28,179 sets of high-quality paired images across five categories.

Result: Achieves average 3.5x speedup over comparable methods while surpassing state-of-the-art methods on key fidelity metrics across VITON-HD, DressCode, and DressCode-MR datasets.

Conclusion: FastFit fundamentally breaks the efficiency bottleneck in virtual try-on technology and enables practical real-world applications through its cacheable architecture and multi-reference support.

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: UTA-Sign: Unsupervised thermal-event fusion method that combines thermal cameras and event cameras to enhance traffic signage detection in low-light conditions, addressing limitations of both modalities.


<details>
  <summary>Details</summary>
Motivation: Thermal cameras struggle with signage detection on similar-material objects, while event cameras have non-uniform sampling. Their complementary characteristics can overcome individual limitations for safer autonomous driving in low-light environments.

Method: Dual-boosting mechanism that fuses thermal frames and event signals. Uses thermal frames as temporal references to align uneven event signals, while event signals add subtle signage details to thermal frames for consistent representation over time.

Result: Validated on real-world datasets, showing superior traffic signage sketching quality and improved detection accuracy at the perceptual level compared to individual modalities.

Conclusion: The proposed unsupervised fusion method effectively leverages complementary strengths of thermal and event cameras, enhancing traffic signage perception in low-illumination environments for autonomous driving applications.

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: Proposes an active defense method using low-frequency perceptual perturbations to disrupt face swapping deepfakes, combining frequency and spatial domain features to prevent manipulation while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Address limitations of passive deepfake detection methods by developing an active defense approach that prevents attacks rather than just detecting them post-event, focusing on disrupting the generative process itself.

Method: Uses discrete wavelet transform (DWT) to extract low-frequency components and generate perturbations that target the generative process. Features encoder, perturbation generator, and decoder architecture combining frequency and spatial domain features while preserving high-frequency details.

Result: Experiments on CelebA-HQ and LFW datasets show significant reduction in face-swapping effectiveness, improved defense success rates, and preservation of visual quality in outputs.

Conclusion: The proposed active defense method effectively disrupts deepfake generation through low-frequency perturbations while maintaining visual plausibility, offering a proactive solution to deepfake threats.

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: Diverse-T2M introduces uncertainty modeling and stochastic sampling to generate diverse 3D human motions from text while maintaining semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Current text-to-motion generation methods produce precise motions but lack diversity, creating a need for approaches that can generate varied motions while preserving text-motion consistency.

Method: Introduces uncertainty via noise signals as diversity carriers in transformer models, creates a continuous text representation latent space instead of rigid mapping, and integrates a latent space sampler for stochastic sampling.

Result: Achieves significantly enhanced diversity while maintaining state-of-the-art text consistency performance on benchmark datasets HumanML3D and KIT-ML.

Conclusion: The proposed method successfully addresses the diversity challenge in text-to-motion generation through explicit uncertainty modeling and stochastic sampling techniques.

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: Proposed optimization-based calibration method using 3D-printed phantom for accurate 3D IVUS volume reconstruction to bridge preoperative CT and intraoperative ultrasound in liver surgery.


<details>
  <summary>Details</summary>
Motivation: Intraoperative ultrasound images are challenging to interpret due to limited field of view and complex anatomy. Bridging preoperative and intraoperative data is crucial for effective surgical guidance in liver surgery.

Method: Optimization-based calibration method using a 3D-printed phantom for accurate 3D Intravascular Ultrasound volume reconstruction. Validated with in vivo swine liver images.

Result: Achieved calibration error from 0.88 to 1.80 mm and registration error from 3.40 to 5.71 mm between 3D IVUS data and corresponding CT scans.

Conclusion: Method provides reliable and accurate calibration and volume reconstruction for registering intraoperative ultrasound with preoperative CT images, enhancing intraoperative guidance in liver surgery.

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: Proposes PI-GenMFI, a physics-informed diffusion model to generate synthetic Magnetic Field Images for semiconductor defect detection, addressing data scarcity issues in training ML models.


<details>
  <summary>Details</summary>
Motivation: Limited availability of MFI datasets due to proprietary concerns creates a bottleneck for training machine learning models in semiconductor defect detection, while X-ray scanning is memory-intensive and time-consuming for large-scale applications.

Method: Physics Informed Generative Models for Magnetic Field Images (PI-GenMFI) using diffusion models with two physical constraints to generate synthetic MFI samples, specifically for power short defects, integrating specific physical information.

Result: The model shows promising results in qualitative and quantitative evaluations using various image generation and signal processing metrics, outperforming state-of-the-art generative models from both VAE and diffusion methods.

Conclusion: PI-GenMFI provides an effective solution to generate synthetic MFI training data, enabling more efficient ML-based defect localization and optimizing the semiconductor manufacturing defect detection process.

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: A novel GAN-based data reconstruction attack framework with progressive feature optimization that significantly outperforms existing attacks on deep neural networks, especially in high-resolution and out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing data reconstruction attacks (DRAs) in split inference systems are only effective on shallow models and fail to leverage semantic priors, limiting reconstruction quality and generalizability across datasets and architectures.

Method: Proposes a GAN-based DRA framework with Progressive Feature Optimization (PFO) that decomposes the generator into hierarchical blocks and incrementally refines intermediate representations. Uses L1-ball constraint to stabilize optimization and improve image realism.

Result: Extensive experiments show the method outperforms prior attacks by a large margin, particularly in high-resolution scenarios, out-of-distribution settings, and against deeper, more complex DNNs.

Conclusion: The proposed framework significantly enhances the effectiveness of data reconstruction attacks in split inference systems, demonstrating superior performance across various challenging scenarios and model architectures.

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST is a diffusion-based framework for text-driven emotional talking head synthesis that addresses limitations in control flexibility, motion naturalness, and expression quality through novel modules and training strategies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for emotional talking head synthesis suffer from limitations in control flexibility, motion naturalness, and expression quality, with datasets primarily collected in lab settings hindering practical real-world applications.

Method: Proposes EmoCAST with two key modules: 1) text-guided decoupled emotive module for appearance modeling and emotion comprehension, 2) emotive audio attention module to capture interplay between emotion and audio. Also constructs emotional dataset and introduces emotion-aware sampling and progressive functional training strategies.

Result: EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos.

Conclusion: The proposed framework successfully addresses the challenges of emotional talking head synthesis through innovative modules and training strategies, enabling practical applications in real-world scenarios.

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: SwinUNETR-based AI framework for breast cancer detection in MRI, achieving 2nd place in multi-center challenge with 511 studies from 6 European centers.


<details>
  <summary>Details</summary>
Motivation: Breast cancer is a leading cause of cancer mortality in women, and MRI is highly sensitive for detection, especially in high-risk patients or those with dense breast tissue where mammography is less effective.

Method: Developed a SwinUNETR-based deep learning framework incorporating breast region masking, extensive data augmentation, and ensemble learning to improve robustness and generalizability across multi-center data.

Result: The method achieved second place on the ODELIA consortium challenge leaderboard, demonstrating strong performance in breast cancer diagnosis and classification.

Conclusion: The framework shows potential to support clinical breast MRI interpretation, and the codebase has been publicly shared to facilitate further research and development.

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack is a plug-and-play framework that reconstructs complete 3D Gaussian head avatars by addressing poor back-head reconstruction through synthetic back-view generation and adaptive spatial alignment.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian Splatting methods for head avatars rely mainly on frontal-view images, resulting in poorly constructed back-head regions with geometric inconsistencies, structural blurring, and reduced realism, limiting overall avatar fidelity.

Method: Proposes AvatarBack with two core innovations: 1) Subject-specific Generator (SSG) that synthesizes identity-consistent back-view pseudo-images from sparse frontal inputs, and 2) Adaptive Spatial Alignment Strategy (ASA) that uses learnable transformation matrices to resolve pose and coordinate discrepancies between synthetic views and 3D Gaussian representation.

Result: Extensive experiments on NeRSemble and K-hairstyle datasets show significant improvements in back-head reconstruction quality while preserving frontal fidelity, with consistent visual realism under diverse motions and full animatability.

Conclusion: AvatarBack successfully addresses the back-head reconstruction challenge in 3D Gaussian avatars through its novel plug-and-play framework, enabling complete and consistent avatar modeling with enhanced realism.

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: Foundation models fine-tuned and integrated with traditional facial recognition networks significantly improve sitter identification in historical paintings, overcoming domain shift and artistic variations that challenge conventional methods.


<details>
  <summary>Details</summary>
Motivation: Automated facial recognition can assist art historians in identifying sitters in historical paintings, but traditional models struggle with artistic domain shift, stylistic variations, and high intra-class variation in artworks.

Method: Fine-tune foundation models and integrate their embeddings with those from conventional facial recognition networks to handle artistic factors like style, skill, intent, and artistic influences.

Result: Demonstrates notable improvements over current state-of-the-art methods in facial recognition for artworks, effectively bridging the performance gap where traditional methods fail.

Conclusion: Foundation models show strong potential for improving facial recognition in historical paintings by addressing domain-specific challenges and artistic variations that complicate traditional recognition approaches.

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti is an end-to-end text-guided graffiti generation framework that preserves facial identity during extreme stylistic transformations by using a style-first approach with face-consistent self-attention and CLIP-guided pose customization.


<details>
  <summary>Details</summary>
Motivation: Preserving facial identity in graffiti art is challenging because subtle distortions can erase recognizability, undermining personal and cultural authenticity in this high-contrast, abstract medium.

Method: Uses LoRA-fine-tuned pretrained diffusion transformer for style transfer, then enforces identity fidelity through face-consistent self-attention with explicit identity embeddings. Pose customization is achieved without keypoints using CLIP-guided prompt extension.

Result: Quantitative results show competitive facial feature consistency and state-of-the-art aesthetic/human preference scores. Qualitative analysis and live festival deployment demonstrate real-world creative impact.

Conclusion: CraftGraffiti advances identity-respectful AI-assisted artistry by blending stylistic freedom with recognizability through a principled style-first, identity-after paradigm.

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: Proposes a self-judgment scoring method for LVLMs to reduce hallucinations and improve safety without external resources, outperforming traditional alignment methods.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for LVLMs rely on external datasets and human annotations, limiting scalability and increasing costs while still suffering from hallucinations and safety issues.

Method: Generates debiased self-judgment scores internally within the model for self-evaluation, enabling autonomous improvement of visual-linguistic alignment without external resources.

Result: Significantly reduces hallucinations, enhances safety, and improves overall capability compared to traditional methods like instruction tuning and preference tuning.

Conclusion: The self-judgment approach provides a more effective and scalable solution for aligning large visual-language models, addressing key challenges in modality integration.

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: S-HArM dataset for intent-aware classification of AI-generated images (Humor/Satire, Art, Misinformation) with 9,576 real-world image-text pairs and synthetic data generation strategies.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal AI efforts overlook the intent behind AI-generated images, creating a gap in understanding whether content is created for humor, art, or misinformation purposes.

Method: Created S-HArM dataset from Twitter/X and Reddit, explored three prompting strategies (image-guided, description-guided, multimodally-guided) with Stable Diffusion for synthetic data, and tested various models including modality fusion, contrastive learning, and vision-language models.

Result: Models trained on image- and multimodally-guided synthetic data generalized better to real-world content due to preserved visual context, but overall performance remained limited.

Conclusion: Inferring intent from AI-generated content is complex and requires specialized architectures beyond current multimodal approaches, highlighting the need for more sophisticated intent-aware classification systems.

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: MobileCLIP2 improves upon MobileCLIP with better teacher ensembles and captioner fine-tuning, achieving state-of-the-art zero-shot accuracy at low latencies while being smaller and faster than comparable models.


<details>
  <summary>Details</summary>
Motivation: To enhance the multi-modal reinforced training of MobileCLIP by improving CLIP teacher ensembles and captioner teachers, enabling more efficient knowledge distillation and better performance in low-latency mobile applications.

Method: Improved multi-modal reinforced training through: 1) better CLIP teacher ensembles trained on DFN dataset, 2) improved captioner teachers trained on DFN and fine-tuned on diverse high-quality image-caption datasets. Used temperature tuning in contrastive knowledge distillation and combined synthetic captions from multiple models.

Result: Achieved state-of-the-art ImageNet-1k zero-shot accuracies at low latencies: 2.2% improvement for MobileCLIP2-B vs MobileCLIP-B. MobileCLIP2-S4 matches SigLIP-SO400M/14 accuracy while being 2x smaller and improves on DFN ViT-L/14 at 2.5x lower latency.

Conclusion: MobileCLIP2 demonstrates significant improvements in zero-shot accuracy with reduced model size and latency, making it highly suitable for mobile applications. The released code enables scalable creation of reinforced datasets with arbitrary teachers.

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: A dynamic neural video compression framework with variable coding routes and rate control agent that achieves precise bitrate control while maintaining competitive rate-distortion performance.


<details>
  <summary>Details</summary>
Motivation: Neural video compression has shown remarkable performance but struggles with precise rate control due to inherent limitations of learning-based codecs, making variable bitrate scenarios challenging.

Method: Proposes Dynamic-Route Autoencoder with variable coding routes (each with partial complexity and distinct RD trade-offs), Rate Control Agent for real-time bitrate estimation and route adjustment, and Joint-Routes Optimization for collaborative training of multiple routes.

Result: Achieves 14.8% average BD-Rate reduction and 0.47dB BD-PSNR gain over state-of-the-art methods with only 1.66% average bitrate error on HEVC and UVG datasets.

Conclusion: The framework successfully achieves Rate-Distortion-Complexity Optimization for various bitrate and bitrate-constrained applications, providing effective variable bitrate neural video compression.

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: CardioMorphNet is a recurrent Bayesian deep learning framework for 3D cardiac motion estimation that uses shape-guided registration instead of intensity-based methods, achieving superior performance on UK Biobank data with lower uncertainty.


<details>
  <summary>Details</summary>
Motivation: Existing cardiac motion estimation methods struggle with accuracy because they rely on intensity-based image registration that may overlook cardiac anatomical regions, leading to suboptimal motion capture.

Method: Proposes a recurrent variational autoencoder framework with two posterior models for bi-ventricular segmentation and motion estimation. Uses Bayesian formulation to guide registration through segmentation maps without intensity-based similarity loss, leveraging sequential SAX volumes and spatio-temporal features.

Result: Superior performance in cardiac motion estimation on UK Biobank dataset, outperforming state-of-the-art methods. Also yields lower uncertainty values for estimated motion fields in cardiac regions compared to other probabilistic methods.

Conclusion: CardioMorphNet effectively addresses limitations of intensity-based registration by focusing on anatomical regions through shape-guided registration, providing more accurate motion estimation with higher confidence predictions.

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: A simple training-time method for domain-robust atypical mitotic figure classification that uses style perturbations, attention feature alignment, and EMA teacher distillation to achieve strong performance across scanner/stain variations.


<details>
  <summary>Details</summary>
Motivation: Atypical mitotic figures are important histopathological markers but challenging to identify consistently due to domain shifts from scanner, stain, and acquisition differences.

Method: Three components: (i) style perturbations at early/mid backbone stages for feature diversity, (ii) attention-refined feature alignment across domains using weak domain labels, (iii) EMA teacher distillation with temperature-scaled KL divergence for prediction stability.

Result: Achieved balanced accuracy of 0.8762, sensitivity of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499 on MIDOG 2025 Task 2 preliminary leaderboard.

Conclusion: The method provides strong, balanced performance with negligible inference overhead and only requires coarse domain metadata, making it competitive for the MIDOG 2025 challenge.

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [60] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: Pref-GRPO addresses reward hacking in text-to-image generation by using pairwise preference rewards instead of pointwise scoring, and introduces UniGenBench for comprehensive model evaluation.


<details>
  <summary>Details</summary>
Motivation: Current GRPO-based methods using pointwise reward models are susceptible to reward hacking, where minimal score differences are amplified, causing unstable image generation and over-optimization for trivial gains.

Method: Pref-GRPO uses pairwise preference comparisons within groups and win rates as reward signals, shifting optimization from score maximization to preference fitting. Also introduces UniGenBench with 600 prompts across 5 themes and detailed evaluation criteria using MLLM.

Result: Pref-GRPO effectively differentiates subtle image quality differences, provides stable advantages, and mitigates reward hacking. UniGenBench reveals strengths/weaknesses of T2I models and validates Pref-GRPO's effectiveness.

Conclusion: The proposed Pref-GRPO method with pairwise preference optimization and the comprehensive UniGenBench benchmark address key limitations in current T2I generation and evaluation methods, enabling more stable training and thorough model assessment.

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [61] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS is a novel framework for generalizable Gaussian Splatting that improves novel view synthesis from sparse input views by incorporating context-aware, cross-dimension, and cross-scale constraints to enhance feature learning and geometric accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for generalizable Gaussian Splatting struggle with encoding discriminative, multi-view consistent features for accurate geometry construction from sparse input views, limiting rendering quality.

Method: Proposes C3-GS framework with three lightweight modules integrated into a unified rendering pipeline: context-aware, cross-dimension, and cross-scale constraints to improve feature fusion without additional supervision.

Result: Achieves state-of-the-art rendering quality and generalization ability on benchmark datasets, enabling photorealistic synthesis from sparse views without per-scene optimization.

Conclusion: C3-GS effectively addresses feature learning limitations in generalizable Gaussian Splatting through multi-constraint integration, demonstrating superior performance in novel view synthesis from sparse inputs.

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [62] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: SeqVLM is a zero-shot 3D visual grounding framework that uses multi-view scene images with spatial information to localize objects from natural language descriptions without scene-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot 3DVG methods suffer from spatial-limited reasoning due to single-view localization and contextual omissions. Supervised methods require scene-specific training, limiting real-world applicability.

Method: Generates 3D instance proposals via semantic segmentation, refines through semantic filtering, uses proposal-guided multi-view projection to preserve spatial relationships, and implements dynamic scheduling for VLM processing of sequence-query prompts.

Result: Achieves state-of-the-art performance on ScanRefer (55.6% Acc@0.25) and Nr3D (53.2% Acc@0.25) benchmarks, surpassing previous zero-shot methods by 4.0% and 5.2% respectively.

Conclusion: SeqVLM advances 3DVG toward greater generalization and real-world applicability by eliminating scene-specific training requirements while maintaining high accuracy through multi-view reasoning and efficient VLM utilization.

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [63] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: CLIP models show varying robustness to occlusion in military vehicle classification, with Transformers outperforming CNNs, and fine-tuning backbones significantly improves occlusion resilience.


<details>
  <summary>Details</summary>
Motivation: Evaluate CLIP's robustness in challenging military environments with occlusion and degraded SNR, which remains underexplored despite its zero-shot classification advantages for defense applications with limited labeled data.

Method: Test CLIP variants on custom military vehicle dataset with 18 classes, using Normalized Area Under the Curve (NAUC) to measure performance across different occlusion percentages, comparing Transformer vs CNN architectures and analyzing fine-tuning effects.

Result: Four key findings: Transformers outperform CNNs; fine-grained occlusions degrade performance more than large contiguous ones; linear-probed models drop sharply at ~35% occlusion; backbone fine-tuning pushes performance drop threshold to >60% occlusion.

Conclusion: Occlusion-specific augmentations during training are crucial, and further research into patch-level sensitivity and architectural resilience is needed for real-world CLIP deployment in military applications.

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [64] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: Proposes SKGE-Swin architecture for autonomous vehicles using Swin Transformer with skip-stage mechanism to enhance global feature representation and maintain critical information throughout network levels, achieving superior driving performance on CARLA platform.


<details>
  <summary>Details</summary>
Motivation: To develop an end-to-end autonomous vehicle model with improved pixel-to-pixel context awareness and better comprehension of complex environmental patterns in vehicle surroundings.

Method: Utilizes Swin Transformer with skip-stage mechanism and Shifted Window-based Multi-head Self-Attention (SW-MSA) to extract information from distant pixels while preserving critical information across network stages.

Result: Achieves superior Driving Score compared to previous methods when evaluated on CARLA platform using adversarial scenarios simulating real-world conditions.

Conclusion: The SKGE-Swin architecture effectively enhances autonomous vehicle perception and performance through improved global feature representation and information retention across network levels.

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [65] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: Survey paper advocating for revisiting abstract concept recognition in videos using modern foundation models, drawing on decades of community experience to avoid reinventing solutions.


<details>
  <summary>Details</summary>
Motivation: Humans can recognize abstract concepts like justice and freedom in videos, while current AI systems mainly understand concrete visible elements. Abstract concept understanding is crucial for aligning models with human reasoning and values.

Method: Survey and analysis of different tasks, datasets, and historical approaches used for abstract concept understanding in video content over decades of research.

Result: The paper identifies that researchers have periodically attempted to solve abstract concept recognition using available tools, and argues that foundation models provide an ideal setting to address this challenge.

Conclusion: Drawing on decades of community experience will help address the important open challenge of abstract concept understanding in videos and avoid reinventing solutions in the era of multi-modal foundation models.

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [66] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: Proposes Global Class Activation Probabilistic Map Evaluation method for trustworthy skin lesion classification, addressing limitations of existing explainability methods by analyzing all classes' activation maps probabilistically at pixel level.


<details>
  <summary>Details</summary>
Motivation: Despite high accuracy in skin lesion classification models, distrust in AI remains due to lack of explainability. Existing methods like LIME suffer from inconsistency and CAM methods fail to consider all classes, limiting diagnostic reliability.

Method: Global Class Activation Probabilistic Map Evaluation that analyzes all classes' activation probability maps probabilistically at pixel level. Uses SafeML for false diagnosis detection and warnings. Evaluated on ISIC datasets with MobileNetV2 and Vision Transformers.

Result: Method provides unified visualization of diagnostic process, helping reduce misdiagnosis risk. SafeML integration enhances detection of false diagnoses and issues warnings to improve diagnostic reliability.

Conclusion: The proposed approach addresses explainability limitations in skin lesion classification, improving trustworthiness and patient safety through probabilistic analysis of all class activations and false diagnosis detection mechanisms.

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [67] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: Diffusion Classifier shows improved compositional generalization compared to CLIP in attribute binding tasks, but all vision-language models struggle with relational reasoning like left/right concepts.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP often fail at compositional semantics, incorrectly combining attributes and objects. The research explores whether generative diffusion-based classifiers have better compositional generalization abilities than discriminative models.

Method: Evaluated three models (Diffusion Classifier, CLIP, and ViLT) on object-attribute binding and relational tasks in zero-shot and generalized zero-shot learning settings. Analyzed embedding similarities to understand performance limitations.

Result: Diffusion Classifier and ViLT performed well on concept binding tasks, but all models struggled significantly with relational reasoning. CLIP embeddings showed overly similar representations for relational concepts like left/right.

Conclusion: While diffusion-based classifiers show promise for compositional generalization, relational reasoning remains a fundamental challenge for vision-language models due to poor representation of spatial relations.

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [68] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: A novel surfel-based pose learning regression method for point cloud registration that uses SE(3) equivariant convolutional features to handle orientations and uncertainties, achieving robust performance against noise and aggressive rotations.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud registration methods ignore point orientations and uncertainties, making them vulnerable to noisy inputs and aggressive rotations like orthogonal transformations, requiring extensive training data with augmentation.

Method: Initialize surfels from Lidar point clouds using virtual perspective camera parameters, learn explicit SE(3) equivariant features (position and rotation) through SE(3) equivariant convolutional kernels, with an encoder, cross-attention mechanism, fully-connected decoder, and Huber loss.

Result: Experimental results on indoor and outdoor datasets demonstrate superior and robust performance on real point-cloud scans compared to state-of-the-art methods.

Conclusion: The proposed surfel-based approach with SE(3) equivariant features effectively addresses orientation and uncertainty issues in point cloud registration, providing robust performance against noise and aggressive transformations.

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [69] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: DVCTNet is a dual-view co-training network that combines global panoramic X-ray screening with detailed tooth-level inspection for superior dental caries detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Current dental caries detection methods have suboptimal accuracy due to subtle contrast variations and diverse lesion morphology in panoramic X-rays.

Method: Uses automated tooth detection to create global (panoramic) and local (tooth-cropped) views, pretrains vision foundation models on both views, and integrates them with a Gated Cross-View Attention module for feature fusion.

Result: Superior performance against state-of-the-art methods on both public dataset and newly curated high-precision dataset with double-verified annotations.

Conclusion: DVCTNet demonstrates clinical applicability and effectiveness for accurate dental caries detection by mimicking dentists' systematic workflow of combining global screening with detailed inspection.

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [70] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: FusionCounting integrates crowd counting with visible-infrared image fusion in a unified multi-task framework, using minimal annotations and dynamic loss weighting to improve both fusion quality and counting accuracy in dense scenes.


<details>
  <summary>Details</summary>
Motivation: Existing VIF methods focus on image quality but lack semantic guidance. Semantic segmentation requires extensive annotations, while object detection struggles in crowded scenes. RGB-T crowd counting has emerged but hasn't been integrated with VIF.

Method: Multi-task learning framework that combines visible-infrared image fusion with crowd counting. Uses dynamic loss function weighting for task balance and adversarial training for robustness. Leverages population density information to guide fusion.

Result: Experimental results show FusionCounting improves both image fusion quality and crowd counting performance on public datasets, demonstrating superior performance in dense scenes.

Conclusion: Crowd counting provides effective semantic guidance for VIF with minimal annotation requirements. The unified multi-task framework with dynamic weighting and adversarial training enables mutual benefits between fusion and counting tasks.

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [71] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: Novel pipeline using Vision Language Models (VLMs) with LoRA fine-tuning for 2D surgical tool keypoint estimation, outperforming traditional CNN/Transformer methods with minimal training.


<details>
  <summary>Details</summary>
Motivation: Traditional CNN and Transformer-based approaches often overfit on small-scale medical datasets, requiring a more generalized approach that leverages pre-trained VLMs for better performance in low-resource scenarios.

Method: Fine-tuned Vision Language Models using Low Rank Adjusting (LoRA) technique with carefully designed prompts for instruction-tuning, aligning visual features with semantic keypoint descriptions.

Result: With only two epochs of fine-tuning, the adapted VLM outperforms baseline models, demonstrating effectiveness of LoRA in low-resource scenarios and improved keypoint detection performance.

Conclusion: This approach not only enhances 2D keypoint estimation for surgical tools but also provides a foundation for future work in 3D surgical hands and tools pose estimation.

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [72] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: PointDGRWKV is the first RWKV-based framework for Domain Generalization in Point Cloud Classification, addressing spatial distortion and attention drift issues through adaptive geometric token shift and cross-domain key distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Existing DG PCC methods using convolutional networks, Transformers, or Mamba architectures suffer from limited receptive fields, high computational cost, or insufficient long-range dependency modeling. RWKV offers linear complexity and global receptive fields but introduces challenges when directly applied to unstructured point clouds.

Method: Proposes PointDGRWKV with two key modules: 1) Adaptive Geometric Token Shift to model local neighborhood structures and improve geometric context awareness, and 2) Cross-Domain key feature Distribution Alignment to mitigate attention drift by aligning key feature distributions across domains.

Result: Extensive experiments on multiple benchmarks demonstrate that PointDGRWKV achieves state-of-the-art performance on DG PCC while maintaining RWKV's linear efficiency.

Conclusion: The framework successfully addresses RWKV's limitations in DG PCC by enhancing spatial modeling and cross-domain robustness, establishing RWKV as a viable architecture for point cloud domain generalization tasks.

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [73] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: PathMR is a cell-level multimodal visual reasoning framework that generates diagnostic explanations and predicts cell distribution patterns from pathological images and textual queries, outperforming state-of-the-art methods in text quality, segmentation accuracy, and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Current deep learning diagnostic tools lack transparency and traceable rationale, limiting clinical adoption. There's a need for models that provide both pixel-level segmentation and semantically aligned textual explanations for dependable AI-assisted pathology.

Method: Proposed PathMR framework that takes pathological images and textual queries as input, generates expert-level diagnostic explanations while simultaneously predicting cell distribution patterns. Evaluated on PathGen dataset and newly developed GADVR dataset.

Result: PathMR consistently outperforms state-of-the-art visual reasoning methods in text generation quality, segmentation accuracy, and cross-modal alignment across both datasets.

Conclusion: PathMR demonstrates strong potential for improving interpretability in AI-driven pathological diagnosis by providing transparent insights through localized lesion regions and expert-style diagnostic narratives.

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [74] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: Deep learning framework using autofluorescence and SHG imaging achieves over 90% accuracy in detecting pancreatic cancer, outperforming manual methods and addressing challenges of limited medical datasets.


<details>
  <summary>Details</summary>
Motivation: Pancreatic ductal adenocarcinoma has extremely low survival rates due to late detection, creating an urgent need for early detection methods to improve patient outcomes.

Method: Developed and validated a deep learning framework using dual-modality imaging (autofluorescence + SHG) on 40 patient samples. Evaluated 6 architectures including CNNs and Vision Transformers, with final optimized ResNet using frozen pre-trained layers and class-weighted training to handle dataset limitations.

Result: Achieved over 90% accuracy in cancer detection, significantly improving upon current manual analysis methods. Successfully distinguished between normal, fibrotic, and cancerous tissue.

Conclusion: Establishes a robust automated PDAC detection pipeline that can augment pathologists' capabilities and provides foundation for expansion to other cancer types. Offers valuable insights for deep learning applications with limited medical imaging datasets.

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [75] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: This thesis develops counterfactual frameworks for explaining, auditing, and mitigating bias in vision classifiers and generative models through systematic attribute variation and causal analysis.


<details>
  <summary>Details</summary>
Motivation: To address interpretability and fairness in AI by using counterfactual reasoning to uncover spurious correlations, probe causal dependencies, and build more robust systems that avoid biased behaviors.

Method: Developed multiple frameworks: CAVLI (integrates attribution with concept-level analysis), ASAC (adversarial counterfactuals with curriculum learning), TIBET (scalable pipeline for prompt-sensitive bias evaluation), BiasConnect (causal graphs for intersectional biases), and InterMit (training-free mitigation via causal sensitivity scores).

Result: Created methods that quantify concept dependencies, improve fairness and accuracy while avoiding stereotypes, enable causal auditing of identity-related biases, diagnose intersectional biases, and provide modular bias mitigation without retraining.

Conclusion: Counterfactuals serve as a unifying framework for interpretability, fairness, and causality across both discriminative and generative models, establishing principled and scalable methods for socially responsible bias evaluation and mitigation.

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [76] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: Survey paper on unified perception for autonomous vehicles, proposing taxonomy and reviewing methods that integrate detection, tracking, and prediction into shared architectures to overcome limitations of modular pipelines.


<details>
  <summary>Details</summary>
Motivation: Traditional modular perception pipelines suffer from error accumulation and limited inter-task synergy. Unified perception offers potential improvements in robustness, contextual reasoning, and efficiency while maintaining interpretability.

Method: Comprehensive survey introducing a taxonomy categorizing methods along task integration, tracking formulation, and representation flow. Defines three paradigms: Early, Late, and Full Unified Perception, and systematically reviews existing methods, architectures, training strategies, datasets, and open-source availability.

Result: Establishes the first comprehensive framework for understanding unified perception, consolidates fragmented research efforts, and provides systematic categorization of existing approaches.

Conclusion: This work provides a foundational framework to guide future research toward more robust, generalizable, and interpretable perception systems for autonomous vehicles, highlighting future research directions in the field.

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [77] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: Dino U-Net leverages DINOv3 vision foundation model for medical image segmentation, achieving SOTA performance across 7 datasets with a parameter-efficient architecture that preserves feature fidelity.


<details>
  <summary>Details</summary>
Motivation: Effectively transferring foundation models' learned representations from natural images to precise medical image segmentation applications remains challenging despite their powerful pre-training.

Method: Proposes Dino U-Net with frozen DINOv3 backbone, specialized adapter for feature fusion, and fidelity-aware projection module (FAPM) to preserve feature quality during dimensionality reduction for the decoder.

Result: Achieves state-of-the-art performance across 7 diverse medical image segmentation datasets, with segmentation accuracy consistently improving as backbone model size increases up to 7B parameters.

Conclusion: Leveraging dense-pretrained features from general-purpose foundation models provides a highly effective and parameter-efficient approach for advancing medical image segmentation accuracy.

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [78] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: An ensemble of ConvNeXtBase models with rule-based refinement achieved 84.02% balanced accuracy for classifying atypical mitotic figures, though the refinement module reduced overall performance while increasing specificity.


<details>
  <summary>Details</summary>
Motivation: Differentiating atypical mitotic figures from normal ones is challenging due to time-consuming and subjective manual annotation in tumor grading.

Method: Trained an ensemble of ConvNeXtBase models using AUCMEDI framework and extended with a rule-based refinement module.

Result: Achieved 84.02% balanced accuracy on MIDOG25 test set. Rule-based refinement increased specificity but reduced sensitivity and overall performance.

Conclusion: Deep ensembles perform well for atypical mitotic figure classification, but rule-based refinement requires further research despite improving specific metrics.

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [79] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: COMETH is a lightweight multi-view human pose fusion algorithm that uses convex optimization and biomechanical constraints to improve accuracy and temporal consistency for real-time industrial monitoring applications.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of multi-camera centralized setups (high computational costs, bandwidth requirements) and edge device constraints (accuracy degradation, temporal/spatial inconsistencies) in human activity monitoring for Industry 5.0.

Method: Proposes COMETH algorithm with three key concepts: 1) integration of kinematic and biomechanical constraints for joint positioning accuracy, 2) convex optimization-based inverse kinematics for spatial fusion, 3) state observer implementation for temporal consistency.

Result: Outperforms state-of-the-art methods in localization, detection, and tracking accuracy on both public and industrial datasets.

Conclusion: COMETH enables accurate and scalable human motion tracking suitable for industrial and safety-critical applications, with publicly available code.

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [80] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: Novel olive tree segmentation method using SAM with geometric constraints achieves 98% accuracy, significantly improving over baseline SAM performance.


<details>
  <summary>Details</summary>
Motivation: Address climate change impacts by maintaining olive biodiversity through early anomaly detection using remote sensing technology for effective agricultural management.

Method: Leverages Segment Anything Model (SAM) with additional corrections based on tree alignment patterns and learnable constraints about tree shape and size for precise segmentation.

Result: Achieved 98% accuracy rate in olive tree segmentation, significantly surpassing the initial SAM baseline performance of 82%.

Conclusion: The integrated approach combining foundational models with geometric constraints provides highly accurate olive tree segmentation, enabling effective remote monitoring and biodiversity conservation.

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [81] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: E-ConvNeXt integrates CSPNet with ConvNeXt to create a lightweight network that reduces complexity by 80% while maintaining high accuracy, achieving 78.3-81.9% Top-1 accuracy on ImageNet with 0.9-3.1GFLOPs.


<details>
  <summary>Details</summary>
Motivation: Many high-performance networks are too complex for lightweight applications, limiting their practical deployment. The paper aims to create a more efficient version of ConvNeXt suitable for resource-constrained scenarios.

Method: Three core innovations: 1) Integrate Cross Stage Partial Network (CSPNet) with ConvNeXt to reduce complexity by 80%, 2) Optimize Stem and Block structures for better feature expression and efficiency, 3) Replace Layer Scale with channel attention mechanism.

Result: E-ConvNeXt-mini achieves 78.3% Top-1 accuracy at 0.9GFLOPs, E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs on ImageNet. Transfer learning tests on object detection confirm strong generalization capability.

Conclusion: E-ConvNeXt successfully creates a lightweight network that maintains high accuracy while significantly reducing computational complexity, making it suitable for deployment in resource-constrained environments with excellent accuracy-efficiency balance.

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [82] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: DrivingGaussian++ is an efficient framework for realistic reconstruction and controllable editing of dynamic autonomous driving scenes using 3D Gaussians and LiDAR priors, supporting training-free editing operations like texture modification, weather simulation, and object manipulation.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of realistic reconstruction and controllable editing of dynamic autonomous driving scenes with accurate positions, occlusions, and photorealistic surround-view synthesis.

Method: Models static background with incremental 3D Gaussians and reconstructs moving objects using composite dynamic Gaussian graph. Integrates LiDAR prior for detailed reconstruction and leverages multi-view images with depth priors. Uses large language models for automatic motion trajectory generation.

Result: Outperforms existing methods in dynamic scene reconstruction and photorealistic surround-view synthesis. Achieves consistent and realistic editing results while significantly enhancing scene diversity.

Conclusion: DrivingGaussian++ provides an effective solution for reconstructing and editing dynamic driving scenes with high realism and supports various editing operations without requiring additional training.

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [83] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: Novel methods to address image manipulation localization data scarcity using web data, featuring automated pixel-level annotation (CAAAv2), quality filtering (QES), large dataset creation (MIMLv2), and Web-IML model achieving 31% performance gain.


<details>
  <summary>Details</summary>
Motivation: Accurate localization of manipulated image regions is challenging due to high data acquisition costs and lack of high-quality annotated datasets, posing risks to social security.

Method: Leverage web data with CAAAv2 for automated pixel-level annotation, QES metric for quality filtering, Object Jitter for artifact generation, and develop Web-IML model for web-scale supervision.

Result: Created MIMLv2 dataset (246,212 images, 120x larger than IMD20), Web-IML achieved 31% performance gain and surpassed previous SOTA TruFor by 24.1 average IoU points.

Conclusion: The approach effectively mitigates data scarcity, significantly improves manipulation localization performance, and provides valuable resources for the research community.

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [84] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: ExpertSim is a deep learning approach using Mixture-of-Generative-Experts architecture to efficiently simulate Zero Degree Calorimeter responses in ALICE experiment, replacing computationally expensive Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo simulations for particle collision detectors at CERN are computationally expensive and strain computational resources, requiring more efficient alternatives.

Method: Uses a Mixture-of-Generative-Experts architecture where each expert specializes in simulating different subsets of data, tailored for the Zero Degree Calorimeter in ALICE experiment.

Result: Provides more precise and efficient generation process with improved accuracy and significant speedup compared to traditional Monte Carlo methods.

Conclusion: ExpertSim offers a promising high-efficiency solution for detector simulations in particle physics experiments at CERN, with code made publicly available.

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [85] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: A modular framework that decouples causal reasoning from answer generation using natural language causal chains as interpretable intermediate representations, outperforming state-of-the-art VideoQA models while improving explainability.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with higher-order reasoning, relying on opaque pipelines that entangle video understanding and causal inference, offering limited interpretability and shallow heuristics.

Method: Two-stage architecture with Causal Chain Extractor (CCE) that generates causal chains from video-question pairs, and Causal Chain-Driven Answerer (CCDA) that produces answers grounded in these chains. Uses LLMs to generate causal chains from existing datasets and introduces CauCo evaluation metric.

Result: Outperforms state-of-the-art models on three large-scale benchmarks, with substantial gains in explainability, user trust, and generalization. CCE serves as reusable causal reasoning engine across domains.

Conclusion: The modular framework with explicit causal chains enables transparent and logically coherent inference, positioning it as an effective solution for causal VideoQA with improved interpretability and performance.

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [86] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: POSE is a one-step distillation framework that accelerates large-scale video diffusion models by 100x, enabling high-quality video generation in a single step through phased stability priming and adversarial equilibrium training.


<details>
  <summary>Details</summary>
Motivation: Existing video acceleration methods suffer from poor temporal coherence modeling and lack single-step distillation capabilities for large-scale video models, creating bottlenecks in sampling efficiency.

Method: Two-phase distillation: (1) Stability priming - warm-up mechanism to stabilize adversarial distillation across SNR regimes, (2) Unified adversarial equilibrium - self-adversarial training toward Nash equilibrium in Gaussian noise space, plus conditional adversarial consistency for semantic and frame consistency.

Result: Outperforms other acceleration methods by average 7.15% on VBench-I2V metrics, reduces latency from 1000 seconds to 10 seconds (100x improvement) while maintaining competitive performance.

Conclusion: POSE successfully bridges the gap in video acceleration by enabling single-step generation with preserved temporal coherence and quality, making large-scale video diffusion models practical for real-time applications.

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [87] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: Training-free method that clusters semantically similar prompts and shares computation in early diffusion steps to reduce redundancy and improve efficiency in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are computationally expensive, and prior work focused on per-inference optimization rather than reducing redundancy across correlated prompts.

Method: Leverages coarse-to-fine nature of diffusion models by clustering prompts based on semantic similarity and sharing computation in early denoising steps that capture shared structures. Uses UnClip's text-to-image prior to enhance diffusion step allocation.

Result: Significantly reduces compute cost while improving image quality for models trained with image embeddings. Method integrates with existing pipelines and scales with prompt sets.

Conclusion: Provides an efficient approach that reduces environmental and financial burden of large-scale text-to-image generation by exploiting redundancy across similar prompts.

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [88] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: A Mamba-based approach using VM-UNet architecture with stain augmentation for mitosis detection under domain shift, submitted to MIDOG challenge with preliminary results showing room for improvement.


<details>
  <summary>Details</summary>
Motivation: Mitosis detection is crucial for tumor assessment, but machine learning algorithms suffer performance drops when tested on domains different from training data. Domain shift is a significant challenge in medical imaging.

Method: Proposes a Mamba-based approach using VM-UNet architecture combined with stain augmentation operations to improve model robustness against domain shift in mitosis detection.

Result: Preliminary experiments on MIDOG++ dataset indicate large room for improvement for the proposed method. The approach has been submitted to track 1 of the MIDOG challenge.

Conclusion: The Mamba-based approach shows potential for mitosis detection under domain shift but requires further development and optimization to achieve better performance on cross-domain evaluation.

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [89] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: Multi-task learning approach for domain generalization in atypical mitosis detection, helping models focus on relevant features while ignoring domain-varying backgrounds.


<details>
  <summary>Details</summary>
Motivation: Machine learning models for recognizing atypical mitotic figures suffer performance drops under domain shift, requiring better generalization across different histopathology datasets.

Method: Multi-task learning with auxiliary tasks correlated to the main classification task, forcing the model to focus on the object to classify while ignoring domain-varying background features.

Result: Promising performance in preliminary evaluation across three distinct datasets: MIDOG 2025 Atypical Training Set, Ami-Br dataset, and MIDOG25 challenge preliminary test set.

Conclusion: The multi-task learning approach shows potential for improving domain generalization in atypical mitosis detection by leveraging auxiliary tasks to enhance model focus on relevant features.

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [90] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN is a one-shot handwriting synthesis framework that uses frequency-aware components to generate realistic, style-consistent handwriting from a single example, addressing limitations of conventional methods in capturing long-range dependencies and fine-grained details.


<details>
  <summary>Details</summary>
Motivation: Handwriting recognition systems suffer from scarce labeled data, and current synthesis methods struggle with long-range dependencies and ignore frequency information crucial for capturing stylistic details.

Method: Proposes FW-GAN with phase-aware Wave-MLP generator, frequency-guided discriminator, and novel Frequency Distribution Loss to align frequency characteristics between synthetic and real handwriting.

Result: Experiments on Vietnamese and English datasets show FW-GAN generates high-quality, style-consistent handwriting that effectively augments training data for low-resource HTR systems.

Conclusion: FW-GAN provides a valuable tool for handwriting data augmentation, successfully addressing frequency modeling challenges and improving visual fidelity in synthetic handwriting generation.

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [91] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: MMG-Vid is a training-free visual token pruning framework that reduces computational overhead in Video LLMs by maximizing marginal gains at segment and token levels, achieving 75% token reduction with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Video LLMs face computational challenges due to excessive visual tokens, and existing methods ignore dynamic characteristics and temporal dependencies in videos.

Method: Divides video into segments based on frame similarity, dynamically allocates token budgets per segment, and uses temporal-guided DPC algorithm to model inter-frame uniqueness and intra-frame diversity.

Result: Maintains over 99.5% of original performance while reducing 75% visual tokens and accelerating prefilling stage by 3.9x on LLaVA-OneVision-7B.

Conclusion: MMG-Vid effectively maximizes limited token budget utilization, significantly improving efficiency while preserving strong video understanding performance.

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [92] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: CogVLA is a cognition-aligned VLA framework that uses instruction-driven routing and sparsification to achieve state-of-the-art performance with 2.5x lower training costs and 2.8x faster inference compared to OpenVLA.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language-Action models require extensive post-training, resulting in high computational overhead that limits scalability and deployment.

Method: 3-stage progressive architecture: 1) EFA-Routing for instruction-aware visual token compression, 2) LFP-Routing for token-level sparsity by pruning irrelevant tokens, 3) V-L-A Coupled Attention for accurate action generation with compressed inputs.

Result: Achieves 97.4% success rate on LIBERO benchmark and 70.0% on real-world robotic tasks, with 2.5x training cost reduction and 2.8x inference latency reduction.

Conclusion: CogVLA provides an efficient and high-performance VLA framework that significantly reduces computational overhead while maintaining state-of-the-art results, making it more scalable and deployable.

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: HydraFake dataset addresses real-world deepfake detection challenges with hierarchical generalization testing, and Veritas MLLM detector uses pattern-aware reasoning for improved performance.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection benchmarks suffer from discrepancies with industrial practice, featuring homogeneous training sources and low-quality testing images that hinder practical deployment.

Method: Introduces HydraFake dataset with diversified deepfake techniques and in-the-wild forgeries, plus Veritas MLLM detector with pattern-aware reasoning (planning and self-reflection) and two-stage training pipeline.

Result: Previous detectors show good cross-model generalization but fail on unseen forgeries and data domains. Veritas achieves significant gains across OOD scenarios with transparent outputs.

Conclusion: HydraFake provides realistic benchmark, and Veritas with pattern-aware reasoning effectively addresses real-world deepfake detection challenges with improved generalization capabilities.

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: FakeParts introduces a new class of deepfakes with subtle, localized manipulations to specific regions of authentic videos, making them highly deceptive and difficult to detect.


<details>
  <summary>Details</summary>
Motivation: Current deepfake detection methods struggle with partial manipulations that blend seamlessly with real content, creating an urgent vulnerability in detection capabilities.

Method: Created FakePartsBench, the first large-scale benchmark dataset with over 25K videos featuring pixel-level and frame-level manipulation annotations for comprehensive evaluation of detection methods.

Result: FakeParts reduces human detection accuracy by over 30% compared to traditional deepfakes, with similar performance degradation in state-of-the-art detection models.

Conclusion: This work identifies critical vulnerabilities in current deepfake detection and provides essential resources to develop more robust methods for detecting partial video manipulations.

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: First data-driven multi-view 3D point tracker that uses 4+ cameras to track arbitrary points in dynamic scenes, overcoming depth ambiguities and occlusion issues of monocular methods while avoiding the high camera count requirements of previous multi-camera approaches.


<details>
  <summary>Details</summary>
Motivation: Existing monocular trackers struggle with depth ambiguities and occlusion, while prior multi-camera methods require over 20 cameras and tedious per-sequence optimization. There's a need for a practical, robust solution that works with fewer cameras.

Method: Feed-forward model that fuses multi-view features into unified point cloud, uses k-nearest-neighbors correlation with transformer-based update to predict 3D correspondences. Requires known camera poses and multi-view depth (sensor-based or estimated). Trained on 5K synthetic multi-view Kubric sequences.

Result: Achieves median trajectory errors of 3.1 cm on Panoptic Studio and 2.0 cm on DexYCB benchmarks. Generalizes well to diverse camera setups (1-8 views) and video lengths (24-150 frames).

Conclusion: Sets new standard for multi-view 3D tracking research with practical camera requirements (4+ cameras), robust performance under occlusion, and provides a practical tool for real-world applications. Code and datasets released to advance the field.

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: OneReward is a unified RL framework that uses a single vision-language model as a reward model to enhance multi-task generation capabilities across different evaluation criteria, eliminating the need for task-specific supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mask-guided image generation tasks rely on task-specific supervised fine-tuning, which limits generalization and training efficiency across diverse tasks with different data distributions and evaluation metrics.

Method: Uses a single vision-language model as a generative reward model that can distinguish winners/losers for any task and evaluation criterion. Applies multi-task reinforcement learning directly on pre-trained base models without task-specific SFT.

Result: The unified edit model (Seedream 3.0 Fill) consistently outperforms both commercial (Ideogram, Adobe Photoshop) and open-source (FLUX Fill [Pro]) competitors across multiple evaluation dimensions.

Conclusion: OneReward provides an effective unified framework for multi-task generation that eliminates the need for task-specific fine-tuning while achieving superior performance across diverse tasks and evaluation criteria.

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: Dress&Dance is a video diffusion framework that generates high-quality virtual try-on videos showing users wearing desired garments while moving according to reference videos, using a single user image and supporting various garment types.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality virtual try-on system that can generate realistic videos of users wearing different garments while maintaining motion fidelity, addressing limitations in existing solutions.

Method: Uses CondNet, a novel conditioning network with attention mechanisms to unify multi-modal inputs (text, images, videos). Trained on heterogeneous data combining limited video data with larger image datasets in a multistage progressive manner.

Result: Generates 5-second 24 FPS videos at 1152x720 resolution. Outperforms existing open source and commercial solutions, enabling high-quality and flexible try-on experiences.

Conclusion: Dress&Dance provides an effective framework for virtual try-on video generation with superior garment registration and motion fidelity compared to current alternatives.

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: Winning solution to NeurIPS 2024 Erasing the Invisible challenge that achieves 95.7% watermark removal with minimal quality impact using adaptive attacks for both black-box and beige-box scenarios.


<details>
  <summary>Details</summary>
Motivation: To stress-test watermark robustness against adversarial attacks and determine if existing watermarks can withstand varying degrees of adversary knowledge.

Method: For beige-box: VAE-based evasion attack with test-time optimization and CIELAB color-contrast restoration. For black-box: clustering by artifacts, then using diffusion models with controlled noise injection and ChatGPT-generated semantic priors.

Result: Achieved near-perfect watermark removal (95.7%) with negligible impact on residual image quality.

Conclusion: The attacks demonstrate vulnerabilities in current watermarking methods and aim to inspire development of more robust image watermarking techniques.

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>
