<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [3D and 4D World Modeling: A Survey](https://arxiv.org/abs/2509.07996)
*Lingdong Kong,Wesley Yang,Jianbiao Mei,Youquan Liu,Ao Liang,Dekai Zhu,Dongyue Lu,Wei Yin,Xiaotao Hu,Mingkai Jia,Junyuan Deng,Kaiwen Zhang,Yang Wu,Tianyi Yan,Shenyuan Gao,Song Wang,Linfeng Li,Liang Pan,Yong Liu,Jianke Zhu,Wei Tsang Ooi,Steven C. H. Hoi,Ziwei Liu*

Main category: cs.CV

TL;DR: First comprehensive survey on 3D/4D world modeling, establishing definitions, taxonomy, and systematic review of methods, datasets, and metrics for video, occupancy, and LiDAR-based approaches.


<details>
  <summary>Details</summary>
Motivation: Address fragmented literature and lack of standardized definitions for 3D/4D world models, while recognizing the gap in comprehensive surveys focusing on native 3D representations beyond 2D image/video methods.

Method: Establishes precise definitions and structured taxonomy covering VideoGen (video-based), OccGen (occupancy-based), and LiDARGen (LiDAR-based) approaches. Systematically summarizes datasets and evaluation metrics tailored to 3D/4D settings.

Result: Provides a coherent foundational reference with comprehensive review of existing literature, practical applications, and identified challenges in 3D/4D world modeling field.

Conclusion: Aims to advance the field by offering standardized framework, highlighting research directions, and serving as foundational reference for future work in 3D/4D world modeling and generation.

Abstract: World modeling has become a cornerstone in AI research, enabling agents to
understand, represent, and predict the dynamic environments they inhabit. While
prior work largely emphasizes generative methods for 2D image and video data,
they overlook the rapidly growing body of work that leverages native 3D and 4D
representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds
for large-scale scene modeling. At the same time, the absence of a standardized
definition and taxonomy for ``world models'' has led to fragmented and
sometimes inconsistent claims in the literature. This survey addresses these
gaps by presenting the first comprehensive review explicitly dedicated to 3D
and 4D world modeling and generation. We establish precise definitions,
introduce a structured taxonomy spanning video-based (VideoGen),
occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and
systematically summarize datasets and evaluation metrics tailored to 3D/4D
settings. We further discuss practical applications, identify open challenges,
and highlight promising research directions, aiming to provide a coherent and
foundational reference for advancing the field. A systematic summary of
existing literature is available at https://github.com/worldbench/survey

</details>


### [2] [An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities](https://arxiv.org/abs/2509.08003)
*Shahid Shafi Dar,Bharat Kaurav,Arnav Jain,Chandravardhan Singh Raghaw,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: XFloodNet is a novel deep learning framework that addresses urban flood classification challenges through hierarchical cross-modal attention, multi-scale feature extraction, and cascading transformer refinement, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Urban flooding is a critical climate change challenge, but traditional flood detection methods fail due to reliance on unimodal data and static rule-based systems. Existing approaches have limitations in hierarchical refinement, cross-modal integration, and adaptability to noisy environments.

Method: XFloodNet integrates three novel components: 1) Hierarchical Cross-Modal Gated Attention for dynamic visual-textual feature alignment, 2) Heterogeneous Convolutional Adaptive Multi-Scale Attention for frequency-enhanced feature prioritization, and 3) Cascading Convolutional Transformer Feature Refinement for hierarchical feature harmonization.

Result: XFloodNet achieves state-of-the-art F1-scores of 93.33% on Chennai Floods, 82.24% on Rhine18 Floods, and 88.60% on Harz17 Floods, significantly outperforming existing methods.

Conclusion: The proposed XFloodNet framework successfully addresses the limitations of traditional flood detection by providing robust, noise-resistant flood classification through advanced deep learning techniques, making it highly effective for urban flood management in climate change scenarios.

Abstract: In an era of escalating climate change, urban flooding has emerged as a
critical challenge for sustainable cities, threatening lives, infrastructure,
and ecosystems. Traditional flood detection methods are constrained by their
reliance on unimodal data and static rule-based systems, which fail to capture
the dynamic, non-linear relationships inherent in flood events. Furthermore,
existing attention mechanisms and ensemble learning approaches exhibit
limitations in hierarchical refinement, cross-modal feature integration, and
adaptability to noisy or unstructured environments, resulting in suboptimal
flood classification performance. To address these challenges, we present
XFloodNet, a novel framework that redefines urban flood classification through
advanced deep-learning techniques. XFloodNet integrates three novel components:
(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically
aligns visual and textual features, enabling precise multi-granularity
interactions and resolving contextual ambiguities; (2) a Heterogeneous
Convolutional Adaptive Multi-Scale Attention module, which leverages
frequency-enhanced channel attention and frequency-modulated spatial attention
to extract and prioritize discriminative flood-related features across spectral
and spatial domains; and (3) a Cascading Convolutional Transformer Feature
Refinement technique that harmonizes hierarchical features through adaptive
scaling and cascading operations, ensuring robust and noise-resistant flood
detection. We evaluate our proposed method on three benchmark datasets, such as
Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves
state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,
surpassing existing methods by significant margins.

</details>


### [3] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

TL;DR: Video Parallel Scaling (VPS) is an inference-time method that improves VideoLLM performance by running multiple parallel inference streams on disjoint frame subsets and aggregating outputs, avoiding computational costs of longer context windows.


<details>
  <summary>Details</summary>
Motivation: VideoLLMs face computational bottlenecks when increasing input frames for temporal detail, leading to prohibitive costs and performance degradation from long context lengths.

Method: VPS runs multiple parallel inference streams, each processing unique disjoint subsets of video frames, then aggregates output probabilities to integrate richer visual information without increasing context window.

Result: Extensive experiments across various model architectures (2B-32B) on benchmarks like Video-MME and EventHallusion show VPS consistently and significantly improves performance, scaling better than alternatives like Self-consistency.

Conclusion: VPS provides a memory-efficient and robust framework for enhancing temporal reasoning in VideoLLMs, effectively contracting the Chinchilla scaling law by leveraging uncorrelated visual evidence without additional training.

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [4] [Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change](https://arxiv.org/abs/2509.08024)
*Lata Pangtey,Omkar Kabde,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: Proposes a multimodal stance detection framework combining text and visual information through hierarchical fusion, achieving state-of-the-art results on climate change stance detection.


<details>
  <summary>Details</summary>
Motivation: Address the gap in existing stance detection approaches that focus only on textual data, while real-world social media content increasingly combines text with visual elements, creating a need for advanced multimodal methods.

Method: Uses a hierarchical fusion approach: Large Language Model for stance-relevant text summaries, domain-aware image caption generator for visual content interpretation, and a specialized transformer module to jointly model text-image interactions for robust stance classification.

Result: Achieved 76.2% accuracy, 76.3% precision, 76.2% recall, and 76.2% F1-score on the MultiClimate dataset, outperforming existing state-of-the-art approaches.

Conclusion: The proposed multimodal framework effectively integrates textual and visual information for stance detection, demonstrating superior performance and addressing the limitations of text-only approaches in handling modern social media content.

Abstract: With the rapid proliferation of information across digital platforms, stance
detection has emerged as a pivotal challenge in social media analysis. While
most of the existing approaches focus solely on textual data, real-world social
media content increasingly combines text with visual elements creating a need
for advanced multimodal methods. To address this gap, we propose a multimodal
stance detection framework that integrates textual and visual information
through a hierarchical fusion approach. Our method first employs a Large
Language Model to retrieve stance-relevant summaries from source text, while a
domain-aware image caption generator interprets visual content in the context
of the target topic. These modalities are then jointly modeled along with the
reply text, through a specialized transformer module that captures interactions
between the texts and images. The proposed modality fusion framework integrates
diverse modalities to facilitate robust stance classification. We evaluate our
approach on the MultiClimate dataset, a benchmark for climate change-related
stance detection containing aligned video frames and transcripts. We achieve
accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%,
respectively, outperforming existing state-of-the-art approaches.

</details>


### [5] [Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.08026)
*Zeinab Ghasemi Darehnaei,Mohammad Shokouhifar,Hossein Yazdanjouei,S. M. J. Rastegar Fatemi*

Main category: cs.CV

TL;DR: SI-EDTL is a two-stage swarm intelligence ensemble deep transfer learning model that combines multiple pre-trained CNN feature extractors with transfer classifiers, optimized by whale optimization algorithm, for superior multi-vehicle detection in UAV images.


<details>
  <summary>Details</summary>
Motivation: To develop an effective method for detecting multiple vehicle types in UAV images by leveraging ensemble learning and transfer learning to overcome challenges in complex aerial imagery and improve detection accuracy.

Method: Two-stage approach combining three pre-trained Faster R-CNN feature extractors (InceptionV3, ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5, Naive Bayes), creating 15 base learners aggregated via weighted averaging. Hyperparameters optimized using whale optimization algorithm.

Result: Outperforms existing methods on the AU-AIR UAV dataset, achieving balanced performance in accuracy, precision, and recall through MATLAB R2020b implementation with parallel processing.

Conclusion: SI-EDTL demonstrates the effectiveness of combining ensemble deep transfer learning with swarm intelligence optimization for robust multi-vehicle detection in UAV imagery, providing superior performance compared to conventional approaches.

Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep
transfer learning model for detecting multiple vehicles in UAV images. It
combines three pre-trained Faster R-CNN feature extractor models (InceptionV3,
ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,
Na\"ive Bayes), resulting in 15 different base learners. These are aggregated
via weighted averaging to classify regions as Car, Van, Truck, Bus, or
background. Hyperparameters are optimized with the whale optimization algorithm
to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with
parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV
dataset.

</details>


### [6] [MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery](https://arxiv.org/abs/2509.08027)
*Rafał Osadnik,Pablo Gómez,Eleni Bohacek,Rickbir Bahia*

Main category: cs.CV

TL;DR: A new Martian DEM dataset called MCTED with 80,898 samples from CTX instrument data, featuring optical image patches, DEM patches, and masks for missing/altered data, with validation showing small U-Net outperforms foundation models.


<details>
  <summary>Details</summary>
Motivation: To create a machine learning-ready dataset for Martian digital elevation model prediction that addresses artefacts and missing data issues in existing large-scale DEM processing pipelines.

Method: Developed a comprehensive pipeline to process high-resolution Mars orthoimage and DEM pairs from Mars Reconnaissance Orbiter CTX data, creating 80,898 samples with training/validation splits and mask patches for altered regions.

Result: A small U-Net architecture trained on MCTED dataset outperformed the zero-shot performance of DepthAnythingV2 foundation model for elevation prediction tasks.

Conclusion: The MCTED dataset provides a valuable resource for Martian DEM prediction with proper data handling capabilities, and specialized training on this dataset yields better performance than general foundation models for this specific task.

Abstract: This work presents a new dataset for the Martian digital elevation model
prediction task, ready for machine learning applications called MCTED. The
dataset has been generated using a comprehensive pipeline designed to process
high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a
dataset consisting of 80,898 data samples. The source images are data gathered
by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very
diverse and comprehensive coverage of the Martian surface. Given the complexity
of the processing pipelines used in large-scale DEMs, there are often artefacts
and missing data points in the original data, for which we developed tools to
solve or mitigate their impact. We divide the processed samples into training
and validation splits, ensuring samples in both splits cover no mutual areas to
avoid data leakage. Every sample in the dataset is represented by the optical
image patch, DEM patch, and two mask patches, indicating values that were
originally missing or were altered by us. This allows future users of the
dataset to handle altered elevation regions as they please. We provide
statistical insights of the generated dataset, including the spatial
distribution of samples, the distributions of elevation values, slopes and
more. Finally, we train a small U-Net architecture on the MCTED dataset and
compare its performance to a monocular depth estimation foundation model,
DepthAnythingV2, on the task of elevation prediction. We find that even a very
small architecture trained on this dataset specifically, beats a zero-shot
performance of a depth estimation foundation model like DepthAnythingV2. We
make the dataset and code used for its generation completely open source in
public repositories.

</details>


### [7] [APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction](https://arxiv.org/abs/2509.08104)
*Sasan Sharifipour,Constantino Álvarez Casado,Mohammad Sabokrou,Miguel Bordallo López*

Main category: cs.CV

TL;DR: APML is a differentiable approximation of one-to-one matching that addresses limitations of Chamfer-based losses by using Sinkhorn iterations with adaptive temperature scaling, achieving near-quadratic runtime and improved performance on point cloud tasks.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud loss functions like Chamfer Distance suffer from many-to-one correspondences causing point congestion and poor coverage, while Earth Mover Distance has prohibitive cubic complexity.

Method: Proposes Adaptive Probabilistic Matching Loss (APML) using Sinkhorn iterations on temperature-scaled similarity matrix with analytically computed temperature to guarantee minimum assignment probability.

Result: APML achieves near-quadratic runtime comparable to Chamfer losses, yields faster convergence, superior spatial distribution in low-density regions, and improved quantitative performance on ShapeNet benchmarks and WiFi-based 3D human point cloud generation.

Conclusion: APML provides an efficient, differentiable alternative to one-to-one matching that outperforms existing loss functions without additional hyperparameter tuning, making it practical for various point cloud prediction tasks.

Abstract: Training deep learning models for point cloud prediction tasks such as shape
completion and generation depends critically on loss functions that measure
discrepancies between predicted and ground-truth point sets. Commonly used
functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on
nearest-neighbor assignments, which often induce many-to-one correspondences,
leading to point congestion in dense regions and poor coverage in sparse
regions. These losses also involve non-differentiable operations due to index
selection, which may affect gradient-based optimization. Earth Mover Distance
(EMD) enforces one-to-one correspondences and captures structural similarity
more effectively, but its cubic computational complexity limits its practical
use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances. We analytically compute the temperature to guarantee a minimum
assignment probability, eliminating manual tuning. APML achieves near-quadratic
runtime, comparable to Chamfer-based losses, and avoids non-differentiable
operations. When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search. The code is available at:
https://github.com/apm-loss/apml.

</details>


### [8] [Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection](https://arxiv.org/abs/2509.08205)
*Jingjing Liu,Yinchao Han,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: L-RPCANet is a lightweight infrared small target detection framework that improves parameter efficiency and noise robustness through hierarchical bottleneck structures, noise reduction modules, and channel attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing deep unfolding networks for infrared small target detection face challenges with parameter lightweightness and noise robustness, limiting their practical deployment.

Method: Proposes L-RPCANet based on robust principal component analysis, featuring hierarchical bottleneck structures for channel-wise feature refinement, noise reduction modules for robustness, and squeeze-and-excitation networks for channel attention.

Result: Extensive experiments on ISTD datasets show superior performance compared to state-of-the-art methods including RPCANet, DRPCANet, and RPCANet++.

Conclusion: The proposed L-RPCANet framework achieves excellent performance while maintaining both lightweight parameter design and strong noise robustness, making it suitable for practical infrared small target detection applications.

Abstract: Infrared small target detection (ISTD) is one of the key techniques in image
processing. Although deep unfolding networks (DUNs) have demonstrated promising
performance in ISTD due to their model interpretability and data adaptability,
existing methods still face significant challenges in parameter lightweightness
and noise robustness. In this regard, we propose a highly lightweight framework
based on robust principal component analysis (RPCA) called L-RPCANet.
Technically, a hierarchical bottleneck structure is constructed to reduce and
increase the channel dimension in the single-channel input infrared image to
achieve channel-wise feature refinement, with bottleneck layers designed in
each module to extract features. This reduces the number of channels in feature
extraction and improves the lightweightness of network parameters. Furthermore,
a noise reduction module is embedded to enhance the robustness against complex
noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a
channel attention mechanism to focus on the varying importance of different
features across channels, thereby achieving excellent performance while
maintaining both lightweightness and robustness. Extensive experiments on the
ISTD datasets validate the superiority of our proposed method compared with
state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code
will be available at https://github.com/xianchaoxiu/L-RPCANet.

</details>


### [9] [Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing](https://arxiv.org/abs/2509.08228)
*Miao Cao,Siming Zheng,Lishun Wang,Ziyang Chen,David Brady,Xin Yuan*

Main category: cs.CV

TL;DR: Ultra-Sparse Sampling (USS) strategy reduces power consumption in gigapixel cameras by using minimal measurements (only one sub-frame per spatial location), achieving 10-100X power reduction with higher dynamic range than random sampling.


<details>
  <summary>Details</summary>
Motivation: Current digital cameras consume unsustainable power (~20W for 4K@30fps), making gigapixel cameras at 100-1000fps impractical. Compressive measurement is needed to reduce power consumption per pixel by 10-100X.

Method: Proposes USS regime where only one sub-frame is set to 1 per spatial location. Built DMD encoding system and developed BSTFormer - a sparse Transformer using local Block attention, global Sparse attention, and global Temporal attention to handle measurement mismatch.

Result: Extensive results on simulated and real-world data show significant outperformance over previous state-of-the-art algorithms. USS strategy provides higher dynamic range than random sampling and enables fixed exposure time for on-chip implementation.

Conclusion: USS strategy is an effective solution for power-efficient gigapixel video capture, offering better performance, higher dynamic range, and practical implementation advantages for complete video SCI systems on chip.

Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode
video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps.
Imagining gigapixel cameras operating at 100-1000 fps, the current processing
model is unsustainable. To address this, physical layer compressive measurement
has been proposed to reduce power consumption per pixel by 10-100X. Video
Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the
optical sensor layer to increase effective frame rate. A commonly used sampling
strategy of video SCI is Random Sampling (RS) where each mask element value is
randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated
that images can be recovered from a fraction of the image pixels. Inspired by
I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial
location, only one sub-frame is set to 1 and all others are set to 0. We then
build a Digital Micro-mirror Device (DMD) encoding system to verify the
effectiveness of our USS strategy. Ideally, we can decompose the USS
measurement into sub-measurements for which we can utilize I2P algorithms to
recover high-speed frames. However, due to the mismatch between the DMD and
CCD, the USS measurement cannot be perfectly decomposed. To this end, we
propose BSTFormer, a sparse TransFormer that utilizes local Block attention,
global Sparse attention, and global Temporal attention to exploit the sparsity
of the USS measurement. Extensive results on both simulated and real-world data
show that our method significantly outperforms all previous state-of-the-art
algorithms. Additionally, an essential advantage of the USS strategy is its
higher dynamic range than that of the RS strategy. Finally, from the
application perspective, the USS strategy is a good choice to implement a
complete video SCI system on chip due to its fixed exposure time.

</details>


### [10] [GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation](https://arxiv.org/abs/2509.08232)
*Seongho Kim,Sejong Ryu,Hyoukjun You,Je Hyeong Hong*

Main category: cs.CV

TL;DR: GTA-Crime dataset and framework using GTA5 to generate fatal video anomalies like shootings/stabbings for improved surveillance detection, with domain adaptation to bridge synthetic-real gap.


<details>
  <summary>Details</summary>
Motivation: Current video anomaly detection struggles with rare fatal incidents due to data scarcity and ethical collection issues.

Method: Created synthetic dataset using GTA5, developed generation framework, and implemented snippet-level domain adaptation with Wasserstein adversarial training.

Result: Experimental validation shows GTA-Crime dataset enhances real-world fatal violence detection accuracy when combined with domain adaptation.

Conclusion: Synthetic data generation with proper domain adaptation effectively addresses fatal incident detection challenges in surveillance systems.

Abstract: Recent advancements in video anomaly detection (VAD) have enabled
identification of various criminal activities in surveillance videos, but
detecting fatal incidents such as shootings and stabbings remains difficult due
to their rarity and ethical issues in data collection. Recognizing this
limitation, we introduce GTA-Crime, a fatal video anomaly dataset and
generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains
fatal situations such as shootings and stabbings, captured from CCTV multiview
perspectives under diverse conditions including action types, weather, time of
day, and viewpoints. To address the rarity of such scenarios, we also release a
framework for generating these types of videos. Additionally, we propose a
snippet-level domain adaptation strategy using Wasserstein adversarial training
to bridge the gap between synthetic GTA-Crime features and real-world features
like UCF-Crime. Experimental results validate our GTA-Crime dataset and
demonstrate that incorporating GTA-Crime with our domain adaptation strategy
consistently enhances real world fatal violence detection accuracy. Our dataset
and the data generation framework are publicly available at
https://github.com/ta-ho/GTA-Crime.

</details>


### [11] [RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification](https://arxiv.org/abs/2509.08234)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: RepViT-CXR uses channel replication to adapt single-channel chest X-rays for Vision Transformers, achieving state-of-the-art performance on TB and pneumonia detection across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are typically pretrained on 3-channel natural images but chest X-rays are grayscale (single-channel), creating an input format mismatch that hinders ViT performance on medical imaging tasks.

Method: Proposed RepViT-CXR with channel replication strategy that converts single-channel CXR images into ViT-compatible format without information loss, enabling direct use of pretrained Vision Transformers.

Result: Achieved 99.9% accuracy/AUC on TB-CXR dataset (surpassing Topo-CXR), 99.0% accuracy on Pediatric Pneumonia dataset (outperforming DCNN/VGG16), and 91.1% accuracy on Shenzhen TB dataset (beating CNN methods).

Conclusion: Simple channel replication strategy effectively bridges the input format gap, allowing ViTs to leverage their full representational power for grayscale medical imaging with state-of-the-art performance for clinical deployment.

Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic
tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.
Recent advances in deep learning, particularly Vision Transformers (ViTs), have
shown strong potential for automated medical image analysis. However, most ViT
architectures are pretrained on natural images and require three-channel
inputs, while CXR scans are inherently grayscale. To address this gap, we
propose RepViT-CXR, a channel replication strategy that adapts single-channel
CXR images into a ViT-compatible format without introducing additional
information loss. We evaluate RepViT-CXR on three benchmark datasets. On the
TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,
surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,
99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%
accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,
outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB
dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a
performance improvement over previously reported CNN-based methods. These
results demonstrate that a simple yet effective channel replication strategy
allows ViTs to fully leverage their representational power on grayscale medical
imaging tasks. RepViT-CXR establishes a new state of the art for TB and
pneumonia detection from chest X-rays, showing strong potential for deployment
in real-world clinical screening systems.

</details>


### [12] [Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI](https://arxiv.org/abs/2509.08243)
*Zheng Yang,Yanteng Zhang,Xupeng Kou,Yang Liu,Chao Ren*

Main category: cs.CV

TL;DR: A novel end-to-end network combining 3D CNN and Symmetry Interactive Transformer for Alzheimer's disease diagnosis, focusing on brain asymmetry caused by atrophy and achieving 92.5% accuracy on ADNI dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods either rely on pretraining or ignore the asymmetrical characteristics caused by brain disorders in Alzheimer's disease, limiting diagnostic performance.

Method: Proposed network with 3D CNN Encoder and Symmetry Interactive Transformer (SIT) that aligns left and right hemisphere features using inter-equal grid block fetch operation to focus on asymmetric regions caused by structural changes.

Result: Achieved 92.5% diagnostic accuracy on ADNI dataset, outperforming several CNN methods and CNN-transformer combinations. Visualization shows better focus on brain atrophy regions and asymmetric pathological characteristics.

Conclusion: The method demonstrates improved diagnostic performance and interpretability by effectively capturing asymmetric brain changes induced by Alzheimer's disease, providing a more effective approach for AD diagnosis using structural MRI.

Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has
achieved remarkable progress in the prediction and diagnosis of Alzheimer's
disease (AD). Existing studies have used CNN and transformer to build a
well-performing network, but most of them are based on pretraining or ignoring
the asymmetrical character caused by brain disorders. We propose an end-to-end
network for the detection of disease-based asymmetric induced by left and right
brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive
Transformer (SIT). Following the inter-equal grid block fetch operation, the
corresponding left and right hemisphere features are aligned and subsequently
fed into the SIT for diagnostic analysis. SIT can help the model focus more on
the regions of asymmetry caused by structural changes, thus improving
diagnostic performance. We evaluated our method based on the ADNI dataset, and
the results show that the method achieves better diagnostic accuracy (92.5\%)
compared to several CNN methods and CNNs combined with a general transformer.
The visualization results show that our network pays more attention in regions
of brain atrophy, especially for the asymmetric pathological characteristics
induced by AD, demonstrating the interpretability and effectiveness of the
method.

</details>


### [13] [EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning](https://arxiv.org/abs/2509.08260)
*Chi Zhang,Xiang Zhang,Chenxu Jiang,Gui-Song Xia,Lei Yu*

Main category: cs.CV

TL;DR: EVDI++ is a self-supervised framework that uses event cameras to deblur videos and interpolate frames, achieving state-of-the-art results without requiring ground truth data.


<details>
  <summary>Details</summary>
Motivation: Traditional frame-based cameras with long exposure times cause visual blurring and information loss between frames, degrading video quality. Event cameras offer high temporal resolution but need effective integration methods.

Method: Uses Learnable Double Integral (LDI) network to map reference frames to sharp images, learning-based division reconstruction for varying exposure intervals, and adaptive parameter-free fusion strategy with confidence weighting from event streams.

Result: Achieves state-of-the-art performance on both synthetic and real-world datasets for video deblurring and interpolation tasks, with demonstrated generalizability using a DAVIS346c camera dataset.

Conclusion: EVDI++ provides an effective unified self-supervised framework that leverages event camera data to significantly improve video quality through deblurring and frame interpolation without requiring labeled training data.

Abstract: Frame-based cameras with extended exposure times often produce perceptible
visual blurring and information loss between frames, significantly degrading
video quality. To address this challenge, we introduce EVDI++, a unified
self-supervised framework for Event-based Video Deblurring and Interpolation
that leverages the high temporal resolution of event cameras to mitigate motion
blur and enable intermediate frame prediction. Specifically, the Learnable
Double Integral (LDI) network is designed to estimate the mapping relation
between reference frames and sharp latent images. Then, we refine the coarse
results and optimize overall training efficiency by introducing a
learning-based division reconstruction module, enabling images to be converted
with varying exposure intervals. We devise an adaptive parameter-free fusion
strategy to obtain the final results, utilizing the confidence embedded in the
LDI outputs of concurrent events. A self-supervised learning framework is
proposed to enable network training with real-world blurry videos and events by
exploring the mutual constraints among blurry frames, latent images, and event
streams. We further construct a dataset with real-world blurry images and
events using a DAVIS346c camera, demonstrating the generalizability of the
proposed EVDI++ in real-world scenarios. Extensive experiments on both
synthetic and real-world datasets show that our method achieves
state-of-the-art performance in video deblurring and interpolation tasks.

</details>


### [14] [Hyperspectral Mamba for Hyperspectral Object Tracking](https://arxiv.org/abs/2509.08265)
*Long Gao,Yunhe Zhang,Yan Jiang,Weiying Xie,Yunsong Li*

Main category: cs.CV

TL;DR: HyMamba is a new hyperspectral object tracking network that uses state space modules to unify spectral, cross-depth, and temporal modeling, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing hyperspectral trackers fail to capture intrinsic spectral information, temporal dependencies, and cross-depth interactions, limiting their effectiveness in challenging scenarios.

Method: Proposes HyMamba with Spectral State Integration (SSI) module and Hyperspectral Mamba (HSM) module that use directional scanning state space models to learn spatial and spectral information synchronously, constructing joint features from false-color and hyperspectral inputs.

Result: Achieves 73.0% AUC score and 96.3% DP@20 score on HOTC2020 dataset, demonstrating state-of-the-art performance across seven benchmark datasets.

Conclusion: HyMamba effectively addresses limitations of existing hyperspectral trackers by unifying spectral, cross-depth, and temporal modeling through state space modules, providing superior object tracking performance.

Abstract: Hyperspectral object tracking holds great promise due to the rich spectral
information and fine-grained material distinctions in hyperspectral images,
which are beneficial in challenging scenarios. While existing hyperspectral
trackers have made progress by either transforming hyperspectral data into
false-color images or incorporating modality fusion strategies, they often fail
to capture the intrinsic spectral information, temporal dependencies, and
cross-depth interactions. To address these limitations, a new hyperspectral
object tracking network equipped with Mamba (HyMamba), is proposed. It unifies
spectral, cross-depth, and temporal modeling through state space modules
(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)
module, which enables progressive refinement and propagation of spectral
features with cross-depth and temporal spectral information. Embedded within
each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial
and spectral information synchronously via three directional scanning SSMs.
Based on SSI and HSM, HyMamba constructs joint features from false-color and
hyperspectral inputs, and enhances them through interaction with original
spectral features extracted from raw hyperspectral images. Extensive
experiments conducted on seven benchmark datasets demonstrate that HyMamba
achieves state-of-the-art performance. For instance, it achieves 73.0\% of the
AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will
be released at https://github.com/lgao001/HyMamba.

</details>


### [15] [Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features](https://arxiv.org/abs/2509.08266)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: VLMs rely on training biases and fail on specific visual questions. Minor image/prompt changes cause large performance variations.


<details>
  <summary>Details</summary>
Motivation: To systematically examine how input characteristics (image properties and prompt specificity) affect VLM performance and attention patterns.

Method: Developed a multi-dimensional examination framework using open-source VLMs, analyzing attention value fluctuations with varying input parameters like image size, object count, background color, and prompt specificity.

Result: Even minor modifications in image characteristics and prompt specificity lead to significant changes in how VLMs formulate answers and their overall performance.

Conclusion: VLMs are highly sensitive to input variations, and systematic analysis of attention patterns can help characterize and understand their performance limitations.

Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on
inherent biases learned during training to respond to questions about visual
properties of an image. These biases are exacerbated when VLMs are asked highly
specific questions that require focusing on specific areas of the image. For
example, a VLM tasked with counting stars on a modified American flag (e.g.,
with more than 50 stars) will often disregard the visual evidence and fail to
answer accurately. We build upon this research and develop a multi-dimensional
examination framework to systematically determine which characteristics of the
input data, including both the image and the accompanying prompt, lead to such
differences in performance. Using open-source VLMs, we further examine how
attention values fluctuate with varying input parameters (e.g., image size,
number of objects in the image, background color, prompt specificity). This
research aims to learn how the behavior of vision language models changes and
to explore methods for characterizing such changes. Our results suggest, among
other things, that even minor modifications in image characteristics and prompt
specificity can lead to large changes in how a VLM formulates its answer and,
subsequently, its overall performance.

</details>


### [16] [Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration](https://arxiv.org/abs/2509.08280)
*Hyeonseok Kim,Byeongkeun Kang,Yeejin Lee*

Main category: cs.CV

TL;DR: E3DPC-GZSL is a novel method for generalized zero-shot semantic segmentation of 3D point clouds that addresses biased predictions toward seen classes using evidence-based uncertainty estimation and dynamic calibration.


<details>
  <summary>Details</summary>
Motivation: Existing 3D point cloud segmentation models suffer from biased predictions favoring seen classes due to smaller training data scales compared to image tasks, requiring better handling of unseen class generalization.

Method: Integrates evidence-based uncertainty estimator into classifier, uses dynamic calibrated stacking factor for probability adjustment, and employs novel training strategy merging learnable parameters with text-derived features to refine semantic space.

Result: Achieves state-of-the-art performance on generalized zero-shot semantic segmentation datasets including ScanNet v2 and S3DIS.

Conclusion: The proposed approach effectively reduces overconfident predictions toward seen classes without separate classifiers, demonstrating superior performance in 3D zero-shot segmentation tasks.

Abstract: Generalized zero-shot semantic segmentation of 3D point clouds aims to
classify each point into both seen and unseen classes. A significant challenge
with these models is their tendency to make biased predictions, often favoring
the classes encountered during training. This problem is more pronounced in 3D
applications, where the scale of the training data is typically smaller than in
image-based tasks. To address this problem, we propose a novel method called
E3DPC-GZSL, which reduces overconfident predictions towards seen classes
without relying on separate classifiers for seen and unseen data. E3DPC-GZSL
tackles the overconfidence problem by integrating an evidence-based uncertainty
estimator into a classifier. This estimator is then used to adjust prediction
probabilities using a dynamic calibrated stacking factor that accounts for
pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel
training strategy that improves uncertainty estimation by refining the semantic
space. This is achieved by merging learnable parameters with text-derived
features, thereby improving model optimization for unseen data. Extensive
experiments demonstrate that the proposed approach achieves state-of-the-art
performance on generalized zero-shot semantic segmentation datasets, including
ScanNet v2 and S3DIS.

</details>


### [17] [Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection](https://arxiv.org/abs/2509.08289)
*Yuelin Guo,Haoyu He,Zhiyuan Chen,Zitong Huang,Renhao Lu,Lu Shi,Zejun Wang,Weizhe Zhang*

Main category: cs.CV

TL;DR: A novel weakly supervised object detection framework that addresses three key limitations in existing methods through heatmap-guided proposal selection, enhanced network architecture with background class representation, and negative certainty supervision for faster convergence.


<details>
  <summary>Details</summary>
Motivation: Existing WSOD methods suffer from three main problems: 1) pseudo GT boxes either focus only on discriminative parts or fail to distinguish adjacent intra-class instances, 2) WSDDN lacks background class representation and has semantic gap between branches, 3) discarded proposals during optimization lead to slow convergence.

Method: Proposes three main components: 1) Heatmap-Guided Proposal Selector (HGPS) with dual thresholds to generate better pseudo GT boxes, 2) Weakly Supervised Basic Detection Network (WSBDN) with background class representation and heatmap pre-supervision, 3) Negative certainty supervision loss on ignored proposals to accelerate convergence.

Result: Achieves state-of-the-art performance with mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and 55.6%/80.5% on VOC 2012, outperforming existing WSOD methods.

Conclusion: The proposed framework effectively addresses key limitations in WSOD through innovative heatmap guidance, enhanced network architecture, and improved optimization strategy, demonstrating superior performance on standard benchmarks.

Abstract: Weakly supervised object detection (WSOD) has attracted significant attention
in recent years, as it does not require box-level annotations. State-of-the-art
methods generally adopt a multi-module network, which employs WSDDN as the
multiple instance detection network module and multiple instance refinement
modules to refine performance. However, these approaches suffer from three key
limitations. First, existing methods tend to generate pseudo GT boxes that
either focus only on discriminative parts, failing to capture the whole object,
or cover the entire object but fail to distinguish between adjacent intra-class
instances. Second, the foundational WSDDN architecture lacks a crucial
background class representation for each proposal and exhibits a large semantic
gap between its branches. Third, prior methods discard ignored proposals during
optimization, leading to slow convergence. To address these challenges, we
first design a heatmap-guided proposal selector (HGPS) algorithm, which
utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo
GT boxes to both capture the full object extent and distinguish between
adjacent intra-class instances. We then present a weakly supervised basic
detection network (WSBDN), which augments each proposal with a background class
representation and uses heatmaps for pre-supervision to bridge the semantic gap
between matrices. At last, we introduce a negative certainty supervision loss
on ignored proposals to accelerate convergence. Extensive experiments on the
challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of
our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and
55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD
methods. Our code is publicly available at
https://github.com/gyl2565309278/DTH-CP.

</details>


### [18] [An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia](https://arxiv.org/abs/2509.08303)
*M. Warizmi Wafiq,Peter Cutter,Ate Poortinga,Daniel Marc G. dela Torre,Karis Tenneson,Vanna Teck,Enikoe Bihari,Chanarun Saisaward,Weraphong Suaruang,Andrea McMahon,Andi Vika Faradiba Muin,Karno B. Batiran,Chairil A,Nurul Qomar,Arya Arismaya Metananda,David Ganz,David Saah*

Main category: cs.CV

TL;DR: Open-access geospatial dataset of Indonesian oil palm plantations created through expert labeling of high-resolution satellite imagery (2020-2024) with polygon-based annotations and hierarchical typology for training remote sensing models.


<details>
  <summary>Details</summary>
Motivation: Oil palm cultivation is a leading cause of deforestation in Indonesia, requiring detailed mapping to support sustainability efforts and regulatory frameworks.

Method: Expert labeling of high-resolution satellite imagery through wall-to-wall digitization over large grids, with multi-interpreter consensus and field validation for quality assurance.

Result: Comprehensive dataset covering various agro-ecological zones with hierarchical typology distinguishing oil palm planting stages and similar perennial crops, suitable for training both conventional CNNs and geospatial foundation models.

Conclusion: The CC-BY licensed dataset fills a key gap in training data for remote sensing, aims to improve land cover mapping accuracy, and supports transparent monitoring of oil palm expansion to contribute to global deforestation reduction goals following FAIR data principles.

Abstract: Oil palm cultivation remains one of the leading causes of deforestation in
Indonesia. To better track and address this challenge, detailed and reliable
mapping is needed to support sustainability efforts and emerging regulatory
frameworks. We present an open-access geospatial dataset of oil palm
plantations and related land cover types in Indonesia, produced through expert
labeling of high-resolution satellite imagery from 2020 to 2024. The dataset
provides polygon-based, wall-to-wall annotations across a range of
agro-ecological zones and includes a hierarchical typology that distinguishes
oil palm planting stages as well as similar perennial crops. Quality was
ensured through multi-interpreter consensus and field validation. The dataset
was created using wall-to-wall digitization over large grids, making it
suitable for training and benchmarking both conventional convolutional neural
networks and newer geospatial foundation models. Released under a CC-BY
license, it fills a key gap in training data for remote sensing and aims to
improve the accuracy of land cover types mapping. By supporting transparent
monitoring of oil palm expansion, the resource contributes to global
deforestation reduction goals and follows FAIR data principles.

</details>


### [19] [SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training](https://arxiv.org/abs/2509.08311)
*Rongsheng Wang,Fenghe Tang,Qingsong Yao,Rui Yan,Xu Zhang,Zhen Huang,Haoran Lai,Zhiyang He,Xiaodong Tao,Zihang Jiang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: SimCroP is a vision-language pre-training framework for chest CT scans that uses similarity-driven alignment and cross-granularity fusion to improve lesion detection and radiograph interpretation in sparse medical images.


<details>
  <summary>Details</summary>
Motivation: CT scans have spatially sparse lesion distributions and complex implicit relationships between pathological descriptions and corresponding image sub-regions, making traditional pre-training methods less effective for this domain.

Method: Uses multi-modal masked modeling for low-level semantics, similarity-driven alignment to match report sentences with correct image patches, and cross-granularity fusion integrating instance-level and word-patch level information.

Result: Outperforms state-of-the-art medical self-supervised learning and vision-language pre-training methods on image classification and segmentation tasks across five public datasets.

Conclusion: SimCroP effectively addresses the challenges of sparse lesion distribution in CT scans and complex text-image relationships, demonstrating superior performance for multi-scale medical imaging tasks.

Abstract: Medical vision-language pre-training shows great potential in learning
representative features from massive paired radiographs and reports. However,
in computed tomography (CT) scans, the distribution of lesions which contain
intricate structures is characterized by spatial sparsity. Besides, the complex
and implicit relationships between different pathological descriptions in each
sentence of the report and their corresponding sub-regions in radiographs pose
additional challenges. In this paper, we propose a Similarity-Driven
Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines
similarity-driven alignment and cross-granularity fusion to improve radiograph
interpretation. We first leverage multi-modal masked modeling to optimize the
encoder for understanding precise low-level semantics from radiographs. Then,
similarity-driven alignment is designed to pre-train the encoder to adaptively
select and align the correct patches corresponding to each sentence in reports.
The cross-granularity fusion module integrates multimodal information across
instance level and word-patch level, which helps the model better capture key
pathology structures in sparse radiographs, resulting in improved performance
for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale
paired CT-reports dataset and validated on image classification and
segmentation tasks across five public datasets. Experimental results
demonstrate that SimCroP outperforms both cutting-edge medical self-supervised
learning methods and medical vision-language pre-training methods. Codes and
models are available at https://github.com/ToniChopp/SimCroP.

</details>


### [20] [Boosted Training of Lightweight Early Exits for Optimizing CNN Image Classification Inference](https://arxiv.org/abs/2509.08318)
*Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: BTS-EE is a sequential training method for early-exit CNNs that addresses covariance shift by aligning branch training with inference distributions, achieving up to 45% computation reduction with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Real-time image classification on resource-constrained platforms requires balancing accuracy with latency/power budgets. Conventional early-exit training suffers from covariance shift where downstream branches are trained on full datasets but only process harder samples at inference.

Method: Boosted Training Scheme for Early Exits (BTS-EE) sequentially trains and calibrates each branch before the next. Uses lightweight 1D convolution branch architecture and Class Precision Margin (CPM) calibration for per-class threshold tuning.

Result: On CINIC-10 dataset with ResNet18 backbone, BTS-EE outperforms non-boosted training across 64 configurations, achieving up to 45% computation reduction with only 2% accuracy degradation.

Conclusion: BTS-EE expands design space for deploying CNNs in real-time systems, offering practical efficiency gains for industrial inspection, embedded vision, and UAV applications.

Abstract: Real-time image classification on resource-constrained platforms demands
inference methods that balance accuracy with strict latency and power budgets.
Early-exit strategies address this need by attaching auxiliary classifiers to
intermediate layers of convolutional neural networks (CNNs), allowing "easy"
samples to terminate inference early. However, conventional training of early
exits introduces a covariance shift: downstream branches are trained on full
datasets, while at inference they process only the harder, non-exited samples.
This mismatch limits efficiency--accuracy trade-offs in practice. We introduce
the Boosted Training Scheme for Early Exits (BTS-EE), a sequential training
approach that aligns branch training with inference-time data distributions.
Each branch is trained and calibrated before the next, ensuring robustness
under selective inference conditions. To further support embedded deployment,
we propose a lightweight branch architecture based on 1D convolutions and a
Class Precision Margin (CPM) calibration method that enables per-class
threshold tuning for reliable exit decisions. Experiments on the CINIC-10
dataset with a ResNet18 backbone demonstrate that BTS-EE consistently
outperforms non-boosted training across 64 configurations, achieving up to 45
percent reduction in computation with only 2 percent accuracy degradation.
These results expand the design space for deploying CNNs in real-time image
processing systems, offering practical efficiency gains for applications such
as industrial inspection, embedded vision, and UAV-based monitoring.

</details>


### [21] [Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis](https://arxiv.org/abs/2509.08338)
*Jihyun Moon,Charmgil Hong*

Main category: cs.CV

TL;DR: Retrieval-augmented vision-language model for melanoma diagnosis that incorporates similar patient cases into prompts, improving accuracy without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: CNNs neglect clinical metadata and require extensive preprocessing, while VLMs trained on general data lack clinical specificity for melanoma diagnosis.

Method: Proposed a retrieval-augmented VLM framework that incorporates semantically similar patient cases into diagnostic prompts to enable informed predictions.

Result: Significantly improves classification accuracy and error correction over conventional baselines without requiring fine-tuning.

Conclusion: Retrieval-augmented prompting provides a robust strategy for clinical decision support in melanoma diagnosis.

Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving
patient outcomes. While convolutional neural networks (CNNs) have shown promise
in dermoscopic image analysis, they often neglect clinical metadata and require
extensive preprocessing. Vision-language models (VLMs) offer a multimodal
alternative but struggle to capture clinical specificity when trained on
general-domain data. To address this, we propose a retrieval-augmented VLM
framework that incorporates semantically similar patient cases into the
diagnostic prompt. Our method enables informed predictions without fine-tuning
and significantly improves classification accuracy and error correction over
conventional baselines. These results demonstrate that retrieval-augmented
prompting provides a robust strategy for clinical decision support.

</details>


### [22] [InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2509.08374)
*Zhongyu Xia,Hansong Yang,Yongtao Wang*

Main category: cs.CV

TL;DR: InsFusion is a 3D object detection method that extracts proposals from both raw and fused features to mitigate accumulated errors in multi-view camera and LiDAR fusion pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the problem of noise and error accumulation that occurs during basic feature extraction, perspective transformation, and feature fusion in 3D object detection from multi-view cameras and LiDAR.

Method: Extracts proposals from both raw and fused features, uses these proposals to query raw features, and incorporates attention mechanisms applied to raw features to reduce accumulated errors.

Result: Achieves new state-of-the-art performance on nuScenes dataset and demonstrates compatibility with various advanced baseline methods.

Conclusion: InsFusion effectively mitigates error accumulation in 3D object detection pipelines and delivers superior performance for autonomous driving applications.

Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a
crucial component for autonomous driving and smart transportation. However, in
the process of basic feature extraction, perspective transformation, and
feature fusion, noise and error will gradually accumulate. To address this
issue, we propose InsFusion, which can extract proposals from both raw and
fused features and utilizes these proposals to query the raw features, thereby
mitigating the impact of accumulated errors. Additionally, by incorporating
attention mechanisms applied to the raw features, it thereby mitigates the
impact of accumulated errors. Experiments on the nuScenes dataset demonstrate
that InsFusion is compatible with various advanced baseline methods and
delivers new state-of-the-art performance for 3D object detection.

</details>


### [23] [Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video](https://arxiv.org/abs/2509.08376)
*Xiao Li,Qi Chen,Xiulian Peng,Kai Yu,Xie Chen,Yan Lu*

Main category: cs.CV

TL;DR: A self-supervised framework that disentangles videos into motion and content components using transformer architecture and vector quantization, enabling motion transfer and generation tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a general video representation learning method that separates dynamic motion from static content with fewer assumptions and inductive biases than previous approaches.

Method: Transformer-based architecture with joint generation of implicit motion and content features, low-bitrate vector quantization as information bottleneck, and denoising diffusion model for self-supervised learning.

Result: Validated on talking head videos for motion transfer and auto-regressive generation, and shown to generalize to 2D cartoon character videos.

Conclusion: Presents a novel perspective for self-supervised learning of disentangled video representations, contributing to video analysis and generation.

Abstract: We propose a novel and general framework to disentangle video data into its
dynamic motion and static content components. Our proposed method is a
self-supervised pipeline with less assumptions and inductive biases than
previous works: it utilizes a transformer-based architecture to jointly
generate flexible implicit features for frame-wise motion and clip-wise
content, and incorporates a low-bitrate vector quantization as an information
bottleneck to promote disentanglement and form a meaningful discrete motion
space. The bitrate-controlled latent motion and content are used as conditional
inputs to a denoising diffusion model to facilitate self-supervised
representation learning. We validate our disentangled representation learning
framework on real-world talking head videos with motion transfer and
auto-regressive motion generation tasks. Furthermore, we also show that our
method can generalize to other types of video data, such as pixel sprites of 2D
cartoon characters. Our work presents a new perspective on self-supervised
learning of disentangled video representations, contributing to the broader
field of video analysis and generation.

</details>


### [24] [Semantic Causality-Aware Vision-Based 3D Occupancy Prediction](https://arxiv.org/abs/2509.08388)
*Dubing Chen,Huan Zheng,Yucheng Zhou,Xianfei Li,Wenlong Liao,Tao He,Pai Peng,Jianbing Shen*

Main category: cs.CV

TL;DR: Proposes a novel causal loss for end-to-end supervision of 2D-to-3D semantic occupancy prediction, enabling differentiable pipeline and improving semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Existing modular pipelines for 3D semantic occupancy prediction suffer from cascading errors due to independent optimization of modules or pre-configured inputs.

Method: Designs a causal loss based on 2D-to-3D semantic causality principle that regulates gradient flow from 3D voxels to 2D features. Proposes three components: Channel-Grouped Lifting, Learnable Camera Offsets, and Normalized Convolution.

Result: Achieves state-of-the-art performance on Occ3D benchmark with significant robustness to camera perturbations and improved 2D-to-3D semantic consistency.

Conclusion: The causal loss enables holistic end-to-end supervision, making previously non-trainable components learnable and unifying the learning process for better 3D semantic occupancy prediction.

Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision
that integrates volumetric 3D reconstruction with semantic understanding.
Existing methods, however, often rely on modular pipelines. These modules are
typically optimized independently or use pre-configured inputs, leading to
cascading errors. In this paper, we address this limitation by designing a
novel causal loss that enables holistic, end-to-end supervision of the modular
2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D
semantic causality, this loss regulates the gradient flow from 3D voxel
representations back to the 2D features. Consequently, it renders the entire
pipeline differentiable, unifying the learning process and making previously
non-trainable components fully learnable. Building on this principle, we
propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises
three components guided by our causal loss: Channel-Grouped Lifting for
adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness
against camera perturbations, and Normalized Convolution for effective feature
propagation. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on the Occ3D benchmark, demonstrating significant
robustness to camera perturbations and improved 2D-to-3D semantic consistency.

</details>


### [25] [VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring](https://arxiv.org/abs/2509.08392)
*Cuong Nguyen,Dung T. Tran,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.CV

TL;DR: Proposes Vertical Residual Autoencoder (VRAE) for enhancing degraded vehicle images in traffic surveillance, achieving significant improvements over existing methods with minimal parameter increase.


<details>
  <summary>Details</summary>
Motivation: Vehicle images in traffic surveillance often suffer from noise and blur due to adverse weather, poor lighting, or high-speed motion, which reduces license plate recognition accuracy, especially when plates occupy small regions.

Method: VRAE architecture with an enhancement strategy using auxiliary blocks that inject input-aware features at each encoding stage to guide representation learning and preserve general information better than conventional autoencoders.

Result: Outperforms Autoencoder, GAN, and Flow-Based approaches - improves PSNR by ~20%, reduces NMSE by ~50%, enhances SSIM by 1%, with only ~1% parameter increase compared to AE at same depth.

Conclusion: VRAE effectively enhances degraded vehicle images for license plate recognition in real-time traffic surveillance applications, demonstrating superior performance with minimal computational overhead.

Abstract: In real-world traffic surveillance, vehicle images captured under adverse
weather, poor lighting, or high-speed motion often suffer from severe noise and
blur. Such degradations significantly reduce the accuracy of license plate
recognition systems, especially when the plate occupies only a small region
within the full vehicle image. Restoring these degraded images a fast realtime
manner is thus a crucial pre-processing step to enhance recognition
performance. In this work, we propose a Vertical Residual Autoencoder (VRAE)
architecture designed for the image enhancement task in traffic surveillance.
The method incorporates an enhancement strategy that employs an auxiliary
block, which injects input-aware features at each encoding stage to guide the
representation learning process, enabling better general information
preservation throughout the network compared to conventional autoencoders.
Experiments on a vehicle image dataset with visible license plates demonstrate
that our method consistently outperforms Autoencoder (AE), Generative
Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at
the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,
and enhances SSIM by 1\%, while requiring only a marginal increase of roughly
1\% in parameters.

</details>


### [26] [Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking](https://arxiv.org/abs/2509.08421)
*Keisuke Toida,Taigo Sakai,Naoki Kato,Kazutoyo Yokota,Takeshi Nakamura,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: SCFusion is a novel multi-view multi-object tracking framework that addresses feature distortion and non-uniform density issues in BEV projection through sparse transformation, density-aware weighting, and multi-view consistency loss.


<details>
  <summary>Details</summary>
Motivation: Maintaining consistent object identities across multiple cameras is challenging due to viewpoint changes, lighting variations, and occlusions. Conventional BEV projection methods suffer from feature distortion and non-uniform density caused by object scale variations with distance.

Method: Three key techniques: 1) Sparse transformation to avoid unnatural interpolation during projection, 2) Density-aware weighting for adaptive feature fusion based on spatial confidence and camera distance, 3) Multi-view consistency loss to encourage discriminative feature learning before fusion.

Result: State-of-the-art performance with 95.9% IDF1 score on WildTrack and 89.2% MODP on MultiviewX, outperforming baseline TrackTacular method.

Conclusion: SCFusion effectively mitigates limitations of conventional BEV projection and provides a robust, accurate solution for multi-view object detection and tracking.

Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such
as surveillance, autonomous driving, and sports analytics. However, maintaining
consistent object identities across multiple cameras remains challenging due to
viewpoint changes, lighting variations, and occlusions, which often lead to
tracking errors.Recent methods project features from multiple cameras into a
unified Bird's-Eye-View (BEV) space to improve robustness against occlusion.
However, this projection introduces feature distortion and non-uniform density
caused by variations in object scale with distance. These issues degrade the
quality of the fused representation and reduce detection and tracking
accuracy.To address these problems, we propose SCFusion, a framework that
combines three techniques to improve multi-view feature integration. First, it
applies a sparse transformation to avoid unnatural interpolation during
projection. Next, it performs density-aware weighting to adaptively fuse
features based on spatial confidence and camera distance. Finally, it
introduces a multi-view consistency loss that encourages each camera to learn
discriminative features independently before fusion.Experiments show that
SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9%
on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline
method TrackTacular. These results demonstrate that SCFusion effectively
mitigates the limitations of conventional BEV projection and provides a robust
and accurate solution for multi-view object detection and tracking.

</details>


### [27] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: LD-ViCE is a novel framework that uses latent diffusion models to generate realistic, temporally coherent counterfactual explanations for video-based AI systems, significantly improving computational efficiency and explanation quality compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Video-based AI systems in safety-critical domains like autonomous driving and healthcare need interpretable decisions, but current explanation techniques lack temporal coherence, robustness, and actionable causal insights. Existing counterfactual methods don't incorporate model guidance, reducing semantic fidelity.

Method: LD-ViCE operates in latent space using a state-of-the-art diffusion model to reduce computational costs, with an additional refinement step to produce realistic and interpretable counterfactuals. It incorporates guidance from the target model for better semantic fidelity.

Result: LD-ViCE outperforms state-of-the-art methods with up to 68% increase in R2 score while reducing inference time by half. It demonstrates effectiveness across three diverse video datasets (EchoNet-Dynamic, FERV39k, Something-Something V2) and generates semantically meaningful, temporally coherent explanations.

Conclusion: LD-ViCE represents a significant advancement toward trustworthy AI deployment in safety-critical domains by providing high-quality, efficient counterfactual explanations that offer valuable insights into model behavior.

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.

</details>


### [28] [Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time](https://arxiv.org/abs/2509.08436)
*Xia Yue,Anfeng Liu,Ning Chen,Chenjia Huang,Hui Liu,Zhou Huang,Leyuan Fang*

Main category: cs.CV

TL;DR: HyperTTA is a unified framework that enhances hyperspectral image classification robustness against various real-world degradations through a multi-degradation dataset, spectral-spatial transformer classifier, and lightweight test-time adaptation strategy.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral image classification models are highly sensitive to distribution shifts caused by real-world degradations like noise, blur, compression, and atmospheric effects, requiring improved robustness.

Method: Constructed multi-degradation hyperspectral dataset with 9 degradation types; designed spectral-spatial transformer classifier with multi-level receptive field and label smoothing; implemented confidence-aware entropy-minimized LayerNorm adapter for test-time adaptation.

Result: Extensive experiments on two benchmark datasets show HyperTTA outperforms existing baselines across various degradation scenarios, validating both the classification backbone and TTA scheme effectiveness.

Conclusion: HyperTTA provides a comprehensive solution for robust hyperspectral image classification under diverse degradation conditions through dataset construction, improved classifier design, and lightweight test-time adaptation without requiring source data or target annotations.

Abstract: Hyperspectral image (HSI) classification models are highly sensitive to
distribution shifts caused by various real-world degradations such as noise,
blur, compression, and atmospheric effects. To address this challenge, we
propose HyperTTA, a unified framework designed to enhance model robustness
under diverse degradation conditions. Specifically, we first construct a
multi-degradation hyperspectral dataset that systematically simulates nine
representative types of degradations, providing a comprehensive benchmark for
robust classification evaluation. Based on this, we design a spectral-spatial
transformer classifier (SSTC) enhanced with a multi-level receptive field
mechanism and label smoothing regularization to jointly capture multi-scale
spatial context and improve generalization. Furthermore, HyperTTA incorporates
a lightweight test-time adaptation (TTA) strategy, the confidence-aware
entropy-minimized LayerNorm adapter (CELA), which updates only the affine
parameters of LayerNorm layers by minimizing prediction entropy on
high-confidence unlabeled target samples. This confidence-aware adaptation
prevents unreliable updates from noisy predictions, enabling robust and dynamic
adaptation without access to source data or target annotations. Extensive
experiments on two benchmark datasets demonstrate that HyperTTA outperforms
existing baselines across a wide range of degradation scenarios, validating the
effectiveness of both its classification backbone and the proposed TTA scheme.
Code will be made available publicly.

</details>


### [29] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: SBDM is a novel diffusion model for forecasting individualized cortical thickness trajectories using spherical Brownian bridge diffusion and conditional spherical U-Net, achieving superior accuracy over previous methods.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting of individualized cortical thickness trajectories is crucial for detecting subtle neurodegenerative changes and enabling earlier interventions, but challenging due to the cortex's non-Euclidean geometry and need for multi-modal data integration.

Method: Proposed Spherical Brownian Bridge Diffusion Model (SBDM) with bidirectional conditional Brownian bridge diffusion process and conditional spherical U-Net (CoS-UNet) that combines spherical convolutions with dense cross-attention to integrate cortical surfaces and tabular data.

Result: SBDM achieves significantly reduced prediction errors compared to previous approaches, as validated on ADNI and OASIS longitudinal datasets, and can generate both factual and counterfactual cortical thickness trajectories.

Conclusion: SBDM provides a novel framework for accurate cortical thickness forecasting and exploring hypothetical cortical development scenarios, offering valuable insights for neurodegenerative disease monitoring and intervention strategies.

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness
(CTh) trajectories is essential for detecting subtle cortical changes,
providing invaluable insights into neurodegenerative processes and facilitating
earlier and more precise intervention strategies. However, CTh forecasting is a
challenging task due to the intricate non-Euclidean geometry of the cerebral
cortex and the need to integrate multi-modal data for subject-specific
predictions. To address these challenges, we introduce the Spherical Brownian
Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional
conditional Brownian bridge diffusion process to forecast CTh trajectories at
the vertex level of registered cortical surfaces. Our technical contribution
includes a new denoising model, the conditional spherical U-Net (CoS-UNet),
which combines spherical convolutions and dense cross-attention to integrate
cortical surfaces and tabular conditions seamlessly. Compared to previous
approaches, SBDM achieves significantly reduced prediction errors, as
demonstrated by our experiments based on longitudinal datasets from the ADNI
and OASIS. Additionally, we demonstrate SBDM's ability to generate individual
factual and counterfactual CTh trajectories, offering a novel framework for
exploring hypothetical scenarios of cortical development.

</details>


### [30] [First-order State Space Model for Lightweight Image Super-resolution](https://arxiv.org/abs/2509.08458)
*Yujie Zhu,Xinyi Zhang,Yekai Lu,Guang Yang,Faming Fang,Guixu Zhang*

Main category: cs.CV

TL;DR: FSSM improves Mamba-based vision models for super-resolution by modifying SSM calculation with first-order hold condition, enhancing performance without extra parameters.


<details>
  <summary>Details</summary>
Motivation: Most Mamba-based vision models focus on architecture and scan paths but neglect SSM module optimization. The paper aims to explore SSM potential for lightweight super-resolution tasks.

Method: Introduces First-order State Space Model (FSSM) that applies first-order hold condition in SSMs, derives new discretized form, and analyzes cumulative error to incorporate token correlations.

Result: FSSM improves MambaIR performance on five benchmark datasets without additional parameters, surpassing current lightweight SR methods and achieving state-of-the-art results.

Conclusion: The proposed FSSM successfully enhances SSM module performance for vision tasks, demonstrating that SSM optimization can significantly improve lightweight super-resolution models.

Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP
tasks and are increasingly applied to vision tasks. However, most Mamba-based
vision models focus on network architecture and scan paths, with little
attention to the SSM module. In order to explore the potential of SSMs, we
modified the calculation process of SSM without increasing the number of
parameters to improve the performance on lightweight super-resolution tasks. In
this paper, we introduce the First-order State Space Model (FSSM) to improve
the original Mamba module, enhancing performance by incorporating token
correlations. We apply a first-order hold condition in SSMs, derive the new
discretized form, and analyzed cumulative error. Extensive experimental results
demonstrate that FSSM improves the performance of MambaIR on five benchmark
datasets without additionally increasing the number of parameters, and
surpasses current lightweight SR methods, achieving state-of-the-art results.

</details>


### [31] [Maximally Useful and Minimally Redundant: The Key to Self Supervised Learning for Imbalanced Data](https://arxiv.org/abs/2509.08469)
*Yash Kumar Sharma,Vineet Nair,Wilson Naik*

Main category: cs.CV

TL;DR: Proposes a multi-view contrastive self-supervised learning approach using mutual information theory to handle imbalanced datasets, achieving state-of-the-art results on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Contrastive self-supervised learning performs poorly on imbalanced datasets, and recent insights suggest extending beyond two views could improve performance for tail classes.

Method: Theoretical justification using mutual information for more than two views, with a loss function that segregates intra and inter discriminatory characteristics and filters extreme features.

Result: Achieved 2% improvement on Cifar10-LT, 5% on Cifar100-LT, and 3% on Imagenet-LT using ResNet architectures, setting new state-of-the-art accuracy.

Conclusion: The multi-view objective effectively addresses dataset imbalance in self-supervised learning by extracting better representations for tail classes through improved feature discrimination.

Abstract: The robustness of contrastive self-supervised learning (CSSL) for imbalanced
datasets is largely unexplored. CSSL usually makes use of \emph{multi-view}
assumptions to learn discriminatory features via similar and dissimilar data
samples. CSSL works well on balanced datasets, but does not generalize well for
imbalanced datasets. In a very recent paper, as part of future work, Yann LeCun
pointed out that the self-supervised multiview framework can be extended to
cases involving \emph{more than two views}. Taking a cue from this insight we
propose a theoretical justification based on the concept of \emph{mutual
information} to support the \emph{more than two views} objective and apply it
to the problem of dataset imbalance in self-supervised learning. The proposed
method helps extract representative characteristics of the tail classes by
segregating between \emph{intra} and \emph{inter} discriminatory
characteristics. We introduce a loss function that helps us to learn better
representations by filtering out extreme features. Experimental evaluation on a
variety of self-supervised frameworks (both contrastive and non-contrastive)
also prove that the \emph{more than two view} objective works well for
imbalanced datasets. We achieve a new state-of-the-art accuracy in
self-supervised imbalanced dataset classification (2\% improvement in
Cifar10-LT using Resnet-18, 5\% improvement in Cifar100-LT using Resnet-18, 3\%
improvement in Imagenet-LT (1k) using Resnet-50).

</details>


### [32] [Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation](https://arxiv.org/abs/2509.08489)
*Kaleem Ahmad*

Main category: cs.CV

TL;DR: A unified pipeline for prompt-driven image analysis that combines detection, segmentation, inpainting, and description into a single workflow from a single natural language prompt.


<details>
  <summary>Details</summary>
Motivation: To create a practical, end-to-end system that converts natural language instructions into multiple image analysis steps with transparent debugging and consistent functionality across UI and CLI interfaces.

Method: Combines open-vocabulary detection, promptable segmentation, text-conditioned inpainting, and vision-language description in a unified workflow with integration choices to reduce brittleness (threshold adjustments, mask inspection, resource-aware defaults).

Result: Achieved over 90% usable mask production with accuracy above 85% in single-word prompt segmentation, identified inpainting as the main runtime bottleneck (60-75% of total time), and developed reliable operational practices.

Conclusion: Provides a transparent, reliable pattern for assembling modern vision models with clear guardrails and operational practices that improve reliability in object replacement, scene augmentation, and removal tasks.

Abstract: Prompt-driven image analysis converts a single natural-language instruction
into multiple steps: locate, segment, edit, and describe. We present a
practical case study of a unified pipeline that combines open-vocabulary
detection, promptable segmentation, text-conditioned inpainting, and
vision-language description into a single workflow. The system works end to end
from a single prompt, retains intermediate artifacts for transparent debugging
(such as detections, masks, overlays, edited images, and before and after
composites), and provides the same functionality through an interactive UI and
a scriptable CLI for consistent, repeatable runs. We highlight integration
choices that reduce brittleness, including threshold adjustments, mask
inspection with light morphology, and resource-aware defaults. In a small,
single-word prompt segment, detection and segmentation produced usable masks in
over 90% of cases with an accuracy above 85% based on our criteria. On a
high-end GPU, inpainting makes up 60 to 75% of total runtime under typical
guidance and sampling settings, which highlights the need for careful tuning.
The study offers implementation-guided advice on thresholds, mask tightness,
and diffusion parameters, and details version pinning, artifact logging, and
seed control to support replay. Our contribution is a transparent, reliable
pattern for assembling modern vision and multimodal models behind a single
prompt, with clear guardrails and operational practices that improve
reliability in object replacement, scene augmentation, and removal.

</details>


### [33] [A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models](https://arxiv.org/abs/2509.08490)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Yi Fang,Zhou Ni*

Main category: cs.CV

TL;DR: This review paper systematically categorizes underwater object detection challenges and explores the potential of large vision-language models to address issues like image degradation, small object detection, and dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection faces numerous performance-compromising challenges that existing methods fail to fully address, particularly in complex underwater environments with image quality degradation and dynamic conditions.

Method: The paper systematically categorizes UOD challenges into five areas, analyzes progression from traditional to modern techniques, explores LVLMs' potential, and presents case studies including synthetic dataset generation with DALL-E 3 and fine-tuning Florence-2 LVLM.

Result: Three key insights: current UOD methods are insufficient for challenges like image degradation and small object detection; synthetic data generation shows potential but needs refinement; LVLMs hold promise but real-time applications require further optimization research.

Conclusion: Large vision-language models represent a promising direction for advancing underwater object detection, though significant research is still needed to address real-time application challenges and ensure synthetic data realism for practical deployment.

Abstract: Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.

</details>


### [34] [Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening](https://arxiv.org/abs/2509.08502)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: Developing time-sensitive video representations that can distinguish temporally opposite actions (chiral pairs) through self-supervised adaptation of frozen image features.


<details>
  <summary>Details</summary>
Motivation: Current video embeddings poorly represent simple visual changes over time needed to distinguish chiral actions like opening/closing doors, which occur frequently in everyday life.

Method: Self-supervised adaptation recipe using auto-encoder with latent space inspired by perceptual straightening to inject time-sensitivity into frozen image feature sequences.

Result: Outperforms larger video models pre-trained on large-scale datasets and improves classification performance on standard benchmarks when combined with existing models.

Conclusion: The proposed method successfully creates compact, time-sensitive video representations that effectively distinguish chiral action pairs across multiple datasets.

Abstract: Our objective is to develop compact video representations that are sensitive
to visual change over time. To measure such time-sensitivity, we introduce a
new task: chiral action recognition, where one needs to distinguish between a
pair of temporally opposite actions, such as "opening vs. closing a door",
"approaching vs. moving away from something", "folding vs. unfolding paper",
etc. Such actions (i) occur frequently in everyday life, (ii) require
understanding of simple visual change over time (in object state, size, spatial
position, count . . . ), and (iii) are known to be poorly represented by many
video embeddings. Our goal is to build time aware video representations which
offer linear separability between these chiral pairs. To that end, we propose a
self-supervised adaptation recipe to inject time-sensitivity into a sequence of
frozen image features. Our model is based on an auto-encoder with a latent
space with inductive bias inspired by perceptual straightening. We show that
this results in a compact but time-sensitive video representation for the
proposed task across three datasets: Something-Something, EPIC-Kitchens, and
Charade. Our method (i) outperforms much larger video models pre-trained on
large-scale video datasets, and (ii) leads to an improvement in classification
performance on standard benchmarks when combined with these existing models.

</details>


### [35] [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)
*Liyang Chen,Tianxiang Ma,Jiawei Liu,Bingchuan Li,Zhuowei Chen,Lijie Liu,Xu He,Gen Li,Qian He,Zhiyong Wu*

Main category: cs.CV

TL;DR: HuMo presents a unified Human-Centric Video Generation framework that addresses multimodal coordination challenges through a two-stage training approach with specialized strategies for subject preservation and audio-visual synchronization.


<details>
  <summary>Details</summary>
Motivation: Existing HCVG methods struggle to effectively coordinate heterogeneous modalities (text, image, audio) due to scarce paired triplet training data and difficulties in collaborating subject preservation with audio-visual sync tasks.

Method: Two-stage progressive multimodal training: 1) Minimal-invasive image injection for subject preservation, 2) Focus-by-predicting strategy for audio-visual sync with audio cross-attention, plus time-adaptive Classifier-Free Guidance for flexible control during inference.

Result: HuMo surpasses specialized state-of-the-art methods in sub-tasks and establishes a unified framework for collaborative multimodal-conditioned human video generation.

Conclusion: The proposed framework successfully addresses multimodal coordination challenges through progressive training and specialized strategies, enabling effective collaborative control across text, image, and audio inputs for human-centric video generation.

Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos
from multimodal inputs, including text, image, and audio. Existing methods
struggle to effectively coordinate these heterogeneous modalities due to two
challenges: the scarcity of training data with paired triplet conditions and
the difficulty of collaborating the sub-tasks of subject preservation and
audio-visual sync with multimodal inputs. In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control. For the first
challenge, we construct a high-quality dataset with diverse and paired text,
reference images, and audio. For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies. For the
subject preservation task, to maintain the prompt following and visual
generation abilities of the foundation model, we adopt the minimal-invasive
image injection strategy. For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions. For
joint learning of controllabilities across multimodal inputs, building on
previously acquired capabilities, we progressively incorporate the audio-visual
sync task. During inference, for flexible and fine-grained multimodal control,
we design a time-adaptive Classifier-Free Guidance strategy that dynamically
adjusts guidance weights across denoising steps. Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG. Project Page:
https://phantom-video.github.io/HuMo.

</details>


### [36] [MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models](https://arxiv.org/abs/2509.08538)
*Garry Yang,Zizhe Chen,Man Hon Wong,Haoyu Lei,Yongqiang Chen,Zhenguo Li,Kaiwen Zhou,James Cheng*

Main category: cs.CV

TL;DR: MESH is a new benchmark for evaluating hallucinations in Large Video Models (LVMs) that uses a QA framework with binary and multi-choice formats to systematically assess video understanding capabilities.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for video hallucination rely heavily on manual categorization and neglect human perception-based processes, making them inadequate for evaluating how LVMs naturally interpret videos.

Method: MESH uses a Question-Answering framework with target and trap instances, following a bottom-up approach that evaluates basic objects, coarse-to-fine subject features, and subject-action pairs to align with human video understanding.

Result: Evaluations show LVMs excel at recognizing basic objects and features but become increasingly susceptible to hallucinations when handling fine details or aligning multiple actions involving various subjects in longer videos.

Conclusion: MESH provides an effective and comprehensive approach for systematically identifying hallucinations in LVMs, addressing limitations of current benchmarks by incorporating human-like perception processes.

Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large
Language Models (LLMs) and vision modules by integrating temporal information
to better understand dynamic video content. Despite their progress, LVMs are
prone to hallucinations-producing inaccurate or irrelevant descriptions.
Current benchmarks for video hallucination depend heavily on manual
categorization of video content, neglecting the perception-based processes
through which humans naturally interpret videos. We introduce MESH, a benchmark
designed to evaluate hallucinations in LVMs systematically. MESH uses a
Question-Answering framework with binary and multi-choice formats incorporating
target and trap instances. It follows a bottom-up approach, evaluating basic
objects, coarse-to-fine subject features, and subject-action pairs, aligning
with human video understanding. We demonstrate that MESH offers an effective
and comprehensive approach for identifying hallucinations in videos. Our
evaluations show that while LVMs excel at recognizing basic objects and
features, their susceptibility to hallucinations increases markedly when
handling fine details or aligning multiple actions involving various subjects
in longer videos.

</details>


### [37] [ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping](https://arxiv.org/abs/2509.08550)
*Robin-Nico Kampa,Fabian Deuser,Konrad Habel,Norbert Oswald*

Main category: cs.CV

TL;DR: ViewSparsifier approach won both plant age prediction and leaf count estimation tasks in the GroMo Grand Challenge by using multi-view embeddings with random view selection to handle redundancy and capture view-invariant features.


<details>
  <summary>Details</summary>
Motivation: Single-view classification/regression models often fail to capture all information needed for accurate plant phenotypic trait estimation, which affects plant health assessment and harvest readiness prediction.

Method: Used multi-view dataset with plants photographed from multiple heights and angles. Incorporated 24 views (selection vector) in random selection to learn view-invariant embeddings. Also experimented with randomized view selection across all five height levels (120 views total) using selection matrices.

Result: The ViewSparsifier approach won both tasks (Plant Age Prediction and Leaf Count Estimation) in the ACM Multimedia 2025 GroMo Grand Challenge.

Conclusion: Multi-view approach with view-invariant embeddings effectively handles information redundancy and improves phenotypic trait estimation, with potential for further improvement through expanded view selection matrices.

Abstract: Plant phenotyping involves analyzing observable characteristics of plants to
better understand their growth, health, and development. In the context of deep
learning, this analysis is often approached through single-view classification
or regression models. However, these methods often fail to capture all
information required for accurate estimation of target phenotypic traits, which
can adversely affect plant health assessment and harvest readiness prediction.
To address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia
2025 provides a multi-view dataset featuring multiple plants and two tasks:
Plant Age Prediction and Leaf Count Estimation. Each plant is photographed from
multiple heights and angles, leading to significant overlap and redundancy in
the captured information. To learn view-invariant embeddings, we incorporate 24
views, referred to as the selection vector, in a random selection. Our
ViewSparsifier approach won both tasks. For further improvement and as a
direction for future research, we also experimented with randomized view
selection across all five height levels (120 views total), referred to as
selection matrices.

</details>


### [38] [Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2509.08570)
*Wenjun Yu,Yinchen Zhou,Jia-Xuan Jiang,Shubin Zeng,Yuee Li,Zhong Wang*

Main category: cs.CV

TL;DR: Proposes EM Aggregation and Text-Guided Pixel Decoder to bridge semantic gap in medical image segmentation, outperforming SOTA methods on cardiac and fundus datasets.


<details>
  <summary>Details</summary>
Motivation: Multimodal models underperform in medical domain due to semantic gap between abstract text prompts and fine-grained medical visual features, causing feature dispersion issues.

Method: Expectation-Maximization (EM) Aggregation mechanism to cluster features into semantic centers, and Text-Guided Pixel Decoder to leverage domain-invariant textual knowledge for guiding visual representations.

Result: Extensive experiments show consistent outperformance over existing SOTA approaches across multiple domain generalization benchmarks on cardiac and fundus datasets.

Conclusion: The synergy between EM Aggregation and Text-Guided Pixel Decoder significantly improves generalization ability for medical image segmentation by addressing semantic gap and feature dispersion challenges.

Abstract: Multimodal models have achieved remarkable success in natural image
segmentation, yet they often underperform when applied to the medical domain.
Through extensive study, we attribute this performance gap to the challenges of
multimodal fusion, primarily the significant semantic gap between abstract
textual prompts and fine-grained medical visual features, as well as the
resulting feature dispersion. To address these issues, we revisit the problem
from the perspective of semantic aggregation. Specifically, we propose an
Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel
Decoder. The former mitigates feature dispersion by dynamically clustering
features into compact semantic centers to enhance cross-modal correspondence.
The latter is designed to bridge the semantic gap by leveraging
domain-invariant textual knowledge to effectively guide deep visual
representations. The synergy between these two mechanisms significantly
improves the model's generalization ability. Extensive experiments on public
cardiac and fundus datasets demonstrate that our method consistently
outperforms existing SOTA approaches across multiple domain generalization
benchmarks.

</details>


### [39] [Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data](https://arxiv.org/abs/2509.08571)
*Bayu Adhi Tama,Homayra Alam,Mostafa Cham,Omar Faruque,Jianwu Wang,Vandana Janeja*

Main category: cs.CV

TL;DR: GraphTopoNet is a graph-learning framework that fuses heterogeneous data and models uncertainty to create accurate subglacial bed maps of Greenland, reducing error by up to 60% compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate maps of Greenland's subglacial bed are crucial for sea-level projections, but current radar observations are sparse and unevenly distributed, limiting their reliability for climate forecasting.

Method: Uses graph-learning framework with spatial graphs built from surface observables (elevation, velocity, mass balance), augmented with gradient features and polynomial trends. Employs Monte Carlo dropout for uncertainty modeling and hybrid loss combining confidence-weighted radar supervision with dynamically balanced regularization.

Result: Outperforms interpolation, convolutional, and graph-based baselines in three Greenland subregions, reducing error by up to 60% while preserving fine-scale glacial features.

Conclusion: GraphTopoNet demonstrates how graph machine learning can convert sparse, uncertain geophysical observations into actionable knowledge at continental scale, improving reliability for operational climate modeling and policy support.

Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level
projections, but radar observations are sparse and uneven. We introduce
GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision
and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built
from surface observables (elevation, velocity, mass balance) are augmented with
gradient features and polynomial trends to capture both local variability and
broad structure. To handle data gaps, we employ a hybrid loss that combines
confidence-weighted radar supervision with dynamically balanced regularization.
Applied to three Greenland subregions, GraphTopoNet outperforms interpolation,
convolutional, and graph-based baselines, reducing error by up to 60 percent
while preserving fine-scale glacial features. The resulting bed maps improve
reliability for operational modeling, supporting agencies engaged in climate
forecasting and policy. More broadly, GraphTopoNet shows how graph machine
learning can convert sparse, uncertain geophysical observations into actionable
knowledge at continental scale.

</details>


### [40] [Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation](https://arxiv.org/abs/2509.08580)
*Mathilde Monvoisin,Louise Piecuch,Blanche Texier,Cédric Hémon,Anaïs Barateau,Jérémie Huet,Antoine Nordez,Anne-Sophie Boureau,Jean-Claude Nunes,Diana Mateus*

Main category: cs.CV

TL;DR: This paper introduces an implicit shape prior method to reduce manual segmentation workload in medical 3D imaging by using sparse slice annotations and automatically selecting the most informative slices for guidance.


<details>
  <summary>Details</summary>
Motivation: To reduce the manual workload for medical professionals in complex 3D segmentation tasks that cannot be fully automated, particularly in radiotherapy planning and diagnosing degenerative diseases like sarcopenia which require accurate organ and muscle segmentation.

Method: The paper introduces an implicit shape prior to segment volumes from sparse slice manual annotations generalized to multi-organ cases, along with a framework for automatically selecting the most informative slices to guide and minimize subsequent interactions.

Result: Experimental validation shows the method's effectiveness on two medical use cases: assisted segmentation of at-risk organs for brain cancer patients, and accelerating the creation of new databases with unseen muscle shapes for sarcopenia patients.

Conclusion: The proposed method successfully reduces manual segmentation burden in medical imaging by leveraging sparse annotations and intelligent slice selection, demonstrating practical value in both radiotherapy planning and degenerative disease diagnosis.

Abstract: The objective of this paper is to significantly reduce the manual workload
required from medical professionals in complex 3D segmentation tasks that
cannot be yet fully automated. For instance, in radiotherapy planning, organs
at risk must be accurately identified in computed tomography (CT) or magnetic
resonance imaging (MRI) scans to ensure they are spared from harmful radiation.
Similarly, diagnosing age-related degenerative diseases such as sarcopenia,
which involve progressive muscle volume loss and strength, is commonly based on
muscular mass measurements often obtained from manual segmentation of medical
volumes. To alleviate the manual-segmentation burden, this paper introduces an
implicit shape prior to segment volumes from sparse slice manual annotations
generalized to the multi-organ case, along with a simple framework for
automatically selecting the most informative slices to guide and minimize the
next interactions. The experimental validation shows the method's effectiveness
on two medical use cases: assisted segmentation in the context of at risks
organs for brain cancer patients, and acceleration of the creation of a new
database with unseen muscle shapes for patients with sarcopenia.

</details>


### [41] [EfficientIML: Efficient High-Resolution Image Manipulation Localization](https://arxiv.org/abs/2509.08583)
*Jinhan Li,Haoyang He,Lei Xie,Jiangning Zhang*

Main category: cs.CV

TL;DR: Proposed EfficientIML model with EfficientRWKV backbone for detecting diffusion-based image forgeries in high-resolution images, outperforming existing methods in localization accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current image forgery detectors lack exposure to diffusion-based manipulations and struggle with computational complexity when handling high-resolution images from modern imaging devices.

Method: Created high-resolution SIF dataset with 1200+ diffusion-generated manipulations, developed EfficientIML model with lightweight EfficientRWKV backbone combining state-space and attention networks, and implemented multi-scale supervision for hierarchical prediction consistency.

Result: Outperformed ViT-based and other state-of-the-art lightweight baselines in localization performance, FLOPs, and inference speed on both the new dataset and standard benchmarks.

Conclusion: The proposed approach is suitable for real-time forensic applications, effectively addressing the computational challenges of detecting diffusion-based forgeries in high-resolution imagery.

Abstract: With imaging devices delivering ever-higher resolutions and the emerging
diffusion-based forgery methods, current detectors trained only on traditional
datasets (with splicing, copy-moving and object removal forgeries) lack
exposure to this new manipulation type. To address this, we propose a novel
high-resolution SIF dataset of 1200+ diffusion-generated manipulations with
semantically extracted masks. However, this also imposes a challenge on
existing methods, as they face significant computational resource constraints
due to their prohibitive computational complexities. Therefore, we propose a
novel EfficientIML model with a lightweight, three-stage EfficientRWKV
backbone. EfficientRWKV's hybrid state-space and attention network captures
global context and local details in parallel, while a multi-scale supervision
strategy enforces consistency across hierarchical predictions. Extensive
evaluations on our dataset and standard benchmarks demonstrate that our
approach outperforms ViT-based and other SOTA lightweight baselines in
localization performance, FLOPs and inference speed, underscoring its
suitability for real-time forensic applications.

</details>


### [42] [CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging](https://arxiv.org/abs/2509.08618)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Shahrooz Faghihroohi,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: CLAPS is a unified automated segmentation method for retinal imaging that combines CLIP-based encoding, automated prompt generation, and SAM to achieve expert-level performance across diverse tasks and modalities.


<details>
  <summary>Details</summary>
Motivation: Current medical image segmentation methods face modality ambiguity in disease descriptions, reliance on manual prompting for SAM workflows, and lack of unified frameworks that work across different tasks and imaging modalities.

Method: Pre-trains CLIP-based image encoder on multi-modal retinal data, uses GroundingDINO for automatic spatial prompt generation, enhances text prompts with modality signatures, and employs SAM for precise segmentation in a fully automated pipeline.

Result: Extensive experiments on 12 datasets across 11 segmentation categories show CLAPS achieves performance comparable to specialized expert models and surpasses existing benchmarks in most metrics.

Conclusion: CLAPS demonstrates strong generalizability as a foundation model for unified retinal image segmentation, effectively addressing current limitations while maintaining high performance across diverse tasks and modalities.

Abstract: Recent advancements in foundation models, such as the Segment Anything Model
(SAM), have significantly impacted medical image segmentation, especially in
retinal imaging, where precise segmentation is vital for diagnosis. Despite
this progress, current methods face critical challenges: 1) modality ambiguity
in textual disease descriptions, 2) a continued reliance on manual prompting
for SAM-based workflows, and 3) a lack of a unified framework, with most
methods being modality- and task-specific. To overcome these hurdles, we
propose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method for
unified segmentation across diverse tasks and modalities in retinal imaging.
Our approach begins by pre-training a CLIP-based image encoder on a large,
multi-modal retinal dataset to handle data scarcity and distribution imbalance.
We then leverage GroundingDINO to automatically generate spatial bounding box
prompts by detecting local lesions. To unify tasks and resolve ambiguity, we
use text prompts enhanced with a unique "modality signature" for each imaging
modality. Ultimately, these automated textual and spatial prompts guide SAM to
execute precise segmentation, creating a fully automated and unified pipeline.
Extensive experiments on 12 diverse datasets across 11 critical segmentation
categories show that CLAPS achieves performance on par with specialized expert
models while surpassing existing benchmarks across most metrics, demonstrating
its broad generalizability as a foundation model.

</details>


### [43] [AdsQA: Towards Advertisement Video Understanding](https://arxiv.org/abs/2509.08621)
*Xinwei Long,Kai Tian,Peng Xu,Guoli Jia,Jingxuan Li,Sa Yang,Yihua Shao,Kaiyan Zhang,Che Jiang,Hao Xu,Yang Liu,Jiaheng Ma,Bowen Zhou*

Main category: cs.CV

TL;DR: First benchmark using ad videos to evaluate LLMs' ability to understand marketing logic and persuasive strategies beyond physical content, with a new model ReAd-R achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Extend LLM applications to specialized domains by leveraging ad videos' rich information density and marketing logic to probe deeper understanding capabilities.

Method: Created AdsQA benchmark from 1,544 ad videos with 10,962 clips, proposed ReAd-R model (Deepseek-R1 style) that reflects on questions and generates answers via reward-driven optimization.

Result: Benchmarked 14 top-tier LLMs, ReAd-R achieved state-of-the-art performance, outperforming strong competitors with long-chain reasoning capabilities by a clear margin.

Conclusion: Ad videos provide a challenging testbed for evaluating LLMs' advanced perception abilities, and reward-driven optimization enables superior performance on marketing-focused reasoning tasks.

Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile,
an increasing number of domain-specific problems such as math and programming
boost these general-purpose models to continuously evolve via learning deeper
expertise. Now is thus the time further to extend the diversity of specialized
applications for knowledgeable LLMs, though collecting high quality data with
unexpected and informative tasks is challenging. In this paper, we propose to
use advertisement (ad) videos as a challenging test-bed to probe the ability of
LLMs in perceiving beyond the objective physical content of common visual
domain. Our motivation is to take full advantage of the clue-rich and
information-dense ad videos' traits, e.g., marketing logic, persuasive
strategies, and audience engagement. Our contribution is three-fold: (1) To our
knowledge, this is the first attempt to use ad videos with well-designed tasks
to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark
derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing
5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that
reflects on questions, and generates answers via reward-driven optimization.
(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves
the state-of-the-art outperforming strong competitors equipped with long-chain
reasoning capabilities by a clear margin.

</details>


### [44] [UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation](https://arxiv.org/abs/2509.08624)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Daniel Zapp,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: Proposes UOPSL framework that uses OCT spatial priors to enhance fundus image disease recognition without requiring paired multimodal data, addressing modality imbalance in ophthalmic diagnosis.


<details>
  <summary>Details</summary>
Motivation: Acquiring paired multimodal ophthalmic images is expensive, with limited OCT data availability. Conventional single-modality approaches fail to capture fine-grained spatial information from different imaging modalities that provide distinct lesion cues.

Method: Uses contrastive learning on unpaired OCT and fundus images to learn predilection sites matrix in OCT latent space. Bridges modalities via disease text descriptions. During inference, uses only fundus images with the learned matrix for classification.

Result: Outperforms existing benchmarks on 9 diverse datasets across 28 critical categories, demonstrating effective disease recognition using only fundus images enhanced by OCT-derived spatial priors.

Conclusion: The framework successfully addresses modality imbalance by leveraging OCT spatial knowledge without requiring paired data, enabling improved fundus-based diagnosis through learned lesion localization patterns.

Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have
led to substantial improvements in ophthalmic disease identification in recent
years. However, acquiring paired multimodal ophthalmic images remains
prohibitively expensive. While fundus photography is simple and cost-effective,
the limited availability of OCT data and inherent modality imbalance hinder
further progress. Conventional approaches that rely solely on fundus or textual
features often fail to capture fine-grained spatial information, as each
imaging modality provides distinct cues about lesion predilection sites. In
this study, we propose a novel unpaired multimodal framework \UOPSL that
utilizes extensive OCT-derived spatial priors to dynamically identify
predilection sites, enhancing fundus image-based disease recognition. Our
approach bridges unpaired fundus and OCTs via extended disease text
descriptions. Initially, we employ contrastive learning on a large corpus of
unpaired OCT and fundus images while simultaneously learning the predilection
sites matrix in the OCT latent space. Through extensive optimization, this
matrix captures lesion localization patterns within the OCT feature space.
During the fine-tuning or inference phase of the downstream classification task
based solely on fundus images, where paired OCT data is unavailable, we
eliminate OCT input and utilize the predilection sites matrix to assist in
fundus image classification learning. Extensive experiments conducted on 9
diverse datasets across 28 critical categories demonstrate that our framework
outperforms existing benchmarks.

</details>


### [45] [LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation](https://arxiv.org/abs/2509.08628)
*Xuqin Wang,Tao Wu,Yanfeng Zhang,Lu Liu,Dong Wang,Mingwei Sun,Yongliang Wang,Niclas Zeller,Daniel Cremers*

Main category: cs.CV

TL;DR: LADB is a semi-supervised framework that uses partially paired data to bridge domain gaps for sample-to-sample translation, avoiding exhaustive retraining while maintaining fidelity and diversity.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle in data-scarce domains where exhaustive retraining or costly paired data are required. Current unpaired methods lack controllability while fully paired approaches need large domain-specific datasets.

Method: Aligns source and target distributions in shared latent space, integrates pretrained source-domain diffusion models with target-domain Latent Aligned Diffusion Model (LADM) trained on partially paired latent representations.

Result: Superior performance in depth-to-image translation under partial supervision. Successfully extended to multi-source translation (depth maps + segmentation masks) and multi-target translation in class-conditioned style transfer.

Conclusion: LADB provides a scalable and versatile solution for real-world domain translation, particularly useful when data annotation is costly or incomplete.

Abstract: Diffusion models excel at generating high-quality outputs but face challenges
in data-scarce domains, where exhaustive retraining or costly paired data are
often required. To address these limitations, we propose Latent Aligned
Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample
translation that effectively bridges domain gaps using partially paired data.
By aligning source and target distributions within a shared latent space, LADB
seamlessly integrates pretrained source-domain diffusion models with a
target-domain Latent Aligned Diffusion Model (LADM), trained on partially
paired latent representations. This approach enables deterministic domain
mapping without the need for full supervision. Compared to unpaired methods,
which often lack controllability, and fully paired approaches that require
large, domain-specific datasets, LADB strikes a balance between fidelity and
diversity by leveraging a mixture of paired and unpaired latent-target
couplings. Our experimental results demonstrate superior performance in
depth-to-image translation under partial supervision. Furthermore, we extend
LADB to handle multi-source translation (from depth maps and segmentation
masks) and multi-target translation in a class-conditioned style transfer task,
showcasing its versatility in handling diverse and heterogeneous use cases.
Ultimately, we present LADB as a scalable and versatile solution for real-world
domain translation, particularly in scenarios where data annotation is costly
or incomplete.

</details>


### [46] [Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network](https://arxiv.org/abs/2509.08661)
*Liangjin Liu,Haoyang Zheng,Pei Zhou*

Main category: cs.CV

TL;DR: DSLNet introduces a dual-reference, dual-stream architecture for isolated sign language recognition that separates hand shape and motion trajectory analysis using wrist-centric and facial-centric coordinate systems, achieving state-of-the-art performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing ISLR methods struggle with morphologically similar but semantically distinct gestures due to geometric ambiguity from relying on single reference frames.

Method: Dual-stream architecture with wrist-centric frame for view-invariant shape analysis (using topology-aware graph convolution) and facial-centric frame for context-aware trajectory modeling (using Finsler geometry-based encoder), integrated via geometry-driven optimal transport fusion.

Result: Achieved 93.70% on WLASL-100, 89.97% on WLASL-300, and 99.79% on LSA64 datasets with significantly fewer parameters than competing models.

Conclusion: DSLNet's dual-reference approach effectively resolves geometric ambiguity in sign language recognition by decoupling shape and trajectory analysis, setting new state-of-the-art performance across multiple challenging datasets.

Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are
morphologically similar yet semantically distinct, a problem rooted in the
complex interplay between hand shape and motion trajectory. Existing methods,
often relying on a single reference frame, struggle to resolve this geometric
ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a
dual-reference, dual-stream architecture that decouples and models gesture
morphology and trajectory in separate, complementary coordinate systems. Our
approach utilizes a wrist-centric frame for view-invariant shape analysis and a
facial-centric frame for context-aware trajectory modeling. These streams are
processed by specialized networks-a topology-aware graph convolution for shape
and a Finsler geometry-based encoder for trajectory-and are integrated via a
geometry-driven optimal transport fusion mechanism. DSLNet sets a new
state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the
challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with
significantly fewer parameters than competing models.

</details>


### [47] [FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization](https://arxiv.org/abs/2509.08670)
*Sara Behnamian,Rasoul Khaksarinezhad,Andreas Langer*

Main category: cs.CV

TL;DR: FractalPINN-Flow is an unsupervised deep learning framework for optical flow estimation that learns directly from grayscale frames without ground truth, using a fractal-inspired recursive encoder-decoder architecture with TV regularization.


<details>
  <summary>Details</summary>
Motivation: To develop an optical flow estimation method that doesn't require ground truth annotations and can handle high-resolution data effectively, especially in scenarios with limited labeled data.

Method: Uses Fractal Deformation Network (FDN) - a recursive encoder-decoder with skip connections inspired by fractal geometry. Combines L1 and L2 data fidelity terms for brightness constancy with total variation regularization for spatial smoothness.

Result: Produces accurate, smooth, and edge-preserving optical flow fields. Particularly effective for high-resolution data and scenarios with limited annotations, as demonstrated on synthetic and benchmark datasets.

Conclusion: The unsupervised fractal-based approach successfully estimates optical flow without ground truth, offering advantages for high-resolution applications and annotation-scarce environments through its unique recursive architecture and variational formulation.

Abstract: We present FractalPINN-Flow, an unsupervised deep learning framework for
dense optical flow estimation that learns directly from consecutive grayscale
frames without requiring ground truth. The architecture centers on the Fractal
Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal
geometry and self-similarity. Unlike traditional CNNs with sequential
downsampling, FDN uses repeated encoder-decoder nesting with skip connections
to capture both fine-grained details and long-range motion patterns. The
training objective is based on a classical variational formulation using total
variation (TV) regularization. Specifically, we minimize an energy functional
that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness
constancy, along with a TV term that promotes spatial smoothness and coherent
flow fields. Experiments on synthetic and benchmark datasets show that
FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow
fields. The model is especially effective for high-resolution data and
scenarios with limited annotations.

</details>


### [48] [Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework](https://arxiv.org/abs/2509.08694)
*Zhen Tian,Christos Anagnostopoulos,Qiyuan Wang,Zhiwei Gao*

Main category: cs.CV

TL;DR: Robust U-Net framework for coastal water segmentation using HSV color space supervision and multi-modal constraints, achieving improved training stability and segmentation quality.


<details>
  <summary>Details</summary>
Motivation: Traditional RGB-based approaches suffer from training instability and poor generalization in complex coastal environments with irregular boundary patterns.

Method: Integrates five components: HSV-guided color supervision, gradient-based coastline optimization, morphological post-processing, sea area cleanup, and connectivity control.

Result: HSV supervision provides highest impact (0.85 influence score), achieves 84% variance reduction in training stability, and shows consistent improvements across multiple evaluation metrics.

Conclusion: The proposed Robust U-Net framework effectively addresses coastal water segmentation challenges while maintaining computational efficiency, with code available for reproducibility.

Abstract: Coastal water segmentation from satellite imagery presents unique challenges
due to complex spectral characteristics and irregular boundary patterns.
Traditional RGB-based approaches often suffer from training instability and
poor generalization in diverse maritime environments. This paper introduces a
systematic robust enhancement framework, referred to as Robust U-Net, that
leverages HSV color space supervision and multi-modal constraints for improved
coastal water segmentation. Our approach integrates five synergistic
components: HSV-guided color supervision, gradient-based coastline
optimization, morphological post-processing, sea area cleanup, and connectivity
control. Through comprehensive ablation studies, we demonstrate that HSV
supervision provides the highest impact (0.85 influence score), while the
complete framework achieves superior training stability (84\% variance
reduction) and enhanced segmentation quality. Our method shows consistent
improvements across multiple evaluation metrics while maintaining computational
efficiency. For reproducibility, our training configurations and code are
available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.

</details>


### [49] [Computational Imaging for Enhanced Computer Vision](https://arxiv.org/abs/2509.08712)
*Humera Shaikh,Kaur Jashanpreet*

Main category: cs.CV

TL;DR: Survey of computational imaging techniques and their impact on computer vision applications, addressing limitations of conventional imaging in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Conventional imaging methods often fail in challenging conditions like low light, motion blur, or high dynamic range scenes, limiting computer vision system performance.

Method: Systematic exploration of computational imaging techniques including light field imaging, HDR imaging, deblurring, high-speed imaging, and glare mitigation, and their synergies with core CV tasks.

Result: Analysis reveals relationships between CI methods and their practical contributions to CV applications, highlighting opportunities for improved robustness and accuracy.

Conclusion: Potential for task-specific adaptive imaging pipelines to enhance performance in real-world scenarios like autonomous navigation, surveillance, AR, and robotics.

Abstract: This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruction processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.

</details>


### [50] [BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion](https://arxiv.org/abs/2509.08715)
*Sike Xiang,Shuang Chen,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: Lightweight MLLM framework with 1.2B parameters for efficient visual question answering, achieving comparable performance to larger models while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Address deployment challenges of large MLLMs in resource-constrained environments, focusing on energy efficiency, computational scalability, and environmental sustainability.

Method: Proposes BreezeCLIP as a compact vision-language encoder within a modular framework called BcQLM (BreezeCLIP-enhanced Q-Gated Multimodal Language Model) for end-to-end visual question answering.

Result: Achieves performance comparable to standard-size MLLMs while significantly reducing computational costs, validated on multiple datasets with effective balance between accuracy and efficiency.

Conclusion: Offers a promising path toward deployable MLLMs under practical hardware constraints with modular and extensible design for broader multimodal tasks.

Abstract: As multimodal large language models (MLLMs) advance, their large-scale
architectures pose challenges for deployment in resource-constrained
environments. In the age of large models, where energy efficiency,
computational scalability and environmental sustainability are paramount, the
development of lightweight and high-performance models is critical for
real-world applications. As such, we propose a lightweight MLLM framework for
end-to-end visual question answering. Our proposed approach centres on
BreezeCLIP, a compact yet powerful vision-language encoder optimised for
efficient multimodal understanding. With only 1.2 billion parameters overall,
our model significantly reduces computational cost while achieving performance
comparable to standard-size MLLMs. Experiments conducted on multiple datasets
further validate its effectiveness in balancing accuracy and efficiency. The
modular and extensible design enables generalisation to broader multimodal
tasks. The proposed lightweight vision-language framework is denoted as BcQLM
(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising
path toward deployable MLLMs under practical hardware constraints. The source
code is available at https://github.com/thico0224/BcQLM.

</details>


### [51] [CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes](https://arxiv.org/abs/2509.08738)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: CrowdQuery (CQ) is a novel method that enhances transformer-based detectors by embedding object density maps into object queries, improving crowd detection in both 2D and 3D without additional data.


<details>
  <summary>Details</summary>
Motivation: Existing crowd detection methods struggle with crowded scenes where object density varies significantly. Current density map definitions are limited to head positions or spatial statistics, lacking consideration for individual bounding box dimensions.

Method: Developed CQ module that predicts and embeds object density maps, then integrates this density information into decoder queries. Created CQ2D and CQ3D architectures that extend density definitions to include bounding box dimensions for both 2D and 3D detection.

Result: Significant performance improvements on STCrowd dataset for both 2D and 3D domains, outperforming most state-of-the-art methods. Further improved performance on CrowdHuman dataset when integrated with existing crowd detectors.

Conclusion: CQ effectively bridges 2D and 3D detection in crowded environments, demonstrating universal applicability and generalizability across different transformer models and challenging datasets.

Abstract: This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing transformer-based
detectors. We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map. The embedded
density information is then systematically integrated into the decoder.
Existing density map definitions typically depend on head positions or
object-based spatial statistics. Our method extends these definitions to
include individual bounding box dimensions. By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes. CQ is universally applicable to both 2D
and 3D detection without requiring additional data. Consequently, we are the
first to design a method that effectively bridges 2D and 3D detection in
crowded environments. We demonstrate the integration of CQ into both a general
2D and 3D transformer-based object detector, introducing the architectures CQ2D
and CQ3D. CQ is not limited to the specific transformer models we selected.
Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods. When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability. The code is released at
https://github.com/mdaehl/CrowdQuery.

</details>


### [52] [ArgoTweak: Towards Self-Updating HD Maps through Structured Priors](https://arxiv.org/abs/2509.08764)
*Lena Wild,Rafael Valencia,Patric Jensfelt*

Main category: cs.CV

TL;DR: ArgoTweak is the first dataset providing realistic map priors, current maps, and sensor data to address the sim2real gap in HD mapping, enabling accurate change detection and integration through bijective mapping.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on synthetic priors that create inconsistencies and sim2real gaps, as no public dataset includes the required triplet of prior maps, current maps, and sensor data.

Method: Uses a bijective mapping framework that breaks down large-scale modifications into fine-grained atomic changes at map element level, ensuring interpretability and preserving unchanged elements.

Result: Training models on ArgoTweak significantly reduces sim2real gap compared to synthetic priors, with extensive ablations showing impact of structured priors and detailed change annotations.

Conclusion: ArgoTweak establishes a benchmark for explainable, prior-aided HD mapping and advances scalable, self-improving mapping solutions by providing realistic data and tools.

Abstract: Reliable integration of prior information is crucial for self-verifying and
self-updating HD maps. However, no public dataset includes the required triplet
of prior maps, current maps, and sensor data. As a result, existing methods
must rely on synthetic priors, which create inconsistencies and lead to a
significant sim2real gap. To address this, we introduce ArgoTweak, the first
dataset to complete the triplet with realistic map priors. At its core,
ArgoTweak employs a bijective mapping framework, breaking down large-scale
modifications into fine-grained atomic changes at the map element level, thus
ensuring interpretability. This paradigm shift enables accurate change
detection and integration while preserving unchanged elements with high
fidelity. Experiments show that training models on ArgoTweak significantly
reduces the sim2real gap compared to synthetic priors. Extensive ablations
further highlight the impact of structured priors and detailed change
annotations. By establishing a benchmark for explainable, prior-aided HD
mapping, ArgoTweak advances scalable, self-improving mapping solutions. The
dataset, baselines, map modification toolbox, and further resources are
available at https://kth-rpl.github.io/ArgoTweak/.

</details>


### [53] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: MMB is a new multimodal-aware method that uses Bayesian prompt ensembles with image clustering to improve text-to-image evaluation by dynamically weighting prompts based on visual characteristics, outperforming existing baselines in accuracy and calibration.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs used as judges for text-to-image generation suffer from biases, overconfidence, and inconsistent performance across different image domains, and standard prompt ensembling methods don't work well for multimodal tasks.

Method: Multimodal Mixture-of-Bayesian Prompt Ensembles (MMB) - Bayesian prompt ensemble approach augmented by image clustering that dynamically assigns prompt weights based on visual characteristics of each sample.

Result: MMB improves accuracy in pairwise preference judgments and greatly enhances calibration. Outperforms existing baselines on HPSv2 and MJBench benchmarks in alignment with human annotations and calibration across varied image content.

Conclusion: Multimodal-specific strategies are crucial for judge calibration, and MMB provides a promising path forward for reliable large-scale text-to-image evaluation.

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


### [54] [An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images](https://arxiv.org/abs/2509.08780)
*Asif Newaz,Asif Ur Rahman Adib,Rajit Sahil,Mashfique Mehzad*

Main category: cs.CV

TL;DR: Deep learning framework using mobile phone images for arsenicosis diagnosis, with Transformer models achieving 86% accuracy and explainable AI for clinical transparency.


<details>
  <summary>Details</summary>
Motivation: Arsenicosis is a serious public health issue in Asia caused by contaminated water, with early skin manifestations often underdiagnosed in rural areas lacking dermatologists. Automated image-based diagnosis can support early detection.

Method: End-to-end framework using mobile-captured skin images. Curated dataset of 20 classes with 11,000+ images. Benchmarked CNN and Transformer models. Integrated LIME and Grad-CAM for interpretability. Developed web-based diagnostic tool.

Result: Transformer models outperformed CNNs, with Swin Transformer achieving 86% accuracy. Visualizations confirmed models focused on lesion-relevant regions. Strong performance on external validation samples demonstrated generalization capability.

Conclusion: Framework shows potential for non-invasive, accessible, and explainable arsenicosis diagnosis from mobile images. Can serve as practical diagnostic aid in resource-limited communities for early detection and timely intervention.

Abstract: Background: Arsenicosis is a serious public health concern in South and
Southeast Asia, primarily caused by long-term consumption of
arsenic-contaminated water. Its early cutaneous manifestations are clinically
significant but often underdiagnosed, particularly in rural areas with limited
access to dermatologists. Automated, image-based diagnostic solutions can
support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis
diagnosis using mobile phone-captured skin images. A dataset comprising 20
classes and over 11000 images of arsenic-induced and other dermatological
conditions was curated. Multiple deep learning architectures, including
convolutional neural networks (CNNs) and Transformer-based models, were
benchmarked for arsenicosis detection. Model interpretability was integrated
via LIME and Grad-CAM, while deployment feasibility was demonstrated through a
web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the
Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM
visualizations confirmed that the models attended to lesion-relevant regions,
increasing clinical transparency and aiding in error analysis. The framework
also demonstrated strong performance on external validation samples, confirming
its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep
learning for non-invasive, accessible, and explainable diagnosis of arsenicosis
from mobile-acquired images. By enabling reliable image-based screening, it can
serve as a practical diagnostic aid in rural and resource-limited communities,
where access to dermatologists is scarce, thereby supporting early detection
and timely intervention.

</details>


### [55] [Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation](https://arxiv.org/abs/2509.08794)
*Dennis Melamed,Connor Hashemi,Scott McCloskey*

Main category: cs.CV

TL;DR: Event-based cameras achieve 18.47 arcsecond accuracy for star tracking by using Earth's rotation as ground truth, demonstrating their potential for low-cost, low-latency attitude determination systems.


<details>
  <summary>Details</summary>
Motivation: Event-based cameras show promise for star tracking but lack accurate ground truth validation in previous studies. This research aims to quantify their accuracy using Earth's regular rotation as a reliable reference.

Method: A static event camera mounted on a ground-based telescope captures star events. The Earth's rotation provides the only motion in the celestial reference frame. Orientation estimates are compared against IERS-measured Earth orientation data.

Result: The system achieved 18.47 arcseconds root mean squared error and 78.84 arcseconds about error, validating event cameras' accuracy for star tracking applications.

Conclusion: Event cameras offer viable star tracking with good accuracy, combined with advantages like sparser data processing, higher dynamic range, lower energy consumption, and faster update rates compared to traditional framing sensors.

Abstract: Event-based cameras (EBCs) are a promising new technology for star
tracking-based attitude determination, but prior studies have struggled to
determine accurate ground truth for real data. We analyze the accuracy of an
EBC star tracking system utilizing the Earth's motion as the ground truth for
comparison. The Earth rotates in a regular way with very small irregularities
which are measured to the level of milli-arcseconds. By keeping an event camera
static and pointing it through a ground-based telescope at the night sky, we
create a system where the only camera motion in the celestial reference frame
is that induced by the Earth's rotation. The resulting event stream is
processed to generate estimates of orientation which we compare to the
International Earth Rotation and Reference System (IERS) measured orientation
of the Earth. The event camera system is able to achieve a root mean squared
across error of 18.47 arcseconds and an about error of 78.84 arcseconds.
Combined with the other benefits of event cameras over framing sensors (reduced
computation due to sparser data streams, higher dynamic range, lower energy
consumption, faster update rates), this level of accuracy suggests the utility
of event cameras for low-cost and low-latency star tracking. We provide all
code and data used to generate our results:
https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.

</details>


### [56] [Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching](https://arxiv.org/abs/2509.08805)
*Matthieu Vilain,Rémi Giraud,Yannick Berthoumieu,Guillaume Bourmaud*

Main category: cs.CV

TL;DR: BEAMER introduces a beam search strategy for dense image matching that preserves multiple correspondent hypotheses per pixel across scales, improving robustness in challenging cases like depth discontinuities and strong zoom-ins.


<details>
  <summary>Details</summary>
Motivation: Current dense matching methods produce only single correspondent hypotheses per pixel at each scale, which fails in challenging scenarios where neighboring pixels have widely spread correspondences, leading to erroneous matches.

Method: Proposes a beam search strategy to propagate multiple hypotheses at each scale and integrates these multiple hypotheses into cross-attention layers, creating the BEAMER architecture that preserves and propagates multiple correspondences.

Result: BEAMER becomes significantly more robust than state-of-the-art methods, particularly at depth discontinuities and when the target image is a strong zoom-in of the source image.

Conclusion: Predicting and preserving multiple correspondent hypotheses per source location at each scale through beam search and cross-attention integration substantially improves dense image matching performance in challenging scenarios.

Abstract: Dense image matching aims to find a correspondent for every pixel of a source
image in a partially overlapping target image. State-of-the-art methods
typically rely on a coarse-to-fine mechanism where a single correspondent
hypothesis is produced per source location at each scale. In challenging cases
-- such as at depth discontinuities or when the target image is a strong
zoom-in of the source image -- the correspondents of neighboring source
locations are often widely spread and predicting a single correspondent
hypothesis per source location at each scale may lead to erroneous matches. In
this paper, we investigate the idea of predicting multiple correspondent
hypotheses per source location at each scale instead. We consider a beam search
strategy to propagat multiple hypotheses at each scale and propose integrating
these multiple hypotheses into cross-attention layers, resulting in a novel
dense matching architecture called BEAMER. BEAMER learns to preserve and
propagate multiple hypotheses across scales, making it significantly more
robust than state-of-the-art methods, especially at depth discontinuities or
when the target image is a strong zoom-in of the source image.

</details>


### [57] [GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts](https://arxiv.org/abs/2509.08818)
*Jenna Kang,Maria Silva,Patsorn Sangkloy,Kenneth Chen,Niall Williams,Qi Sun*

Main category: cs.CV

TL;DR: GeneVA is a large-scale annotated dataset for benchmarking spatio-temporal artifacts in text-to-video generation models, addressing the lack of systematic evaluation tools for video generation quality.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on image generation, but video generation introduces unique spatio-temporal complexities and artifacts like impossible physics and temporal inconsistency that need systematic evaluation.

Method: Created a large-scale artifact dataset with rich human annotations specifically focusing on spatio-temporal artifacts in videos generated from natural text prompts.

Result: GeneVA provides a comprehensive benchmark tool that enables systematic evaluation of text-to-video generation models and identification of various artifact types.

Conclusion: GeneVA bridges the gap in video generation evaluation and can assist critical applications like benchmarking model performance and improving generative video quality.

Abstract: Recent advances in probabilistic generative models have extended capabilities
from static image synthesis to text-driven video generation. However, the
inherent randomness of their generation process can lead to unpredictable
artifacts, such as impossible physics and temporal inconsistency. Progress in
addressing these challenges requires systematic benchmarks, yet existing
datasets primarily focus on generative images due to the unique spatio-temporal
complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale
artifact dataset with rich human annotations that focuses on spatio-temporal
artifacts in videos generated from natural text prompts. We hope GeneVA can
enable and assist critical applications, such as benchmarking model performance
and improving generative video quality.

</details>


### [58] [RewardDance: Reward Scaling in Visual Generation](https://arxiv.org/abs/2509.08826)
*Jie Wu,Yu Gao,Zilyu Ye,Ming Li,Liang Li,Hanzhong Guo,Jie Liu,Zeyue Xue,Xiaoxia Hou,Wei Liu,Yan Zeng,Weilin Huang*

Main category: cs.CV

TL;DR: RewardDance is a scalable reward modeling framework that introduces a generative reward paradigm to overcome limitations of existing approaches, enabling scaling to 26B parameters and resolving reward hacking issues in visual generation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing reward models for visual generation suffer from architectural constraints, modality limitations, and misalignment with VLM architectures. CLIP-based RMs have input modality issues, while Bradley-Terry losses don't align with next-token prediction. Reward hacking also plagues RLHF optimization.

Method: RewardDance reformulates reward scoring as the model's probability of predicting a "yes" token when comparing generated vs reference images. This generative reward paradigm intrinsically aligns with VLM architectures and enables scaling across model size (up to 26B parameters) and context (instructions, references, chain-of-thought).

Result: Extensive experiments show RewardDance significantly outperforms state-of-the-art methods in text-to-image, text-to-video, and image-to-video generation. Large-scale RMs maintain high reward variance during RL fine-tuning, demonstrating resistance to reward hacking and ability to produce diverse, high-quality outputs.

Conclusion: RewardDance successfully addresses fundamental limitations in visual generation reward modeling, enabling scalable reward models that resist reward hacking and alleviate mode collapse problems that affect smaller models.

Abstract: Reward Models (RMs) are critical for improving generation models via
Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation
remains largely unexplored. It primarily due to fundamental limitations in
existing approaches: CLIP-based RMs suffer from architectural and input
modality constraints, while prevalent Bradley-Terry losses are fundamentally
misaligned with the next-token prediction mechanism of Vision-Language Models
(VLMs), hindering effective scaling. More critically, the RLHF optimization
process is plagued by Reward Hacking issue, where models exploit flaws in the
reward signal without improving true quality. To address these challenges, we
introduce RewardDance, a scalable reward modeling framework that overcomes
these barriers through a novel generative reward paradigm. By reformulating the
reward score as the model's probability of predicting a "yes" token, indicating
that the generated image outperforms a reference image according to specific
criteria, RewardDance intrinsically aligns reward objectives with VLM
architectures. This alignment unlocks scaling across two dimensions: (1) Model
Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context
Scaling: Integration of task-specific instructions, reference examples, and
chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that
RewardDance significantly surpasses state-of-the-art methods in text-to-image,
text-to-video, and image-to-video generation. Crucially, we resolve the
persistent challenge of "reward hacking": Our large-scale RMs exhibit and
maintain high reward variance during RL fine-tuning, proving their resistance
to hacking and ability to produce diverse, high-quality outputs. It greatly
relieves the mode collapse problem that plagues smaller models.

</details>


### [59] [SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video](https://arxiv.org/abs/2509.08828)
*David Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: Novel approach combining 3D geometry reconstruction and appearance estimation for fabrics using only monocular RGB video, with improved reconstruction accuracy and sharp detail recovery.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of 3D dynamic scene reconstruction, particularly for fabrics, using only monocular RGB video input while overcoming depth ambiguity problems.

Method: Combines physical simulation of cloth geometry with differentiable rendering, introduces two novel regularization terms to address depth ambiguity in monocular video reconstruction.

Result: Reduced 3D reconstruction error by factor of 2.64 compared to recent methods, with medium runtime of 30 min per scene. Achieved high-quality appearance estimation recovering sharp details.

Conclusion: The proposed system successfully performs both 3D reconstruction and appearance estimation for deforming fabrics from monocular video, demonstrating significant improvement in reconstruction accuracy and detail recovery.

Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established
yet challenging task within the domain of computer vision. In this paper, we
propose a novel approach that combines the domains of 3D geometry
reconstruction and appearance estimation for physically based rendering and
present a system that is able to perform both tasks for fabrics, utilizing only
a single monocular RGB video sequence as input. In order to obtain realistic
and high-quality deformations and renderings, a physical simulation of the
cloth geometry and differentiable rendering are employed. In this paper, we
introduce two novel regularization terms for the 3D reconstruction task that
improve the plausibility of the reconstruction by addressing the depth
ambiguity problem in monocular video. In comparison with the most recent
methods in the field, we have reduced the error in the 3D reconstruction by a
factor of 2.64 while requiring a medium runtime of 30 min per scene.
Furthermore, the optimized motion achieves sufficient quality to perform an
appearance estimation of the deforming object, recovering sharp details from
this single monocular RGB video.

</details>
