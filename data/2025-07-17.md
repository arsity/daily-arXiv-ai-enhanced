<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 86]
- [eess.IV](#eess.IV) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: A hardware-friendly optimization framework for Deformable Attention Transformers (DAT) is proposed, using NAS and a new slicing strategy to balance accuracy and efficiency, validated on FPGA.


<details>
  <summary>Details</summary>
Motivation: DAT's irregular memory access patterns hinder efficient hardware deployment; existing methods compromise accuracy or hardware efficiency.

Method: Proposes a NAS-based method with a new slicing strategy to divide input features uniformly, avoiding memory conflicts, and designs an FPGA verification system.

Result: Achieves only 0.2% accuracy drop on ImageNet-1K and reduces DRAM access times to 18% compared to existing methods.

Conclusion: The framework effectively balances accuracy and hardware efficiency, making DAT more deployable on edge-side hardware.

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [2] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: Proposes Deformable Dynamic Convolution Network (DDCN) for efficient and accurate spatio-temporal traffic prediction, addressing limitations of GNNs and traditional CNNs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with heterogeneity in traffic patterns and scalability issues, especially with large-scale data. GNNs require predefined adjacency matrices and are computationally complex.

Method: DDCN uses deformable filters dynamically based on offsets, decomposes into an encoder-decoder structure, and employs spatial and spatio-temporal attention blocks for feature emphasis.

Result: DDCN achieves competitive performance on four real-world datasets, demonstrating accuracy and efficiency.

Conclusion: DDCN shows the potential of CNN-based approaches for spatio-temporal traffic prediction, overcoming traditional limitations.

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [3] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: Inversion-DPO is a novel alignment framework for diffusion models that avoids reward modeling by using DDIM inversion, improving training efficiency and precision.


<details>
  <summary>Details</summary>
Motivation: Existing alignment methods for diffusion models are computationally intensive and may reduce accuracy. Inversion-DPO aims to address these limitations.

Method: The method reformulates Direct Preference Optimization (DPO) with DDIM inversion, enabling posterior sampling without auxiliary reward models.

Result: Inversion-DPO outperforms existing post-training methods, achieving high-fidelity and compositionally coherent image generation.

Conclusion: Inversion-DPO offers an efficient, high-precision alignment approach for diffusion models, enhancing their use in complex generation tasks.

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [4] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: ST-VFM adapts Vision Foundation Models (VFMs) for spatio-temporal forecasting by addressing temporal modeling and modality gaps through a dual-branch architecture and reprogramming stages.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack spatio-temporal correlation modeling, while VFMs offer spatial priors but lack temporal capacity and face modality gaps with ST data.

Method: ST-VFM uses a dual-branch architecture with raw ST and auxiliary flow inputs, plus pre- and post-VFM reprogramming stages for temporal embedding and dynamic interaction.

Result: ST-VFM outperforms baselines on ten datasets, proving robust across VFM backbones.

Conclusion: ST-VFM is a strong general framework for spatio-temporal forecasting, leveraging VFMs effectively.

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [5] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: xOp-GAN introduces multiple expert generators for underwater image restoration, outperforming single-regressor models with higher PSNR scores.


<details>
  <summary>Details</summary>
Motivation: Underwater image restoration is challenging due to diverse deformation artifacts; single deep regressor networks struggle with heterogeneous degradation.

Method: xOp-GAN uses multiple expert generators, each trained on specific image quality subsets, and a discriminator to select the best output.

Result: Achieves PSNR up to 25.16 dB on the LSUI dataset, surpassing single-regressor models.

Conclusion: xOp-GAN effectively addresses heterogeneous degradation in underwater images, offering superior restoration performance.

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [6] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: The paper reviews gait-based age estimation, analyzing 59 studies and large datasets to compare methods, identify key metrics, and optimize deep learning models for accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Age estimation from gait has applications in healthcare, security, and human-computer interaction, but performance varies across methods and datasets.

Method: The study combines meta-analysis of existing research, large-scale dataset analysis (OU-ISIR and VersatileGait), and fine-tuning of ResNet34 with Grad-CAM for interpretability.

Result: Convolutional neural networks achieve 96% accuracy with <0.1s processing time, and multi-sensor fusion reduces error to 3.4 years. Key gait metrics correlate with age (coefficients â‰¥0.27).

Conclusion: The work provides performance baselines and guidelines to reduce gait-age error below three years in real-world scenarios.

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [7] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: The paper explores using a modified part-pose guided network (PPGNet-Cat) for re-identifying feral cats in the wild, achieving high accuracy (mAP 0.86, rank-1 0.95).


<details>
  <summary>Details</summary>
Motivation: Feral cats are a major threat to Australian wildlife, and improved monitoring methods like re-ID are needed to mitigate their impact.

Method: Adapted PPGNet (originally for Amur tigers) to feral cats, incorporating specific modifications and testing contrastive learning (e.g., ArcFace loss).

Result: PPGNet-Cat performed exceptionally well, with mAP 0.86 and rank-1 accuracy 0.95.

Conclusion: PPGNet-Cat is a highly effective model for feral cat re-ID, offering a valuable tool for wildlife monitoring.

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [8] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: SketchDNN introduces a generative model for CAD sketches using a unified continuous-discrete diffusion process, improving generation quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in CAD sketch generation, such as heterogeneous primitive parameterizations and permutation invariance.

Method: Uses Gaussian-Softmax diffusion, blending Gaussian noise with softmax for discrete variables.

Result: Reduces FID from 16.04 to 7.80 and NLL from 84.8 to 81.33, setting a new benchmark.

Conclusion: SketchDNN achieves state-of-the-art performance in CAD sketch generation.

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [9] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: A Variational Autoencoder (VAE) is used to improve lymph node metastasis (LNM) staging in rectal cancer, outperforming traditional CNN-based methods with higher interpretability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current radiological criteria for LNM staging are limited in accuracy. A VAE is proposed to directly encode visual features and create a more interpretable latent space.

Method: The VAE replaces large pre-trained CNNs, focusing on reconstructing images to encode meaningful patterns. The model, 'VAE-MLP', is tested on an in-house MRI dataset of 168 patients.

Result: VAE-MLP achieved state-of-the-art performance with AUC 0.86, Sensitivity 0.79, and Specificity 0.85.

Conclusion: The VAE-based approach offers improved accuracy and interpretability for LNM staging, demonstrating its potential for clinical use.

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [10] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: The paper proposes a posture-based method to infer mental states, validated in cricket, achieving high accuracy in intent discrimination and suggesting broader applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges in vision-based mental state diagnosis due to sensitive human data, the study explores sports settings as a viable data source.

Method: The approach uses motion analysis of posture in cricket videos to discriminate aggressive and defensive shot intent, leveraging weak supervision for validation.

Result: The method achieves over 75% F1 score and 80% AUC-ROC, showing posture effectively leaks intent signals despite data noise.

Conclusion: Posture-based intent inference is viable, with potential applications in sports analytics and broader human behavior analysis.

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [11] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: VISTA is a novel monocular global localization framework using object-based segmentation and tracking, achieving robust performance across viewpoints and seasons with minimal memory usage.


<details>
  <summary>Details</summary>
Motivation: Global localization in unstructured environments is challenging due to appearance changes and occlusions, which traditional methods often fail to address.

Method: VISTA combines object-based segmentation/tracking with submap correspondence search for frame alignment, requiring no domain-specific training.

Result: VISTA improves recall by 69% over baselines and uses a map only 0.6% the size of the most memory-conservative baseline.

Conclusion: VISTA offers a scalable, efficient solution for global localization in diverse environments, suitable for resource-constrained platforms.

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [12] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: Benchmarking VLMs vs. CNN-based OCR for outdoor billboard text visibility, showing VLMs excel in holistic reasoning but CNNs are cost-effective for cropped text.


<details>
  <summary>Details</summary>
Motivation: Accurate verification of billboard text visibility in real-world conditions is challenging due to complex scenes, fonts, and weather noise.

Method: Systematically compared VLMs (Qwen 2.5 VL 3B, InternVL3, SmolVLM2) and CNN-based OCR (PaddleOCRv4) on datasets (ICDAR 2015, SVT) with synthetic weather distortions.

Result: VLMs perform better in holistic scene understanding, but lightweight CNNs are more efficient for cropped text recognition.

Conclusion: CNN pipelines remain competitive for edge deployment due to lower computational costs, while VLMs offer superior scene reasoning. Public benchmark and code released for future research.

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [13] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: The paper proposes UCGS, a unified framework for solving multiple abstract visual reasoning tasks without task-specific retraining.


<details>
  <summary>Details</summary>
Motivation: Current deep AVR solvers require task-specific designs or retraining, increasing costs. UCGS aims to address this limitation by unifying multiple AVR tasks under one framework.

Method: Reformulates AVR tasks as predictability estimation of target images and uses a single conditional generative model for multi-task training.

Result: UCGS achieves abstract reasoning across various tasks with a single training round and demonstrates zero-shot reasoning on unseen tasks.

Conclusion: UCGS offers a cost-effective and scalable solution for AVR tasks, eliminating the need for task-specific retraining or tuning.

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [14] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE is a novel framework for robust correspondence pruning in cross-domain and cross-scene scenarios, using style mixing and adaptive feature fusion.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with diverse scene structures and domain shifts, limiting their robustness.

Method: Proposes De-stylization Dual Branch for domain shift and Bi-Fusion Mixture of Experts for scene diversity.

Result: Outperforms state-of-the-art methods in accuracy and generalization on benchmarks.

Conclusion: CorrMoE effectively addresses cross-domain and cross-scene challenges, offering superior performance.

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [15] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: ProtoConNet improves open-set few-shot image classification by integrating contextual information, enhancing feature diversity, and aligning prototypes, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook contextual information, leading to spurious associations in few-shot scenarios. ProtoConNet addresses this by leveraging background data.

Method: ProtoConNet uses three modules: CDS for diverse data patterns, CSR for contextual integration, and PA for prototype alignment.

Result: Experiments on two datasets show ProtoConNet enhances representation learning and open-set sample identification.

Conclusion: ProtoConNet outperforms existing methods by effectively integrating context and refining feature spaces in few-shot scenarios.

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [16] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: GRACE improves DFER by aligning refined text and visual signals, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize emotional cues in text and lack mechanisms to filter irrelevant facial dynamics.

Method: GRACE integrates dynamic motion modeling, semantic text refinement (CATE), and token-level cross-modal alignment.

Result: Achieves SOTA performance on three datasets, especially in ambiguous/imbalanced emotion classes.

Conclusion: GRACE effectively localizes emotional features and enhances recognition accuracy.

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [17] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: The paper proposes Spatial Frequency Modulation (SFM) to preserve high-frequency details in semantic segmentation by modulating and demodulating features during downsampling and upsampling, respectively.


<details>
  <summary>Details</summary>
Motivation: High-frequency details are crucial for semantic segmentation but are prone to aliasing during downsampling.

Method: SFM modulates high-frequency features to lower frequencies before downsampling using adaptive resampling (ARS) and recovers them during upsampling with Multi-Scale Adaptive Upsampling (MSAU).

Result: The method effectively reduces aliasing and retains details, improving segmentation accuracy. It also shows broad applicability in other tasks like classification and instance segmentation.

Conclusion: SFM is a versatile and effective solution for preserving high-frequency information in various architectures and tasks.

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [18] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: SEPose is a synthetic event-based dataset for human pose estimation in pedestrian monitoring, addressing data scarcity in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: Limited availability of event-based data for pedestrian monitoring in safety-critical scenarios like distracted walking.

Method: Created SEPose using CARLA simulator with dynamic vision sensors, annotating 350K pedestrians with pose keypoints under diverse conditions.

Result: Trained models like RVT and YOLOv8 on SEPose, showing sim-to-real generalization on real event-based data.

Conclusion: SEPose effectively bridges the data gap for event-based pedestrian pose estimation, demonstrating practical utility.

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [19] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Dark-EvGS is a novel event-assisted 3D Gaussian Splatting framework for reconstructing bright frames in low-light conditions, addressing noise, frame quality, and color consistency issues.


<details>
  <summary>Details</summary>
Motivation: Conventional cameras fail in low-light, while event cameras and 3D GS show promise but face challenges like noise and inconsistent color.

Method: Proposes Dark-EvGS with triplet-level supervision for holistic and granular details, and a color tone matching block for consistency. Introduces a real-captured dataset.

Result: Outperforms existing methods in radiance field reconstruction under low-light conditions.

Conclusion: Dark-EvGS successfully tackles low-light challenges, enabling high-quality multi-view frame synthesis.

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [20] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: The paper introduces Hyperphantasia, a benchmark to evaluate mental visualization in MLLMs, revealing a significant performance gap between humans and models.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus on passive visual perception, neglecting active mental visualization, a critical human cognitive skill.

Method: Hyperphantasia uses four procedurally generated puzzles at three difficulty levels to assess MLLMs. Reinforcement learning is explored for improvement.

Result: State-of-the-art MLLMs show limited mental visualization, with a notable gap compared to humans.

Conclusion: Robust mental visualization remains a challenge for MLLMs, despite partial pattern recognition abilities.

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [21] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: The paper introduces RaDL, a relation-aware disentangled learning framework, to improve multi-instance image generation by addressing relationship discrepancies and attribute leakage.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with generating multiple instances in a single image while maintaining accurate relationships and attributes.

Method: RaDL uses learnable parameters for instance-specific attributes and Relation Attention for relation-aware features, leveraging action verbs from prompts.

Result: RaDL outperforms existing methods on benchmarks like COCO-Position and DrawBench, improving positional accuracy and attribute handling.

Conclusion: RaDL effectively addresses multi-instance image generation challenges by considering relationships and attributes, setting a new standard for such tasks.

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [22] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: The paper introduces PPAR, a framework for generalizable semantic segmentation, addressing challenges in prototypical alignment and source data reliability using CLIP-derived prototypes and progressive alignment.


<details>
  <summary>Details</summary>
Motivation: To improve generalization in semantic segmentation by overcoming limitations of existing prototypical alignment methods, such as coarse strategies, overfitting, and equal treatment of source samples.

Method: Proposes PPAR, using CLIP to generate Original Text Prototype (OTP) and Visual Text Prototype (VTP), progressive alignment, and prototypical reweighting to mitigate negative transfer.

Result: PPAR achieves state-of-the-art performance across multiple benchmarks, demonstrating its effectiveness.

Conclusion: PPAR successfully addresses key challenges in generalizable semantic segmentation, offering a robust solution with theoretical backing and empirical validation.

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [23] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: LG-CAV-MAE integrates text, audio, and visual modalities for improved representation learning, using auto-generated triplets from videos. It outperforms existing methods in retrieval and classification tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance audio-visual representation learning by incorporating text guidance and leveraging unlabeled videos for training.

Method: Integrates a pretrained text encoder into contrastive audio-visual masked autoencoders. Uses auto-generated audio-visual-text triplets from videos via CLAP-based filtering.

Result: Achieves 5.6% improvement in recall@10 for retrieval and 3.2% for classification over existing methods.

Conclusion: LG-CAV-MAE effectively learns cross-modal representations and outperforms prior approaches, demonstrating the value of text-guided learning.

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [24] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: The paper evaluates the robustness of Multimodal Large Language Models (MLLMs) in short-form video contexts, introducing a tri-modal safety framework, the SVMA dataset, and the ChimeraBreak attack strategy. Results reveal significant vulnerabilities and model biases.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations for MLLMs focus on unimodal attacks, neglecting combined vulnerabilities in short-form video contexts.

Method: The study introduces the SVMA dataset with synthetic adversarial attacks and proposes ChimeraBreak, a tri-modal attack strategy targeting visual, auditory, and semantic reasoning.

Result: Experiments show high Attack Success Rates (ASR) and distinct failure modes, with model biases in misclassifying content.

Conclusion: The findings highlight vulnerabilities in MLLMs and provide insights for developing safer models, using LLM-as-a-judge for evaluation.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [25] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: GS-Bias introduces a TTA method for VLMs using global and spatial biases to improve efficiency and performance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods for VLMs struggle with balancing performance and efficiency, often due to excessive tuning or unstable enhancements.

Method: GS-Bias learns global and spatial biases from test images, adding them to VLM logits to avoid full backpropagation.

Result: Achieves SOTA on 15 benchmarks, with 2.23% and 2.72% improvements over TPT in cross-dataset and domain generalization, using only 6.5% of TPT's memory.

Conclusion: GS-Bias is an efficient and effective TTA paradigm for VLMs, offering significant performance gains with minimal overhead.

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [26] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: EC-Diff accelerates cloud-edge collaboration in diffusion models by optimizing noise estimation and handoff timing, improving speed and quality.


<details>
  <summary>Details</summary>
Motivation: Addressing the trade-off between cloud inference time and edge model output consistency in hybrid edge-cloud frameworks.

Method: Uses a K-step noise approximation strategy and a two-stage greedy search algorithm to optimize noise estimation and handoff timing.

Result: Achieves up to 2Ã— speedup in inference while maintaining generation quality compared to cloud inference.

Conclusion: EC-Diff effectively balances speed and quality in diffusion model inference through optimized cloud-edge collaboration.

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [27] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: MPAE introduces an unsupervised method for part discovery in images, using masked autoencoding to align part descriptors with actual shapes, robust across categories and scenarios.


<details>
  <summary>Details</summary>
Motivation: Overcoming the lack of fine-grained labels and improving robustness in unsupervised part discovery for diverse categories and scenarios.

Method: Uses a Masked Part Autoencoder (MPAE) to learn part descriptors and fill masked regions based on similarity, guided by unmasked patches.

Result: Robustly discovers meaningful parts aligned with object shapes, even in complex scenarios, and handles occlusion and cross-category part similarity.

Conclusion: MPAE provides an effective unsupervised solution for part discovery, with broad applicability and strong performance demonstrated in experiments.

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [28] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: A zero-shot diffusion pipeline is proposed for blending multiple artistic styles in a controlled, region-specific manner by fusing denoised latents from separately trained models using spatial masks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based text-to-image models struggle with applying distinct painting techniques regionally due to entangled latent spaces and lack of smooth interpolation, often leading to one style dominating.

Method: The method involves style composition on denoised latents from separately trained models, leveraging lower-noise latents for stylistic fusion with spatial masks. Depth-map conditioning via ControlNet ensures structural coherence.

Result: Qualitative and quantitative experiments show successful region-specific style mixing according to given masks, preserving individual style fidelity.

Conclusion: The proposed pipeline effectively enables precise, user-guided style blending while maintaining structural coherence and style fidelity.

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [29] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: ID-EA improves personalized portrait generation by aligning text embeddings with visual identity embeddings, enhancing facial identity consistency and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current Textual Inversion methods fail to maintain consistent facial identity due to semantic misalignments between textual and visual embeddings.

Method: ID-EA uses an ID-driven Enhancer and ID-conditioned Adapter to refine visual identity embeddings and adapt text conditions, ensuring identity preservation.

Result: ID-EA outperforms state-of-the-art methods in identity preservation and is 15 times faster.

Conclusion: ID-EA effectively addresses identity preservation in personalized portrait generation, offering superior performance and efficiency.

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [30] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: SAMST is a semi-supervised method for remote sensing semantic segmentation, combining SAM's zero-shot generalization with iterative pseudo-label refinement to improve accuracy with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Public remote sensing datasets lack universality due to resolution variability and inconsistent land cover definitions, limiting their utility.

Method: SAMST uses supervised self-training and a SAM-based Pseudo-label Refiner with three modules: Threshold Filter, Prompt Generation, and Label Refinement.

Result: Experiments on the Potsdam dataset show SAMST improves pseudo-label accuracy and model performance.

Conclusion: SAMST effectively addresses limited labeled data challenges in remote sensing semantic segmentation.

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [31] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: The paper introduces AUBlendSet, a 3D facial dataset for fine-grained expression manipulation, and AUBlendNet, a network for stylized facial animation.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of datasets for fine-grained stylized 3D facial expression manipulation.

Method: AUBlendSet provides blendshape data based on 32 AUs across 500 identities. AUBlendNet learns AU-Blendshape basis vectors for stylized manipulation.

Result: Validated through tasks like expression manipulation, speech-driven animation, and emotion recognition.

Conclusion: AUBlendSet and AUBlendNet are pioneering tools for continuous 3D facial expression manipulation.

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [32] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: The paper introduces Frequency-Dynamic Attention Modulation (FDAM), a method to address frequency vanishing in Vision Transformers (ViTs) by modulating their frequency response using Attention Inversion and Frequency Dynamic Scaling.


<details>
  <summary>Details</summary>
Motivation: ViTs lose critical details and textures due to their low-pass filtering nature and frequency vanishing in stacked layers.

Method: Proposes FDAM, combining Attention Inversion (AttInv) for high-pass filtering and Frequency Dynamic Scaling (FreqScale) for fine-grained frequency adjustments.

Result: FDAM improves performance in tasks like semantic segmentation, object detection, and instance segmentation, achieving state-of-the-art results in remote sensing detection.

Conclusion: FDAM effectively mitigates frequency vanishing in ViTs, enhancing their performance across various vision tasks.

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [33] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: MaskTwins reframes masked reconstruction in UDA as a sparse signal problem, proving complementary masks enhance domain-agnostic feature extraction, outperforming baselines in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MIM approaches in UDA lack theoretical grounding and underutilize masked reconstruction's potential for feature learning.

Method: Proposes MaskTwins, integrating masked reconstruction into UDA training, enforcing consistency between complementary masked image predictions.

Result: Outperforms baselines in natural and biological image segmentation, demonstrating domain-invariant feature extraction.

Conclusion: MaskTwins offers a novel, effective UDA paradigm without separate pre-training, advancing domain-adaptive segmentation.

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [34] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: An end-to-end deep neural encoder-decoder model is proposed to encode and decode brain activity from fMRI data, bridging the temporal gap between movie stimuli and fMRI. It predicts visual cortex activity and reconstructs visual inputs, with saliency maps identifying key brain regions.


<details>
  <summary>Details</summary>
Motivation: To bridge the temporal resolution gap between naturalistic stimuli (films) and fMRI data, and to understand visual processing by decoding brain activity.

Method: Uses a deep neural encoder-decoder model with temporal convolutional layers to process consecutive film frames and fMRI data, predicting voxel activity and reconstructing visual inputs.

Result: The model identifies middle occipital, fusiform, and calcarine areas as key contributors, aligning with their roles in shape perception, face recognition, and basic visual features.

Conclusion: The model offers a way to probe visual processing in films using deep learning, suggesting its potential for understanding brain activity.

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [35] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: The paper proposes a decoupling-coupling framework (SS-DC) for unsupervised domain adaptive object detection (UDAOD) from visible to infrared domains, addressing subdomain variations and improving feature separation.


<details>
  <summary>Details</summary>
Motivation: Existing UDAOD methods treat the RGB domain as unified, ignoring subdomains like daytime, nighttime, and foggy scenes, which limits performance. Decoupling domain-invariant (DI) and domain-specific (DS) features is proposed to enhance adaptation.

Method: The SS-DC framework uses a Spectral Adaptive Idempotent Decoupling (SAID) module for spectral decomposition, a filter bank-based paradigm, and self-distillation-driven loss for DI/DS separation. A spatial-spectral coupling method integrates DI features.

Result: The method outperforms existing UDAOD approaches on RGB-IR datasets, including a new FLIR-ADAS protocol, demonstrating significant baseline improvements.

Conclusion: Decoupling and coupling DI/DS features across subdomains effectively improves RGB-IR domain adaptation, validated by superior performance and a novel experimental protocol.

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [36] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: The paper introduces DOV4MM, a method to verify dataset ownership for masked models, addressing a gap in existing techniques tailored for supervised and contrastive models.


<details>
  <summary>Details</summary>
Motivation: Protecting high-quality open-source datasets from exploitation is critical, but current verification methods are unsuitable for masked models.

Method: DOV4MM leverages the difficulty of reconstructing masked information in the embedding space to verify if a model was pre-trained on a target dataset.

Result: Tests on masked image and language models show DOV4MM's effectiveness, rejecting the null hypothesis with a p-value below 0.05.

Conclusion: DOV4MM successfully addresses the challenge of dataset ownership verification for masked models, outperforming prior methods.

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [37] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: Proposes MVAR, a model for multivariate air pollutant forecasting, addressing interactions among pollutants and spatial responses, with improved efficiency and long-term forecasting capability.


<details>
  <summary>Details</summary>
Motivation: Existing models focus on single-pollutant forecasting, ignoring interactions and spatial diversity, which are critical for accurate pollution warnings and policy-making.

Method: Introduces MVAR with a Multivariate Autoregressive Training Paradigm and Meteorological Coupled Spatial Transformer block, leveraging AI-based meteorological forecasts and pollutant interactions.

Result: MVAR outperforms state-of-the-art methods, achieving 120-hour long-term forecasting and efficient data utilization.

Conclusion: MVAR addresses practical needs in air pollutant forecasting, validated by experimental results and a comprehensive dataset.

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [38] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: 3D-MoRe is a framework for generating large-scale 3D-language datasets using foundational models, improving performance in QA and dense captioning tasks.


<details>
  <summary>Details</summary>
Motivation: Address the need for diverse and scalable data in indoor scene tasks like question answering and dense captioning.

Method: Integrates multi-modal embedding, cross-modal interaction, and a language model decoder to process 3D scenes and natural language. Uses data augmentation and semantic filtering for quality.

Result: Generates 62,000 QA pairs and 73,000 object descriptions. Improves CIDEr scores by 2.15% on ScanQA and 1.84% on ScanRefer.

Conclusion: 3D-MoRe effectively enhances reasoning and response generation in 3D environments, with publicly released datasets and code.

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [39] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc is a localization system that regresses camera poses from 3D Gaussian Splatting (3DGS) using semantic info, achieving high accuracy without initial pose priors.


<details>
  <summary>Details</summary>
Motivation: To enable accurate camera pose estimation without requiring prior pose information by leveraging semantic relationships between 2D images and 3D scene representations.

Method: Uses a multi-level pose regression strategy and a semantic-based global retrieval algorithm to align 2D images with 3DGS maps, refining poses iteratively.

Result: Outperforms baselines on 12scenes and 7scenes datasets, excelling in global localization without initial pose priors.

Conclusion: SGLoc effectively estimates camera poses using semantic info and 3DGS, demonstrating robust performance in global localization.

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [40] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: Proposes IICMVNCD, a framework for multi-view novel class discovery, addressing limitations of single-view focus and pseudo-label instability.


<details>
  <summary>Details</summary>
Motivation: Existing NCD methods overlook multi-view data and rely on unstable pseudo-labels, limiting performance.

Method: Uses matrix factorization for intra-view distributional similarity and inter-view weighted fusion of factor matrices for clustering.

Result: Experimental results confirm the effectiveness of the proposed IICMVNCD framework.

Conclusion: IICMVNCD successfully addresses multi-view NCD challenges, improving clustering stability and performance.

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [41] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: MoViAD is a modular library for Video Anomaly Detection (VAD), offering state-of-the-art models, datasets, and utilities to accelerate research and deployment in unsupervised and challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: The scarcity of anomalous data and the need for unsupervised training in VAD pose significant challenges. MoViAD aims to simplify and speed up research and deployment in this field.

Method: MoViAD provides a modular framework with tools for training, evaluation, and deployment, including support for Edge/IoT settings, quantization, and compression.

Result: The library integrates diverse backbones, robust metrics, and profiling tools, enabling fast deployment and flexibility for both engineers and researchers.

Conclusion: MoViAD facilitates efficient VAD research and deployment, addressing practical challenges while offering extensibility for future advancements.

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [42] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: InstructFLIP is a novel framework for face anti-spoofing (FAS) that uses vision-language models (VLMs) and meta-domain learning to improve generalization and reduce training redundancy.


<details>
  <summary>Details</summary>
Motivation: Addressing limited semantic understanding of attack types and training redundancy across domains in FAS.

Method: Integrates VLMs for better visual input perception and employs a meta-domain strategy for unified learning. The framework decouples instructions into content (spoofing semantics) and style (environment/camera variations).

Result: Outperforms state-of-the-art models in accuracy and significantly reduces training redundancy.

Conclusion: InstructFLIP effectively enhances FAS generalization and efficiency, validated by extensive experiments.

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [43] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: MS-DETR is a framework for Video Moment Retrieval and Highlight Detection, leveraging motion-semantics relationships and contrastive denoising to outperform state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To harness untapped potential in the relationships between temporal motion and spatial semantics in video content for MR/HD tasks.

Method: Proposes MS-DETR, which models intra-modal correlations in motion and semantics, uses task-wise correlations for localization, and employs contrastive denoising to address dataset sparsity.

Result: Outperforms existing models on four MR/HD benchmarks.

Conclusion: MS-DETR effectively captures motion-semantics features and addresses dataset sparsity, achieving superior performance.

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [44] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: The paper proposes a 'First Reasoning, Then Forecasting' strategy for motion forecasting in autonomous driving, using an interpretable, reward-driven intention reasoner and a hierarchical decoder for accurate trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Motion forecasting is crucial for autonomous driving safety. Existing data-driven approaches lack interpretability and direct trajectory prediction may not account for behavior intentions.

Method: The method involves a query-centric IRL scheme to derive reward distributions, policy rollouts for intention reasoning, and a hierarchical DETR-like decoder for trajectory generation.

Result: The approach significantly improves trajectory prediction confidence and achieves competitive performance on Argoverse and nuScenes datasets.

Conclusion: The proposed strategy enhances interpretability and accuracy in motion forecasting, outperforming state-of-the-art methods.

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [45] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: The paper presents a winning solution for tracking small, agile multi-objects (SMOT) like birds from UAVs, addressing challenges like scarce features, complex motion, and occlusions with innovative detection and tracking methods.


<details>
  <summary>Details</summary>
Motivation: Tracking small, agile objects from UAVs is challenging due to limited appearance features, complex motion, and frequent occlusions. The paper aims to solve these issues for real-world applications.

Method: The solution combines a detection enhancement framework (SliceTrain) for small objects and a robust tracker (OC-SORT with motion direction maintenance and adaptive similarity metrics) independent of appearance.

Result: Achieves state-of-the-art performance with an SO-HOTA score of 55.205 on the SMOT4SB test set.

Conclusion: The framework effectively addresses complex SMOT problems, validated by its top performance, and the code is made publicly available.

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [46] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: The paper introduces a method for 3D vehicle reconstruction from sparse-view inputs, improving Gaussian Splatting with selective photometric loss and DUSt3R for pose estimation, validated on a new dataset.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D vehicle reconstruction is crucial for applications like inspection and urban planning, but existing methods require dense views, limiting practicality.

Method: Enhances Gaussian Splatting with selective photometric loss and DUSt3R for pose estimation, using depth maps and sparse inputs.

Result: Achieves state-of-the-art performance on benchmarks, enabling high-quality reconstructions from sparse views.

Conclusion: The proposed method effectively addresses sparse-view reconstruction challenges, validated by a novel dataset and superior results.

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [47] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: The paper introduces DeepShade, a diffusion-based model for synthesizing shade variations to improve heatwave-aware routing systems, using a dataset of simulated shadows and satellite images.


<details>
  <summary>Details</summary>
Motivation: Heatwaves threaten public health, but current routing systems lack shade information due to noisy satellite imagery and limited training data.

Method: 1) Build a dataset using Blender-based 3D simulations and satellite images. 2) Propose DeepShade, a diffusion model combining RGB and edge features with contrastive learning.

Result: DeepShade improves shade image generation, enabling shade ratio calculations for route planning in Tempe, Arizona.

Conclusion: The work aids urban planning in extreme heat and has practical environmental applications.

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [48] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: Med-OoD improves biomedical segmentation by using Out-of-Distribution (OoD) data supervision without external data, feature regularization, or extra annotations, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Biomedical segmentation networks often misclassify pixels due to limited and imperfect datasets. OoD data, effective in other tasks, is leveraged to address this.

Method: Proposes Med-OoD, a framework integrating OoD data supervision into fully-supervised segmentation without architectural changes or additional requirements.

Result: Med-OoD reduces misclassification and improves performance, achieving 76.1% mIoU when trained solely on OoD data.

Conclusion: Med-OoD demonstrates the potential of OoD data in medical segmentation, suggesting a new learning paradigm for the field.

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [49] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: A novel method for generating adversarial faces exploits FRS feature space subspheres, achieving high success rates with minimal queries.


<details>
  <summary>Details</summary>
Motivation: Address security and privacy threats posed by adversarial attacks on face recognition systems used for identity verification.

Method: Leverages structural characteristics of FRS feature space, specifically attributed subspheres, to generate adversarial faces with minimal queries.

Result: Achieves over 93% success rate against AWS's CompareFaces API with just one non-adaptive query of 100 images.

Conclusion: The method is efficient, non-adaptive, and capable of producing adversarial faces with chosen attributes, outperforming existing approaches.

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [50] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: LidarPainter is a real-time diffusion model for high-fidelity driving scene reconstruction, outperforming existing methods in speed, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for dynamic driving scene reconstruction suffer from inconsistency, deformation, and inefficiency when deviating from input trajectories.

Method: Proposes LidarPainter, a one-step diffusion model that recovers consistent views from sparse LiDAR data and corrupted renderings.

Result: LidarPainter is 7x faster than StreetCrafter, uses 1/5 GPU memory, and supports stylized generation via text prompts.

Conclusion: LidarPainter enables efficient, high-quality, and versatile driving scene reconstruction.

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [51] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: OVIGo-3DHSG is a method for open-vocabulary indoor object grounding using a 3D hierarchical scene graph, integrating foundation models and LLMs for spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance spatial understanding and object grounding in complex indoor environments by leveraging hierarchical representations and open-vocabulary models.

Method: Uses a 3D hierarchical scene graph derived from RGB-D data, combined with open-vocabulary foundation models and a Large Language Model for multistep reasoning.

Result: Demonstrates efficient scene comprehension and robust object grounding, outperforming existing methods.

Conclusion: OVIGo-3DHSG shows strong potential for applications requiring spatial reasoning in indoor environments.

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [52] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: BSPF-ViT introduces a block-based symmetric pruning and fusion method to jointly optimize Q/K token pruning in ViTs, improving accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Existing ViT pruning methods degrade performance by independently pruning Q/K tokens and ignoring token interactions.

Method: BSPF-ViT jointly prunes Q/K tokens, evaluates token interactions, and compresses retained tokens via similarity fusion. It leverages symmetric attention matrices for efficiency.

Result: BSPF-ViT outperforms state-of-the-art methods, increasing ImageNet accuracy by 1.3% on DeiT-T and 2.0% on DeiT-S, while reducing computation by 50% and achieving 40% speedup.

Conclusion: BSPF-ViT effectively balances accuracy and efficiency in ViTs by addressing token interaction and symmetric pruning.

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [53] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: A new framework, BPAM, combines bilateral grids and MLPs for image enhancement, addressing limitations of linear transformations and global parameter sharing.


<details>
  <summary>Details</summary>
Motivation: Existing methods are limited to linear transformations and struggle with localized variations, prompting the need for a more flexible approach.

Method: BPAM uses bilateral grids with MLP parameters, dynamically retrieving unique transformations per pixel and employing grid decomposition for efficient parameter usage.

Result: The method outperforms state-of-the-art techniques on public datasets while maintaining real-time processing.

Conclusion: BPAM effectively integrates spatial and non-linear modeling, offering superior performance in image enhancement.

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [54] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: AD-GS is a self-supervised framework for high-quality free-viewpoint rendering of driving scenes, using a novel motion model and simplified pseudo 2D segmentation, outperforming annotation-free methods and competing with annotation-dependent ones.


<details>
  <summary>Details</summary>
Motivation: Current methods for dynamic urban driving scenes rely on costly manual annotations or fail in accuracy and scene decomposition, leading to rendering artifacts.

Method: AD-GS integrates a learnable motion model (locality-aware B-spline curves and global-aware trigonometric functions), pseudo 2D segmentation, dynamic Gaussians, and bidirectional temporal visibility masks, with visibility reasoning and rigid regularization.

Result: The model outperforms state-of-the-art annotation-free methods and competes with annotation-dependent approaches.

Conclusion: AD-GS offers a robust, annotation-free solution for high-quality rendering of dynamic driving scenes.

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [55] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: A data-driven method using normalizing flows (RealNVP) to model a neural prior over human body poses in 6D rotation format, ensuring stable learning and compatibility with rotation-based frameworks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling distributions on the manifold of valid 6D rotations, providing a flexible and principled alternative to heuristic or low-expressivity methods.

Method: Leverages RealNVP to learn a density over poses, inverts the Gram-Schmidt process for stable training, and ensures framework-agnostic reproducibility.

Result: Demonstrated effectiveness through qualitative and quantitative evaluations, with ablation studies validating the impact.

Conclusion: Provides a probabilistic foundation for integrating pose priors into human motion capture and reconstruction pipelines.

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [56] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: The paper introduces TGDA, a novel training framework for fine-grained image recognition (FGIR) that eliminates reliance on pretrained models, enabling task-specific architectures and efficient training from scratch.


<details>
  <summary>Details</summary>
Motivation: Existing FGIR methods depend on pretrained models, limiting adaptability and task-specific design. The goal is to develop a framework for high-performance FGIR without pretraining.

Method: TGDA integrates data-aware augmentation and weak supervision via a fine-grained-aware teacher model using knowledge distillation. It supports task-specific architectures like LRNets and ViTFS.

Result: TGDA matches or outperforms pretrained models on FGIR benchmarks, with LRNets improving accuracy by 23% and reducing parameters by 20.6x. ViTFS-T matches ViT B-16 performance with 15.3x fewer parameters.

Conclusion: TGDA offers an efficient, adaptable alternative to pretraining, enabling more efficient fine-grained vision systems.

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [57] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: A double ensembling framework combining pre-trained DL models and fine-tuned ML classifiers improves brain tumor classification accuracy in MRI images.


<details>
  <summary>Details</summary>
Motivation: Human evaluation of MRI images can lead to diagnostic errors due to fatigue, limited expertise, or insufficient detail. This study aims to enhance precision by leveraging AI.

Method: The approach involves preprocessing, augmentation, transfer learning with pre-trained DL models (CNNs and vision transformers), and fine-tuning ML classifiers. Ensembles of features and classifiers are tested on Kaggle datasets.

Result: The proposed framework outperforms existing methods, with hyperparameter fine-tuning significantly boosting performance. An ablation study highlights each component's contribution.

Conclusion: The double ensembling framework enhances brain tumor classification accuracy, demonstrating the value of combining DL and ML techniques.

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [58] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: A wavelet-based method decouples feature space for low-light stereo image enhancement, using low-frequency for illumination and high-frequency for texture, with cross-view interaction and detail enhancement modules.


<details>
  <summary>Details</summary>
Motivation: Existing methods entangle degradation factors in a single latent space, leading to shortcut learning. Wavelet transform allows independent processing of low- and high-frequency information for better enhancement.

Method: Uses wavelet transform to decompose feature space into low-frequency (illumination) and high-frequency (texture) branches. Introduces HF-CIM for cross-view interaction and DTEM for detail enhancement.

Result: Significant advantages in light adjustment and high-frequency recovery on real and synthetic images.

Conclusion: The proposed method effectively decouples feature space and enhances both illumination and texture, outperforming existing approaches.

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [59] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: The paper introduces three advanced techniques (Fractal Convolution, Self-Sensitive Tile Filling, and Super Resolution) for preserving Indian cultural heritage using AI and image processing, balancing tradition and innovation.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of preserving Indian monuments by integrating modern digitized approaches like AI and computer vision into multidisciplinary projects.

Method: Proposes Fractal Convolution for architectural pattern segmentation, Self-Sensitive Tile Filling (SSTF) for terracotta temples, and Super Resolution for image upscaling, with a novel MosaicSlice data augmentation technique.

Result: Achieves seamless region-filling, detailed tile preservation, and high-quality image upscaling at affordable costs, enhancing cultural heritage protection.

Conclusion: The study advances heritage preservation with efficient, automated solutions that maintain authenticity and aesthetic excellence, bridging tradition and innovation.

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [60] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: RODS is a new diffusion sampling method that detects and corrects hallucinations using optimization-inspired techniques, improving fidelity and robustness without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from hallucinations due to score approximation errors, impacting sampling quality.

Method: RODS uses geometric cues from the loss landscape to detect and correct high-risk steps, enforcing smoother trajectories and adaptive perturbations.

Result: RODS detects 70% of hallucinations and corrects 25%, enhancing fidelity and robustness on datasets like AFHQv2 and FFHQ.

Conclusion: RODS effectively mitigates hallucinations in diffusion models with minimal overhead, improving sampling quality.

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [61] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: The paper introduces DD-VQA+, an extended VQA dataset with richer attributes, and MGFFD-VLM, a novel forgery detection framework integrating hybrid LoRA and multi-granularity prompt learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs for deepfake detection lack face quality-related attributes and effective training strategies, limiting their accuracy and interpretability.

Method: Extends the VQA dataset to DD-VQA+ and introduces MGFFD-VLM with Attribute-Driven Hybrid LoRA, Multi-Granularity Prompt Learning, and Forgery-Aware Training Strategy.

Result: Achieves superior accuracy in text-based forgery judgment and analysis, outperforming existing methods.

Conclusion: The proposed framework enhances forgery detection and interpretability, addressing limitations of prior methods.

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [62] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix NÃ¼tzel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: Generative text-to-image diffusion models outperform discriminative methods for zero-shot phrase grounding in medical imaging, especially when fine-tuned with domain-specific language models. A novel post-processing technique, Bimodal Bias Merging (BBM), further enhances accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve disease localization in medical imaging by leveraging generative models for phrase grounding, addressing limitations of current discriminative methods.

Method: Uses generative text-to-image diffusion models with cross-attention maps, fine-tuned with a domain-specific language model (CXR-BERT). Introduces BBM for refining cross-attention maps.

Result: Achieves superior zero-shot performance, doubling mIoU scores of discriminative methods. BBM further improves localization accuracy.

Conclusion: Generative models are more effective for phrase grounding in medical imaging, offering robust and interpretable solutions for clinical applications.

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [63] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper introduces a dataset for automated temporal segmentation of static calisthenics skills in videos, proposing a baseline method for skill duration estimation.


<details>
  <summary>Details</summary>
Motivation: To assist athletes and judges by automating the evaluation of static calisthenics skills, addressing a gap in video understanding literature.

Method: Creation of an annotated dataset of static calisthenics skills and a baseline approach for temporal video segmentation.

Result: Feasibility of automated skill segmentation is demonstrated, though improvements are needed.

Conclusion: The study lays groundwork for future automated tools in calisthenics skill evaluation.

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [64] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida NezoviÄ‡,Jalal Romano,Nada MariÄ‡,Medina Kapo,Amila AkagiÄ‡*

Main category: cs.CV

TL;DR: Comparative analysis of CNN performance in Keras, PyTorch, and JAX for medical image classification using PathMNIST, focusing on training efficiency, accuracy, and inference speed.


<details>
  <summary>Details</summary>
Motivation: The comparative performance of deep learning frameworks (Keras, PyTorch, JAX) in medical imaging tasks is underexplored.

Method: Implemented CNNs across Keras, PyTorch, and JAX, evaluated on PathMNIST dataset for training efficiency, accuracy, and inference speed.

Result: Findings reveal trade-offs between computational speed and model accuracy.

Conclusion: Provides insights for researchers and practitioners in medical image analysis on framework selection.

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [65] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno BÃ¼cker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: A deep learning model using early chest X-rays predicts moderate/severe BPD in preterm infants with 0.78 AUROC, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Early prediction of BPD is crucial to avoid unnecessary interventions in low-risk infants, and routine admission radiographs offer a non-invasive solution.

Method: Fine-tuned ResNet-50 on adult chest X-rays, using progressive layer freezing, discriminative learning rates, CutMix augmentation, and linear probing.

Result: Achieved AUROC of 0.78, balanced accuracy of 0.69, and F1-score of 0.67, outperforming ImageNet initialization and traditional IRDS grades.

Conclusion: Domain-specific pretraining and computational efficiency make this approach viable for clinical use and federated learning.

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [66] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: FADE is a novel concept erasure method for text-to-image diffusion models, combining trajectory-aware fine-tuning and adversarial objectives to remove sensitive concepts while preserving model fidelity.


<details>
  <summary>Details</summary>
Motivation: Address privacy and fairness risks in diffusion models by enabling targeted removal of harmful or sensitive concepts without retraining.

Method: FADE uses trajectory-aware fine-tuning and an adversarial objective to minimize mutual information between erased concepts and model outputs.

Result: FADE outperforms baselines (ESD, UCE, MACE, ANT) in concept removal and image quality, improving the harmonic mean by 5-10%.

Conclusion: FADE sets a new standard for safe and fair generative modeling by effectively unlearning specified concepts.

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [67] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: The paper proposes a direct method for calisthenics skill recognition using depth estimation and athlete patch retrieval, bypassing costly pose estimation. It achieves faster inference and higher accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional pose estimation methods for calisthenics skill recognition are computationally expensive and impractical for real-time or mobile use. This work aims to address these limitations.

Method: The approach uses Depth Anything V2 for depth estimation and YOLOv10 for athlete localization, segmenting the subject for classification without pose estimation.

Result: The method is 38.3x faster than skeleton-based approaches and improves classification accuracy (0.837 vs. 0.815).

Conclusion: The proposed pipeline is efficient, accurate, and modular, enabling future enhancements and real-world applications.

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [68] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: The paper argues that diffusion models' success stems from input conditioning and introduces Discrete Latent Code (DLC) for improved fidelity, ease of generation, and compositionality, achieving state-of-the-art results in image generation.


<details>
  <summary>Details</summary>
Motivation: To improve diffusion models by investigating ideal conditioning representations that enhance sample fidelity, ease of generation, and compositionality for out-of-training samples.

Method: Introduces Discrete Latent Code (DLC), a discrete token sequence derived from Simplicial Embeddings, trained with self-supervised learning. Evaluates DLCs in diffusion models for image generation.

Result: DLCs improve generation fidelity, set a new state-of-the-art for unconditional image generation on ImageNet, and enable coherent out-of-distribution samples. Text-to-image generation is also demonstrated.

Conclusion: DLCs are effective for conditioning diffusion models, offering superior fidelity, compositionality, and versatility in generating novel samples, including text-to-image applications.

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [69] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: KeyDiff3D is an unsupervised framework for monocular 3D keypoints estimation using single-view images, leveraging a pretrained multi-view diffusion model for supervision and feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing methods require costly manual annotations or multi-view images. KeyDiff3D aims to enable 3D keypoints estimation from single-view images without such dependencies.

Method: The framework uses a pretrained multi-view diffusion model to generate multi-view images from a single image, providing 3D geometric cues. It also extracts 2D multi-view features and constructs 3D feature volumes from the model's intermediate representations.

Result: KeyDiff3D achieves accurate 3D keypoints estimation and enables manipulation of 3D objects. It performs well on diverse datasets like Human3.6M and Stanford Dogs, demonstrating generalization and accuracy.

Conclusion: KeyDiff3D effectively leverages diffusion models for unsupervised 3D keypoints estimation and manipulation, offering a cost-efficient alternative to traditional methods.

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [70] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet OÄŸuz SaltÄ±k,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: The paper explores Channel-wise Knowledge Distillation (CWD) and Masked Generative Distillation (MGD) to improve lightweight weed detection models, showing notable accuracy gains without added complexity.


<details>
  <summary>Details</summary>
Motivation: Enhancing weed detection for precision agriculture, especially on resource-limited platforms, by differentiating visually similar weed species.

Method: Uses CWD and MGD to transfer knowledge from a teacher model (YOLO11x) to a student model (YOLO11n), tested on a real-world dataset of sugar beet crops and four weed types.

Result: CWD improves mAP50 by 2.5% and MGD by 1.9% over baseline, with successful real-time deployment on embedded devices like Jetson Orin Nano and Raspberry Pi 5.

Conclusion: CWD and MGD are effective, efficient, and practical for improving weed detection in precision agriculture and plant phenotyping.

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [71] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: CueCo combines contrastive learning and clustering for unsupervised visual representation learning, achieving high accuracy on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance unsupervised learning by integrating contrastive learning and clustering for better feature separation and compactness.

Method: Uses two neural networks (query and key) with contrastive loss for inter-class separation and clustering for intra-class compactness.

Result: Achieves 91.40% top-1 accuracy on CIFAR-10, 68.56% on CIFAR-100, and 78.65% on ImageNet-100.

Conclusion: CueCo advances unsupervised visual representation learning by merging contrastive and clustering approaches.

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [72] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: A novel framework, Text-SemiSeg, enhances semi-supervised medical image segmentation using textual data for better visual semantic understanding.


<details>
  <summary>Details</summary>
Motivation: High annotation costs in medical imaging and the underutilization of textual data for 3D tasks motivated the development of a text-driven approach.

Method: The framework includes Text-enhanced Multiplanar Representation (TMR), Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation (DCA) for text-visual interaction and feature enhancement.

Result: Experiments on three datasets show the model outperforms others by effectively integrating textual information.

Conclusion: Text-SemiSeg successfully leverages textual data to improve semi-supervised segmentation, demonstrating robustness and superior performance.

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [73] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: The paper introduces two benchmarks, OD-VIRAT Large and Tiny, for evaluating object detection in surveillance imagery, featuring diverse scenes and rich annotations. It benchmarks state-of-the-art models under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To advance robust human and object detection in surveillance by providing diverse, realistic datasets for training and evaluation.

Method: Creation of two benchmarks with extensive annotations and benchmarking of models like RETMDET, YOLOX, and DETR on challenging surveillance data.

Result: OD-VIRAT Large has 8.7M annotations in 599,996 images; Tiny has 288,901 in 19,860 images. Performance insights for models under complex conditions are provided.

Conclusion: The benchmarks and evaluations set a foundation for developing more efficient and robust object detection architectures in surveillance.

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [74] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: AutoVDC uses Vision-Language Models (VLMs) to automatically detect and clean erroneous annotations in vision datasets for autonomous driving, improving data quality without manual review.


<details>
  <summary>Details</summary>
Motivation: Human annotations are imperfect and costly to review; automating error detection can enhance dataset reliability.

Method: AutoVDC leverages VLMs to identify errors in datasets like KITTI and nuImages, testing with injected errors and comparing VLM performance.

Result: High error detection rates show AutoVDC's effectiveness in improving dataset accuracy.

Conclusion: AutoVDC can significantly enhance large-scale dataset reliability for autonomous driving.

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [75] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: QuRe improves Composed Image Retrieval by reducing false negatives via a reward model and hard negative sampling, aligning better with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods ignore the relevance of non-target images, leading to false negatives and reduced user satisfaction.

Method: Proposes Query-Relevant Retrieval (QuRe) with a reward model and hard negative sampling strategy.

Result: QuRe achieves state-of-the-art performance on FashionIQ and CIRR datasets and aligns best with human preferences on HP-FashionIQ.

Conclusion: QuRe effectively addresses false negatives in CIR, improving retrieval quality and user satisfaction.

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [76] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: InterpIoU replaces handcrafted geometric penalties in bounding box regression with interpolated box IoU, improving small object detection and avoiding box enlargement issues.


<details>
  <summary>Details</summary>
Motivation: Existing IoU-based losses are sensitive to box shape, size, and distribution, leading to suboptimal performance for small objects and undesired behaviors like box enlargement.

Method: Proposes InterpIoU, using interpolated boxes for meaningful gradients, and Dynamic InterpIoU, adjusting interpolation coefficients dynamically.

Result: Outperforms state-of-the-art IoU-based losses on COCO, VisDrone, and PASCAL VOC, especially in small object detection.

Conclusion: InterpIoU and Dynamic InterpIoU effectively address limitations of existing IoU-based losses, enhancing bounding box regression performance.

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [77] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: DVFL-Net is a lightweight video recognition model that uses knowledge distillation and spatiotemporal modulation to balance performance and efficiency, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Transformers for video recognition are computationally expensive, limiting real-time deployment. DVFL-Net addresses this by distilling knowledge from a large model into a compact one.

Method: DVFL-Net employs knowledge distillation and spatiotemporal focal modulation, using forward KL divergence to transfer context from a teacher model (Video-FocalNet Base) to a student model (VFL-Net).

Result: DVFL-Net achieves strong accuracy with lower memory usage and reduced GFLOPs, outperforming benchmarks on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400.

Conclusion: DVFL-Net offers a practical, efficient solution for real-time Human Action Recognition, balancing performance and computational cost.

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [78] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: The paper introduces TA-STGCN, a model for pedestrian intention prediction that integrates traffic signal states and contextual information, outperforming baseline methods by 4.75% in accuracy.


<details>
  <summary>Details</summary>
Motivation: Current models for pedestrian intention estimation often overlook dynamic traffic signals and scene context, which are vital for real-world autonomous vehicle safety.

Method: The proposed TA-STGCN integrates traffic signal states (Red, Yellow, Green) and bounding box size as features, capturing spatial and temporal dependencies in urban environments.

Result: TA-STGCN achieves a 4.75% higher accuracy than the baseline model on the PIE dataset.

Conclusion: The model effectively improves pedestrian intention prediction by incorporating dynamic traffic signals and contextual information.

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [79] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DAM-QA leverages the region-aware capabilities of the Describe Anything Model (DAM) for text-rich Visual Question Answering (VQA), outperforming baselines and narrowing the gap with generalist vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of text-rich VQA, where fine-grained textual extraction is crucial, by utilizing DAM's region-level descriptive capabilities.

Method: Introduces DAM-QA, a framework with a tailored evaluation protocol, aggregating answers from multiple regional views of image content.

Result: Outperforms baseline DAM on six VQA benchmarks, with a 7+ point gain on DocVQA, and achieves top performance among region-aware models.

Conclusion: DAM-like models show promise for text-rich VQA when paired with efficient usage and integration strategies.

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [80] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: An efficient obstacle avoidance pipeline for autonomous vehicles using camera-only perception and Frenet-Pure Pursuit planning, evaluated in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in autonomous navigation by improving obstacle avoidance in complex environments.

Method: Combines YOLOv11 for object detection and Depth Anything V2 for monocular depth estimation, integrated with Frenet-Pure Pursuit planning.

Result: Effective handling of diverse obstacles, demonstrated through real-world testing on a university campus.

Conclusion: The proposed pipeline enhances autonomous navigation safety and efficiency, validated by experimental results.

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [81] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: SENTINEL reduces hallucinations in MLLMs by early intervention at the sentence level, using in-domain preference learning without human annotations, achieving a 90% reduction in hallucinations.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in MLLMs persist despite existing methods, often due to high costs or data mismatches. Early-stage intervention is identified as key.

Method: SENTINEL bootstraps in-domain preference pairs, validates object existence, and trains models with context-aware preference loss (C-DPO).

Result: SENTINEL reduces hallucinations by over 90% and outperforms prior methods on benchmarks.

Conclusion: SENTINEL is a superior, generalizable solution for hallucination mitigation in MLLMs.

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [82] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: RadGazeIntent, a deep learning model, predicts radiologists' diagnostic intent from gaze data, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing models fail to capture the intent behind radiologists' gaze fixations during medical image interpretation.

Method: A transformer-based architecture processes temporal and spatial gaze data to model diagnostic intent, tested on intention-labeled datasets (RadSeq, RadExplore, RadHybrid).

Result: RadGazeIntent outperforms baselines in predicting radiologists' focus on specific findings.

Conclusion: The model successfully interprets radiologists' gaze intent, enhancing understanding of their diagnostic process.

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [83] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos, unifying point tracking, depth, and camera pose estimation into a single, high-performing system.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of modular pipelines by integrating intrinsic connections between point tracking, monocular depth, and camera pose estimation into a unified framework.

Method: Decomposes 3D motion into scene geometry, camera ego-motion, and object motion using a fully differentiable, end-to-end architecture trained on diverse datasets.

Result: Outperforms existing 3D tracking methods by 30% and matches dynamic 3D reconstruction accuracy while running 50x faster.

Conclusion: SpatialTrackerV2 demonstrates superior performance and efficiency by jointly learning geometry and motion from heterogeneous data.

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [84] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: The paper introduces MMHU, a large-scale benchmark for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources.


<details>
  <summary>Details</summary>
Motivation: Understanding human behavior is crucial for safe driving systems, but a comprehensive benchmark for evaluating such understanding is lacking.

Method: The authors propose MMHU, a dataset with 57k human motion clips and 1.73M frames, annotated using a human-in-the-loop pipeline. It includes motion, trajectories, text descriptions, intentions, and safety labels.

Result: The dataset supports multiple tasks like motion prediction, generation, and behavior question answering, providing a broad evaluation suite.

Conclusion: MMHU fills a gap in human behavior analysis for autonomous driving, offering a valuable resource for future research.

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [85] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. GÃ¶tze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: CytoSAE, a sparse autoencoder, is applied to hematology for explainable AI in medical imaging, identifying morphologically relevant concepts and aiding in disease detection.


<details>
  <summary>Details</summary>
Motivation: The lack of tools for explaining inferences in medical imaging foundation models inspired the adaptation of sparse autoencoders (SAEs) for hematology.

Method: CytoSAE, trained on 40,000+ blood cell images, generalizes to diverse datasets, including bone marrow cytology, and identifies validated medical concepts.

Result: CytoSAE detects patient- and disease-specific concepts, aids in AML subtype classification, and matches state-of-the-art performance with added explainability.

Conclusion: CytoSAE successfully bridges the gap in explainable AI for medical imaging, offering sub-cellular insights and validated medical relevance.

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [86] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces PhysX, a framework for physical-grounded 3D asset generation, addressing the lack of physics-aware 3D models. It includes PhysXNet, a physics-annotated dataset, and PhysXGen, an image-to-3D generation model integrating physical properties.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models focus on geometry and textures but neglect physical properties, limiting real-world applications like simulation and embodied AI. This gap motivates the development of PhysX.

Method: 1) PhysXNet: A physics-grounded 3D dataset annotated across five dimensions (scale, material, affordance, kinematics, function) using a human-in-the-loop pipeline. 2) PhysXGen: A dual-branch framework injecting physical knowledge into pre-trained 3D structural space for physics-aware generation.

Result: PhysX achieves superior performance and generalization, producing 3D assets with plausible physical predictions while maintaining geometry quality.

Conclusion: PhysX bridges the gap in physics-aware 3D generation, offering a scalable solution with potential applications in simulation and AI. The framework's code, data, and models will be released to support future research.

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [87] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: CompressedVQA-HDR is a new VQA framework for HDR videos, using Swin Transformer and SigLip 2 for FR and NR models, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing VQA methods lack generalization for diverse video types, especially HDR content.

Method: Uses Swin Transformer (FR) and SigLip 2 (NR) for feature extraction, with pre-training on SDR data and fine-tuning on HDRSDR-VQA.

Result: State-of-the-art performance; CompressedVQA-HDR-FR won first place in IEEE ICME 2025 challenge.

Conclusion: CompressedVQA-HDR effectively addresses HDR video quality assessment challenges.

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [88] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: eess.IV

TL;DR: Unsupervised machine learning identifies liver tissue patterns in MRI to quantify treatment response in diffuse liver disease, outperforming non-imaging measures.


<details>
  <summary>Details</summary>
Motivation: Quantifiable image patterns are needed to guide treatment and develop therapies for diffuse liver disease.

Method: Deep clustering networks encode and cluster liver tissue patches into a low-dimensional latent space to create a tissue vocabulary.

Result: The method identifies treatment-specific liver tissue changes and predicts biopsy-derived features from imaging data, validated on a replication cohort.

Conclusion: The proposed unsupervised learning approach effectively quantifies treatment response and has potential for clinical application.

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [89] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: A benchmark for cortical lesion detection in MS using MRI, leveraging nnU-Net for improved segmentation and detection, with public model availability.


<details>
  <summary>Details</summary>
Motivation: Cortical lesions in MS are diagnostically valuable but clinically underutilized due to MRI subtlety, annotation challenges, and lack of standardized methods.

Method: Multi-centric MRI dataset (656 scans) with expert annotations, using nnU-Net adapted for CL detection, evaluated via out-of-distribution testing.

Result: Achieved F1-scores of 0.64 (in-domain) and 0.5 (out-of-domain), with analysis of model features and errors.

Conclusion: The study highlights data variability and protocol impacts, offering recommendations for clinical adoption, with open-source models for reproducibility.

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>
