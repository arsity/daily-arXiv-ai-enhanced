<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 81]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Structured Captions Improve Prompt Adherence in Text-to-Image Models (Re-LAION-Caption 19M)](https://arxiv.org/abs/2507.05300)
*Nicholas Merchant,Haitz Sáez de Ocáriz Borde,Andrei Cristian Popescu,Carlos Garcia Jurado Suarez*

Main category: cs.CV

TL;DR: Enforcing structured captions in training improves text-to-image model controllability and alignment, demonstrated using Re-LAION-Caption 19M.


<details>
  <summary>Details</summary>
Motivation: Generative models struggle with prompt adherence due to noisy datasets, requiring heavy prompt engineering.

Method: Created Re-LAION-Caption 19M with structured captions (subject, setting, aesthetics, camera details) and fine-tuned PixArt-Σ and Stable Diffusion 2.

Result: Structured captions yielded higher text-image alignment scores in VQA evaluations.

Conclusion: Structured captions enhance model performance and controllability, offering a practical solution for noisy datasets.

Abstract: We argue that generative text-to-image models often struggle with prompt
adherence due to the noisy and unstructured nature of large-scale datasets like
LAION-5B. This forces users to rely heavily on prompt engineering to elicit
desirable outputs. In this work, we propose that enforcing a consistent caption
structure during training can significantly improve model controllability and
alignment. We introduce Re-LAION-Caption 19M, a high-quality subset of
Re-LAION-5B, comprising 19 million 1024x1024 images with captions generated by
a Mistral 7B Instruct-based LLaVA-Next model. Each caption follows a four-part
template: subject, setting, aesthetics, and camera details. We fine-tune
PixArt-$\Sigma$ and Stable Diffusion 2 using both structured and randomly
shuffled captions, and show that structured versions consistently yield higher
text-image alignment scores using visual question answering (VQA) models. The
dataset is publicly available at
https://huggingface.co/datasets/supermodelresearch/Re-LAION-Caption19M.

</details>


### [2] [CorrDetail: Visual Detail Enhanced Self-Correction for Face Forgery Detection](https://arxiv.org/abs/2507.05302)
*Binjia Zhou,Hengrui Lou,Lizhe Chen,Haoyuan Li,Dawei Luo,Shuai Chen,Jie Lei,Zunlei Feng,Yijun Bei*

Main category: cs.CV

TL;DR: CorrDetail is a visual detail-enhanced self-correction framework for interpretable deepfake detection, outperforming existing methods by focusing on authentic forgery details and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: The rise of facial deepfakes demands reliable detection methods. Current approaches either lack interpretability (visual-based) or suffer from hallucinations (multimodal).

Method: CorrDetail uses error-guided questioning to correct forgery details, includes a visual fine-grained detail enhancement module, and employs a fusion decision strategy for better discrimination.

Result: CorrDetail achieves state-of-the-art performance, accurately identifies forged details, and shows strong generalization.

Conclusion: CorrDetail addresses key limitations in deepfake detection, offering interpretability and reliability.

Abstract: With the swift progression of image generation technology, the widespread
emergence of facial deepfakes poses significant challenges to the field of
security, thus amplifying the urgent need for effective deepfake
detection.Existing techniques for face forgery detection can broadly be
categorized into two primary groups: visual-based methods and multimodal
approaches. The former often lacks clear explanations for forgery details,
while the latter, which merges visual and linguistic modalities, is more prone
to the issue of hallucinations.To address these shortcomings, we introduce a
visual detail enhanced self-correction framework, designated CorrDetail, for
interpretable face forgery detection. CorrDetail is meticulously designed to
rectify authentic forgery details when provided with error-guided questioning,
with the aim of fostering the ability to uncover forgery details rather than
yielding hallucinated responses. Additionally, to bolster the reliability of
its findings, a visual fine-grained detail enhancement module is incorporated,
supplying CorrDetail with more precise visual forgery details. Ultimately, a
fusion decision strategy is devised to further augment the model's
discriminative capacity in handling extreme samples, through the integration of
visual information compensation and model bias reduction.Experimental results
demonstrate that CorrDetail not only achieves state-of-the-art performance
compared to the latest methodologies but also excels in accurately identifying
forged details, all while exhibiting robust generalization capabilities.

</details>


### [3] [YOLO-APD: Enhancing YOLOv8 for Robust Pedestrian Detection on Complex Road Geometries](https://arxiv.org/abs/2507.05376)
*Aquino Joctum,John Kandiri*

Main category: cs.CV

TL;DR: YOLO-APD enhances YOLOv8 for pedestrian detection on complex roadways, achieving 77.7% mAP and 96% recall at 100 FPS.


<details>
  <summary>Details</summary>
Motivation: Improving pedestrian detection on geometrically complex roadways where RGB cameras struggle.

Method: Integrates SimAM attention, C3Ghost modules, SimSPPF, Mish activation, and IGD module, with adaptive ROI processing.

Result: 77.7% mAP@0.5:0.95, 96% recall, 100 FPS, outperforming YOLOv8.

Conclusion: YOLO-APD offers accurate, efficient, adaptable perception for autonomous navigation in challenging environments.

Abstract: Autonomous vehicle perception systems require robust pedestrian detection,
particularly on geometrically complex roadways like Type-S curved surfaces,
where standard RGB camera-based methods face limitations. This paper introduces
YOLO-APD, a novel deep learning architecture enhancing the YOLOv8 framework
specifically for this challenge. YOLO-APD integrates several key architectural
modifications: a parameter-free SimAM attention mechanism, computationally
efficient C3Ghost modules, a novel SimSPPF module for enhanced multi-scale
feature pooling, the Mish activation function for improved optimization, and an
Intelligent Gather & Distribute (IGD) module for superior feature fusion in the
network's neck. The concept of leveraging vehicle steering dynamics for
adaptive region-of-interest processing is also presented. Comprehensive
evaluations on a custom CARLA dataset simulating complex scenarios demonstrate
that YOLO-APD achieves state-of-the-art detection accuracy, reaching 77.7%
mAP@0.5:0.95 and exceptional pedestrian recall exceeding 96%, significantly
outperforming baseline models, including YOLOv8. Furthermore, it maintains
real-time processing capabilities at 100 FPS, showcasing a superior balance
between accuracy and efficiency. Ablation studies validate the synergistic
contribution of each integrated component. Evaluation on the KITTI dataset
confirms the architecture's potential while highlighting the need for domain
adaptation. This research advances the development of highly accurate,
efficient, and adaptable perception systems based on cost-effective sensors,
contributing to enhanced safety and reliability for autonomous navigation in
challenging, less-structured driving environments.

</details>


### [4] [Foreground-aware Virtual Staining for Accurate 3D Cell Morphological Profiling](https://arxiv.org/abs/2507.05383)
*Alexandr A. Kalinin,Paula Llanos,Theresa Maria Sommer,Giovanni Sestini,Xinhai Hou,Jonathan Z. Sexton,Xiang Wan,Ivo D. Dinov,Brian D. Athey,Nicolas Rivron,Anne E. Carpenter,Beth Cimini,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: Spotlight improves virtual staining by focusing on biologically meaningful signals using histogram-based foreground estimation and Dice loss, outperforming methods that treat all pixels equally.


<details>
  <summary>Details</summary>
Motivation: Existing virtual staining methods treat all pixels equally, reproducing noise and artifacts instead of focusing on relevant cellular structures.

Method: Spotlight uses histogram-based foreground estimation to mask pixel-wise loss and calculates a Dice loss on soft-thresholded predictions for shape-aware learning.

Result: Applied to a 3D benchmark dataset, Spotlight improves morphological representation while preserving pixel-level accuracy.

Conclusion: Spotlight produces virtual stains better suited for downstream tasks like segmentation and profiling.

Abstract: Microscopy enables direct observation of cellular morphology in 3D, with
transmitted-light methods offering low-cost, minimally invasive imaging and
fluorescence microscopy providing specificity and contrast. Virtual staining
combines these strengths by using machine learning to predict fluorescence
images from label-free inputs. However, training of existing methods typically
relies on loss functions that treat all pixels equally, thus reproducing
background noise and artifacts instead of focusing on biologically meaningful
signals. We introduce Spotlight, a simple yet powerful virtual staining
approach that guides the model to focus on relevant cellular structures.
Spotlight uses histogram-based foreground estimation to mask pixel-wise loss
and to calculate a Dice loss on soft-thresholded predictions for shape-aware
learning. Applied to a 3D benchmark dataset, Spotlight improves morphological
representation while preserving pixel-level accuracy, resulting in virtual
stains better suited for downstream tasks such as segmentation and profiling.

</details>


### [5] [From General to Specialized: The Need for Foundational Models in Agriculture](https://arxiv.org/abs/2507.05390)
*Vishal Nedungadi,Xingguo Xiong,Aike Potze,Ron Van Bree,Tao Lin,Marc Rußwurm,Ioannis N. Athanasiadis*

Main category: cs.CV

TL;DR: The paper evaluates existing foundation models for agricultural tasks, proposes a framework for an ideal agricultural model (CropFM), and highlights the need for a dedicated agricultural foundation model.


<details>
  <summary>Details</summary>
Motivation: Addressing food security challenges by leveraging foundation models for agricultural monitoring, as current applications in agriculture are under-explored.

Method: Quantitative evaluation of existing foundational models, development of a requirements framework (CropFM), and empirical evaluation of two models in three agricultural tasks.

Result: Existing models show potential but lack specialization for agriculture, emphasizing the need for a dedicated model.

Conclusion: A tailored foundational model (CropFM) is necessary to effectively address agricultural challenges like crop mapping, phenology, and yield estimation.

Abstract: Food security remains a global concern as population grows and climate change
intensifies, demanding innovative solutions for sustainable agricultural
productivity. Recent advances in foundation models have demonstrated remarkable
performance in remote sensing and climate sciences, and therefore offer new
opportunities for agricultural monitoring. However, their application in
challenges related to agriculture-such as crop type mapping, crop phenology
estimation, and crop yield estimation-remains under-explored. In this work, we
quantitatively evaluate existing foundational models to assess their
effectivity for a representative set of agricultural tasks. From an
agricultural domain perspective, we describe a requirements framework for an
ideal agricultural foundation model (CropFM). We then survey and compare
existing general-purpose foundational models in this framework and empirically
evaluate two exemplary of them in three representative agriculture specific
tasks. Finally, we highlight the need for a dedicated foundational model
tailored specifically to agriculture.

</details>


### [6] [Enhancing Underwater Images Using Deep Learning with Subjective Image Quality Integration](https://arxiv.org/abs/2507.05393)
*Jose M. Montero,Jose-Luis Lisani*

Main category: cs.CV

TL;DR: A deep learning approach enhances underwater images by integrating human subjective assessments, using GANs trained on expert-labeled datasets, achieving improved quality in both metrics and perception.


<details>
  <summary>Details</summary>
Motivation: To improve underwater image quality by leveraging deep learning and incorporating human subjective evaluations for more effective enhancement.

Method: Train a classifier to distinguish image quality, then use GANs with enhancement criteria (e.g., color fidelity, sharpness) to refine low-quality images.

Result: The model significantly improves image quality, validated by metrics (PSNR, SSIM, UIQM) and qualitative analysis.

Conclusion: Incorporating human assessments and specific enhancement criteria in GANs leads to better underwater image quality.

Abstract: Recent advances in deep learning, particularly neural networks, have
significantly impacted a wide range of fields, including the automatic
enhancement of underwater images. This paper presents a deep learning-based
approach to improving underwater image quality by integrating human subjective
assessments into the training process. To this end, we utilize publicly
available datasets containing underwater images labeled by experts as either
high or low quality. Our method involves first training a classifier network to
distinguish between high- and low-quality images. Subsequently, generative
adversarial networks (GANs) are trained using various enhancement criteria to
refine the low-quality images. The performance of the GAN models is evaluated
using quantitative metrics such as PSNR, SSIM, and UIQM, as well as through
qualitative analysis. Results demonstrate that the proposed model --
particularly when incorporating criteria such as color fidelity and image
sharpness -- achieves substantial improvements in both perceived and measured
image quality.

</details>


### [7] [pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models](https://arxiv.org/abs/2507.05394)
*Sajjad Ghiasvand,Mahnoosh Alizadeh,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: pFedMMA is a personalized federated learning framework using multi-modal adapters for vision-language tasks, balancing personalization and generalization efficiently.


<details>
  <summary>Details</summary>
Motivation: Adapting Vision-Language Models (VLMs) like CLIP to decentralized, heterogeneous data without sacrificing generalization is challenging. Existing methods prioritize personalization over generalization.

Method: pFedMMA employs multi-modal adapters with modality-specific layers and a globally shared projection. An asymmetric optimization strategy allows local adaptation while improving global generalization.

Result: Experiments on eleven datasets show pFedMMA achieves state-of-the-art trade-offs between personalization and generalization, outperforming federated prompt tuning methods.

Conclusion: pFedMMA effectively addresses the challenge of adapting VLMs in federated learning, offering a communication-efficient solution with superior performance.

Abstract: Vision-Language Models (VLMs) like CLIP have demonstrated remarkable
generalization in zero- and few-shot settings, but adapting them efficiently to
decentralized, heterogeneous data remains a challenge. While prompt tuning has
emerged as a popular parameter-efficient approach in personalized federated
learning, existing methods often sacrifice generalization in favor of
personalization, struggling particularly on unseen classes or domains. In this
work, we propose pFedMMA, the first personalized federated learning framework
that leverages multi-modal adapters for vision-language tasks. Each adapter
contains modality-specific up- and down-projection layers alongside a globally
shared projection that aligns cross-modal features. Our asymmetric optimization
strategy allows clients to locally adapt to personalized data distributions
while collaboratively training the shared projection to improve global
generalization. This design is also communication-efficient, as only the shared
component is exchanged during rounds. Through extensive experiments across
eleven datasets, including domain- and label-shift scenarios, we show that
pFedMMA achieves state-of-the-art trade-offs between personalization and
generalization, outperforming recent federated prompt tuning methods. The code
is available at https://github.com/sajjad-ucsb/pFedMMA.

</details>


### [8] [Neural-Driven Image Editing](https://arxiv.org/abs/2507.05397)
*Pengfei Zhou,Jie Xia,Xiaopeng Peng,Wangbo Zhao,Zilong Ye,Zekai Li,Suorong Yang,Jiadong Pan,Yuanxiang Chen,Ziqiao Wang,Kai Wang,Qian Zheng,Xiaojun Chang,Gang Pan,Shurong Dong,Kaipeng Zhang,Yang You*

Main category: cs.CV

TL;DR: LoongX is a hands-free image editing system using neurophysiological signals (EEG, fNIRS, PPG, and head motion) to drive generative models, achieving performance comparable to text-driven methods.


<details>
  <summary>Details</summary>
Motivation: Traditional image editing is labor-intensive and inaccessible to some users. LoongX aims to make it more accessible by leveraging brain-computer interfaces and generative models.

Method: LoongX uses diffusion models trained on a dataset of image editing pairs with synchronized neurophysiological signals. It integrates CS3 and DGF modules for feature encoding and fusion, aligning with edit semantics via fine-tuning on a diffusion transformer.

Result: LoongX matches text-driven methods (CLIP-I: 0.6605 vs. 0.6558) and outperforms them when combined with speech (CLIP-T: 0.2588 vs. 0.2549).

Conclusion: LoongX demonstrates the potential of neural-driven generative models for accessible image editing and cognitive-driven creative technologies.

Abstract: Traditional image editing typically relies on manual prompting, making it
labor-intensive and inaccessible to individuals with limited motor control or
language abilities. Leveraging recent advances in brain-computer interfaces
(BCIs) and generative models, we propose LoongX, a hands-free image editing
approach driven by multimodal neurophysiological signals. LoongX utilizes
state-of-the-art diffusion models trained on a comprehensive dataset of 23,928
image editing pairs, each paired with synchronized electroencephalography
(EEG), functional near-infrared spectroscopy (fNIRS), photoplethysmography
(PPG), and head motion signals that capture user intent. To effectively address
the heterogeneity of these signals, LoongX integrates two key modules. The
cross-scale state space (CS3) module encodes informative modality-specific
features. The dynamic gated fusion (DGF) module further aggregates these
features into a unified latent space, which is then aligned with edit semantics
via fine-tuning on a diffusion transformer (DiT). Additionally, we pre-train
the encoders using contrastive learning to align cognitive states with semantic
intentions from embedded natural language. Extensive experiments demonstrate
that LoongX achieves performance comparable to text-driven methods (CLIP-I:
0.6605 vs. 0.6558; DINO: 0.4812 vs. 0.4636) and outperforms them when neural
signals are combined with speech (CLIP-T: 0.2588 vs. 0.2549). These results
highlight the promise of neural-driven generative models in enabling
accessible, intuitive image editing and open new directions for
cognitive-driven creative technologies. Datasets and code will be released to
support future work and foster progress in this emerging area.

</details>


### [9] [Motion Generation: A Survey of Generative Approaches and Benchmarks](https://arxiv.org/abs/2507.05419)
*Aliasghar Khani,Arianna Rampini,Bruno Roy,Larasika Nadela,Noa Kaplan,Evan Atherton,Derek Cheung,Jacky Bibliowicz*

Main category: cs.CV

TL;DR: A survey categorizing recent motion generation methods by generative strategies, focusing on advancements since 2023, analyzing architectures, conditioning, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The rapid progress in motion generation with diverse modeling paradigms necessitates a structured review to compare approaches and identify challenges.

Method: Categorizes methods by generative strategies, reviews top-tier papers since 2023, and analyzes architectures, conditioning, and evaluation metrics.

Result: Provides a detailed overview of current methods, enabling clearer comparisons and highlighting open challenges.

Conclusion: Offers a foundational reference for researchers to navigate the evolving field of motion generation.

Abstract: Motion generation, the task of synthesizing realistic motion sequences from
various conditioning inputs, has become a central problem in computer vision,
computer graphics, and robotics, with applications ranging from animation and
virtual agents to human-robot interaction. As the field has rapidly progressed
with the introduction of diverse modeling paradigms including GANs,
autoencoders, autoregressive models, and diffusion-based techniques, each
approach brings its own advantages and limitations. This growing diversity has
created a need for a comprehensive and structured review that specifically
examines recent developments from the perspective of the generative approach
employed.
  In this survey, we provide an in-depth categorization of motion generation
methods based on their underlying generative strategies. Our main focus is on
papers published in top-tier venues since 2023, reflecting the most recent
advancements in the field. In addition, we analyze architectural principles,
conditioning mechanisms, and generation settings, and compile a detailed
overview of the evaluation metrics and datasets used across the literature. Our
objective is to enable clearer comparisons and identify open challenges,
thereby offering a timely and foundational reference for researchers and
practitioners navigating the rapidly evolving landscape of motion generation.

</details>


### [10] [Mastering Regional 3DGS: Locating, Initializing, and Editing with Diverse 2D Priors](https://arxiv.org/abs/2507.05426)
*Lanqing Guo,Yufei Wang,Hezhen Hu,Yan Zheng,Yeying Jin,Siyu Huang,Zhangyang Wang*

Main category: cs.CV

TL;DR: A method for precise 3D scene local editing using 2D diffusion and inverse rendering, achieving state-of-the-art performance with a 4× speedup.


<details>
  <summary>Details</summary>
Motivation: 3D semantic parsing underperforms compared to 2D, making targeted 3D edits difficult. This work aims to improve fidelity and control in 3D scene editing.

Method: Leverages 2D diffusion for region identification, inverse rendering for 3D localization, and refines views iteratively with depth maps from a 2D model.

Result: Achieves state-of-the-art performance with a 4× speedup.

Conclusion: The method provides an efficient and effective approach for precise 3D local editing, enhancing coherence across perspectives.

Abstract: Many 3D scene editing tasks focus on modifying local regions rather than the
entire scene, except for some global applications like style transfer, and in
the context of 3D Gaussian Splatting (3DGS), where scenes are represented by a
series of Gaussians, this structure allows for precise regional edits, offering
enhanced control over specific areas of the scene; however, the challenge lies
in the fact that 3D semantic parsing often underperforms compared to its 2D
counterpart, making targeted manipulations within 3D spaces more difficult and
limiting the fidelity of edits, which we address by leveraging 2D diffusion
editing to accurately identify modification regions in each view, followed by
inverse rendering for 3D localization, then refining the frontal view and
initializing a coarse 3DGS with consistent views and approximate shapes derived
from depth maps predicted by a 2D foundation model, thereby supporting an
iterative, view-consistent editing process that gradually enhances structural
details and textures to ensure coherence across perspectives. Experiments
demonstrate that our method achieves state-of-the-art performance while
delivering up to a $4\times$ speedup, providing a more efficient and effective
approach to 3D scene local editing.

</details>


### [11] [OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts](https://arxiv.org/abs/2507.05427)
*Shiting Xiao,Rishabh Kabra,Yuhang Li,Donghyun Lee,Joao Carreira,Priyadarshini Panda*

Main category: cs.CV

TL;DR: OpenWorldSAM extends SAM2 for open-vocabulary segmentation using multi-modal embeddings, achieving efficiency, instance awareness, and strong generalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of segmenting objects based on diverse and unseen language prompts by grounding textual semantics into precise spatial masks.

Method: Integrates multi-modal embeddings from a lightweight VLM into SAM2, with frozen pre-trained components, training only 4.5M parameters. Uses positional tie-breaker embeddings and cross-attention for instance awareness.

Result: Achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across benchmarks like ADE20k, PASCAL, ScanNet, and SUN-RGBD.

Conclusion: OpenWorldSAM successfully extends SAM2 for open-vocabulary tasks with efficiency, flexibility, and strong zero-shot generalization.

Abstract: The ability to segment objects based on open-ended language prompts remains a
critical challenge, requiring models to ground textual semantics into precise
spatial masks while handling diverse and unseen categories. We present
OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model
v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings
extracted from a lightweight vision-language model (VLM). Our approach is
guided by four key principles: i) Unified prompting: OpenWorldSAM supports a
diverse range of prompts, including category-level and sentence-level language
descriptions, providing a flexible interface for various segmentation tasks.
ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we
train only 4.5 million parameters on the COCO-stuff dataset, achieving
remarkable resource efficiency. iii) Instance Awareness: We enhance the model's
spatial understanding through novel positional tie-breaker embeddings and
cross-attention layers, enabling effective segmentation of multiple instances.
iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities,
generalizing well on unseen categories and an open vocabulary of concepts
without additional training. Extensive experiments demonstrate that
OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic,
instance, and panoptic segmentation across multiple benchmarks, including
ADE20k, PASCAL, ScanNet, and SUN-RGBD.

</details>


### [12] [Robotic System with AI for Real Time Weed Detection, Canopy Aware Spraying, and Droplet Pattern Evaluation](https://arxiv.org/abs/2507.05432)
*Inayat Rasool,Pappu Kumar Yadav,Amee Parmar,Hasan Mirzakhaninafchi,Rikesh Budhathoki,Zain Ul Abideen Usmani,Supriya Paudel,Ivan Perez Olivera,Eric Jone*

Main category: cs.CV

TL;DR: An AI-driven variable rate sprayer system was developed to reduce herbicide overuse by detecting weeds and adjusting spray in real time, showing promising results in indoor trials.


<details>
  <summary>Details</summary>
Motivation: Address the issues of high herbicide costs, environmental pollution, and herbicide-resistant weeds caused by uniform herbicide application.

Method: Integration of YOLO11n and YOLO11n-seg models on NVIDIA Jetson Orin Nano for real-time weed detection and canopy segmentation, coupled with Arduino-controlled nozzles.

Result: High precision (0.99) and recall (~1.0) for detection (mAP@50: 0.98), and spray coverage adjusted by canopy size (16.22% to 21.65%).

Conclusion: The system demonstrates potential for selective herbicide application using deep learning and low-cost hardware, with plans to expand weed detection and field validation.

Abstract: Uniform and excessive herbicide application in modern agriculture contributes
to increased input costs, environmental pollution, and the emergence of
herbicide resistant weeds. To address these challenges, we developed a vision
guided, AI-driven variable rate sprayer system capable of detecting weed
presence, estimating canopy size, and dynamically adjusting nozzle activation
in real time. The system integrates lightweight YOLO11n and YOLO11n-seg deep
learning models, deployed on an NVIDIA Jetson Orin Nano for onboard inference,
and uses an Arduino Uno-based relay interface to control solenoid actuated
nozzles based on canopy segmentation results. Indoor trials were conducted
using 15 potted Hibiscus rosa sinensis plants of varying canopy sizes to
simulate a range of weed patch scenarios. The YOLO11n model achieved a mean
average precision (mAP@50) of 0.98, with a precision of 0.99 and a recall close
to 1.0. The YOLO11n-seg segmentation model achieved a mAP@50 of 0.48, precision
of 0.55, and recall of 0.52. System performance was validated using water
sensitive paper, which showed an average spray coverage of 24.22% in zones
where canopy was present. An upward trend in mean spray coverage from 16.22%
for small canopies to 21.46% and 21.65% for medium and large canopies,
respectively, demonstrated the system's capability to adjust spray output based
on canopy size in real time. These results highlight the potential of combining
real time deep learning with low-cost embedded hardware for selective herbicide
application. Future work will focus on expanding the detection capabilities to
include three common weed species in South Dakota: water hemp (Amaranthus
tuberculatus), kochia (Bassia scoparia), and foxtail (Setaria spp.), followed
by further validation in both indoor and field trials within soybean and corn
production systems.

</details>


### [13] [Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video](https://arxiv.org/abs/2507.05463)
*Md Zahid Hasan,Guillermo Basulto-Elias,Jun Ha Chang,Sahuna Hallmark,Matthew Rizzo,Anuj Sharma,Soumik Sarkar*

Main category: cs.CV

TL;DR: A framework using large vision models and naturalistic driving videos to identify cognitive decline in older drivers, enabling early detection and proactive intervention.


<details>
  <summary>Details</summary>
Motivation: Current diagnostic methods for cognitive decline (e.g., Alzheimer's, MCI) are costly and time-consuming. Real-world driving behavior offers a scalable, non-invasive alternative for early detection.

Method: Analyzes driving videos with large vision models to extract 'digital fingerprints' correlating with cognitive decline, classifying status and predicting progression.

Result: Identifies early warning signs of cognitive impairment, supporting scalable monitoring systems.

Conclusion: Enhances early detection of cognitive decline, reducing societal and economic burdens through proactive interventions.

Abstract: We introduce scenario-based cognitive status identification in older drivers
from Naturalistic driving videos and large vision models. In recent times,
cognitive decline, including Alzheimer's disease (AD) and mild cognitive
impairment (MCI), is often underdiagnosed due to the time-consuming and costly
nature of current diagnostic methods. By analyzing real-world driving behavior
captured through in-vehicle systems, this research aims to extract "digital
fingerprints" that correlate with functional decline and clinical features of
MCI and AD. Moreover, modern large vision models can draw meaningful insights
from everyday driving patterns of older patients to early detect cognitive
decline. We propose a framework that uses large vision models and naturalistic
driving videos to analyze driver behavior, classify cognitive status and
predict disease progression. We leverage the strong relationship between
real-world driving behavior as an observation of the current cognitive status
of the drivers where the vehicle can be utilized as a "diagnostic tool". Our
method identifies early warning signs of functional impairment, contributing to
proactive intervention strategies. This work enhances early detection and
supports the development of scalable, non-invasive monitoring systems to
mitigate the growing societal and economic burden of cognitive decline in the
aging population.

</details>


### [14] [Cloud Diffusion Part 1: Theory and Motivation](https://arxiv.org/abs/2507.05496)
*Andrew Randono*

Main category: cs.CV

TL;DR: The paper introduces "Cloud Diffusion Models," which replace white noise with scale-invariant noise in diffusion models for image generation, promising faster inference, better details, and more control.


<details>
  <summary>Details</summary>
Motivation: Natural images exhibit scale invariance, unlike the white noise used in traditional diffusion models, suggesting a need for noise profiles that align better with natural image statistics.

Method: Proposes using scale-invariant noise profiles in diffusion models instead of white noise, forming "Cloud Diffusion Models."

Result: Expected benefits include faster inference, improved high-frequency details, and greater controllability.

Conclusion: A follow-up paper will implement and compare Cloud Diffusion Models with traditional white noise models to validate these claims.

Abstract: Diffusion models for image generation function by progressively adding noise
to an image set and training a model to separate out the signal from the noise.
The noise profile used by these models is white noise -- that is, noise based
on independent normal distributions at each point whose mean and variance is
independent of the scale. By contrast, most natural image sets exhibit a type
of scale invariance in their low-order statistical properties characterized by
a power-law scaling. Consequently, natural images are closer (in a quantifiable
sense) to a different probability distribution that emphasizes large scale
correlations and de-emphasizes small scale correlations. These scale invariant
noise profiles can be incorporated into diffusion models in place of white
noise to form what we will call a ``Cloud Diffusion Model". We argue that these
models can lead to faster inference, improved high-frequency details, and
greater controllability. In a follow-up paper, we will build and train a Cloud
Diffusion Model that uses scale invariance at a fundamental level and compare
it to classic, white noise diffusion models.

</details>


### [15] [LoomNet: Enhancing Multi-View Image Generation via Latent Space Weaving](https://arxiv.org/abs/2507.05499)
*Giulio Federico,Fabio Carrara,Claudio Gennaro,Giuseppe Amato,Marco Di Benedetto*

Main category: cs.CV

TL;DR: LoomNet is a multi-view diffusion architecture that generates consistent multi-view images from a single input, improving 3D mesh quality by leveraging a shared latent space for view consistency.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of spatial inconsistency in multi-view image generation, which degrades 3D mesh quality in surface reconstruction.

Method: Uses parallel diffusion models to collaboratively build a shared latent space, fusing encodings from multiple views into aggregated planes for coherent rendering.

Result: Generates 16 high-quality, coherent views in 15 seconds, outperforming state-of-the-art methods in image quality and reconstruction metrics.

Conclusion: LoomNet effectively produces diverse, plausible novel views while maintaining consistency, advancing multi-view image generation.

Abstract: Generating consistent multi-view images from a single image remains
challenging. Lack of spatial consistency often degrades 3D mesh quality in
surface reconstruction. To address this, we propose LoomNet, a novel multi-view
diffusion architecture that produces coherent images by applying the same
diffusion model multiple times in parallel to collaboratively build and
leverage a shared latent space for view consistency. Each viewpoint-specific
inference generates an encoding representing its own hypothesis of the novel
view from a given camera pose, which is projected onto three orthogonal planes.
For each plane, encodings from all views are fused into a single aggregated
plane. These aggregated planes are then processed to propagate information and
interpolate missing regions, combining the hypotheses into a unified, coherent
interpretation. The final latent space is then used to render consistent
multi-view images. LoomNet generates 16 high-quality and coherent views in just
15 seconds. In our experiments, LoomNet outperforms state-of-the-art methods on
both image quality and reconstruction metrics, also showing creativity by
producing diverse, plausible novel views from the same input.

</details>


### [16] [Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model](https://arxiv.org/abs/2507.05513)
*Mengyao Xu,Gabriel Moreira,Ronay Ak,Radek Osmulski,Yauhen Babakhin,Zhiding Yu,Benedikt Schifferer,Even Oldridge*

Main category: cs.CV

TL;DR: The paper introduces llama-nemoretriever-colembed, a state-of-the-art unified text-image retrieval model with two variants (1B and 3B), achieving top performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the demand for cross-modal retrieval systems by improving accuracy and efficiency.

Method: Modifies NVIDIA Eagle2 VLM with bidirectional attention and integrates ColBERT-style late interaction for fine-grained retrieval. Uses a two-stage training strategy.

Result: The 3B model scores NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, leading benchmarks as of June 2025.

Conclusion: The model excels in retrieval accuracy but faces trade-offs in storage and efficiency, analyzed comprehensively.

Abstract: Motivated by the growing demand for retrieval systems that operate across
modalities, we introduce llama-nemoretriever-colembed, a unified text-image
retrieval model that delivers state-of-the-art performance across multiple
benchmarks. We release two model variants, 1B and 3B. The 3B model achieves
state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on
ViDoRe V2, placing first on both leaderboards as of June 27, 2025.
  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM),
modifies its architecture by replacing causal attention with bidirectional
attention, and integrates a ColBERT-style late interaction mechanism to enable
fine-grained multimodal retrieval in a shared embedding space. While this
mechanism delivers superior retrieval accuracy, it introduces trade-offs in
storage and efficiency. We provide a comprehensive analysis of these
trade-offs. Additionally, we adopt a two-stage training strategy to enhance the
model's retrieval capabilities.

</details>


### [17] [Simulating Refractive Distortions and Weather-Induced Artifacts for Resource-Constrained Autonomous Perception](https://arxiv.org/abs/2507.05536)
*Moseli Mots'oehli,Feimei Chen,Hok Wai Chan,Itumeleng Tlali,Thulani Babeli,Kyungim Baek,Huaijin Chen*

Main category: cs.CV

TL;DR: A pipeline for augmenting low-cost dashcam footage with realistic distortions and weather effects for African driving scenarios, addressing dataset scarcity in developing regions.


<details>
  <summary>Details</summary>
Motivation: The lack of autonomous vehicle datasets from Africa's diverse roads hinders robust perception in low-resource settings.

Method: Procedural augmentation pipeline with refractive (lens distortion, noise, warps) and weather (fog, lens flare) modules. Evaluated using three image restoration models.

Result: Augmented dataset splits, distortion toolkit, and benchmark results released to support perception research in underrepresented African contexts.

Conclusion: The toolkit enables cost-effective perception research in Africa without expensive data collection or simulation.

Abstract: The scarcity of autonomous vehicle datasets from developing regions,
particularly across Africa's diverse urban, rural, and unpaved roads, remains a
key obstacle to robust perception in low-resource settings. We present a
procedural augmentation pipeline that enhances low-cost monocular dashcam
footage with realistic refractive distortions and weather-induced artifacts
tailored to challenging African driving scenarios. Our refractive module
simulates optical effects from low-quality lenses and air turbulence, including
lens distortion, Perlin noise, Thin-Plate Spline (TPS), and divergence-free
(incompressible) warps. The weather module adds homogeneous fog, heterogeneous
fog, and lens flare. To establish a benchmark, we provide baseline performance
using three image restoration models. To support perception research in
underrepresented African contexts, without costly data collection, labeling, or
simulation, we release our distortion toolkit, augmented dataset splits, and
benchmark results.

</details>


### [18] [ReLayout: Integrating Relation Reasoning for Content-aware Layout Generation with Multi-modal Large Language Models](https://arxiv.org/abs/2507.05568)
*Jiaxu Tian,Xuehui Yu,Yaoxing Wang,Pan Wang,Guangqian Guo,Shan Gao*

Main category: cs.CV

TL;DR: ReLayout improves content-aware layout generation by using relation-CoT to address spatial relationship interpretation issues in LLM-based methods, resulting in more structured and diverse layouts.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based methods for layout generation struggle with interpreting spatial relationships, leading to structural and diversity issues.

Method: ReLayout introduces relation-CoT to enhance layout annotations with explicit spatial relationships and a layout prototype rebalance sampler to address uniformity issues.

Result: ReLayout outperforms baselines, generating more structured, diverse, and aesthetically coherent layouts.

Conclusion: ReLayout effectively addresses limitations of LLM-based methods, producing layouts better aligned with human aesthetics and explainability.

Abstract: Content-aware layout aims to arrange design elements appropriately on a given
canvas to convey information effectively. Recently, the trend for this task has
been to leverage large language models (LLMs) to generate layouts
automatically, achieving remarkable performance. However, existing LLM-based
methods fail to adequately interpret spatial relationships among visual themes
and design elements, leading to structural and diverse problems in layout
generation. To address this issue, we introduce ReLayout, a novel method that
leverages relation-CoT to generate more reasonable and aesthetically coherent
layouts by fundamentally originating from design concepts. Specifically, we
enhance layout annotations by introducing explicit relation definitions, such
as region, salient, and margin between elements, with the goal of decomposing
the layout into smaller, structured, and recursive layouts, thereby enabling
the generation of more structured layouts. Furthermore, based on these defined
relationships, we introduce a layout prototype rebalance sampler, which defines
layout prototype features across three dimensions and quantifies distinct
layout styles. This sampler addresses uniformity issues in generation that
arise from data bias in the prototype distribution balance process. Extensive
experimental results verify that ReLayout outperforms baselines and can
generate structural and diverse layouts that are more aligned with human
aesthetics and more explainable.

</details>


### [19] [Multi-Modal Face Anti-Spoofing via Cross-Modal Feature Transitions](https://arxiv.org/abs/2507.05575)
*Jun-Xiong Chong,Fang-Yu Hsu,Ming-Tsung Hsu,Yi-Ting Lin,Kai-Heng Chien,Chiou-Ting Hsu,Pei-Kai Huang*

Main category: cs.CV

TL;DR: The paper proposes CTNet, a cross-modal transition-guided network, to address challenges in multi-modal face anti-spoofing (FAS) by leveraging consistent feature transitions for live samples and inconsistent ones for spoof detection, while also handling missing modalities.


<details>
  <summary>Details</summary>
Motivation: Multi-modal FAS faces distribution discrepancies and missing modalities during inference. The paper exploits smaller visual differences in live faces and consistent cross-modal transitions to improve robustness.

Method: CTNet learns consistent cross-modal feature transitions for live samples and inconsistent ones for spoof detection. It also learns complementary IR and depth features from RGB to handle missing modalities.

Result: CTNet outperforms previous multi-modal FAS methods across most protocols, demonstrating superior performance.

Conclusion: The proposed CTNet effectively addresses multi-modal FAS challenges by leveraging feature transitions and auxiliary modalities, enhancing robustness and performance.

Abstract: Multi-modal face anti-spoofing (FAS) aims to detect genuine human presence by
extracting discriminative liveness cues from multiple modalities, such as RGB,
infrared (IR), and depth images, to enhance the robustness of biometric
authentication systems. However, because data from different modalities are
typically captured by various camera sensors and under diverse environmental
conditions, multi-modal FAS often exhibits significantly greater distribution
discrepancies across training and testing domains compared to single-modal FAS.
Furthermore, during the inference stage, multi-modal FAS confronts even greater
challenges when one or more modalities are unavailable or inaccessible. In this
paper, we propose a novel Cross-modal Transition-guided Network (CTNet) to
tackle the challenges in the multi-modal FAS task. Our motivation stems from
that, within a single modality, the visual differences between live faces are
typically much smaller than those of spoof faces. Additionally, feature
transitions across modalities are more consistent for the live class compared
to those between live and spoof classes. Upon this insight, we first propose
learning consistent cross-modal feature transitions among live samples to
construct a generalized feature space. Next, we introduce learning the
inconsistent cross-modal feature transitions between live and spoof samples to
effectively detect out-of-distribution (OOD) attacks during inference. To
further address the issue of missing modalities, we propose learning
complementary infrared (IR) and depth features from the RGB modality as
auxiliary modalities. Extensive experiments demonstrate that the proposed CTNet
outperforms previous two-class multi-modal FAS methods across most protocols.

</details>


### [20] [Semi-Supervised Defect Detection via Conditional Diffusion and CLIP-Guided Noise Filtering](https://arxiv.org/abs/2507.05588)
*Shuai Li,Shihan Chen,Wanru Geng,Zhaohua Xu,Xiaolu Liu,Can Dong,Zhen Tian,Changlin Chen*

Main category: cs.CV

TL;DR: A semi-supervised defect detection framework (DSYM) using conditional diffusion and pseudo-labels improves efficiency and reduces labeling dependency in industrial quality inspection.


<details>
  <summary>Details</summary>
Motivation: Traditional defect detection methods are inefficient and costly, prompting the need for a more robust and data-efficient solution.

Method: DSYM employs a two-stage collaborative training mechanism, pseudo-label generation, and a conditional diffusion model with noise filtering.

Result: Achieves 78.4% mAP@0.5 with full labeled data and 75.1% with only 40%, demonstrating superior data efficiency.

Conclusion: DSYM offers a high-precision, low-labeling-dependent solution for industrial defect detection, with open-source availability.

Abstract: In the realm of industrial quality inspection, defect detection stands as a
critical component, particularly in high-precision, safety-critical sectors
such as automotive components aerospace, and medical devices. Traditional
methods, reliant on manual inspection or early image processing algorithms,
suffer from inefficiencies, high costs, and limited robustness. This paper
introduces a semi-supervised defect detection framework based on conditional
diffusion (DSYM), leveraging a two-stage collaborative training mechanism and a
staged joint optimization strategy. The framework utilizes labeled data for
initial training and subsequently incorporates unlabeled data through the
generation of pseudo-labels. A conditional diffusion model synthesizes
multi-scale pseudo-defect samples, while a CLIP cross-modal feature-based noise
filtering mechanism mitigates label contamination. Experimental results on the
NEU-DET dataset demonstrate a 78.4% mAP@0.5 with the same amount of labeled
data as traditional supervised methods, and 75.1% mAP@0.5 with only 40% of the
labeled data required by the original supervised model, showcasing significant
advantages in data efficiency. This research provides a high-precision,
low-labeling-dependent solution for defect detection in industrial quality
inspection scenarios. The work of this article has been open-sourced at
https://github.com/cLin-c/Semisupervised-DSYM.

</details>


### [21] [GSVR: 2D Gaussian-based Video Representation for 800+ FPS with Hybrid Deformation Field](https://arxiv.org/abs/2507.05594)
*Zhizhuo Pang,Zhihui Ke,Xiaobo Zhou,Tie Qiu*

Main category: cs.CV

TL;DR: GSVR, a 2D Gaussian-based video representation, achieves high decoding speed (800+ FPS) and quality (35+ PSNR) with fast training (2s/frame), addressing limitations of convolution-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing implicit neural video representations focus on reconstruction quality but neglect decoding speed and training time, which are hindered by high computation in convolutional networks.

Method: Proposes GSVR with a hybrid deformation field (tri-plane and polynomial motion), Dynamic-aware Time Slicing for GOP division, and quantization-aware fine-tuning for compact representation.

Result: Achieves 800+ FPS, 35+ PSNR, and 2s/frame training on Bunny, with 10x faster decoding and comparable interpolation performance to SOTA.

Conclusion: GSVR outperforms existing methods in speed and training efficiency while maintaining high video quality and compression performance.

Abstract: Implicit neural representations for video have been recognized as a novel and
promising form of video representation. Existing works pay more attention to
improving video reconstruction quality but little attention to the decoding
speed. However, the high computation of convolutional network used in existing
methods leads to low decoding speed. Moreover, these convolution-based video
representation methods also suffer from long training time, about 14 seconds
per frame to achieve 35+ PSNR on Bunny. To solve the above problems, we propose
GSVR, a novel 2D Gaussian-based video representation, which achieves 800+ FPS
and 35+ PSNR on Bunny, only needing a training time of $2$ seconds per frame.
Specifically, we propose a hybrid deformation field to model the dynamics of
the video, which combines two motion patterns, namely the tri-plane motion and
the polynomial motion, to deal with the coupling of camera motion and object
motion in the video. Furthermore, we propose a Dynamic-aware Time Slicing
strategy to adaptively divide the video into multiple groups of pictures(GOP)
based on the dynamic level of the video in order to handle large camera motion
and non-rigid movements. Finally, we propose quantization-aware fine-tuning to
avoid performance reduction after quantization and utilize image codecs to
compress Gaussians to achieve a compact representation. Experiments on the
Bunny and UVG datasets confirm that our method converges much faster than
existing methods and also has 10x faster decoding speed compared to other
methods. Our method has comparable performance in the video interpolation task
to SOTA and attains better video compression performance than NeRV.

</details>


### [22] [PaddleOCR 3.0 Technical Report](https://arxiv.org/abs/2507.05595)
*Cheng Cui,Ting Sun,Manhui Lin,Tingquan Gao,Yubo Zhang,Jiaxuan Liu,Xueqing Wang,Zelun Zhang,Changda Zhou,Hongen Liu,Yue Zhang,Wenyu Lv,Kui Huang,Yichao Zhang,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR 3.0 is an open-source OCR toolkit with multilingual text recognition, document parsing, and key information extraction capabilities, offering competitive accuracy and efficiency with fewer parameters than large VLMs.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for document understanding in the era of large language models by providing efficient and accurate OCR solutions.

Method: Introduces three solutions: PP-OCRv5 for multilingual text recognition, PP-StructureV3 for hierarchical document parsing, and PP-ChatOCRv4 for key information extraction.

Result: Achieves competitive accuracy and efficiency with fewer than 100 million parameters, rivaling billion-parameter VLMs.

Conclusion: PaddleOCR 3.0 provides a high-quality OCR model library, efficient tools for training and deployment, and supports heterogeneous hardware acceleration, enabling easy development of document applications.

Abstract: This technical report introduces PaddleOCR 3.0, an Apache-licensed
open-source toolkit for OCR and document parsing. To address the growing demand
for document understanding in the era of large language models, PaddleOCR 3.0
presents three major solutions: (1) PP-OCRv5 for multilingual text recognition,
(2) PP-StructureV3 for hierarchical document parsing, and (3) PP-ChatOCRv4 for
key information extraction. Compared to mainstream vision-language models
(VLMs), these models with fewer than 100 million parameters achieve competitive
accuracy and efficiency, rivaling billion-parameter VLMs. In addition to
offering a high-quality OCR model library, PaddleOCR 3.0 provides efficient
tools for training, inference, and deployment, supports heterogeneous hardware
acceleration, and enables developers to easily build intelligent document
applications.

</details>


### [23] [Rethinking Layered Graphic Design Generation with a Top-Down Approach](https://arxiv.org/abs/2507.05601)
*Jingye Chen,Zhaowen Wang,Nanxuan Zhao,Li Zhang,Difan Liu,Jimei Yang,Qifeng Chen*

Main category: cs.CV

TL;DR: Accordion is a framework converting AI-generated pixel designs into editable layered designs, refining nonsensical text with user prompts, and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: AI-generated designs lack editability but inspire human designers. Accordion bridges this gap by transforming non-layered designs into editable formats.

Method: Uses a vision language model (VLM) in three stages, guided by prompts, and leverages vision experts like SAM. Trained on Design39K dataset with AI-generated images.

Result: Outperforms on DesignIntention benchmark in tasks like text-to-template and text de-rendering, and excels in design variations.

Conclusion: Accordion successfully converts AI designs into editable layers, refining text and improving workflow for designers.

Abstract: Graphic design is crucial for conveying ideas and messages. Designers usually
organize their work into objects, backgrounds, and vectorized text layers to
simplify editing. However, this workflow demands considerable expertise. With
the rise of GenAI methods, an endless supply of high-quality graphic designs in
pixel format has become more accessible, though these designs often lack
editability. Despite this, non-layered designs still inspire human designers,
influencing their choices in layouts and text styles, ultimately guiding the
creation of layered designs. Motivated by this observation, we propose
Accordion, a graphic design generation framework taking the first attempt to
convert AI-generated designs into editable layered designs, meanwhile refining
nonsensical AI-generated text with meaningful alternatives guided by user
prompts. It is built around a vision language model (VLM) playing distinct
roles in three curated stages. For each stage, we design prompts to guide the
VLM in executing different tasks. Distinct from existing bottom-up methods
(e.g., COLE and Open-COLE) that gradually generate elements to create layered
designs, our approach works in a top-down manner by using the visually
harmonious reference image as global guidance to decompose each layer.
Additionally, it leverages multiple vision experts such as SAM and element
removal models to facilitate the creation of graphic layers. We train our
method using the in-house graphic design dataset Design39K, augmented with
AI-generated design images coupled with refined ground truth created by a
customized inpainting model. Experimental results and user studies by designers
show that Accordion generates favorable results on the DesignIntention
benchmark, including tasks such as text-to-template, adding text to background,
and text de-rendering, and also excels in creating design variations.

</details>


### [24] [Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration](https://arxiv.org/abs/2507.05604)
*Yuyang Hu,Kangfu Mei,Mojtaba Sahraee-Ardakan,Ulugbek S. Kamilov,Peyman Milanfar,Mauricio Delbracio*

Main category: cs.CV

TL;DR: Kernel Density Steering (KDS) improves diffusion models for image restoration by using an ensemble of samples to guide outputs toward higher-fidelity regions, reducing artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for image restoration often produce inconsistent fidelity and artifacts, prompting the need for a more robust solution.

Method: KDS employs an N-particle ensemble to compute patch-wise kernel density gradients, steering samples toward shared high-density regions for better fidelity.

Result: KDS enhances both quantitative and qualitative performance in super-resolution and image inpainting tasks.

Conclusion: KDS is a plug-and-play framework that improves diffusion model outputs without retraining, offering higher-quality results at increased computational cost.

Abstract: Diffusion models show promise for image restoration, but existing methods
often struggle with inconsistent fidelity and undesirable artifacts. To address
this, we introduce Kernel Density Steering (KDS), a novel inference-time
framework promoting robust, high-fidelity outputs through explicit local
mode-seeking. KDS employs an $N$-particle ensemble of diffusion samples,
computing patch-wise kernel density estimation gradients from their collective
outputs. These gradients steer patches in each particle towards shared,
higher-density regions identified within the ensemble. This collective local
mode-seeking mechanism, acting as "collective wisdom", steers samples away from
spurious modes prone to artifacts, arising from independent sampling or model
imperfections, and towards more robust, high-fidelity structures. This allows
us to obtain better quality samples at the expense of higher compute by
simultaneously sampling multiple particles. As a plug-and-play framework, KDS
requires no retraining or external verifiers, seamlessly integrating with
various diffusion samplers. Extensive numerical validations demonstrate KDS
substantially improves both quantitative and qualitative performance on
challenging real-world super-resolution and image inpainting tasks.

</details>


### [25] [Generative Head-Mounted Camera Captures for Photorealistic Avatars](https://arxiv.org/abs/2507.05620)
*Shaojie Bai,Seunghyeon Seo,Yida Wang,Chenghui Li,Owen Wang,Te-Li Wang,Tianyang Ma,Jason Saragih,Shih-En Wei,Nojun Kwak,Hyung Jun Kim*

Main category: cs.CV

TL;DR: Proposes GenHMC, a generative method for photorealistic avatar animations by leveraging unpaired HMC captures, improving ground truth accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining synchronized ground truth for VR/AR avatar animations due to partial HMC observations and expensive paired data collection.

Method: GenHMC uses unpaired HMC captures to generate synthetic HMC images from dome-captured avatar states, disentangling expression and appearance.

Result: Achieves accurate ground truth, generalizes to unseen identities, and improves face encoder performance.

Conclusion: GenHMC offers a scalable, efficient solution for high-quality avatar animations without paired data.

Abstract: Enabling photorealistic avatar animations in virtual and augmented reality
(VR/AR) has been challenging because of the difficulty of obtaining ground
truth state of faces. It is physically impossible to obtain synchronized images
from head-mounted cameras (HMC) sensing input, which has partial observations
in infrared (IR), and an array of outside-in dome cameras, which have full
observations that match avatars' appearance. Prior works relying on
analysis-by-synthesis methods could generate accurate ground truth, but suffer
from imperfect disentanglement between expression and style in their
personalized training. The reliance of extensive paired captures (HMC and dome)
for the same subject makes it operationally expensive to collect large-scale
datasets, which cannot be reused for different HMC viewpoints and lighting. In
this work, we propose a novel generative approach, Generative HMC (GenHMC),
that leverages large unpaired HMC captures, which are much easier to collect,
to directly generate high-quality synthetic HMC images given any conditioning
avatar state from dome captures. We show that our method is able to properly
disentangle the input conditioning signal that specifies facial expression and
viewpoint, from facial appearance, leading to more accurate ground truth.
Furthermore, our method can generalize to unseen identities, removing the
reliance on the paired captures. We demonstrate these breakthroughs by both
evaluating synthetic HMC images and universal face encoders trained from these
new HMC-avatar correspondences, which achieve better data efficiency and
state-of-the-art accuracy.

</details>


### [26] [AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework](https://arxiv.org/abs/2507.05621)
*Suoxiang Zhang,Xiaxi Li,Hongrui Chang,Zhuoyan Hou,Guoxin Wu,Ronghua Ji*

Main category: cs.CV

TL;DR: AdaptaGen is a hierarchical semantic optimization framework for domain-specific image generation, addressing semantic accuracy and detail fidelity by integrating prompt optimization and cross-modal adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to jointly address prompt engineering and model adaptation, leading to semantic deviations and hallucinations in specialized domains.

Method: Proposes AdaptaGen with matrix-based prompt optimization, multi-perspective understanding, cross-modal adaptation, and two-phase caption semantic transformation.

Result: Achieves superior performance in image quality, diversity, and semantic consistency across 40 categories with only 16 images per category.

Conclusion: AdaptaGen effectively mitigates hallucinations and semantic deviations, ensuring domain-specific constraints are met while enhancing visual diversity.

Abstract: Domain-specific image generation aims to produce high-quality visual content
for specialized fields while ensuring semantic accuracy and detail fidelity.
However, existing methods exhibit two critical limitations: First, current
approaches address prompt engineering and model adaptation separately,
overlooking the inherent dependence between semantic understanding and visual
representation in specialized domains. Second, these techniques inadequately
incorporate domain-specific semantic constraints during content synthesis,
resulting in generation outcomes that exhibit hallucinations and semantic
deviations. To tackle these issues, we propose AdaptaGen, a hierarchical
semantic optimization framework that integrates matrix-based prompt
optimization with multi-perspective understanding, capturing comprehensive
semantic relationships from both global and local perspectives. To mitigate
hallucinations in specialized domains, we design a cross-modal adaptation
mechanism, which, when combined with intelligent content synthesis, enables
preserving core thematic elements while incorporating diverse details across
images. Additionally, we introduce a two-phase caption semantic transformation
during the generation phase. This approach maintains semantic coherence while
enhancing visual diversity, ensuring the generated images adhere to
domain-specific constraints. Experimental results confirm our approach's
effectiveness, with our framework achieving superior performance across 40
categories from diverse datasets using only 16 images per category,
demonstrating significant improvements in image quality, diversity, and
semantic consistency.

</details>


### [27] [OFFSET: Segmentation-based Focus Shift Revision for Composed Image Retrieval](https://arxiv.org/abs/2507.05631)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Xuemeng Song,Liqiang Nie*

Main category: cs.CV

TL;DR: The paper introduces OFFSET, a method for Composed Image Retrieval (CIR) that addresses feature degradation and visual focus bias by using dominant portion segmentation and dual focus mapping, along with textually guided focus revision.


<details>
  <summary>Details</summary>
Motivation: CIR faces limitations due to ignored inhomogeneity in visual data and overlooked textual priority, leading to degraded query features and visual focus bias.

Method: Proposes OFFSET, featuring a focus mapping-based extractor with dominant portion segmentation and dual focus mapping, plus a textually guided focus revision module.

Result: Comprehensive experiments on four datasets confirm OFFSET's superiority in addressing CIR challenges.

Conclusion: OFFSET effectively improves CIR by reducing noise interference and enhancing modification focus perception.

Abstract: Composed Image Retrieval (CIR) represents a novel retrieval paradigm that is
capable of expressing users' intricate retrieval requirements flexibly. It
enables the user to give a multimodal query, comprising a reference image and a
modification text, and subsequently retrieve the target image. Notwithstanding
the considerable advances made by prevailing methodologies, CIR remains in its
nascent stages due to two limitations: 1) inhomogeneity between dominant and
noisy portions in visual data is ignored, leading to query feature degradation,
and 2) the priority of textual data in the image modification process is
overlooked, which leads to a visual focus bias. To address these two
limitations, this work presents a focus mapping-based feature extractor, which
consists of two modules: dominant portion segmentation and dual focus mapping.
It is designed to identify significant dominant portions in images and guide
the extraction of visual and textual data features, thereby reducing the impact
of noise interference. Subsequently, we propose a textually guided focus
revision module, which can utilize the modification requirements implied in the
text to perform adaptive focus revision on the reference image, thereby
enhancing the perception of the modification focus on the composed features.
The aforementioned modules collectively constitute the segmentatiOn-based Focus
shiFt reviSion nETwork (\mbox{OFFSET}), and comprehensive experiments on four
benchmark datasets substantiate the superiority of our proposed method. The
codes and data are available on https://zivchen-ty.github.io/OFFSET.github.io/

</details>


### [28] [Knowledge-guided Complex Diffusion Model for PolSAR Image Classification in Contourlet Domain](https://arxiv.org/abs/2507.05666)
*Junfei Shi,Yu Cheng,Haiyan Jin,Junhuai Li,Zhaolin Xiao,Maoguo Gong,Weisi Lin*

Main category: cs.CV

TL;DR: A structural knowledge-guided complex diffusion model in the Contourlet domain is proposed for PolSAR image classification, outperforming traditional methods by preserving edge details and region homogeneity.


<details>
  <summary>Details</summary>
Motivation: Traditional real-valued diffusion models struggle with complex-valued phase information and fine structural details in PolSAR data.

Method: Uses Contourlet transform for multiscale/multidirectional representations, extracts statistical/boundary features, and designs a knowledge-guided complex diffusion network.

Result: Outperforms state-of-the-art methods on three PolSAR datasets, especially in edge preservation and region homogeneity.

Conclusion: The proposed model effectively addresses limitations of traditional diffusion models for PolSAR data, enhancing classification accuracy and detail preservation.

Abstract: Diffusion models have demonstrated exceptional performance across various
domains due to their ability to model and generate complicated data
distributions. However, when applied to PolSAR data, traditional real-valued
diffusion models face challenges in capturing complex-valued phase
information.Moreover, these models often struggle to preserve fine structural
details. To address these limitations, we leverage the Contourlet transform,
which provides rich multiscale and multidirectional representations well-suited
for PolSAR imagery. We propose a structural knowledge-guided complex diffusion
model for PolSAR image classification in the Contourlet domain. Specifically,
the complex Contourlet transform is first applied to decompose the data into
low- and high-frequency subbands, enabling the extraction of statistical and
boundary features. A knowledge-guided complex diffusion network is then
designed to model the statistical properties of the low-frequency components.
During the process, structural information from high-frequency coefficients is
utilized to guide the diffusion process, improving edge preservation.
Furthermore, multiscale and multidirectional high-frequency features are
jointly learned to further boost classification accuracy. Experimental results
on three real-world PolSAR datasets demonstrate that our approach surpasses
state-of-the-art methods, particularly in preserving edge details and
maintaining region homogeneity in complex terrain.

</details>


### [29] [Dynamic Rank Adaptation for Vision-Language Models](https://arxiv.org/abs/2507.05668)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: Proposes Dynamic Rank Adaptation (DRA) to enhance generalization in vision-language models by dynamically adjusting feature ranks based on token importance, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for fine-tuning VLMs struggle with generalization to unseen classes due to equal treatment of all tokens, leading to overfitting on less informative features.

Method: DRA dynamically allocates adaptation ranks by grouping tokens by importance using sequence attention, assigns higher ranks to important tokens, and introduces a channel response mechanism and L1 regularization.

Result: DRA outperforms existing methods, improving performance on new classes across benchmarks, including base-new classes, cross-datasets, and domain generalization.

Conclusion: DRA effectively preserves general knowledge and enhances new class generalization, demonstrating superior performance over existing approaches.

Abstract: Pre-trained large vision-language models (VLMs) like CLIP demonstrate
impressive generalization ability. Existing prompt-based and adapter-based
works have made significant progress in fine-tuning VLMs but still face the
challenges of maintaining strong generalization abilities, particularly towards
unseen new classes. This limitation partly arises from these methods treating
all tokens of the image and text encoder equally, which can lead to overfitting
on less informative features (e.g., background noise, template words) and
degrade the general representations that are crucial for novel concept
recognition. To address this issue, we propose Dynamic Rank Adaptation (DRA), a
novel adapter variant method, designed specifically to enhance new class
generalization. DRA dynamically allocates adaptation ranks based on the
importance of features during training to preserve general knowledge. DRA first
employs token importance grouping, using sequence attention to evaluate and
group tokens by their importance. Then, we adopt rank adaptation according to
the importance of each token group dynamically by assigning higher feature
ranks to the more important tokens. Also, we design a new channel response
mechanism to prioritize the preservation and adaptation of feature channels
identified as the most informative for each instance. In addition, a L1
regularization term is introduced to stabilize the training. Extensive
experiments demonstrate the effectiveness and superiority of our proposed DRA
over existing works, especially on enhancing the performance of new classes on
various benchmarks, including base-new classes, cross-datasets evaluation and
domain generalization. The source code will be published after the paper is
received.

</details>


### [30] [Modeling and Reversing Brain Lesions Using Diffusion Models](https://arxiv.org/abs/2507.05670)
*Omar Zamzam,Haleh Akrami,Anand Joshi,Richard Leahy*

Main category: cs.CV

TL;DR: A diffusion model-based framework is introduced to segment, reverse deformations, and inpaint brain lesions, improving accuracy in lesion analysis compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing lesion segmentation methods fail to distinguish between damaged and deformed brain tissue, limiting accurate analysis.

Method: The framework segments abnormal regions, reverses tissue deformations, isolates core lesions, and inpaints to estimate pre-lesion healthy brain.

Result: Improved accuracy in lesion segmentation, characterization, and brain labeling, validated via synthetic lesioned brain images.

Conclusion: The proposed framework offers a robust tool for clinical and research applications in brain lesion analysis.

Abstract: Brain lesions are abnormalities or injuries in brain tissue that are often
detectable using magnetic resonance imaging (MRI), which reveals structural
changes in the affected areas. This broad definition of brain lesions includes
areas of the brain that are irreversibly damaged, as well as areas of brain
tissue that are deformed as a result of lesion growth or swelling. Despite the
importance of differentiating between damaged and deformed tissue, existing
lesion segmentation methods overlook this distinction, labeling both of them as
a single anomaly. In this work, we introduce a diffusion model-based framework
for analyzing and reversing the brain lesion process. Our pipeline first
segments abnormal regions in the brain, then estimates and reverses tissue
deformations by restoring displaced tissue to its original position, isolating
the core lesion area representing the initial damage. Finally, we inpaint the
core lesion area to arrive at an estimation of the pre-lesion healthy brain.
This proposed framework reverses a forward lesion growth process model that is
well-established in biomechanical studies that model brain lesions. Our results
demonstrate improved accuracy in lesion segmentation, characterization, and
brain labeling compared to traditional methods, offering a robust tool for
clinical and research applications in brain lesion analysis. Since pre-lesion
healthy versions of abnormal brains are not available in any public dataset for
validation of the reverse process, we simulate a forward model to synthesize
multiple lesioned brain images.

</details>


### [31] [R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding](https://arxiv.org/abs/2507.05673)
*Joonhyung Park,Peng Tang,Sagnik Das,Srikar Appalaraju,Kunwar Yashraj Singh,R. Manmatha,Shabnam Ghadar*

Main category: cs.CV

TL;DR: R-VLM improves GUI grounding accuracy by 13% using zoomed-in region proposals and an IoU-aware objective function, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing GUI agents struggle with precise element grounding due to cluttered screenshots and ineffective loss functions.

Method: Proposes R-VLM, leveraging zoomed-in region proposals and an IoU-aware objective for precise element localization.

Result: Achieves 13% better grounding accuracy on benchmarks and 3.2-9.7% improvements in GUI navigation tasks.

Conclusion: R-VLM bridges VLMs and object detection, enhancing GUI automation accuracy.

Abstract: Visual agent models for automating human activities on Graphical User
Interfaces (GUIs) have emerged as a promising research direction, driven by
advances in large Vision Language Models (VLMs). A critical challenge in GUI
automation is the precise grounding of interface elements across diverse
platforms. Existing vision-only GUI agents directly ground elements from large
and cluttered screenshots, requiring them to process substantial irrelevant
information that compromises their accuracy. In addition, these approaches
typically employ basic cross-entropy loss for learning grounding objectives,
which fails to effectively capture grounding quality compared to established
object detection metrics like Intersection-over-Union (IoU). To address these
issues, we introduce R-VLM, a novel GUI grounding approach that leverages
zoomed-in region proposals for precise element localization. We also propose an
IoU-aware objective function that facilitates model convergence toward high IoU
predictions. Our approach bridges the gap between VLMs and conventional object
detection techniques, improving the state-of-the-art grounding accuracy by 13%
across diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and
AgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy
improvements in GUI navigation tasks on the AITW and Mind2Web benchmarks.

</details>


### [32] [MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos](https://arxiv.org/abs/2507.05675)
*Rongsheng Wang,Junying Chen,Ke Ji,Zhenyang Cai,Shunian Chen,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: The paper introduces MedVideoCap-55K, a large-scale dataset for medical video generation, and MedGen, a model achieving high performance in visual quality and medical accuracy.


<details>
  <summary>Details</summary>
Motivation: Medical video generation is underexplored despite its importance for clinical training and education, with current models lacking accuracy due to inadequate datasets.

Method: The authors create MedVideoCap-55K, a dataset of 55,000 curated medical clips, and develop MedGen, a model trained on this dataset.

Result: MedGen outperforms open-source models and competes with commercial systems in visual quality and medical accuracy.

Conclusion: The dataset and model aim to advance research in medical video generation, offering a valuable resource for the community.

Abstract: Recent advances in video generation have shown remarkable progress in
open-domain settings, yet medical video generation remains largely
underexplored. Medical videos are critical for applications such as clinical
training, education, and simulation, requiring not only high visual fidelity
but also strict medical accuracy. However, current models often produce
unrealistic or erroneous content when applied to medical prompts, largely due
to the lack of large-scale, high-quality datasets tailored to the medical
domain. To address this gap, we introduce MedVideoCap-55K, the first
large-scale, diverse, and caption-rich dataset for medical video generation. It
comprises over 55,000 curated clips spanning real-world medical scenarios,
providing a strong foundation for training generalist medical video generation
models. Built upon this dataset, we develop MedGen, which achieves leading
performance among open-source models and rivals commercial systems across
multiple benchmarks in both visual quality and medical accuracy. We hope our
dataset and model can serve as a valuable resource and help catalyze further
research in medical video generation. Our code and data is available at
https://github.com/FreedomIntelligence/MedGen

</details>


### [33] [Integrated Structural Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.05677)
*Jiahui Wang,Qin Xu,Bo Jiang,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces Integrated Structural Prompt (ISP) for Vision-Language Models (VLMs) to improve prompt learning by modeling structural relationships between prompts and tokens, and dynamically adjusting loss coefficients for better generalization.


<details>
  <summary>Details</summary>
Motivation: Existing prompt learning methods for VLMs often overlook structural relationships between prompts and tokens and struggle with balancing performance across base and new classes.

Method: Proposes ISP with self-structural and cross-structural prompt modules to enhance inter-modal interactions and a sample probing module for dynamic loss adjustment.

Result: ISP achieves competitive performance in base-to-new generalization, cross-dataset evaluation, and domain generalization tasks.

Conclusion: ISP effectively improves VLM transferability by addressing structural relationships and sample difficulty, outperforming existing methods.

Abstract: Prompt learning methods have significantly extended the transferability of
pre-trained Vision-Language Models (VLMs) like CLIP for various downstream
tasks. These methods adopt handcraft templates or learnable vectors to provide
text or image instructions in fine-tuning VLMs. However, most existing works
ignore the structural relationships between learnable prompts and tokens within
and between modalities. Moreover, balancing the performance of base and new
classes remains a significant challenge. In this paper, we propose an
Integrated Structural Prompt (ISP) for VLMs to enhance the interaction of
information representations between the text and image branches. ISP introduces
self-structural and cross-structural prompt modules to model the structural
relationships between learnable prompts and frozen tokens within and across
modalities. This enables efficient information transfer while preserving
feature stability. Additionally, we propose a sample probing module that
dynamically adjusts loss coefficients based on sample difficulty, preventing
the mode from overfitting to simple samples and improving generalization
ability to new classes. Extensive experiments on three widely used settings:
base-to-new generalization, cross-dataset evaluation, and domain generalization
demonstrate that the proposed ISP achieves competitive performance against
state-of-the-art methods.

</details>


### [34] [LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion](https://arxiv.org/abs/2507.05678)
*Yisu Zhang,Chenjie Cao,Chaohui Yu,Jianke Zhu*

Main category: cs.CV

TL;DR: LiON-LoRA improves video diffusion models by enabling precise control over camera trajectories and object motion through linear scalability, orthogonality, and norm consistency.


<details>
  <summary>Details</summary>
Motivation: Achieving precise control over both camera trajectories and object motion in VDMs is challenging due to unstable fusion and non-linear scalability.

Method: LiON-LoRA introduces three principles: linear scalability, orthogonality, and norm consistency, along with a controllable token in the diffusion transformer for decoupled control.

Result: LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy and motion strength adjustment, with superior generalization using minimal training data.

Conclusion: LiON-LoRA effectively addresses the challenges of precise control in VDMs, unifying spatial and temporal controllability.

Abstract: Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in
synthesizing realistic videos by learning from large-scale data. Although
vanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal
movement to driven VDMs with constrained data, achieving precise control over
both camera trajectories and object motion remains challenging due to the
unstable fusion and non-linear scalability. To address these issues, we propose
LiON-LoRA, a novel framework that rethinks LoRA fusion through three core
principles: Linear scalability, Orthogonality, and Norm consistency. First, we
analyze the orthogonality of LoRA features in shallow VDM layers, enabling
decoupled low-level controllability. Second, norm consistency is enforced
across layers to stabilize fusion during complex camera motion combinations.
Third, a controllable token is integrated into the diffusion transformer (DiT)
to linearly adjust motion amplitudes for both cameras and objects with a
modified self-attention mechanism to ensure decoupled control. Additionally, we
extend LiON-LoRA to temporal generation by leveraging static-camera videos,
unifying spatial and temporal controllability. Experiments demonstrate that
LiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy
and motion strength adjustment, achieving superior generalization with minimal
training data. Project Page: https://fuchengsu.github.io/lionlora.github.io/

</details>


### [35] [Event-RGB Fusion for Spacecraft Pose Estimation Under Harsh Lighting](https://arxiv.org/abs/2507.05698)
*Mohsi Jawaid,Marcus Märtens,Tat-Jun Chin*

Main category: cs.CV

TL;DR: A sensor fusion approach combining RGB and event sensors improves spacecraft pose estimation by leveraging their complementary strengths under harsh lighting conditions.


<details>
  <summary>Details</summary>
Motivation: Vision-based pose estimation is critical for autonomous space operations but is hindered by extreme lighting conditions. Event sensors offer resilience but have limitations like low resolution and noise.

Method: A beam-splitter prism aligns RGB and event sensors optically and temporally. A RANSAC-based technique fuses data, with dropout uncertainty estimation to detect extreme conditions.

Result: The method demonstrated efficacy in a real dataset under challenging illumination, supporting the use of event sensors for pose estimation.

Conclusion: The fusion approach addresses individual sensor limitations, and the publicly released dataset encourages further research in this area.

Abstract: Spacecraft pose estimation is crucial for autonomous in-space operations,
such as rendezvous, docking and on-orbit servicing. Vision-based pose
estimation methods, which typically employ RGB imaging sensors, is a compelling
solution for spacecraft pose estimation, but are challenged by harsh lighting
conditions, which produce imaging artifacts such as glare, over-exposure,
blooming and lens flare. Due to their much higher dynamic range, neuromorphic
or event sensors are more resilient to extreme lighting conditions. However,
event sensors generally have lower spatial resolution and suffer from reduced
signal-to-noise ratio during periods of low relative motion. This work
addresses these individual sensor limitations by introducing a sensor fusion
approach combining RGB and event sensors. A beam-splitter prism was employed to
achieve precise optical and temporal alignment. Then, a RANSAC-based technique
was developed to fuse the information from the RGB and event channels to
achieve pose estimation that leveraged the strengths of the two modalities. The
pipeline was complemented by dropout uncertainty estimation to detect extreme
conditions that affect either channel. To benchmark the performance of the
proposed event-RGB fusion method, we collected a comprehensive real dataset of
RGB and event data for satellite pose estimation in a laboratory setting under
a variety of challenging illumination conditions. Encouraging results on the
dataset demonstrate the efficacy of our event-RGB fusion approach and further
supports the usage of event sensors for spacecraft pose estimation. To support
community research on this topic, our dataset will be released publicly.

</details>


### [36] [Hyperspectral Anomaly Detection Methods: A Survey and Comparative Study](https://arxiv.org/abs/2507.05730)
*Aayushma Pant,Arbind Agrahari Baniya,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: The paper compares hyperspectral anomaly detection (HAD) methods, highlighting deep learning's accuracy and statistical models' speed, while addressing challenges like computational complexity and noise sensitivity.


<details>
  <summary>Details</summary>
Motivation: To address challenges in HAD (e.g., computational complexity, noise sensitivity) and provide a comprehensive comparison of methods for researchers and practitioners.

Method: Categorizes and evaluates HAD techniques (statistical, representation-based, classical ML, deep learning) across 17 datasets using metrics like ROC, AUC, and separability maps.

Result: Deep learning models achieved highest accuracy; statistical models were fastest.

Conclusion: The study offers insights for advancing HAD, emphasizing deep learning's potential and statistical models' efficiency.

Abstract: Hyperspectral images are high-dimensional datasets consisting of hundreds of
contiguous spectral bands, enabling detailed material and surface analysis.
Hyperspectral anomaly detection (HAD) refers to the technique of identifying
and locating anomalous targets in such data without prior information about a
hyperspectral scene or target spectrum. This technology has seen rapid
advancements in recent years, with applications in agriculture, defence,
military surveillance, and environmental monitoring. Despite this significant
progress, existing HAD methods continue to face challenges such as high
computational complexity, sensitivity to noise, and limited generalisation
across diverse datasets. This study presents a comprehensive comparison of
various HAD techniques, categorising them into statistical models,
representation-based methods, classical machine learning approaches, and deep
learning models. We evaluated these methods across 17 benchmarking datasets
using different performance metrics, such as ROC, AUC, and separability map to
analyse detection accuracy, computational efficiency, their strengths,
limitations, and directions for future research.The research shows that deep
learning models achieved the highest detection accuracy, while statistical
models demonstrated exceptional speed across all datasets. This study aims to
provide valuable insights for researchers and practitioners working to advance
the field of hyperspectral anomaly detection methods.

</details>


### [37] [SenseShift6D: Multimodal RGB-D Benchmarking for Robust 6D Pose Estimation across Environment and Sensor Variations](https://arxiv.org/abs/2507.05751)
*Yegyu Han,Taegyoon Yoon,Dayeon Woo,Sojeong Kim,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: SenseShift6D is a new RGB-D dataset addressing real-world variations in illumination and sensor settings for 6D object-pose estimation, showing that test-time sensor control outperforms data augmentation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack real-world variations in illumination and sensor settings, limiting robustness in 6D pose estimation.

Method: Introduces SenseShift6D, a dataset with 101.9k RGB and 10k depth images, covering 1,380 sensor-lighting permutations per object pose. Evaluates state-of-the-art models with sensor control during testing.

Result: Test-time sensor control improves performance more than data augmentation, with multimodal RGB-D adaptation yielding the best results.

Conclusion: SenseShift6D advances 6D-pose evaluation by emphasizing sensor-aware robustness, enabling adaptive perception systems for uncertain environments.

Abstract: Recent advances on 6D object-pose estimation has achieved high performance on
representative benchmarks such as LM-O, YCB-V, and T-Less. However, these
datasets were captured under fixed illumination and camera settings, leaving
the impact of real-world variations in illumination, exposure, gain or
depth-sensor mode - and the potential of test-time sensor control to mitigate
such variations - largely unexplored. To bridge this gap, we introduce
SenseShift6D, the first RGB-D dataset that physically sweeps 13 RGB exposures,
9 RGB gains, auto-exposure, 4 depth-capture modes, and 5 illumination levels.
For three common household objects (spray, pringles, and tincase), we acquire
101.9k RGB and 10k depth images, which can provide 1,380 unique sensor-lighting
permutations per object pose. Experiments with state-of-the-art models on our
dataset show that applying sensor control during test-time induces greater
performance improvement over digital data augmentation, achieving performance
comparable to or better than costly increases in real-world training data
quantity and diversity. Adapting either RGB or depth sensors individually is
effective, while jointly adapting multimodal RGB-D configurations yields even
greater improvements. SenseShift6D extends the 6D-pose evaluation paradigm from
data-centered to sensor-aware robustness, laying a foundation for adaptive,
self-tuning perception systems capable of operating robustly in uncertain
real-world environments. Our dataset is available at:
huggingface.co/datasets/Yegyu/SenseShift6D Associated scripts can be found at:
github.com/yegyu-han/SenseShift6D

</details>


### [38] [Normal Patch Retinex Robust Alghoritm for White Balancing in Digital Microscopy](https://arxiv.org/abs/2507.05757)
*Radoslaw Roszczyk,Artur Krupa,Izabella Antoniuk*

Main category: cs.CV

TL;DR: An automatic white-balancing algorithm for microscopic images outperforms classical photography methods, validated on 200 images of pathomorphology specimens.


<details>
  <summary>Details</summary>
Motivation: Accurate color balance in microscopy is challenging, even for experts, necessitating an automated solution.

Method: An automatic white-balancing mechanism was developed and tested on 200 microscopic images, including hematoxylin-phloxine-saffron and immunohistochemical stains.

Result: The algorithm proved more effective than traditional photography white-balance methods for microscopic images.

Conclusion: The proposed automatic white-balancing solution is superior for microscopy, particularly with specific stains.

Abstract: The acquisition of accurately coloured, balanced images in an optical
microscope can be a challenge even for experienced microscope operators. This
article presents an entirely automatic mechanism for balancing the white level
that allows the correction of the microscopic colour images adequately. The
results of the algorithm have been confirmed experimentally on a set of two
hundred microscopic images. The images contained scans of three microscopic
specimens commonly used in pathomorphology. Also, the results achieved were
compared with other commonly used white balance algorithms in digital
photography. The algorithm applied in this work is more effective than the
classical algorithms used in colour photography for microscopic images stained
with hematoxylin-phloxine-saffron and for immunohistochemical staining images.

</details>


### [39] [DreamArt: Generating Interactable Articulated Objects from a Single Image](https://arxiv.org/abs/2507.05763)
*Ruijie Lu,Yu Liu,Jiaxiang Tang,Junfeng Ni,Yuxiang Wang,Diwen Wan,Gang Zeng,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: DreamArt is a framework for generating high-fidelity, interactable articulated 3D objects from single-view images, addressing limitations of current methods.


<details>
  <summary>Details</summary>
Motivation: Current methods lack part decomposition and articulation modeling, and rely on dense data, limiting scalability. DreamArt aims to overcome these challenges.

Method: DreamArt uses a three-stage pipeline: 1) part-segmented 3D mesh reconstruction, 2) video diffusion for articulation priors, and 3) motion optimization and texture refinement.

Result: DreamArt generates articulated objects with accurate part shapes, high appearance fidelity, and plausible articulation.

Conclusion: DreamArt provides a scalable solution for articulated asset generation, advancing applications in Embodied AI and AR/VR.

Abstract: Generating articulated objects, such as laptops and microwaves, is a crucial
yet challenging task with extensive applications in Embodied AI and AR/VR.
Current image-to-3D methods primarily focus on surface geometry and texture,
neglecting part decomposition and articulation modeling. Meanwhile, neural
reconstruction approaches (e.g., NeRF or Gaussian Splatting) rely on dense
multi-view or interaction data, limiting their scalability. In this paper, we
introduce DreamArt, a novel framework for generating high-fidelity,
interactable articulated assets from single-view images. DreamArt employs a
three-stage pipeline: firstly, it reconstructs part-segmented and complete 3D
object meshes through a combination of image-to-3D generation, mask-prompted 3D
segmentation, and part amodal completion. Second, we fine-tune a video
diffusion model to capture part-level articulation priors, leveraging movable
part masks as prompt and amodal images to mitigate ambiguities caused by
occlusion. Finally, DreamArt optimizes the articulation motion, represented by
a dual quaternion, and conducts global texture refinement and repainting to
ensure coherent, high-quality textures across all parts. Experimental results
demonstrate that DreamArt effectively generates high-quality articulated
objects, possessing accurate part shape, high appearance fidelity, and
plausible articulation, thereby providing a scalable solution for articulated
asset generation. Our project page is available at
https://dream-art-0.github.io/DreamArt/.

</details>


### [40] [TalkFashion: Intelligent Virtual Try-On Assistant Based on Multimodal Large Language Model](https://arxiv.org/abs/2507.05790)
*Yujie Hu,Xuanyu Zhang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: TalkFashion enables text-guided virtual try-on for full outfit changes and local edits, using large language models for task analysis and multi-modal models for automation, outperforming current methods in quality and consistency.


<details>
  <summary>Details</summary>
Motivation: Existing virtual try-on methods lack versatility and flexibility, being limited to single tasks and requiring manual input for local edits.

Method: TalkFashion uses large language models to interpret user instructions and activate task-specific pipelines, along with an instruction-based local repainting model for automated edits.

Result: The approach achieves better semantic consistency and visual quality than current methods.

Conclusion: TalkFashion enhances virtual try-on flexibility and automation, demonstrating superior performance in text-guided tasks.

Abstract: Virtual try-on has made significant progress in recent years. This paper
addresses how to achieve multifunctional virtual try-on guided solely by text
instructions, including full outfit change and local editing. Previous methods
primarily relied on end-to-end networks to perform single try-on tasks, lacking
versatility and flexibility. We propose TalkFashion, an intelligent try-on
assistant that leverages the powerful comprehension capabilities of large
language models to analyze user instructions and determine which task to
execute, thereby activating different processing pipelines accordingly.
Additionally, we introduce an instruction-based local repainting model that
eliminates the need for users to manually provide masks. With the help of
multi-modal models, this approach achieves fully automated local editings,
enhancing the flexibility of editing tasks. The experimental results
demonstrate better semantic consistency and visual quality compared to the
current methods.

</details>


### [41] [SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning](https://arxiv.org/abs/2507.05798)
*Xin Hu,Ke Qin,Guiduo Duan,Ming Li,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: SPADE, a novel framework for Panoptic Scene Graph Generation (PSG), leverages denoising diffusion models and spatial-aware reasoning to improve relation prediction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods using vision-language models (VLMs) struggle with spatial relation reasoning, leading to suboptimal predictions. SPADE addresses this by preserving spatial structure through diffusion models.

Method: SPADE involves (1) inversion-guided calibration for UNet adaptation and (2) spatial-aware context reasoning using a relation graph transformer.

Result: SPADE outperforms state-of-the-art methods in PSG and Visual Genome datasets, especially in spatial relationship prediction.

Conclusion: SPADE effectively addresses spatial reasoning limitations in PSG, offering superior performance in both closed- and open-set scenarios.

Abstract: Panoptic Scene Graph Generation (PSG) integrates instance segmentation with
relation understanding to capture pixel-level structural relationships in
complex scenes. Although recent approaches leveraging pre-trained
vision-language models (VLMs) have significantly improved performance in the
open-vocabulary setting, they commonly ignore the inherent limitations of VLMs
in spatial relation reasoning, such as difficulty in distinguishing object
relative positions, which results in suboptimal relation prediction. Motivated
by the denoising diffusion model's inversion process in preserving the spatial
structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork)
framework -- a novel approach for open-vocabulary PSG. SPADE consists of two
key steps: (1) inversion-guided calibration for the UNet adaptation, and (2)
spatial-aware context reasoning. In the first step, we calibrate a general
pre-trained teacher diffusion model into a PSG-specific denoising network with
cross-attention maps derived during inversion through a lightweight LoRA-based
fine-tuning strategy. In the second step, we develop a spatial-aware relation
graph transformer that captures both local and long-range contextual
information, facilitating the generation of high-quality relation queries.
Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate
that SPADE outperforms state-of-the-art methods in both closed- and open-set
scenarios, particularly for spatial relationship prediction.

</details>


### [42] [DREAM: Document Reconstruction via End-to-end Autoregressive Model](https://arxiv.org/abs/2507.05805)
*Xin Li,Mingming Gong,Yunfei Wu,Jianxin Dai,Antai Guo,Xinghua Jiang,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun*

Main category: cs.CV

TL;DR: The paper introduces DREAM, an autoregressive model for end-to-end document reconstruction, addressing limitations of multi-stage and generative models by preserving layout information and achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Current document reconstruction methods suffer from error propagation in multi-stage approaches and lack layout preservation in generative models, necessitating a more comprehensive solution.

Method: Proposes DREAM, an autoregressive model that transforms text images into document sequences end-to-end, incorporating layout and element information. Introduces DSM metric and DocRec1K dataset for evaluation.

Result: DREAM achieves state-of-the-art performance in document reconstruction and performs competitively on subtasks like layout analysis, text recognition, and formula recognition.

Conclusion: DREAM effectively addresses existing limitations, offering a robust, end-to-end solution for document reconstruction with broad applicability across subtasks.

Abstract: Document reconstruction constitutes a significant facet of document analysis
and recognition, a field that has been progressively accruing interest within
the scholarly community. A multitude of these researchers employ an array of
document understanding models to generate predictions on distinct subtasks,
subsequently integrating their results into a holistic document reconstruction
format via heuristic principles. Nevertheless, these multi-stage methodologies
are hindered by the phenomenon of error propagation, resulting in suboptimal
performance. Furthermore, contemporary studies utilize generative models to
extract the logical sequence of plain text, tables and mathematical expressions
in an end-to-end process. However, this approach is deficient in preserving the
information related to element layouts, which are vital for document
reconstruction. To surmount these aforementioned limitations, we in this paper
present an innovative autoregressive model specifically designed for document
reconstruction, referred to as Document Reconstruction via End-to-end
Autoregressive Model (DREAM). DREAM transmutes the text image into a sequence
of document reconstruction in a comprehensive, end-to-end process,
encapsulating a broader spectrum of document element information. In addition,
we establish a standardized definition of the document reconstruction task, and
introduce a novel Document Similarity Metric (DSM) and DocRec1K dataset for
assessing the performance of the task. Empirical results substantiate that our
methodology attains unparalleled performance in the realm of document
reconstruction. Furthermore, the results on a variety of subtasks, encompassing
document layout analysis, text recognition, table structure recognition,
formula recognition and reading order detection, indicate that our model is
competitive and compatible with various tasks.

</details>


### [43] [Towards Solar Altitude Guided Scene Illumination](https://arxiv.org/abs/2507.05812)
*Samed Doğan,Maximilian Hoh,Nico Leuze,Nicolas R. -Peña,Alfred Schöttl*

Main category: cs.CV

TL;DR: The paper addresses the challenge of generating synthetic camera sensor data for autonomous driving by introducing solar altitude as a global conditioning variable, reducing reliance on manual labeling.


<details>
  <summary>Details</summary>
Motivation: Real-world data acquisition for autonomous driving is costly and limited by labeling, safety, and scenario diversity. Synthetic data generation is a solution, but lacks focus on daytime variation due to label scarcity.

Method: Proposes using solar altitude (derived from latitude-longitude and time) as a conditioning variable, with a tailored normalization approach to handle daylight sensitivity.

Result: Demonstrates accurate capture of lighting characteristics and illumination-dependent noise in diffusion models.

Conclusion: Solar altitude is an effective, label-free conditioning variable for synthetic data generation, addressing daytime variation gaps.

Abstract: The development of safe and robust autonomous driving functions is heavily
dependent on large-scale, high-quality sensor data. However, real-word data
acquisition demands intensive human labor and is strongly limited by factors
such as labeling cost, driver safety protocols and diverse scenario coverage.
Thus, multiple lines of work focus on the conditional generation of synthetic
camera sensor data. We identify a significant gap in research regarding daytime
variation, presumably caused by the scarcity of available labels. Consequently,
we present the solar altitude as global conditioning variable. It is readily
computable from latitude-longitude coordinates and local time, eliminating the
need for extensive manual labeling. Our work is complemented by a tailored
normalization approach, targeting the sensitivity of daylight towards small
numeric changes in altitude. We demonstrate its ability to accurately capture
lighting characteristics and illumination-dependent image noise in the context
of diffusion models.

</details>


### [44] [Empowering Bridge Digital Twins by Bridging the Data Gap with a Unified Synthesis Framework](https://arxiv.org/abs/2507.05814)
*Wang Wang,Mingyu Shi,Jun Jiang,Wenqian Ma,Chong Liu,Yasutaka Narazaki,Xuguang Wang*

Main category: cs.CV

TL;DR: The paper proposes a framework for generating synthetic 3D bridge point clouds with annotations to address data incompleteness in real-world scans, improving segmentation and completion tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional bridge inspections are inefficient, and real-world 3D point cloud data is often incomplete, limiting its application. Synthetic data methods lack generalization.

Method: A systematic framework generates complete 3D bridge point clouds with annotations, colors, and normal vectors, and simulates incomplete data for training segmentation and completion networks.

Result: A PointNet++ model trained with synthetic data achieves 84.2% mIoU in real-world segmentation, and KT-Net excels in component completion.

Conclusion: The framework provides a novel methodology and dataset for 3D bridge analysis, advancing automated infrastructure management.

Abstract: As critical transportation infrastructure, bridges face escalating challenges
from aging and deterioration, while traditional manual inspection methods
suffer from low efficiency. Although 3D point cloud technology provides a new
data-driven paradigm, its application potential is often constrained by the
incompleteness of real-world data, which results from missing labels and
scanning occlusions. To overcome the bottleneck of insufficient generalization
in existing synthetic data methods, this paper proposes a systematic framework
for generating 3D bridge data.
  This framework can automatically generate complete point clouds featuring
component-level instance annotations, high-fidelity color, and precise normal
vectors. It can be further extended to simulate the creation of diverse and
physically realistic incomplete point clouds, designed to support the training
of segmentation and completion networks, respectively. Experiments demonstrate
that a PointNet++ model trained with our synthetic data achieves a mean
Intersection over Union (mIoU) of 84.2% in real-world bridge semantic
segmentation. Concurrently, a fine-tuned KT-Net exhibits superior performance
on the component completion task.
  This research offers an innovative methodology and a foundational dataset for
the 3D visual analysis of bridge structures, holding significant implications
for advancing the automated management and maintenance of infrastructure.

</details>


### [45] [2D Instance Editing in 3D Space](https://arxiv.org/abs/2507.05819)
*Yuhuan Xie,Aoxuan Pan,Ming-Xian Lin,Wei Huang,Yi-Hua Huang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: A novel '2D-3D-2D' framework improves 2D image editing by leveraging 3D representations for consistency and identity preservation.


<details>
  <summary>Details</summary>
Motivation: Existing 2D generative models struggle with consistency and object identity due to pixel-level manipulations.

Method: The framework lifts 2D objects to 3D, edits them in a rigidity-constrained 3D environment, and reprojects them back to 2D with seamless inpainting.

Result: Outperforms methods like DragGAN and DragDiffusion, achieving highly consistent edits and robust identity preservation.

Conclusion: The 3D-based approach significantly enhances 2D image editing quality.

Abstract: Generative models have achieved significant progress in advancing 2D image
editing, demonstrating exceptional precision and realism. However, they often
struggle with consistency and object identity preservation due to their
inherent pixel-manipulation nature. To address this limitation, we introduce a
novel "2D-3D-2D" framework. Our approach begins by lifting 2D objects into 3D
representation, enabling edits within a physically plausible,
rigidity-constrained 3D environment. The edited 3D objects are then reprojected
and seamlessly inpainted back into the original 2D image. In contrast to
existing 2D editing methods, such as DragGAN and DragDiffusion, our method
directly manipulates objects in a 3D environment. Extensive experiments
highlight that our framework surpasses previous methods in general performance,
delivering highly consistent edits while robustly preserving object identity.

</details>


### [46] [Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models](https://arxiv.org/abs/2507.05822)
*L'ea Dubois,Klaus Schmidt,Chengyu Wang,Ji-Hoon Park,Lin Wang,Santiago Munoz*

Main category: cs.CV

TL;DR: A novel framework combines Vision Foundation Models (VFMs) and Large Language Models (LLMs) to enhance video understanding, enabling advanced reasoning and prediction tasks through a sophisticated fusion module and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Current video models lack commonsense knowledge for high-level cognitive tasks like causal reasoning and future prediction.

Method: Proposes a fusion module (inspired by Q-Former) to align visual features with language for LLM reasoning, trained via large-scale alignment and targeted fine-tuning.

Result: Achieves state-of-the-art performance, with strong zero-shot generalization and validated contributions from each component.

Conclusion: Advances machine perception from recognition to cognitive understanding, benefiting AI in robotics and human-computer interaction.

Abstract: Current video understanding models excel at recognizing "what" is happening
but fall short in high-level cognitive tasks like causal reasoning and future
prediction, a limitation rooted in their lack of commonsense world knowledge.
To bridge this cognitive gap, we propose a novel framework that synergistically
fuses a powerful Vision Foundation Model (VFM) for deep visual perception with
a Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our
key technical innovation is a sophisticated fusion module, inspired by the
Q-Former architecture, which distills complex spatiotemporal and object-centric
visual features into a concise, language-aligned representation. This enables
the LLM to effectively ground its inferential processes in direct visual
evidence. The model is trained via a two-stage strategy, beginning with
large-scale alignment pre-training on video-text data, followed by targeted
instruction fine-tuning on a curated dataset designed to elicit advanced
reasoning and prediction skills. Extensive experiments demonstrate that our
model achieves state-of-the-art performance on multiple challenging benchmarks.
Notably, it exhibits remarkable zero-shot generalization to unseen reasoning
tasks, and our in-depth ablation studies validate the critical contribution of
each architectural component. This work pushes the boundary of machine
perception from simple recognition towards genuine cognitive understanding,
paving the way for more intelligent and capable AI systems in robotics,
human-computer interaction, and beyond.

</details>


### [47] [I$^2$R: Inter and Intra-image Refinement in Few Shot Segmentation](https://arxiv.org/abs/2507.05838)
*Ourui Fu,Hangzhou He,Xinliang Zhang,Lei Zhu,Shuang Zeng,ZhaoHeng Xie,Yanye Lu*

Main category: cs.CV

TL;DR: The paper introduces I²R, a novel few-shot segmentation method addressing inter- and intra-image discrepancies to improve segmentation performance.


<details>
  <summary>Details</summary>
Motivation: The annotation bottleneck in semantic segmentation and limitations of current few-shot segmentation methods, such as semantic gaps and visually similar regions causing errors, drive the need for a better approach.

Method: I²R uses category-specific high-level representations for precise inter-image region localization and a directional masking strategy to suppress inconsistent support-query pixel pairs.

Result: The method outperforms state-of-the-art approaches, achieving 1.9% and 2.1% mIoU improvements on PASCAL-5ⁱ and COCO-20ⁱ benchmarks under 1-shot settings.

Conclusion: I²R effectively addresses key limitations in few-shot segmentation, demonstrating superior performance over existing methods.

Abstract: The annotation bottleneck in semantic segmentation has driven significant
interest in few-shot segmentation, which aims to develop segmentation models
capable of generalizing rapidly to novel classes using minimal exemplars.
Conventional training paradigms typically generate query prior maps by
extracting masked-area features from support images, followed by making
predictions guided by these prior maps. However, current approaches remain
constrained by two critical limitations stemming from inter- and intra-image
discrepancies, both of which significantly degrade segmentation performance: 1)
The semantic gap between support and query images results in mismatched
features and inaccurate prior maps; 2) Visually similar yet semantically
distinct regions within support or query images lead to false negative or false
positive predictions. We propose a novel FSS method called \textbf{I$^2$R}: 1)
Using category-specific high level representations which aggregate global
semantic cues from support and query images, enabling more precise inter-image
region localization and address the first limitation. 2) Directional masking
strategy that suppresses inconsistent support-query pixel pairs, which exhibit
high feature similarity but conflicting mask, to mitigate the second issue.
Experiments demonstrate that our method outperforms state-of-the-art
approaches, achieving improvements of 1.9\% and 2.1\% in mIoU under the 1-shot
setting on PASCAL-5$^i$ and COCO-20$^i$ benchmarks, respectively.

</details>


### [48] [USIGAN: Unbalanced Self-Information Feature Transport for Weakly Paired Image IHC Virtual Staining](https://arxiv.org/abs/2507.05843)
*Yue Peng,Bing Xiong,Fuqiang Chen,De Eybo,RanRan Zhang,Wanming Hu,Jing Cai,Wenjian Qin*

Main category: cs.CV

TL;DR: The paper proposes USIGAN, a method for virtual IHC staining from H&E images, addressing challenges of weak pairing and spatial heterogeneity to improve pathological consistency.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of weakly paired conditions in IHC virtual staining, which cause inaccurate mappings and inconsistent pathological semantics.

Method: Introduces USIGAN with Unbalanced Optimal Transport Consistency (UOT-CTM) and Pathology Self-Correspondence (PC-SCM) mechanisms to enhance global morphological semantics and mitigate weak pairing effects.

Result: Outperforms on clinical metrics like IoD and Pearson-R correlation, showing better clinical relevance.

Conclusion: USIGAN effectively improves content and pathological consistency in virtual IHC staining, offering a robust solution for pathological analysis.

Abstract: Immunohistochemical (IHC) virtual staining is a task that generates virtual
IHC images from H\&E images while maintaining pathological semantic consistency
with adjacent slices. This task aims to achieve cross-domain mapping between
morphological structures and staining patterns through generative models,
providing an efficient and cost-effective solution for pathological analysis.
However, under weakly paired conditions, spatial heterogeneity between adjacent
slices presents significant challenges. This can lead to inaccurate one-to-many
mappings and generate results that are inconsistent with the pathological
semantics of adjacent slices. To address this issue, we propose a novel
unbalanced self-information feature transport for IHC virtual staining, named
USIGAN, which extracts global morphological semantics without relying on
positional correspondence.By removing weakly paired terms in the joint marginal
distribution, we effectively mitigate the impact of weak pairing on joint
distributions, thereby significantly improving the content consistency and
pathological semantic consistency of the generated results. Moreover, we design
the Unbalanced Optimal Transport Consistency (UOT-CTM) mechanism and the
Pathology Self-Correspondence (PC-SCM) mechanism to construct correlation
matrices between H\&E and generated IHC in image-level and real IHC and
generated IHC image sets in intra-group level.. Experiments conducted on two
publicly available datasets demonstrate that our method achieves superior
performance across multiple clinically significant metrics, such as IoD and
Pearson-R correlation, demonstrating better clinical relevance.

</details>


### [49] [DFYP: A Dynamic Fusion Framework with Spectral Channel Attention and Adaptive Operator learning for Crop Yield Prediction](https://arxiv.org/abs/2507.05849)
*Juli Zhang,Zeyu Yan,Jing Zhang,Qiguang Miao,Quan Wang*

Main category: cs.CV

TL;DR: DFYP is a dynamic fusion framework for crop yield prediction, improving robustness via spectral channel attention, edge-adaptive spatial modeling, and a learnable fusion mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack spatial modeling capacity and generalization across crop types and years, necessitating a more robust solution.

Method: DFYP integrates Resolution-aware Channel Attention (RCA), Adaptive Operator Learning Network (AOL-Net), and a dual-branch fusion mechanism to enhance spectral and spatial feature extraction.

Result: DFYP outperforms state-of-the-art baselines in RMSE, MAE, and R2 across diverse datasets (MODIS, Sentinel-2) and conditions.

Conclusion: DFYP is effective and robust for real-world agricultural monitoring, demonstrating superior generalization across resolutions, crops, and time periods.

Abstract: Accurate remote sensing-based crop yield prediction remains a fundamental
challenging task due to complex spatial patterns, heterogeneous spectral
characteristics, and dynamic agricultural conditions. Existing methods often
suffer from limited spatial modeling capacity, weak generalization across crop
types and years. To address these challenges, we propose DFYP, a novel Dynamic
Fusion framework for crop Yield Prediction, which combines spectral channel
attention, edge-adaptive spatial modeling and a learnable fusion mechanism to
improve robustness across diverse agricultural scenarios. Specifically, DFYP
introduces three key components: (1) a Resolution-aware Channel Attention (RCA)
module that enhances spectral representation by adaptively reweighting input
channels based on resolution-specific characteristics; (2) an Adaptive Operator
Learning Network (AOL-Net) that dynamically selects operators for convolutional
kernels to improve edge-sensitive spatial feature extraction under varying crop
and temporal conditions; and (3) a dual-branch architecture with a learnable
fusion mechanism, which jointly models local spatial details and global
contextual information to support cross-resolution and cross-crop
generalization. Extensive experiments on multi-year datasets MODIS and
multi-crop dataset Sentinel-2 demonstrate that DFYP consistently outperforms
current state-of-the-art baselines in RMSE, MAE, and R2 across different
spatial resolutions, crop types, and time periods, showcasing its effectiveness
and robustness for real-world agricultural monitoring.

</details>


### [50] [D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos](https://arxiv.org/abs/2507.05859)
*Wenkang Zhang,Yan Zhao,Qiang Wang,Li Song,Zhengxue Cheng*

Main category: cs.CV

TL;DR: D-FCGS is a feedforward framework for compressing dynamic Gaussian Splatting sequences, using Group-of-Frames coding and motion tensors, achieving high compression rates without per-scene optimization.


<details>
  <summary>Details</summary>
Motivation: Efficient compression of dynamic 3D representations for free-viewpoint video (FVV) is challenging, and existing methods lack generalizability due to coupling with optimization-dependent coding.

Method: Introduces a Group-of-Frames structure with I-P frame coding, motion extraction via sparse control points, and a dual prior-aware entropy model for compression. Reconstruction uses motion compensation and a refinement network.

Result: Achieves over 40 times compression in under 2 seconds, matching optimization-based methods in rate-distortion performance while preserving visual quality.

Conclusion: D-FCGS advances feedforward compression for dynamic 3DGS, enabling scalable FVV transmission and storage in immersive applications.

Abstract: Free-viewpoint video (FVV) enables immersive 3D experiences, but efficient
compression of dynamic 3D representations remains a major challenge. Recent
advances in 3D Gaussian Splatting (3DGS) and its dynamic extensions have
enabled high-fidelity scene modeling. However, existing methods often couple
scene reconstruction with optimization-dependent coding, which limits
generalizability. This paper presents Feedforward Compression of Dynamic
Gaussian Splatting (D-FCGS), a novel feedforward framework for compressing
temporally correlated Gaussian point cloud sequences. Our approach introduces a
Group-of-Frames (GoF) structure with I-P frame coding, where inter-frame
motions are extracted via sparse control points. The resulting motion tensors
are compressed in a feedforward manner using a dual prior-aware entropy model
that combines hyperprior and spatial-temporal priors for accurate rate
estimation. For reconstruction, we perform control-point-guided motion
compensation and employ a refinement network to enhance view-consistent
fidelity. Trained on multi-view video-derived Gaussian frames, D-FCGS
generalizes across scenes without per-scene optimization. Experiments show that
it matches the rate-distortion performance of optimization-based methods,
achieving over 40 times compression in under 2 seconds while preserving visual
quality across viewpoints. This work advances feedforward compression for
dynamic 3DGS, paving the way for scalable FVV transmission and storage in
immersive applications.

</details>


### [51] [GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing](https://arxiv.org/abs/2507.05887)
*Xianzhi Ma,Jianhui Li,Changhua Pei,Hao Liu*

Main category: cs.CV

TL;DR: GeoMag is a new framework for remote sensing image understanding, addressing limitations of existing Vision-Language Models (VLMs) by dynamically adjusting attention and resolution for multi-granularity tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RS-VLMs struggle with pixel-level tasks, small-object recognition, and high computational costs for high-resolution images.

Method: GeoMag uses Task-driven Multi-granularity Resolution Adjustment (TMRA) and Prompt-guided Semantic-aware Cropping (PSC) to focus on relevant regions and reduce redundancy.

Result: GeoMag outperforms existing RS-VLMs on 10 benchmarks, excelling in pixel-level tasks while maintaining performance across other granularities.

Conclusion: GeoMag offers a scalable and efficient solution for multi-granularity remote sensing image parsing, reducing computational costs and improving accuracy.

Abstract: The application of Vision-Language Models (VLMs) in remote sensing (RS) image
understanding has achieved notable progress, demonstrating the basic ability to
recognize and describe geographical entities. However, existing RS-VLMs are
mostly limited to image-level and region-level tasks, lacking the capability to
handle pixel-level tasks and performing poorly in small-object recognition
scenarios. Moreover, RS-VLMs consume significant computational resources when
processing high-resolution RS images, further restricting their practical
applicability. In this context, we propose GeoMag (Geographical Magnifier), an
end-to-end general-purpose large model framework for RS. GeoMag dynamically
focuses the attention scope based on prompt semantics to effectively perform
remote sensing image parsing across multiple levels of granularity. This method
introduces Task-driven Multi-granularity Resolution Adjustment (TMRA) and
Prompt-guided Semantic-aware Cropping (PSC), which adaptively reduce the
spatial resolution of task-irrelevant regions while enhancing the visual
representation of task-relevant areas. This approach improves the model's
perception of critical target regions, suppresses background redundancy, and
reduces the computational cost of interpreting high-resolution RS imagery.
Extensive comparative experiments on 10 benchmarks demonstrate that GeoMag not
only excels in handling pixel-level tasks but also maintains competitive
performance across tasks of other granularities compared to existing RS-VLMs.

</details>


### [52] [What You Have is What You Track: Adaptive and Robust Multimodal Tracking](https://arxiv.org/abs/2507.05899)
*Yuedong Tan,Jiawei Shao,Eduard Zamfir,Ruanjun Li,Zhaochong An,Chao Ma,Danda Paudel,Luc Van Gool,Radu Timofte,Zongwei Wu*

Main category: cs.CV

TL;DR: The paper introduces a flexible framework for robust multimodal tracking, addressing performance degradation in existing trackers due to temporally incomplete data. The proposed model dynamically adapts to missing modalities and scene complexity, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing trackers struggle with temporally incomplete multimodal data due to rigid architectures, leading to performance degradation. This gap in adaptability is underexplored.

Method: A flexible framework with a Heterogeneous Mixture-of-Experts fusion mechanism and video-level masking strategy is proposed to dynamically handle missing modalities and ensure consistency.

Result: The model outperforms existing trackers, achieving SOTA performance on 9 benchmarks in both complete and missing modality settings.

Conclusion: The proposed framework effectively addresses the challenges of incomplete multimodal data, offering adaptability and superior performance.

Abstract: Multimodal data is known to be helpful for visual tracking by improving
robustness to appearance variations. However, sensor synchronization challenges
often compromise data availability, particularly in video settings where
shortages can be temporal. Despite its importance, this area remains
underexplored. In this paper, we present the first comprehensive study on
tracker performance with temporally incomplete multimodal data. Unsurprisingly,
under such a circumstance, existing trackers exhibit significant performance
degradation, as their rigid architectures lack the adaptability needed to
effectively handle missing modalities. To address these limitations, we propose
a flexible framework for robust multimodal tracking. We venture that a tracker
should dynamically activate computational units based on missing data rates.
This is achieved through a novel Heterogeneous Mixture-of-Experts fusion
mechanism with adaptive complexity, coupled with a video-level masking strategy
that ensures both temporal consistency and spatial completeness which is
critical for effective video tracking. Surprisingly, our model not only adapts
to varying missing rates but also adjusts to scene complexity. Extensive
experiments show that our model achieves SOTA performance across 9 benchmarks,
excelling in both conventional complete and missing modality settings. The code
and benchmark will be publicly available at
https://github.com/supertyd/FlexTrack/tree/main.

</details>


### [53] [On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification](https://arxiv.org/abs/2507.05916)
*Jonas Klotz,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: The paper evaluates the suitability of explainable AI (xAI) methods and metrics for remote sensing (RS) scene classification, identifying limitations and providing guidelines for their use.


<details>
  <summary>Details</summary>
Motivation: Most xAI methods and metrics are designed for natural images in computer vision, and their direct application to RS may not be appropriate.

Method: The study analyzes ten explanation metrics across five categories, applied to five feature attribution methods on three RS datasets.

Result: Perturbation-based methods depend on baselines and scene characteristics; gradient-based methods struggle with multi-label images; some metrics are unreliable for large spatial extents. Robustness and randomization metrics are more stable.

Conclusion: Guidelines are provided for selecting xAI methods, metrics, and hyperparameters in RS scene classification.

Abstract: The development of explainable artificial intelligence (xAI) methods for
scene classification problems has attracted great attention in remote sensing
(RS). Most xAI methods and the related evaluation metrics in RS are initially
developed for natural images considered in computer vision (CV), and their
direct usage in RS may not be suitable. To address this issue, in this paper,
we investigate the effectiveness of explanation methods and metrics in the
context of RS image scene classification. In detail, we methodologically and
experimentally analyze ten explanation metrics spanning five categories
(faithfulness, robustness, localization, complexity, randomization), applied to
five established feature attribution methods (Occlusion, LIME, GradCAM, LRP,
and DeepLIFT) across three RS datasets. Our methodological analysis identifies
key limitations in both explanation methods and metrics. The performance of
perturbation-based methods, such as Occlusion and LIME, heavily depends on
perturbation baselines and spatial characteristics of RS scenes. Gradient-based
approaches like GradCAM struggle when multiple labels are present in the same
image, while some relevance propagation methods (LRP) can distribute relevance
disproportionately relative to the spatial extent of classes. Analogously, we
find limitations in evaluation metrics. Faithfulness metrics share the same
problems as perturbation-based methods. Localization metrics and complexity
metrics are unreliable for classes with a large spatial extent. In contrast,
robustness metrics and randomization metrics consistently exhibit greater
stability. Our experimental results support these methodological findings.
Based on our analysis, we provide guidelines for selecting explanation methods,
metrics, and hyperparameters in the context of RS image scene classification.

</details>


### [54] [High-Resolution Visual Reasoning via Multi-Turn Grounding-Based Reinforcement Learning](https://arxiv.org/abs/2507.05920)
*Xinyu Huang,Yuhao Dong,Weiwei Tian,Bo Li,Rui Feng,Ziwei Liu*

Main category: cs.CV

TL;DR: MGPO is a reinforcement learning framework that helps large multi-modal models (LMMs) focus on key visual regions by cropping sub-images iteratively, improving grounding abilities without costly annotations.


<details>
  <summary>Details</summary>
Motivation: LMMs struggle with high-resolution images due to irrelevant visual tokens. Current methods like supervised fine-tuning require expensive grounding annotations.

Method: MGPO uses RL to iteratively crop sub-images based on predicted grounding coordinates in a multi-turn conversation framework, leveraging a binary reward function.

Result: MGPO improves grounding capabilities, achieving 5.4% and 5.2% gains on MME-Realworld and V* Bench, respectively, outperforming OpenAI models on OOD tasks.

Conclusion: MGPO demonstrates robust grounding emergence in LMMs through RL, offering a cost-effective alternative to supervised methods.

Abstract: State-of-the-art large multi-modal models (LMMs) face challenges when
processing high-resolution images, as these inputs are converted into enormous
visual tokens, many of which are irrelevant to the downstream task. In this
paper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an
end-to-end reinforcement learning (RL) framework that enables LMMs to
iteratively focus on key visual regions by automatically cropping sub-images,
based on model-predicted grounding coordinates within a multi-turn conversation
framework. Compared to supervised fine-tuning (SFT), which requires costly
additional grounding annotations, our approach highlights that LMMs can emerge
robust grounding abilities during the RL training process, leveraging only a
binary reward function derived from the correctness of the final answer.
Additionally, we observe that LMMs struggle to autonomously trigger visual
grounding during the rollout process. To address this cold start problem, we
design a multi-turn conversational template and restrict policy loss
computation to model outputs generated across multiple dialogue rounds, thereby
promoting stable optimization. Extensive experiments demonstrate that, when
trained on standard visual-question-short answering data without grounding
annotations, MGPO effectively elicits stronger grounding capabilities compared
to GRPO, leading to 5.4\% improvement on in-distribution MME-Realworld and
5.2\% improvement on the challenging out-of-distribution (OOD) V* Bench.
Notably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses
OpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at
https://github.com/EvolvingLMMs-Lab/MGPO.

</details>


### [55] [Beyond Appearance: Geometric Cues for Robust Video Instance Segmentation](https://arxiv.org/abs/2507.05948)
*Quanzhu Niu,Yikang Zhou,Shihao Chen,Tao Zhang,Shunping Ji*

Main category: cs.CV

TL;DR: The paper introduces geometric awareness via monocular depth estimation to improve Video Instance Segmentation (VIS), addressing challenges like occlusions and motion blur. Three methods (EDC, SV, DS) are explored, with EDC and SV showing significant robustness improvements. EDC achieves a state-of-the-art 56.2 AP on OVIS benchmark.


<details>
  <summary>Details</summary>
Motivation: VIS struggles with occlusions, motion blur, and appearance variations during temporal association. The work aims to enhance VIS robustness by incorporating depth cues.

Method: Three integration paradigms are investigated: Expanding Depth Channel (EDC), Sharing ViT (SV), and Depth Supervision (DS). EDC adds depth maps as input, SV shares a ViT backbone, and DS uses depth prediction for auxiliary training.

Result: EDC and SV significantly improve VIS robustness, with EDC achieving 56.2 AP on OVIS benchmark, setting a new state-of-the-art. DS shows limited effectiveness.

Conclusion: Depth cues are critical for robust video understanding, with EDC and SV proving effective in enhancing VIS performance.

Abstract: Video Instance Segmentation (VIS) fundamentally struggles with pervasive
challenges including object occlusions, motion blur, and appearance variations
during temporal association. To overcome these limitations, this work
introduces geometric awareness to enhance VIS robustness by strategically
leveraging monocular depth estimation. We systematically investigate three
distinct integration paradigms. Expanding Depth Channel (EDC) method
concatenates the depth map as input channel to segmentation networks; Sharing
ViT (SV) designs a uniform ViT backbone, shared between depth estimation and
segmentation branches; Depth Supervision (DS) makes use of depth prediction as
an auxiliary training guide for feature learning. Though DS exhibits limited
effectiveness, benchmark evaluations demonstrate that EDC and SV significantly
enhance the robustness of VIS. When with Swin-L backbone, our EDC method gets
56.2 AP, which sets a new state-of-the-art result on OVIS benchmark. This work
conclusively establishes depth cues as critical enablers for robust video
understanding.

</details>


### [56] [High-Fidelity and Generalizable Neural Surface Reconstruction with Sparse Feature Volumes](https://arxiv.org/abs/2507.05952)
*Aoxiang Fan,Corentin Dumery,Nicolas Talabot,Hieu Le,Pascal Fua*

Main category: cs.CV

TL;DR: A sparse representation method for neural surface reconstruction improves memory efficiency and enables higher-resolution reconstructions without performance loss.


<details>
  <summary>Details</summary>
Motivation: Dense 3D feature volumes limit reconstruction quality due to poor scalability with increasing voxel resolutions.

Method: A two-stage approach: predict voxel occupancies from images and depth maps, then compute features and render only in high-occupancy voxels. Custom algorithms handle sparse volumes efficiently.

Result: Reduces storage by 50x, enables 512³ resolution (vs. 128³), and achieves superior accuracy over state-of-the-art methods.

Conclusion: Sparse representation significantly enhances scalability and quality in neural surface reconstruction.

Abstract: Generalizable neural surface reconstruction has become a compelling technique
to reconstruct from few images without per-scene optimization, where dense 3D
feature volume has proven effective as a global representation of scenes.
However, the dense representation does not scale well to increasing voxel
resolutions, severely limiting the reconstruction quality. We thus present a
sparse representation method, that maximizes memory efficiency and enables
significantly higher resolution reconstructions on standard hardware. We
implement this through a two-stage approach: First training a network to
predict voxel occupancies from posed images and associated depth maps, then
computing features and performing volume rendering only in voxels with
sufficiently high occupancy estimates. To support this sparse representation,
we developed custom algorithms for efficient sampling, feature aggregation, and
querying from sparse volumes-overcoming the dense-volume assumptions inherent
in existing works. Experiments on public datasets demonstrate that our approach
reduces storage requirements by more than 50 times without performance
degradation, enabling reconstructions at $512^3$ resolution compared to the
typical $128^3$ on similar hardware, and achieving superior reconstruction
accuracy over current state-of-the-art methods.

</details>


### [57] [Tora2: Motion and Appearance Customized Diffusion Transformer for Multi-Entity Video Generation](https://arxiv.org/abs/2507.05963)
*Zhenghao Zhang,Junchao Liao,Xiangyu Meng,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Tora2 enhances Tora with improved appearance and motion customization, introducing a decoupled personalization extractor, gated self-attention, and contrastive loss for better performance in multi-entity video generation.


<details>
  <summary>Details</summary>
Motivation: To advance motion-guided video generation by enabling simultaneous multi-entity customization of appearance and motion, addressing misalignment in multimodal conditioning.

Method: Introduces a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss for joint optimization of trajectory dynamics and entity consistency.

Result: Tora2 achieves competitive performance with state-of-the-art methods, offering advanced motion control and multi-entity customization.

Conclusion: Tora2 marks a critical advancement in multi-condition video generation, setting a new benchmark for simultaneous appearance and motion customization.

Abstract: Recent advances in diffusion transformer models for motion-guided video
generation, such as Tora, have shown significant progress. In this paper, we
present Tora2, an enhanced version of Tora, which introduces several design
improvements to expand its capabilities in both appearance and motion
customization. Specifically, we introduce a decoupled personalization extractor
that generates comprehensive personalization embeddings for multiple open-set
entities, better preserving fine-grained visual details compared to previous
methods. Building on this, we design a gated self-attention mechanism to
integrate trajectory, textual description, and visual information for each
entity. This innovation significantly reduces misalignment in multimodal
conditioning during training. Moreover, we introduce a contrastive loss that
jointly optimizes trajectory dynamics and entity consistency through explicit
mapping between motion and personalization embeddings. Tora2 is, to our best
knowledge, the first method to achieve simultaneous multi-entity customization
of appearance and motion for video generation. Experimental results demonstrate
that Tora2 achieves competitive performance with state-of-the-art customization
methods while providing advanced motion control capabilities, which marks a
critical advancement in multi-condition video generation. Project page:
https://github.com/alibaba/Tora .

</details>


### [58] [T-LoRA: Single Image Diffusion Model Customization Without Overfitting](https://arxiv.org/abs/2507.05964)
*Vera Soboleva,Aibek Alanov,Andrey Kuznetsov,Konstantin Sobolev*

Main category: cs.CV

TL;DR: T-LoRA introduces a timestep-dependent low-rank adaptation framework for fine-tuning diffusion models with limited data, addressing overfitting and improving generalization.


<details>
  <summary>Details</summary>
Motivation: Overfitting in diffusion model fine-tuning with limited samples compromises generalization and diversity. Single-image customization is highly practical but challenging.

Method: T-LoRA uses a dynamic fine-tuning strategy adjusting rank-constrained updates by timestep and a weight parametrization technique with orthogonal initialization.

Result: T-LoRA outperforms standard LoRA and other methods, balancing concept fidelity and text alignment effectively.

Conclusion: T-LoRA is a promising solution for data-limited scenarios, enhancing diffusion model personalization.

Abstract: While diffusion model fine-tuning offers a powerful approach for customizing
pre-trained models to generate specific objects, it frequently suffers from
overfitting when training samples are limited, compromising both generalization
capability and output diversity. This paper tackles the challenging yet most
impactful task of adapting a diffusion model using just a single concept image,
as single-image customization holds the greatest practical potential. We
introduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework
specifically designed for diffusion model personalization. In our work we show
that higher diffusion timesteps are more prone to overfitting than lower ones,
necessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates
two key innovations: (1) a dynamic fine-tuning strategy that adjusts
rank-constrained updates based on diffusion timesteps, and (2) a weight
parametrization technique that ensures independence between adapter components
through orthogonal initialization. Extensive experiments show that T-LoRA and
its individual components outperform standard LoRA and other diffusion model
personalization techniques. They achieve a superior balance between concept
fidelity and text alignment, highlighting the potential of T-LoRA in
data-limited and resource-constrained scenarios. Code is available at
https://github.com/ControlGenAI/T-LoRA.

</details>


### [59] [Automatic Synthesis of High-Quality Triplet Data for Composed Image Retrieval](https://arxiv.org/abs/2507.05970)
*Haiwen Li,Delong Liu,Zhaohui Hou,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: Proposes a scalable pipeline for automatic triplet generation and a synthetic dataset (CIRHS) for Composed Image Retrieval (CIR), along with a novel framework (CoAlign) for robust zero-shot and supervised performance.


<details>
  <summary>Details</summary>
Motivation: Existing CIR methods rely on costly manual triplet labeling, limiting scalability and zero-shot capability.

Method: Uses LLM-generated prompts to create synthetic image pairs, forming the CIRHS dataset, and introduces CoAlign for global and local alignment.

Result: CoAlign achieves outstanding zero-shot performance and outperforms state-of-the-art supervised CIR methods.

Conclusion: Demonstrates feasibility of training CIR models on synthetic data and validates effectiveness of CoAlign.

Abstract: As a challenging vision-language (VL) task, Composed Image Retrieval (CIR)
aims to retrieve target images using multimodal (image+text) queries. Although
many existing CIR methods have attained promising performance, their reliance
on costly, manually labeled triplets hinders scalability and zero-shot
capability. To address this issue, we propose a scalable pipeline for automatic
triplet generation, along with a fully synthetic dataset named Composed Image
Retrieval on High-quality Synthetic Triplets (CIRHS). Our pipeline leverages a
large language model (LLM) to generate diverse prompts, controlling a
text-to-image generative model to produce image pairs with identical elements
in each pair, which are then filtered and reorganized to form the CIRHS
dataset. In addition, we introduce Hybrid Contextual Alignment (CoAlign), a
novel CIR framework, which can accomplish global alignment and local reasoning
within a broader context, enabling the model to learn more robust and
informative representations. By utilizing the synthetic CIRHS dataset, CoAlign
achieves outstanding zero-shot performance on three commonly used benchmarks,
demonstrating for the first time the feasibility of training CIR models on a
fully synthetic dataset. Furthermore, under supervised training, our method
outperforms all the state-of-the-art supervised CIR approaches, validating the
effectiveness of our proposed retrieval framework. The code and the CIRHS
dataset will be released soon.

</details>


### [60] [Exploring Partial Multi-Label Learning via Integrating Semantic Co-occurrence Knowledge](https://arxiv.org/abs/2507.05992)
*Xin Wu,Fei Teng,Yue Feng,Kaibo Shi,Zhuosheng Lin,Ji Zhang,James Wang*

Main category: cs.CV

TL;DR: SCINet is a new framework for partial multi-label learning, using semantic co-occurrence patterns and multimodal fusion to improve label-instance relationships, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Partial multi-label learning deals with incompletely annotated data, requiring accurate identification of ambiguous label-instance relationships.

Method: SCINet uses a bi-dominant prompter module for text-image correlation, cross-modality fusion for interdependencies, and semantic augmentation for better data understanding.

Result: SCINet outperforms state-of-the-art methods on four benchmark datasets.

Conclusion: SCINet effectively addresses partial multi-label learning challenges by leveraging semantic co-occurrence and multimodal fusion.

Abstract: Partial multi-label learning aims to extract knowledge from incompletely
annotated data, which includes known correct labels, known incorrect labels,
and unknown labels. The core challenge lies in accurately identifying the
ambiguous relationships between labels and instances. In this paper, we
emphasize that matching co-occurrence patterns between labels and instances is
key to addressing this challenge. To this end, we propose Semantic
Co-occurrence Insight Network (SCINet), a novel and effective framework for
partial multi-label learning. Specifically, SCINet introduces a bi-dominant
prompter module, which leverages an off-the-shelf multimodal model to capture
text-image correlations and enhance semantic alignment. To reinforce
instance-label interdependencies, we develop a cross-modality fusion module
that jointly models inter-label correlations, inter-instance relationships, and
co-occurrence patterns across instance-label assignments. Moreover, we propose
an intrinsic semantic augmentation strategy that enhances the model's
understanding of intrinsic data semantics by applying diverse image
transformations, thereby fostering a synergistic relationship between label
confidence and sample difficulty. Extensive experiments on four widely-used
benchmark datasets demonstrate that SCINet surpasses state-of-the-art methods.

</details>


### [61] [Ensemble-Based Deepfake Detection using State-of-the-Art Models with Robust Cross-Dataset Generalisation](https://arxiv.org/abs/2507.05996)
*Haroon Wahab,Hassan Ugail,Lujain Jaleel*

Main category: cs.CV

TL;DR: Ensemble-based deepfake detection improves generalization across diverse datasets compared to single models.


<details>
  <summary>Details</summary>
Motivation: Address performance drop of deepfake detection models on out-of-distribution data.

Method: Combine prediction probabilities from asymmetric state-of-the-art models.

Result: Ensemble predictions outperform single models in stability and reliability.

Conclusion: Asymmetric ensembling is robust and scalable for real-world deepfake detection.

Abstract: Machine learning-based Deepfake detection models have achieved impressive
results on benchmark datasets, yet their performance often deteriorates
significantly when evaluated on out-of-distribution data. In this work, we
investigate an ensemble-based approach for improving the generalization of
deepfake detection systems across diverse datasets. Building on a recent
open-source benchmark, we combine prediction probabilities from several
state-of-the-art asymmetric models proposed at top venues. Our experiments span
two distinct out-of-domain datasets and demonstrate that no single model
consistently outperforms others across settings. In contrast, ensemble-based
predictions provide more stable and reliable performance in all scenarios. Our
results suggest that asymmetric ensembling offers a robust and scalable
solution for real-world deepfake detection where prior knowledge of forgery
type or quality is often unavailable.

</details>


### [62] [Geo-Registration of Terrestrial LiDAR Point Clouds with Satellite Images without GNSS](https://arxiv.org/abs/2507.05999)
*Xinyu Wang,Muhammad Ibrahim,Atif Mansoor,Ajmal Mian*

Main category: cs.CV

TL;DR: A method for geo-registering LiDAR point clouds in GNSS-denied urban areas using satellite images and Point Transformer models, improving alignment accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: Challenges in geo-registration of LiDAR point clouds in GNSS-denied urban areas due to unreliable GNSS/IMU data.

Method: Uses a Point Transformer model to segment roads, extracts road skeletons and intersection points for alignment, performs global rigid alignment and local refinement with RBF interpolation, and corrects elevation using SRTM data.

Result: Achieved 55.3% and 77.4% improvement in planimetric alignment on KITTI and Perth datasets, respectively, and significant elevation correlation gains.

Conclusion: The proposed method effectively addresses GNSS-denied geo-registration challenges, improving accuracy for urban 3D mapping.

Abstract: Accurate geo-registration of LiDAR point clouds presents significant
challenges in GNSS signal denied urban areas with high-rise buildings and
bridges. Existing methods typically rely on real-time GNSS and IMU data, that
require pre-calibration and assume stable positioning during data collection.
However, this assumption often fails in dense urban areas, resulting in
localization errors. To address this, we propose a structured geo-registration
and spatial correction method that aligns 3D point clouds with satellite
images, enabling frame-wise recovery of GNSS information and reconstruction of
city scale 3D maps without relying on prior localization. The proposed approach
employs a pre-trained Point Transformer model to segment the road points and
then extracts the road skeleton and intersection points from the point cloud as
well as the target map for alignment. Global rigid alignment of the two is
performed using the intersection points, followed by local refinement using
radial basis function (RBF) interpolation. Elevation correction is then applied
to the point cloud based on terrain information from SRTM dataset to resolve
vertical discrepancies. The proposed method was tested on the popular KITTI
benchmark and a locally collected Perth (Western Australia) CBD dataset. On the
KITTI dataset, our method achieved an average planimetric alignment standard
deviation (STD) of 0.84~m across sequences with intersections, representing a
55.3\% improvement over the original dataset. On the Perth dataset, which lacks
GNSS information, our method achieved an average STD of 0.96~m compared to the
GPS data extracted from Google Maps API. This corresponds to a 77.4\%
improvement from the initial alignment. Our method also resulted in elevation
correlation gains of 30.5\% on the KITTI dataset and 50.4\% on the Perth
dataset.

</details>


### [63] [TextPixs: Glyph-Conditioned Diffusion with Character-Aware Attention and OCR-Guided Supervision](https://arxiv.org/abs/2507.06033)
*Syeda Anshrah Gillani,Mirza Samad Ahmed Baig,Osama Ahmed Khan,Shahid Munir Shah,Umema Mujeeb,Maheen Ali*

Main category: cs.CV

TL;DR: GCDA introduces a framework to improve text rendering in diffusion models, achieving state-of-the-art results in readability and image quality.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image models struggle with generating readable and correctly spelled text, limiting practical applications like advertising and design.

Method: GCDA extends diffusion models with a dual-stream text encoder, character-aware attention, and OCR-in-the-loop fine-tuning.

Result: GCDA outperforms benchmarks with lower error rates (CER: 0.08, WER: 0.15) and high image fidelity (FID: 14.3).

Conclusion: GCDA significantly improves text rendering in diffusion models, enabling broader practical use.

Abstract: The modern text-to-image diffusion models boom has opened a new era in
digital content production as it has proven the previously unseen ability to
produce photorealistic and stylistically diverse imagery based on the semantics
of natural-language descriptions. However, the consistent disadvantage of these
models is that they cannot generate readable, meaningful, and correctly spelled
text in generated images, which significantly limits the use of practical
purposes like advertising, learning, and creative design. This paper introduces
a new framework, namely Glyph-Conditioned Diffusion with Character-Aware
Attention (GCDA), using which a typical diffusion backbone is extended by three
well-designed modules. To begin with, the model has a dual-stream text encoder
that encodes both semantic contextual information and explicit glyph
representations, resulting in a character-aware representation of the input
text that is rich in nature. Second, an attention mechanism that is aware of
the character is proposed with a new attention segregation loss that aims to
limit the attention distribution of each character independently in order to
avoid distortion artifacts. Lastly, GCDA has an OCR-in-the-loop fine-tuning
phase, where a full text perceptual loss, directly optimises models to be
legible and accurately spell. Large scale experiments to benchmark datasets,
such as MARIO-10M and T2I-CompBench, reveal that GCDA sets a new
state-of-the-art on all metrics, with better character based metrics on text
rendering (Character Error Rate: 0.08 vs 0.21 for the previous best; Word Error
Rate: 0.15 vs 0.25), human perception, and comparable image synthesis quality
on high-fidelity (FID: 14.3).

</details>


### [64] [VisualSpeaker: Visually-Guided 3D Avatar Lip Synthesis](https://arxiv.org/abs/2507.06060)
*Alexandre Symeonidis-Herzig,Özge Mercanoğlu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: VisualSpeaker improves 3D facial animation using photorealistic rendering and perceptual lip-reading loss, outperforming prior methods by 56.1% in Lip Vertex Error.


<details>
  <summary>Details</summary>
Motivation: High-fidelity 3D facial animations are needed for expressive avatars, but existing mesh-based methods lag behind 2D innovations.

Method: Uses photorealistic differentiable rendering supervised by visual speech recognition, with a perceptual lip-reading loss from 3D Gaussian Splatting and a pre-trained Visual Automatic Speech Recognition model.

Result: Achieves a 56.1% improvement in Lip Vertex Error on the MEAD dataset and enhances perceptual quality while maintaining mesh-driven controllability.

Conclusion: VisualSpeaker bridges the gap between 2D and 3D facial animation, supporting accurate mouthings for applications like sign language avatars.

Abstract: Realistic, high-fidelity 3D facial animations are crucial for expressive
avatar systems in human-computer interaction and accessibility. Although prior
methods show promising quality, their reliance on the mesh domain limits their
ability to fully leverage the rapid visual innovations seen in 2D computer
vision and graphics. We propose VisualSpeaker, a novel method that bridges this
gap using photorealistic differentiable rendering, supervised by visual speech
recognition, for improved 3D facial animation. Our contribution is a perceptual
lip-reading loss, derived by passing photorealistic 3D Gaussian Splatting
avatar renders through a pre-trained Visual Automatic Speech Recognition model
during training. Evaluation on the MEAD dataset demonstrates that VisualSpeaker
improves both the standard Lip Vertex Error metric by 56.1% and the perceptual
quality of the generated animations, while retaining the controllability of
mesh-driven animation. This perceptual focus naturally supports accurate
mouthings, essential cues that disambiguate similar manual signs in sign
language avatars.

</details>


### [65] [MEDTalk: Multimodal Controlled 3D Facial Animation with Dynamic Emotions by Disentangled Embedding](https://arxiv.org/abs/2507.06071)
*Chang Liu,Ye Pan,Chenyang Ding,Susanto Rahardja,Xiaokang Yang*

Main category: cs.CV

TL;DR: MEDTalk is a framework for dynamic emotional 3D facial animation, disentangling content and emotion for realistic expressions using audio, text, and multimodal inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on static emotion labels, limiting diversity and naturalness in facial animations.

Method: Disentangles content and emotion via cross-reconstruction, integrates audio and text for dynamic expressions, and uses multimodal inputs for control.

Result: Generates fine-grained, dynamic emotional expressions suitable for industrial pipelines like MetaHuman.

Conclusion: MEDTalk advances emotional facial animation by enabling independent control and realistic, personalized expressions.

Abstract: Audio-driven emotional 3D facial animation aims to generate synchronized lip
movements and vivid facial expressions. However, most existing approaches focus
on static and predefined emotion labels, limiting their diversity and
naturalness. To address these challenges, we propose MEDTalk, a novel framework
for fine-grained and dynamic emotional talking head generation. Our approach
first disentangles content and emotion embedding spaces from motion sequences
using a carefully designed cross-reconstruction process, enabling independent
control over lip movements and facial expressions. Beyond conventional
audio-driven lip synchronization, we integrate audio and speech text,
predicting frame-wise intensity variations and dynamically adjusting static
emotion features to generate realistic emotional expressions. Furthermore, to
enhance control and personalization, we incorporate multimodal inputs-including
text descriptions and reference expression images-to guide the generation of
user-specified facial expressions. With MetaHuman as the priority, our
generated results can be conveniently integrated into the industrial production
pipeline.

</details>


### [66] [MCAM: Multimodal Causal Analysis Model for Ego-Vehicle-Level Driving Video Understanding](https://arxiv.org/abs/2507.06072)
*Tongtong Cheng,Rongzhen Li,Yixin Xiong,Tao Zhang,Jing Wang,Kai Liu*

Main category: cs.CV

TL;DR: MCAM proposes a multimodal causal analysis model for autonomous driving, addressing shallow causality and spurious correlations by leveraging visual-language alignment and dynamic DAG-based scenario modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack deep causal analysis and ignore ego-vehicle causality, limiting accurate driving behavior recognition.

Method: MCAM uses a multi-level feature extractor, causal analysis module with DAG, and vision-language transformer for alignment.

Result: Achieves SOTA on BDD-X and CoVLA datasets, excelling in causal relationship learning and video sequence analysis.

Conclusion: MCAM effectively models causality in autonomous driving, demonstrating superior performance and applicability.

Abstract: Accurate driving behavior recognition and reasoning are critical for
autonomous driving video understanding. However, existing methods often tend to
dig out the shallow causal, fail to address spurious correlations across
modalities, and ignore the ego-vehicle level causality modeling. To overcome
these limitations, we propose a novel Multimodal Causal Analysis Model (MCAM)
that constructs latent causal structures between visual and language
modalities. Firstly, we design a multi-level feature extractor to capture
long-range dependencies. Secondly, we design a causal analysis module that
dynamically models driving scenarios using a directed acyclic graph (DAG) of
driving states. Thirdly, we utilize a vision-language transformer to align
critical visual features with their corresponding linguistic expressions.
Extensive experiments on the BDD-X, and CoVLA datasets demonstrate that MCAM
achieves SOTA performance in visual-language causal relationship learning.
Furthermore, the model exhibits superior capability in capturing causal
characteristics within video sequences, showcasing its effectiveness for
autonomous driving applications. The code is available at
https://github.com/SixCorePeach/MCAM.

</details>


### [67] [Discontinuity-aware Normal Integration for Generic Central Camera Models](https://arxiv.org/abs/2507.06075)
*Francesco Milano,Manuel López-Antequera,Naina Dhingra,Roland Siegwart,Robert Thiel*

Main category: cs.CV

TL;DR: A novel method for 3D surface recovery from normal maps, handling discontinuities and generic central cameras better than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing normal integration methods poorly handle depth discontinuities and are limited to specific camera models.

Method: Uses a local planarity assumption, modeling constraints between surface normals and ray directions.

Result: Achieves state-of-the-art accuracy on benchmarks and supports generic central cameras.

Conclusion: The proposed method improves normal integration by explicitly modeling discontinuities and accommodating diverse camera models.

Abstract: Recovering a 3D surface from its surface normal map, a problem known as
normal integration, is a key component for photometric shape reconstruction
techniques such as shape-from-shading and photometric stereo. The vast majority
of existing approaches for normal integration handle only implicitly the
presence of depth discontinuities and are limited to orthographic or ideal
pinhole cameras. In this paper, we propose a novel formulation that allows
modeling discontinuities explicitly and handling generic central cameras. Our
key idea is based on a local planarity assumption, that we model through
constraints between surface normals and ray directions. Compared to existing
methods, our approach more accurately approximates the relation between depth
and surface normals, achieves state-of-the-art results on the standard normal
integration benchmark, and is the first to directly handle generic central
camera models.

</details>


### [68] [ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models](https://arxiv.org/abs/2507.06078)
*Chihan Huang,Hao Tang*

Main category: cs.CV

TL;DR: ScoreAdv introduces a diffusion model-based method for generating natural, unrestricted adversarial examples (UAEs) with high success rates and image quality, outperforming GAN-based and traditional methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are vulnerable to adversarial attacks, but existing methods rely on unnatural perturbation constraints or suffer from poor image quality. ScoreAdv aims to address these limitations.

Method: ScoreAdv uses an interpretable adversarial guidance mechanism and saliency maps to shift the sampling distribution toward adversarial examples, leveraging diffusion models' denoising capabilities.

Result: ScoreAdv achieves state-of-the-art attack success rates and image quality on ImageNet and CelebA datasets, attacking both classification and retrieval models.

Conclusion: ScoreAdv demonstrates robustness under defensive measures and outperforms existing methods, offering a promising approach for generating natural UAEs.

Abstract: Despite the success of deep learning across various domains, it remains
vulnerable to adversarial attacks. Although many existing adversarial attack
methods achieve high success rates, they typically rely on $\ell_{p}$-norm
perturbation constraints, which do not align with human perceptual
capabilities. Consequently, researchers have shifted their focus toward
generating natural, unrestricted adversarial examples (UAEs). GAN-based
approaches suffer from inherent limitations, such as poor image quality due to
instability and mode collapse. Meanwhile, diffusion models have been employed
for UAE generation, but they still rely on iterative PGD perturbation
injection, without fully leveraging their central denoising capabilities. In
this paper, we introduce a novel approach for generating UAEs based on
diffusion models, named ScoreAdv. This method incorporates an interpretable
adversarial guidance mechanism to gradually shift the sampling distribution
towards the adversarial distribution, while using an interpretable saliency map
to inject the visual information of a reference image into the generated
samples. Notably, our method is capable of generating an unlimited number of
natural adversarial examples and can attack not only classification models but
also retrieval models. We conduct extensive experiments on ImageNet and CelebA
datasets, validating the performance of ScoreAdv across ten target models in
both black-box and white-box settings. Our results demonstrate that ScoreAdv
achieves state-of-the-art attack success rates and image quality. Furthermore,
the dynamic balance between denoising and adversarial perturbation enables
ScoreAdv to remain robust even under defensive measures.

</details>


### [69] [CAST-Phys: Contactless Affective States Through Physiological signals Database](https://arxiv.org/abs/2507.06080)
*Joaquim Comas,Alexander Joel Vera,Xavier Vives,Eleonora De Filippi,Alexandre Pereda,Federico Sukno*

Main category: cs.CV

TL;DR: The paper introduces CAST-Phys, a dataset for remote multi-modal emotion recognition using facial and physiological signals, addressing limitations of contact-based methods.


<details>
  <summary>Details</summary>
Motivation: The lack of non-contact, multi-modal datasets and the influence of contact-based devices on genuine emotional responses motivate the need for remote physiological emotion recognition.

Method: The study presents the CAST-Phys dataset, combining facial video recordings with physiological signals (PPG, EDA, RR) for remote emotion recognition.

Result: The dataset enables remote signal recovery and demonstrates the importance of physiological signals when facial expressions alone are insufficient.

Conclusion: CAST-Phys advances contactless emotion recognition by validating the effectiveness of multi-modal fusion in realistic scenarios.

Abstract: In recent years, affective computing and its applications have become a
fast-growing research topic. Despite significant advancements, the lack of
affective multi-modal datasets remains a major bottleneck in developing
accurate emotion recognition systems. Furthermore, the use of contact-based
devices during emotion elicitation often unintentionally influences the
emotional experience, reducing or altering the genuine spontaneous emotional
response. This limitation highlights the need for methods capable of extracting
affective cues from multiple modalities without physical contact, such as
remote physiological emotion recognition. To address this, we present the
Contactless Affective States Through Physiological Signals Database
(CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal
remote physiological emotion recognition using facial and physiological cues.
The dataset includes diverse physiological signals, such as
photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate
(RR), alongside high-resolution uncompressed facial video recordings, enabling
the potential for remote signal recovery. Our analysis highlights the crucial
role of physiological signals in realistic scenarios where facial expressions
alone may not provide sufficient emotional information. Furthermore, we
demonstrate the potential of remote multi-modal emotion recognition by
evaluating the impact of individual and fused modalities, showcasing its
effectiveness in advancing contactless emotion recognition technologies.

</details>


### [70] [Tile-Based ViT Inference with Visual-Cluster Priors for Zero-Shot Multi-Species Plant Identification](https://arxiv.org/abs/2507.06093)
*Murilo Gustineli,Anthony Miyaguchi,Adrian Cheung,Divyansh Khattak*

Main category: cs.CV

TL;DR: DS@GT's second-place solution for PlantCLEF 2025 uses a Vision Transformer, tiling strategy, and domain-prior adaptation to achieve a macro-averaged F1 of 0.348.


<details>
  <summary>Details</summary>
Motivation: To address multi-species plant identification in vegetation quadrat images effectively.

Method: Combines fine-tuned Vision Transformer (ViTD2PC24All), 4x4 tiling, and domain-prior adaptation (PaCMAP + K-Means clustering and geolocation filtering).

Result: Achieved a macro-averaged F1 score of 0.348 on the private leaderboard.

Conclusion: The pipeline is effective for plant identification without additional training, with all resources publicly available.

Abstract: We describe DS@GT's second-place solution to the PlantCLEF 2025 challenge on
multi-species plant identification in vegetation quadrat images. Our pipeline
combines (i) a fine-tuned Vision Transformer ViTD2PC24All for patch-level
inference, (ii) a 4x4 tiling strategy that aligns patch size with the network's
518x518 receptive field, and (iii) domain-prior adaptation through PaCMAP +
K-Means visual clustering and geolocation filtering. Tile predictions are
aggregated by majority vote and re-weighted with cluster-specific Bayesian
priors, yielding a macro-averaged F1 of 0.348 (private leaderboard) while
requiring no additional training. All code, configuration files, and
reproducibility scripts are publicly available at
https://github.com/dsgt-arc/plantclef-2025.

</details>


### [71] [Reflections Unlock: Geometry-Aware Reflection Disentanglement in 3D Gaussian Splatting for Photorealistic Scenes Rendering](https://arxiv.org/abs/2507.06103)
*Jiayi Song,Zihan Ye,Qingyuan Zhou,Weidong Yang,Ben Fei,Jingyi Xu,Ying He,Wanli Ouyang*

Main category: cs.CV

TL;DR: Ref-Unlock improves novel view synthesis for reflective scenes by disentangling reflections from geometry using 3D Gaussian Splatting, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NeRF and 3DGS misinterpret reflections as geometry, degrading reconstructions. Ref-Unlock aims to address this.

Method: Uses a dual-branch representation with spherical harmonics, reflection removal, pseudo-depth maps, and geometry-aware constraints.

Result: Outperforms GS-based methods and competes with NeRF models, enabling reflection editing.

Conclusion: Ref-Unlock provides an efficient, generalizable solution for realistic rendering of reflective scenes.

Abstract: Accurately rendering scenes with reflective surfaces remains a significant
challenge in novel view synthesis, as existing methods like Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS) often misinterpret reflections
as physical geometry, resulting in degraded reconstructions. Previous methods
rely on incomplete and non-generalizable geometric constraints, leading to
misalignment between the positions of Gaussian splats and the actual scene
geometry. When dealing with real-world scenes containing complex geometry, the
accumulation of Gaussians further exacerbates surface artifacts and results in
blurred reconstructions. To address these limitations, in this work, we propose
Ref-Unlock, a novel geometry-aware reflection modeling framework based on 3D
Gaussian Splatting, which explicitly disentangles transmitted and reflected
components to better capture complex reflections and enhance geometric
consistency in real-world scenes. Our approach employs a dual-branch
representation with high-order spherical harmonics to capture high-frequency
reflective details, alongside a reflection removal module providing pseudo
reflection-free supervision to guide clean decomposition. Additionally, we
incorporate pseudo-depth maps and a geometry-aware bilateral smoothness
constraint to enhance 3D geometric consistency and stability in decomposition.
Extensive experiments demonstrate that Ref-Unlock significantly outperforms
classical GS-based reflection methods and achieves competitive results with
NeRF-based models, while enabling flexible vision foundation models (VFMs)
driven reflection editing. Our method thus offers an efficient and
generalizable solution for realistic rendering of reflective scenes. Our code
is available at https://ref-unlock.github.io/.

</details>


### [72] [Omni-Video: Democratizing Unified Video Understanding and Generation](https://arxiv.org/abs/2507.06119)
*Zhiyu Tan,Hao Yang,Luozheng Qin,Jia Gong,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video is a unified framework for video understanding, generation, and editing, leveraging MLLMs and diffusion decoders with efficient training and lightweight design.


<details>
  <summary>Details</summary>
Motivation: Current foundational models focus on images, leaving a gap in unified video understanding and generation. Omni-Video aims to bridge this gap.

Method: Integrates MLLMs to produce visual clues for diffusion decoders, with lightweight architectural improvements and efficient multi-stage training.

Result: Demonstrates satisfactory generalization across video generation, editing, and understanding tasks.

Conclusion: Omni-Video effectively unifies video tasks, showcasing potential for broader applications.

Abstract: Notable breakthroughs in unified understanding and generation modeling have
led to remarkable advancements in image understanding, reasoning, production
and editing, yet current foundational models predominantly focus on processing
images, creating a gap in the development of unified models for video
understanding and generation. This report presents Omni-Video, an efficient and
effective unified framework for video understanding, generation, as well as
instruction-based editing. Our key insight is to teach existing multimodal
large language models (MLLMs) to produce continuous visual clues that are used
as the input of diffusion decoders, which produce high-quality videos
conditioned on these visual clues. To fully unlock the potential of our system
for unified video modeling, we integrate several technical improvements: 1) a
lightweight architectural design that respectively attaches a vision head on
the top of MLLMs and a adapter before the input of diffusion decoders, the
former produce visual tokens for the latter, which adapts these visual tokens
to the conditional space of diffusion decoders; and 2) an efficient multi-stage
training scheme that facilitates a fast connection between MLLMs and diffusion
decoders with limited data and computational resources. We empirically
demonstrate that our model exhibits satisfactory generalization abilities
across video generation, editing and understanding tasks.

</details>


### [73] [Prompt-Free Conditional Diffusion for Multi-object Image Augmentation](https://arxiv.org/abs/2507.06146)
*Haoyu Wang,Lei Zhang,Wei Wei,Chen Ding,Yanning Zhang*

Main category: cs.CV

TL;DR: A prompt-free conditional diffusion framework is proposed for multi-object image augmentation, addressing deviations and diversity issues by using local-global semantic fusion and LoRA knowledge injection.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-object image generation either deviate from original data due to text reliance or lack diversity due to over-reliance on original images.

Method: The framework uses local-global semantic fusion to replace text, injects knowledge via LoRA, and employs a reward-based counting loss alongside reconstruction loss.

Result: The method outperforms state-of-the-art baselines, showing strong downstream task gains and out-of-domain generalization.

Conclusion: The proposed framework effectively mitigates deviations and enhances diversity in multi-object image augmentation.

Abstract: Diffusion models has underpinned much recent advances of dataset augmentation
in various computer vision tasks. However, when involving generating
multi-object images as real scenarios, most existing methods either rely
entirely on text condition, resulting in a deviation between the generated
objects and the original data, or rely too much on the original images,
resulting in a lack of diversity in the generated images, which is of limited
help to downstream tasks. To mitigate both problems with one stone, we propose
a prompt-free conditional diffusion framework for multi-object image
augmentation. Specifically, we introduce a local-global semantic fusion
strategy to extract semantics from images to replace text, and inject knowledge
into the diffusion model through LoRA to alleviate the category deviation
between the original model and the target dataset. In addition, we design a
reward model based counting loss to assist the traditional reconstruction loss
for model training. By constraining the object counts of each category instead
of pixel-by-pixel constraints, bridging the quantity deviation between the
generated data and the original data while improving the diversity of the
generated data. Experimental results demonstrate the superiority of the
proposed method over several representative state-of-the-art baselines and
showcase strong downstream task gain and out-of-domain generalization
capabilities. Code is available at
\href{https://github.com/00why00/PFCD}{here}.

</details>


### [74] [SoftReMish: A Novel Activation Function for Enhanced Convolutional Neural Networks for Visual Recognition Performance](https://arxiv.org/abs/2507.06148)
*Mustafa Bayram Gücen*

Main category: cs.CV

TL;DR: SoftReMish, a new activation function, outperforms ReLU, Tanh, and Mish in CNN image classification tasks, achieving superior training loss and validation accuracy on MNIST.


<details>
  <summary>Details</summary>
Motivation: To improve CNN performance in image classification by introducing a better activation function (SoftReMish) compared to existing ones like ReLU, Tanh, and Mish.

Method: Implemented a standard CNN architecture on MNIST, replacing activation functions with SoftReMish and comparing performance metrics (training loss, validation accuracy).

Result: SoftReMish achieved the lowest training loss (3.14e-8) and highest validation accuracy (99.41%), outperforming other functions.

Conclusion: SoftReMish enhances convergence and generalization, making it a strong candidate for visual recognition tasks.

Abstract: In this study, SoftReMish, a new activation function designed to improve the
performance of convolutional neural networks (CNNs) in image classification
tasks, is proposed. Using the MNIST dataset, a standard CNN architecture
consisting of two convolutional layers, max pooling, and fully connected layers
was implemented. SoftReMish was evaluated against popular activation functions
including ReLU, Tanh, and Mish by replacing the activation function in all
trainable layers. The model performance was assessed in terms of minimum
training loss and maximum validation accuracy. Results showed that SoftReMish
achieved a minimum loss (3.14e-8) and a validation accuracy (99.41%),
outperforming all other functions tested. These findings demonstrate that
SoftReMish offers better convergence behavior and generalization capability,
making it a promising candidate for visual recognition tasks.

</details>


### [75] [Normalizing Diffusion Kernels with Optimal Transport](https://arxiv.org/abs/2507.06161)
*Nathan Kessler,Robin Magnet,Jean Feydy*

Main category: cs.CV

TL;DR: The paper introduces a class of smoothing operators for irregular data, derived from similarity matrices, and normalizes them using a symmetric Sinkhorn algorithm to mimic Laplacian properties.


<details>
  <summary>Details</summary>
Motivation: Traditional Laplacian-based smoothing requires well-structured domains, which are often unavailable. Simple convolution kernels and message-passing layers are biased against boundaries, creating a need for a more general approach.

Method: The authors propose a symmetric Sinkhorn algorithm to rescale positive smoothing operators, enabling Laplacian-like smoothing for irregular data like point clouds or sparse grids.

Result: The derived operators approximate heat diffusion and retain spectral properties of Laplacians, useful for shape analysis and matching.

Conclusion: This work bridges the gap between Laplacian-based smoothing and irregular data, offering a principled approach with theoretical guarantees.

Abstract: Smoothing a signal based on local neighborhoods is a core operation in
machine learning and geometry processing. On well-structured domains such as
vector spaces and manifolds, the Laplace operator derived from differential
geometry offers a principled approach to smoothing via heat diffusion, with
strong theoretical guarantees. However, constructing such Laplacians requires a
carefully defined domain structure, which is not always available. Most
practitioners thus rely on simple convolution kernels and message-passing
layers, which are biased against the boundaries of the domain. We bridge this
gap by introducing a broad class of smoothing operators, derived from general
similarity or adjacency matrices, and demonstrate that they can be normalized
into diffusion-like operators that inherit desirable properties from
Laplacians. Our approach relies on a symmetric variant of the Sinkhorn
algorithm, which rescales positive smoothing operators to match the structural
behavior of heat diffusion. This construction enables Laplacian-like smoothing
and processing of irregular data such as point clouds, sparse voxel grids or
mixture of Gaussians. We show that the resulting operators not only approximate
heat diffusion but also retain spectral information from the Laplacian itself,
with applications to shape analysis and matching.

</details>


### [76] [OmniPart: Part-Aware 3D Generation with Semantic Decoupling and Structural Cohesion](https://arxiv.org/abs/2507.06165)
*Yunhan Yang,Yufan Zhou,Yuan-Chen Guo,Zi-Xin Zou,Yukun Huang,Ying-Tian Liu,Hao Xu,Ding Liang,Yan-Pei Cao,Xihui Liu*

Main category: cs.CV

TL;DR: OmniPart is a part-aware 3D object generation framework that decouples the task into structure planning and part synthesis, enabling editable and interpretable 3D content.


<details>
  <summary>Details</summary>
Motivation: Current generative methods produce monolithic 3D shapes, limiting utility for interactive applications requiring editable part structures.

Method: OmniPart uses a two-stage approach: (1) autoregressive structure planning with 2D part masks for intuitive control, and (2) a rectified flow model for consistent part synthesis within the planned layout.

Result: OmniPart achieves state-of-the-art performance, supporting user-defined part granularity and precise localization.

Conclusion: The framework advances interpretable, editable, and versatile 3D content generation.

Abstract: The creation of 3D assets with explicit, editable part structures is crucial
for advancing interactive applications, yet most generative methods produce
only monolithic shapes, limiting their utility. We introduce OmniPart, a novel
framework for part-aware 3D object generation designed to achieve high semantic
decoupling among components while maintaining robust structural cohesion.
OmniPart uniquely decouples this complex task into two synergistic stages: (1)
an autoregressive structure planning module generates a controllable,
variable-length sequence of 3D part bounding boxes, critically guided by
flexible 2D part masks that allow for intuitive control over part decomposition
without requiring direct correspondences or semantic labels; and (2) a
spatially-conditioned rectified flow model, efficiently adapted from a
pre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and
consistently within the planned layout. Our approach supports user-defined part
granularity, precise localization, and enables diverse downstream applications.
Extensive experiments demonstrate that OmniPart achieves state-of-the-art
performance, paving the way for more interpretable, editable, and versatile 3D
content.

</details>


### [77] [Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling](https://arxiv.org/abs/2507.06183)
*Prahitha Movva,Naga Harshita Marupaka*

Main category: cs.CV

TL;DR: The paper addresses challenges in visual question answering (VQA) for scientific data, proposing an approach for SciVQA 2025 using large models and ensemble methods.


<details>
  <summary>Details</summary>
Motivation: Current VQA methods lack precision in scientific data interpretation, especially for numerical values and multi-step reasoning.

Method: Experiments with 5B-8B parameter models, including InternVL3, and ensemble VLMs, using prompt optimization and chain-of-thought reasoning.

Result: InternVL3 achieved ROUGE-1/L F1 of 0.740 and BERTScore 0.983; ensemble improved performance but InternVL3 was the best standalone.

Conclusion: Prompt optimization, chain-of-thought reasoning, and ensemble modeling enhance VQA performance for scientific data.

Abstract: Technical reports and articles often contain valuable information in the form
of semi-structured data like charts, and figures. Interpreting these and using
the information from them is essential for downstream tasks such as question
answering (QA). Current approaches to visual question answering often struggle
with the precision required for scientific data interpretation, particularly in
handling numerical values, multi-step reasoning over visual elements, and
maintaining consistency between visual observation and textual reasoning. We
present our approach to the SciVQA 2025 shared task, focusing on answering
visual and non-visual questions grounded in scientific figures from scholarly
articles.
  We conducted a series of experiments using models with 5B to 8B parameters.
Our strongest individual model, InternVL3, achieved ROUGE-1 and ROUGE-L F1
scores of \textbf{0.740} and a BERTScore of \textbf{0.983} on the SciVQA test
split. We also developed an ensemble model with multiple vision language models
(VLMs). Through error analysis on the validation split, our ensemble approach
improved performance compared to most individual models, though InternVL3
remained the strongest standalone performer. Our findings underscore the
effectiveness of prompt optimization, chain-of-thought reasoning and ensemble
modeling in improving the model's ability in visual question answering.

</details>


### [78] [CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions](https://arxiv.org/abs/2507.06210)
*Yuchen Huang,Zhiyuan Fan,Zhitao He,Sandeep Polisetty,Wenyan Li,Yi R. Fung*

Main category: cs.CV

TL;DR: The paper introduces CultureCLIP, a fine-tuned version of CLIP, to address its limitations in distinguishing culturally distinct but visually similar concepts. It uses a synthetic dataset, CulTwin, and customized contrastive learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP struggle with fine-grained cultural distinctions due to lack of culture-specific datasets and contextual knowledge.

Method: Construct CulTwin, a synthetic dataset of concept-caption-image triplets, and fine-tune CLIP using customized contrastive learning to create CultureCLIP.

Result: CultureCLIP improves fine-grained concept recognition by up to 5.49% while maintaining CLIP's generalization.

Conclusion: The approach effectively captures subtle cultural distinctions, validating the data synthesis and training paradigm.

Abstract: Pretrained vision-language models (VLMs) such as CLIP excel in multimodal
understanding but struggle with contextually relevant fine-grained visual
features, making it difficult to distinguish visually similar yet culturally
distinct concepts. This limitation stems from the scarcity of high-quality
culture-specific datasets, the lack of integrated contextual knowledge, and the
absence of hard negatives highlighting subtle distinctions. To address these
challenges, we first design a data curation pipeline that leverages
open-sourced VLMs and text-to-image diffusion models to construct CulTwin, a
synthetic cultural dataset. This dataset consists of paired
concept-caption-image triplets, where concepts visually resemble each other but
represent different cultural contexts. Then, we fine-tune CLIP on CulTwin to
create CultureCLIP, which aligns cultural concepts with contextually enhanced
captions and synthetic images through customized contrastive learning, enabling
finer cultural differentiation while preserving generalization capabilities.
Experiments on culturally relevant benchmarks show that CultureCLIP outperforms
the base CLIP, achieving up to a notable 5.49% improvement in fine-grained
concept recognition on certain tasks, while preserving CLIP's original
generalization ability, validating the effectiveness of our data synthesis and
VLM backbone training paradigm in capturing subtle cultural distinctions.

</details>


### [79] [Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion](https://arxiv.org/abs/2507.06230)
*Aleksandar Jevtić,Christoph Reich,Felix Wimbauer,Oliver Hahn,Christian Rupprecht,Stefan Roth,Daniel Cremers*

Main category: cs.CV

TL;DR: SceneDINO introduces an unsupervised method for semantic scene completion (SSC) using self-supervised learning, achieving state-of-the-art results without ground-truth annotations.


<details>
  <summary>Details</summary>
Motivation: Prior SSC methods rely on expensive ground-truth annotations, limiting scalability. SceneDINO aims to overcome this by leveraging unsupervised techniques.

Method: SceneDINO uses multi-view consistency self-supervision and 3D feature distillation to infer 3D geometry and semantics from single images without ground truth.

Result: SceneDINO achieves state-of-the-art segmentation accuracy in unsupervised 3D and 2D scene understanding, matching supervised SSC performance in some cases.

Conclusion: SceneDINO demonstrates strong potential for unsupervised SSC, offering domain generalization and multi-view consistency, paving the way for foundational 3D scene understanding.

Abstract: Semantic scene completion (SSC) aims to infer both the 3D geometry and
semantics of a scene from single images. In contrast to prior work on SSC that
heavily relies on expensive ground-truth annotations, we approach SSC in an
unsupervised setting. Our novel method, SceneDINO, adapts techniques from
self-supervised representation learning and 2D unsupervised scene understanding
to SSC. Our training exclusively utilizes multi-view consistency
self-supervision without any form of semantic or geometric ground truth. Given
a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO
features in a feed-forward manner. Through a novel 3D feature distillation
approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised
scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.
Linear probing our 3D features matches the segmentation accuracy of a current
supervised SSC approach. Additionally, we showcase the domain generalization
and multi-view consistency of SceneDINO, taking the first steps towards a
strong foundation for single image 3D scene understanding.

</details>


### [80] [RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models](https://arxiv.org/abs/2507.06231)
*Keyan Chen,Chenyang Liu,Bowen Chen,Jiafan Zhang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: RSRefSeg 2 introduces a decoupling paradigm for remote sensing image segmentation, improving accuracy and semantic interpretation by separating coarse localization and fine segmentation, leveraging CLIP and SAM models.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with complex semantic relationships and cross-modal alignment due to coupled processing, leading to error propagation and limited generalizability.

Method: Proposes a dual-stage framework: coarse localization using CLIP for feature activation and prompt generation, followed by fine segmentation with SAM guided by optimized semantic prompts.

Result: Outperforms existing methods by ~3% gIoU in segmentation accuracy and excels in complex semantic interpretation.

Conclusion: RSRefSeg 2's decoupling approach enhances precision and generalizability, validated by extensive experiments.

Abstract: Referring Remote Sensing Image Segmentation provides a flexible and
fine-grained framework for remote sensing scene analysis via vision-language
collaborative interpretation. Current approaches predominantly utilize a
three-stage pipeline encompassing dual-modal encoding, cross-modal interaction,
and pixel decoding. These methods demonstrate significant limitations in
managing complex semantic relationships and achieving precise cross-modal
alignment, largely due to their coupled processing mechanism that conflates
target localization with boundary delineation. This architectural coupling
amplifies error propagation under semantic ambiguity while restricting model
generalizability and interpretability. To address these issues, we propose
RSRefSeg 2, a decoupling paradigm that reformulates the conventional workflow
into a collaborative dual-stage framework: coarse localization followed by fine
segmentation. RSRefSeg 2 integrates CLIP's cross-modal alignment strength with
SAM's segmentation generalizability through strategic foundation model
collaboration. Specifically, CLIP is employed as the dual-modal encoder to
activate target features within its pre-aligned semantic space and generate
localization prompts. To mitigate CLIP's misactivation challenges in
multi-entity scenarios described by referring texts, a cascaded second-order
prompter is devised, which enhances precision through implicit reasoning via
decomposition of text embeddings into complementary semantic subspaces. These
optimized semantic prompts subsequently direct the SAM to generate pixel-level
refined masks, thereby completing the semantic transmission pipeline. Extensive
experiments (RefSegRS, RRSIS-D, and RISBench) demonstrate that RSRefSeg 2
surpasses contemporary methods in segmentation accuracy (+~3% gIoU) and complex
semantic interpretation. Code is available at:
https://github.com/KyanChen/RSRefSeg2.

</details>


### [81] [Learning to Track Any Points from Human Motion](https://arxiv.org/abs/2507.06233)
*Inès Hyeonsu Kim,Seokju Cho,Jahyeok Koo,Junghyun Park,Jiahui Huang,Joon-Young Lee,Seungryong Kim*

Main category: cs.CV

TL;DR: AnthroTAP automates pseudo-labeled training data generation for point tracking using the SMPL model, achieving state-of-the-art performance with minimal data and resources.


<details>
  <summary>Details</summary>
Motivation: Human motion is complex and rich for point tracking but manual annotation is laborious, necessitating an automated solution.

Method: Fit SMPL to humans in videos, project 3D vertices to 2D, handle occlusions with ray-casting, and filter tracks using optical flow consistency.

Result: AnthroTAP-trained model outperforms others on TAP-Vid benchmark, using far less data and compute (1 day on 4 GPUs vs. 256 GPUs).

Conclusion: AnthroTAP provides an efficient, scalable solution for point tracking training data, reducing reliance on manual annotation.

Abstract: Human motion, with its inherent complexities, such as non-rigid deformations,
articulated movements, clothing distortions, and frequent occlusions caused by
limbs or other individuals, provides a rich and challenging source of
supervision that is crucial for training robust and generalizable point
trackers. Despite the suitability of human motion, acquiring extensive training
data for point tracking remains difficult due to laborious manual annotation.
Our proposed pipeline, AnthroTAP, addresses this by proposing an automated
pipeline to generate pseudo-labeled training data, leveraging the Skinned
Multi-Person Linear (SMPL) model. We first fit the SMPL model to detected
humans in video frames, project the resulting 3D mesh vertices onto 2D image
planes to generate pseudo-trajectories, handle occlusions using ray-casting,
and filter out unreliable tracks based on optical flow consistency. A point
tracking model trained on AnthroTAP annotated dataset achieves state-of-the-art
performance on the TAP-Vid benchmark, surpassing other models trained on real
videos while using 10,000 times less data and only 1 day in 4 GPUs, compared to
256 GPUs used in recent state-of-the-art.

</details>
