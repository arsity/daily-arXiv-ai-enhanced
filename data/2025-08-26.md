<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 195]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

TL;DR: A novel iToF-RGB fusion framework that addresses iToF depth sensing limitations by combining narrow-FoV iToF depth with wide-FoV RGB through geometric alignment and dual-encoder fusion, achieving superior depth accuracy and expanded field-of-view.


<details>
  <summary>Details</summary>
Motivation: To overcome inherent limitations of indirect Time-of-Flight (iToF) depth sensing including low spatial resolution, limited field-of-view, and structural distortion in complex scenes.

Method: Reprojects narrow-FoV iToF depth map onto wide-FoV RGB coordinate system using geometric calibration, then employs dual-encoder fusion network with monocular depth priors to extract complementary features and perform depth super-resolution.

Result: Significantly outperforms state-of-the-art methods in accuracy, structural consistency, and visual quality on both synthetic and real-world datasets, achieving enhanced depth accuracy, improved edge sharpness, and seamless FoV expansion.

Conclusion: The proposed iToF-RGB fusion framework effectively addresses iToF limitations by leveraging cross-modal fusion and geometric alignment, demonstrating superior performance in depth sensing applications.

Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the
inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as
low spatial resolution, limited field-of-view (FoV), and structural distortion
in complex scenes. The proposed method first reprojects the narrow-FoV iToF
depth map onto the wide-FoV RGB coordinate system through a precise geometric
calibration and alignment module, ensuring pixel-level correspondence between
modalities. A dual-encoder fusion network is then employed to jointly extract
complementary features from the reprojected iToF depth and RGB image, guided by
monocular depth priors to recover fine-grained structural details and perform
depth super-resolution. By integrating cross-modal structural cues and depth
consistency constraints, our approach achieves enhanced depth accuracy,
improved edge sharpness, and seamless FoV expansion. Extensive experiments on
both synthetic and real-world datasets demonstrate that the proposed framework
significantly outperforms state-of-the-art methods in terms of accuracy,
structural consistency, and visual quality.

</details>


### [2] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep Lladós,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

TL;DR: CountLoop is a training-free framework that enables diffusion models to generate scenes with precise object instance counts through iterative multimodal feedback and attention masking techniques.


<details>
  <summary>Details</summary>
Motivation: Diffusion models struggle with generating scenes containing exact numbers of object instances, especially in complex, high-density settings where counting accuracy and spatial arrangements are challenging.

Method: Uses iterative structured feedback alternating between image generation and multimodal agent evaluation. Includes language-guided planner/critic for count assessment, instance-driven attention masking, and compositional generation techniques to improve object separation.

Result: Achieves up to 98% counting accuracy on COCO Count, T2I CompBench, and new high-instance benchmarks while maintaining spatial fidelity and visual quality, outperforming layout-based and gradient-guided baselines with a score of 0.97.

Conclusion: CountLoop provides effective training-free instance control for diffusion models through structured iterative feedback, significantly improving counting accuracy in complex scene generation without compromising visual quality.

Abstract: Diffusion models have shown remarkable progress in photorealistic image
synthesis, yet they remain unreliable for generating scenes with a precise
number of object instances, particularly in complex and high-density settings.
We present CountLoop, a training-free framework that provides diffusion models
with accurate instance control through iterative structured feedback. The
approach alternates between image generation and multimodal agent evaluation,
where a language-guided planner and critic assess object counts, spatial
arrangements, and attribute consistency. This feedback is then used to refine
layouts and guide subsequent generations. To further improve separation between
objects, especially in occluded scenes, we introduce instance-driven attention
masking and compositional generation techniques. Experiments on COCO Count, T2I
CompBench, and two new high-instance benchmarks show that CountLoop achieves
counting accuracy of up to 98% while maintaining spatial fidelity and visual
quality, outperforming layout-based and gradient-guided baselines with a score
of 0.97.

</details>


### [3] [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
*Ashwath Vaithinathan Aravindan,Abha Jha,Mihir Kulkarni*

Main category: cs.CV

TL;DR: VLMs struggle with compositional generalization and object binding due to superposition in MLP neurons, where individual neurons represent multiple features, hindering compositional reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models show strong performance but fail at compositional generalization and object binding, limiting their ability to handle novel object-attribute combinations. The research aims to uncover the root causes of these failures.

Method: Used mechanistic interpretability techniques to analyze CLIP's vision encoder, specifically examining how individual neurons in MLP layers represent features and contribute to superposition effects.

Result: Found evidence that superposition in MLP neurons (where single neurons represent multiple features) directly hinders compositional feature representation, which consequently impairs compositional reasoning and object binding capabilities.

Conclusion: This study provides initial insights into the mechanistic roots of compositional failures in VLMs, with superposition identified as a key limitation. The findings serve as a foundation for future improvements in VLM architecture and performance.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in
integrating visual and textual information for tasks such as image captioning
and visual question answering. However, these models struggle with
compositional generalization and object binding, which limit their ability to
handle novel combinations of objects and their attributes. Our work explores
the root causes of these failures using mechanistic interpretability
techniques. We show evidence that individual neurons in the MLP layers of
CLIP's vision encoder represent multiple features, and this "superposition"
directly hinders its compositional feature representation which consequently
affects compositional reasoning and object binding capabilities. We hope this
study will serve as an initial step toward uncovering the mechanistic roots of
compositional failures in VLMs. The code and supporting results can be found
https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .

</details>


### [4] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: MSNav is a novel framework that addresses VLN challenges by integrating memory, spatial reasoning, and decision modules, achieving state-of-the-art performance on navigation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current VLN approaches using single LLMs suffer from poor spatial reasoning, weak cross-modal grounding, and memory overload in long-horizon tasks.

Method: MSNav integrates three modules: Memory Module for dynamic map memory with selective node pruning, Spatial Module for spatial reasoning and object relationship inference, and Decision Module for LLM-based path planning. Also introduces I-O-S dataset and fine-tunes Qwen3-4B into Qwen-Spatial model.

Result: Outperforms leading commercial LLMs in object list extraction with higher F1 and NDCG scores. Achieves significant improvements in Success Rate (SR) and SPL on R2R and REVERIE datasets.

Conclusion: MSNav transforms fragile VLN inference into robust, integrated intelligence by systematically addressing core vulnerabilities through synergistic module integration.

Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural
language instructions and navigate complex environments. Current approaches
often adopt a "black-box" paradigm, where a single Large Language Model (LLM)
makes end-to-end decisions. However, it is plagued by critical vulnerabilities,
including poor spatial reasoning, weak cross-modal grounding, and memory
overload in long-horizon tasks. To systematically address these issues, we
propose Memory Spatial Navigation(MSNav), a framework that fuses three modules
into a synergistic architecture, which transforms fragile inference into a
robust, integrated intelligence. MSNav integrates three modules: Memory Module,
a dynamic map memory module that tackles memory overload through selective node
pruning, enhancing long-range exploration; Spatial Module, a module for spatial
reasoning and object relationship inference that improves endpoint recognition;
and Decision Module, a module using LLM-based path planning to execute robust
actions. Powering Spatial Module, we also introduce an Instruction-Object-Space
(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),
which outperforms leading commercial LLMs in object list extraction, achieving
higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the
Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art
performance with significant improvements in Success Rate (SR) and Success
weighted by Path Length (SPL).

</details>


### [5] [Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm](https://arxiv.org/abs/2508.16660)
*Yasir Nooruldeen Ibrahim,Fawziya Mahmood Ramo,Mahmood Siddeeq Qadir,Muna Jaffer Al-Shamdeen*

Main category: cs.CV

TL;DR: This paper presents an intelligent soil classification system using Convolutional Neural Networks optimized with swarm algorithms (Whale and Particle Swarm Optimization) to improve agricultural and environmental management.


<details>
  <summary>Details</summary>
Motivation: Soil classification is crucial for better land management, increased agricultural output, and environmental solutions. Understanding soil quality aids risk reduction, performance improvement, and decision-making in agriculture, civil engineering, and natural resource management.

Method: Used Convolutional Neural Networks for soil image classification, enhanced with machine learning algorithms. Employed Whale Optimization Algorithm and Particle Swarm Optimization to select optimal hyperparameters for the CNN network, comparing both swarm algorithms' performance.

Result: The proposed system achieved efficient results in multiple soil type classification, evaluated using Accuracy and F1 measures. Both swarm optimization algorithms contributed to improved CNN performance.

Conclusion: Swarm optimization algorithms (Whale and Particle Swarm) effectively enhance CNN performance for soil classification, providing valuable results for practical applications in agriculture and environmental management.

Abstract: Classifying soil images contributes to better land management, increased
agricultural output, and practical solutions for environmental issues. The
development of various disciplines, particularly agriculture, civil
engineering, and natural resource management, is aided by understanding of soil
quality since it helps with risk reduction, performance improvement, and sound
decision-making . Artificial intelligence has recently been used in a number of
different fields. In this study, an intelligent model was constructed using
Convolutional Neural Networks to classify soil kinds, and machine learning
algorithms were used to enhance the performance of soil classification . To
achieve better implementation and performance of the Convolutional Neural
Networks algorithm and obtain valuable results for the process of classifying
soil type images, swarm algorithms were employed to obtain the best performance
by choosing Hyper parameters for the Convolutional Neural Networks network
using the Whale optimization algorithm and the Particle swarm optimization
algorithm, and comparing the results of using the two algorithms in the process
of multiple classification of soil types. The Accuracy and F1 measures were
adopted to test the system, and the results of the proposed work were efficient
result

</details>


### [6] [QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models](https://arxiv.org/abs/2508.16661)
*Qiaojie Zheng,Jiucai Zhang,Joy Gockel,Michael B. Wakin,Craig Brice,Xiaoli Zhang*

Main category: cs.CV

TL;DR: QA-VLM framework uses vision-language models with domain knowledge to provide interpretable quality assessment in additive manufacturing, outperforming standard VLMs in validity and consistency.


<details>
  <summary>Details</summary>
Motivation: Current machine learning methods for image-based quality assessment in additive manufacturing are black-box systems that lack interpretable justifications, limiting trust and adoption in real-world applications.

Method: A novel QA-VLM framework that leverages vision-language models' attention mechanisms and reasoning capabilities, enriched with application-specific knowledge from peer-reviewed journal articles to generate human-interpretable quality assessments.

Result: Evaluated on 24 single-bead samples from laser wire direct energy deposition, the framework demonstrated higher validity and consistency in explanation quality compared to off-the-shelf VLMs.

Conclusion: The approach shows potential for enabling trustworthy, interpretable quality assessment in additive manufacturing applications by providing human-understandable justifications.

Abstract: Image-based quality assessment (QA) in additive manufacturing (AM) often
relies heavily on the expertise and constant attention of skilled human
operators. While machine learning and deep learning methods have been
introduced to assist in this task, they typically provide black-box outputs
without interpretable justifications, limiting their trust and adoption in
real-world settings. In this work, we introduce a novel QA-VLM framework that
leverages the attention mechanisms and reasoning capabilities of
vision-language models (VLMs), enriched with application-specific knowledge
distilled from peer-reviewed journal articles, to generate human-interpretable
quality assessments. Evaluated on 24 single-bead samples produced by laser wire
direct energy deposition (DED-LW), our framework demonstrates higher validity
and consistency in explanation quality than off-the-shelf VLMs. These results
highlight the potential of our approach to enable trustworthy, interpretable
quality assessment in AM applications.

</details>


### [7] [The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers](https://arxiv.org/abs/2508.16663)
*Naren Sengodan*

Main category: cs.CV

TL;DR: The Loupe is a lightweight plug-and-play attention module that improves FGVC performance while providing interpretable attention maps, boosting Swin-Base accuracy by 2.66% on CUB-200-2011 without part annotations.


<details>
  <summary>Details</summary>
Motivation: FGVC requires identifying subtle visual cues for critical applications like biodiversity monitoring and medical diagnostics, but current Vision Transformers lack interpretability needed for trust and verification.

Method: A novel lightweight attention module inserted into pre-trained backbones like Swin Transformer, trained end-to-end with composite loss to focus on discriminative object parts without explicit part-level annotations.

Result: Improved Swin-Base model accuracy from 85.40% to 88.06% on CUB-200-2011 dataset (2.66% gain), with attention maps effectively localizing semantically meaningful features.

Conclusion: Simple intrinsic attention mechanism serves as powerful regularizer, significantly boosting performance while providing clear visual explanations for model interpretability and trust.

Abstract: Fine-Grained Visual Classification (FGVC) is a critical and challenging area
within computer vision, demanding the identification of highly subtle,
localized visual cues. The importance of FGVC extends to critical applications
such as biodiversity monitoring and medical diagnostics, where precision is
paramount. While large-scale Vision Transformers have achieved state-of-the-art
performance, their decision-making processes often lack the interpretability
required for trust and verification in such domains. In this paper, we
introduce The Loupe, a novel, lightweight, and plug-and-play attention module
designed to be inserted into pre-trained backbones like the Swin Transformer.
The Loupe is trained end-to-end with a composite loss function that implicitly
guides the model to focus on the most discriminative object parts without
requiring explicit part-level annotations. Our unique contribution lies in
demonstrating that a simple, intrinsic attention mechanism can act as a
powerful regularizer, significantly boosting performance while simultaneously
providing clear visual explanations. Our experimental evaluation on the
challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of
a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.
Crucially, our qualitative analysis of the learned attention maps reveals that
The Loupe effectively localizes semantically meaningful features, providing a
valuable tool for understanding and trusting the model's decision-making
process.

</details>


### [8] [COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture](https://arxiv.org/abs/2508.16670)
*Deborup Sanyal*

Main category: cs.CV

TL;DR: Using CNN to analyze lung CT scans for predicting COVID-19 severity and outcomes within one month of positive test


<details>
  <summary>Details</summary>
Motivation: Address acute shortage of medical resources and help doctors assess COVID-19 severity more accurately through automated CT scan analysis to prevent deaths from respiratory failure

Method: Convolutional Neural Network model trained on patient CT scans to predict infection severity and outcomes (intubation or death)

Result: Model aims to provide severity assessment within one month of positive COVID-19 test based on CT scan analysis

Conclusion: Machine learning approach using CNN can help healthcare systems better predict COVID-19 severity and allocate limited resources more effectively to save lives

Abstract: COVID19 took the world by storm since December 2019. A highly infectious
communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,
the World Health Organization (WHO) declared COVID19 as a global pandemic. A
pandemic in the 21st century after almost 100 years was something the world was
not prepared for, which resulted in the deaths of around 1.6 million people
worldwide. The most common symptoms of COVID19 were associated with the
respiratory system and resembled a cold, flu, or pneumonia. After extensive
research, doctors and scientists concluded that the main reason for lives being
lost due to COVID19 was failure of the respiratory system. Patients were dying
gasping for breath. Top healthcare systems of the world were failing badly as
there was an acute shortage of hospital beds, oxygen cylinders, and
ventilators. Many were dying without receiving any treatment at all. The aim of
this project is to help doctors decide the severity of COVID19 by reading the
patient's Computed Tomography (CT) scans of the lungs. Computer models are less
prone to human error, and Machine Learning or Neural Network models tend to
give better accuracy as training improves over time. We have decided to use a
Convolutional Neural Network model. Given that a patient tests positive, our
model will analyze the severity of COVID19 infection within one month of the
positive test result. The severity of the infection may be promising or
unfavorable (if it leads to intubation or death), based entirely on the CT
scans in the dataset.

</details>


### [9] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

TL;DR: MedRepBench is a comprehensive benchmark for evaluating structured medical report interpretation using 1,900 real-world Chinese medical reports, featuring both objective field-level recall metrics and automated subjective LLM-based evaluation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardized benchmarks to assess structured interpretation quality in medical reports, despite recent advances in vision-language models and large language models for document understanding.

Method: Built from 1,900 de-identified real-world Chinese medical reports spanning diverse departments and formats. Includes text-only evaluation using OCR+LLM as an upper-bound comparison. Uses objective field-level recall metrics and automated subjective evaluation with LLM scoring for factuality, interpretability, and reasoning quality.

Result: Applied Group Relative Policy Optimization (GRPO) to improve a mid-scale VLM, achieving up to 6% recall gain. OCR+LLM pipeline showed strong performance but suffers from layout-blindness and latency issues.

Conclusion: The benchmark enables controlled comparisons and reveals limitations of current approaches, motivating further progress toward robust, fully vision-based medical report understanding systems.

Abstract: Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [10] [Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection](https://arxiv.org/abs/2508.16739)
*Yanbing Bai,Rui-Yang Ju,Lemeng Zhao,Junjie Hu,Jianchao Bi,Erick Mas,Shunichi Koshimura*

Main category: cs.CV

TL;DR: Lightweight two-stage framework for real-time wildfire monitoring on UAVs using frame compression and improved YOLOv8 for fire detection with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: UAVs have limited computational resources, making it challenging to run large models for real-time aerial video analysis in disaster emergency response situations like wildfire monitoring.

Method: Two-stage approach: Stage 1 uses policy network with frame compression and station point mechanism to identify redundant clips; Stage 2 employs improved YOLOv8 model for fire source localization when fire is detected.

Result: Significantly reduces computational costs while maintaining classification accuracy in Stage 1, and achieves higher detection accuracy with similar inference time in Stage 2 compared to baseline methods.

Conclusion: The proposed framework enables efficient real-time wildfire monitoring on resource-constrained UAV platforms through intelligent frame selection and optimized fire detection.

Abstract: Unmanned Aerial Vehicles (UAVs) have become increasingly important in
disaster emergency response by enabling real-time aerial video analysis. Due to
the limited computational resources available on UAVs, large models cannot be
run independently for real-time analysis. To overcome this challenge, we
propose a lightweight and efficient two-stage framework for real-time wildfire
monitoring and fire source detection on UAV platforms. Specifically, in Stage
1, we utilize a policy network to identify and discard redundant video clips
using frame compression techniques, thereby reducing computational costs. In
addition, we introduce a station point mechanism that leverages future frame
information within the sequential policy network to improve prediction
accuracy. In Stage 2, once the frame is classified as "fire", we employ the
improved YOLOv8 model to localize the fire source. We evaluate the Stage 1
method using the FLAME and HMDB51 datasets, and the Stage 2 method using the
Fire & Smoke dataset. Experimental results show that our method significantly
reduces computational costs while maintaining classification accuracy in Stage
1, and achieves higher detection accuracy with similar inference time in Stage
2 compared to baseline methods.

</details>


### [11] [CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction](https://arxiv.org/abs/2508.16742)
*Abdul Rehman Akbar,Usama Sajjad,Ziyu Su,Wencheng Li,Fei Xing,Jimmy Ruiz,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: CellEcoNet is a spatially aware deep learning framework that models lung cancer pathology images as a language, achieving superior recurrence prediction compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: About 70% of invasive lung adenocarcinoma patients recur within five years after surgery, and current clinical tools fail to identify those who need adjuvant therapy, creating an unmet clinical need.

Method: CellEcoNet treats whole slide images through natural language analogy: cells as words, cellular neighborhoods as phrases, and tissue architecture as sentences. It automatically learns context-dependent meanings and spatial interactions to predict recurrence risk.

Result: On 456 H&E-stained whole slide images, CellEcoNet achieved AUC:77.8% and HR:9.54, outperforming IASLC grading (AUC:71.4%), AJCC Stage (AUC:64.0%), and state-of-the-art computational methods (AUCs:62.2-67.4%). It showed fairness across diverse demographic and clinical subgroups.

Conclusion: CellEcoNet represents a paradigm shift by decoding the tumor microenvironment's cellular 'language' to reveal how subtle cell variations encode recurrence risk, providing superior prognostic capabilities for lung adenocarcinoma patients.

Abstract: Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)
patients recur within five years, and current tools fail to identify those
needing adjuvant therapy. To address this unmet clinical need, we introduce
CellEcoNet, a novel spatially aware deep learning framework that models whole
slide images (WSIs) through natural language analogy, defining a "language of
pathology," where cells act as words, cellular neighborhoods become phrases,
and tissue architecture forms sentences. CellEcoNet learns these
context-dependent meanings automatically, capturing how subtle variations and
spatial interactions derive recurrence risk. On a dataset of 456 H&E-stained
WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),
outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%
HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).
CellEcoNet demonstrated fairness and consistent performance across diverse
demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a
paradigm shift by decoding the tumor microenvironment's cellular "language" to
reveal how subtle cell variations encode recurrence risk.

</details>


### [12] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

TL;DR: A method for evaluating fairness and utility in text-to-image models using Pareto-optimal frontiers across hyperparameter configurations, showing that default settings are often suboptimal and better configurations can be easily found.


<details>
  <summary>Details</summary>
Motivation: Current fairness evaluation methods for text-to-image models rely on qualitative judgment and narrow comparisons, which are error-prone, difficult to replicate, and limit comprehensive assessment of both fairness and utility.

Method: Proposes using Pareto-optimal frontiers across hyperparameterization of debiasing methods, employing Normalized Shannon Entropy for fairness evaluation and ClipScore for utility evaluation to compare different text-to-image models.

Result: Evaluation of Stable Diffusion, Fair Diffusion, SDXL, DeCoDi, and FLUX models shows that most default hyperparameterizations are dominated solutions in the fairness-utility space, and better hyperparameters can be straightforwardly identified.

Conclusion: The proposed method enables reproducible assessment of debiasing methods and demonstrates that current default model configurations are suboptimal, with significant room for improvement in balancing fairness and utility through better hyperparameter selection.

Abstract: Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [13] [WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation](https://arxiv.org/abs/2508.16763)
*Rabiul Awal,Mahsa Massoud,Aarash Feizi,Zichao Li,Suyuchen Wang,Christopher Pal,Aishwarya Agrawal,David Vazquez,Siva Reddy,Juan A. Rodriguez,Perouz Taslakian,Spandana Gella,Sai Rajeswar*

Main category: cs.CV

TL;DR: WebMMU is a multilingual benchmark that evaluates multimodal large language models on three unified web tasks: website VQA, code editing, and mockup-to-code generation, revealing significant limitations in reasoning and grounding capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark that unifies core web development tasks using real-world data, addressing the limitations of prior benchmarks that treated these tasks separately and lacked assessment of complex reasoning and functional understanding.

Method: Developed WebMMU benchmark with expert-annotated real-world web data covering three tasks: website visual question answering, HTML/CSS/JavaScript code editing, and mockup-to-code generation, designed to evaluate multi-step reasoning, element grounding, and functional UI comprehension.

Result: Multimodal LLMs perform well on basic information extraction but struggle significantly with reasoning and grounding tasks, code editing that preserves functionality, and generating design-to-code that maintains hierarchy and supports multilingual content.

Conclusion: Current MLLMs have key limitations in multimodal and cross-lingual reasoning, highlighting the need for improved capabilities to develop future web agents capable of automating diverse web development tasks.

Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web
tasks: (1) website visual question answering, (2) code editing involving
HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks
that treat these tasks separately, WebMMU unifies them using expert-annotated,
real-world web data to assess models' abilities in complex multi-step
reasoning, precise element grounding, and functional UI comprehension and
coding. Our evaluation shows that while multimodal large language models
(MLLMs) perform well on basic information extraction, they struggle with
reasoning and grounding, editing code to preserve functionality, and generating
design-to-code that maintains hierarchy and supports multilingual content.
These findings reveal key limitations in current MLLMs and underscore the need
for improved multimodal and cross-lingual reasoning to build future web agents
capable of automating diverse web development tasks.

</details>


### [14] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: RoentGen-v2 is a text-to-image diffusion model that generates diverse chest radiographs with demographic control, enabling creation of balanced synthetic datasets that improve downstream classification performance and fairness when used for supervised pretraining.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in achieving robust performance and fairness across diverse patient populations in medical imaging AI, particularly due to limitations in dataset scale and demographic diversity.

Method: Developed RoentGen-v2 diffusion model for chest radiographs with fine-grained control over findings and demographics. Created large synthetic dataset (565k+ images) and proposed synthetic pretraining strategy followed by real data fine-tuning.

Result: Synthetic pretraining improved downstream classification accuracy by 6.5% (vs 2.7% with naive combination), reduced underdiagnosis fairness gap by 19.3%, and enhanced generalization across 5 institutions (137k+ real images).

Conclusion: Synthetic imaging with demographic conditioning enables more equitable and generalizable medical AI by addressing data constraints through strategic synthetic pretraining approaches.

Abstract: Achieving robust performance and fairness across diverse patient populations
remains a challenge in developing clinically deployable deep learning models
for diagnostic imaging. Synthetic data generation has emerged as a promising
strategy to address limitations in dataset scale and diversity. We introduce
RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables
fine-grained control over both radiographic findings and patient demographic
attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first
model to generate clinically plausible images with demographic conditioning,
facilitating the creation of a large, demographically balanced synthetic
dataset comprising over 565,000 images. We use this large synthetic dataset to
evaluate optimal training pipelines for downstream disease classification
models. In contrast to prior work that combines real and synthetic data
naively, we propose an improved training strategy that leverages synthetic data
for supervised pretraining, followed by fine-tuning on real data. Through
extensive evaluation on over 137,000 chest radiographs from five institutions,
we demonstrate that synthetic pretraining consistently improves model
performance, generalization to out-of-distribution settings, and fairness
across demographic subgroups. Across datasets, synthetic pretraining led to a
6.5% accuracy increase in the performance of downstream classification models,
compared to a modest 2.7% increase when naively combining real and synthetic
data. We observe this performance improvement simultaneously with the reduction
of the underdiagnosis fairness gap by 19.3%. These results highlight the
potential of synthetic imaging to advance equitable and generalizable medical
deep learning under real-world data constraints. We open source our code,
trained models, and synthetic dataset at
https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [15] [Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes](https://arxiv.org/abs/2508.16812)
*Xinhao Xiang,Kuan-Chuan Peng,Suhas Lohit,Michael J. Jones,Jiawei Zhang*

Main category: cs.CV

TL;DR: OVODA is an open-vocabulary 3D object and attribute detection framework that uses foundation models to bridge 3D features with text, enabling detection of novel objects and their attributes without requiring anchor size knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing 3D object detection methods are limited by closed-set assumptions and struggle with novel objects and attributes in real-world autonomous systems.

Method: Uses foundation model feature concatenation, prompt tuning strategies, perspective-specified prompts, and horizontal flip augmentation. Introduces OVAD dataset with comprehensive attribute annotations.

Result: Outperforms state-of-the-art methods on nuScenes and Argoverse 2 datasets in open-vocabulary 3D object detection while successfully recognizing object attributes.

Conclusion: OVODA enables effective open-vocabulary 3D object and attribute detection without requiring novel class anchor sizes, advancing real-world autonomous system capabilities.

Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing
methods are limited by closed-set assumptions and struggle to recognize novel
objects and their attributes in real-world scenarios. We propose OVODA, a novel
framework enabling both open-vocabulary 3D object and attribute detection with
no need to know the novel class anchor size. OVODA uses foundation models to
bridge the semantic gap between 3D features and texts while jointly detecting
attributes, e.g., spatial relationships, motion states, etc. To facilitate such
research direction, we propose OVAD, a new dataset that supplements existing 3D
object detection benchmarks with comprehensive attribute annotations. OVODA
incorporates several key innovations, including foundation model feature
concatenation, prompt tuning strategies, and specialized techniques for
attribute detection, including perspective-specified prompts and horizontal
flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets
show that under the condition of no given anchor sizes of novel classes, OVODA
outperforms the state-of-the-art methods in open-vocabulary 3D object detection
while successfully recognizing object attributes. Our OVAD dataset is released
here: https://doi.org/10.5281/zenodo.16904069 .

</details>


### [16] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

TL;DR: The AIM 2025 Low-Light RAW Video Denoising Challenge focused on developing methods to denoise low-light RAW video by leveraging temporal redundancy while working within exposure-time constraints and adapting to sensor-specific noise patterns.


<details>
  <summary>Details</summary>
Motivation: To advance low-light video denoising techniques by creating a standardized benchmark that addresses the challenges of processing RAW video data under various illumination and exposure conditions across multiple smartphone sensors.

Method: Introduced a new benchmark with 756 ten-frame sequences captured using 14 smartphone camera sensors across nine different conditions (varying illumination and exposure times), with high-SNR references obtained through burst averaging. Participants processed linear RAW sequences and output denoised 10th frames while preserving the Bayer pattern.

Result: The challenge established a comprehensive evaluation framework with submissions assessed on a private test set using full-reference PSNR and SSIM metrics, with final rankings determined by the mean of per-metric ranks.

Conclusion: The AIM 2025 challenge successfully created a standardized benchmark for low-light RAW video denoising, facilitating the development and comparison of advanced denoising methods that can handle real-world smartphone camera conditions and sensor-specific noise characteristics.

Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light
RAW Video Denoising Challenge. The task is to develop methods that denoise
low-light RAW video by exploiting temporal redundancy while operating under
exposure-time limits imposed by frame rate and adapting to sensor-specific,
signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences
captured with 14 smartphone camera sensors across nine conditions
(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR
references obtained via burst averaging. Participants process linear RAW
sequences and output the denoised 10th frame while preserving the Bayer
pattern. Submissions are evaluated on a private test set using full-reference
PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This
report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [17] [Transformer-Based Neural Network for Transient Detection without Image Subtraction](https://arxiv.org/abs/2508.16844)
*Adi Inada,Masao Sako,Tatiana Acero-Cuellar,Federica Bianco*

Main category: cs.CV

TL;DR: Transformer-based neural network for astronomical transient classification that achieves 97.4% accuracy on DES data without needing difference imaging, outperforming traditional CNNs.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and efficiency in classifying real vs bogus transient detections in astronomical images by eliminating computationally expensive difference imaging while maintaining high performance.

Method: Transformer-based neural network architecture designed for detailed pixel-by-pixel comparison of search and template images only, removing the need for difference imaging.

Result: Achieved 97.4% classification accuracy on Dark Energy Survey autoScan dataset, with performance utility for difference images diminishing as training set size increased. Network maintained similar performance even when input images weren't centered on supernova candidates.

Conclusion: The transformer-based network effectively enhances both accuracy and efficiency of supernova detection in large-scale astronomical surveys by eliminating computational bottlenecks while maintaining high classification performance.

Abstract: We introduce a transformer-based neural network for the accurate
classification of real and bogus transient detections in astronomical images.
This network advances beyond the conventional convolutional neural network
(CNN) methods, widely used in image processing tasks, by adopting an
architecture better suited for detailed pixel-by-pixel comparison. The
architecture enables efficient analysis of search and template images only,
thus removing the necessity for computationally-expensive difference imaging,
while maintaining high performance. Our primary evaluation was conducted using
the autoScan dataset from the Dark Energy Survey (DES), where the network
achieved a classification accuracy of 97.4% and diminishing performance utility
for difference image as the size of the training set grew. Further experiments
with DES data confirmed that the network can operate at a similar level even
when the input images are not centered on the supernova candidate. These
findings highlight the network's effectiveness in enhancing both accuracy and
efficiency of supernova detection in large-scale astronomical surveys.

</details>


### [18] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

TL;DR: NinA replaces diffusion-based action decoders with Normalizing Flows for faster one-shot sampling in Vision-Language-Action models, maintaining performance while significantly reducing inference time.


<details>
  <summary>Details</summary>
Motivation: Diffusion models in VLA architectures require multiple iterative denoising steps at inference, limiting practicality for real-world high-frequency control applications.

Method: Replaces diffusion action decoder with Normalizing Flow (NF) that enables one-shot sampling through invertible transformation. Integrated into FLOWER VLA architecture and fine-tuned on LIBERO benchmark.

Result: NinA matches performance of diffusion-based counterpart under same training regime while achieving substantially faster inference time.

Conclusion: NinA offers efficient, high-frequency VLA control without compromising performance, providing a promising alternative to diffusion-based decoders.

Abstract: Recent advances in Vision-Language-Action (VLA) models have established a
two-component architecture, where a pre-trained Vision-Language Model (VLM)
encodes visual observations and task descriptions, and an action decoder maps
these representations to continuous actions. Diffusion models have been widely
adopted as action decoders due to their ability to model complex, multimodal
action distributions. However, they require multiple iterative denoising steps
at inference time or downstream techniques to speed up sampling, limiting their
practicality in real-world settings where high-frequency control is crucial. In
this work, we present NinA (Normalizing Flows in Action), a fast and expressive
alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion
action decoder with a Normalizing Flow (NF) that enables one-shot sampling
through an invertible transformation, significantly reducing inference time. We
integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO
benchmark. Our experiments show that NinA matches the performance of its
diffusion-based counterpart under the same training regime, while achieving
substantially faster inference. These results suggest that NinA offers a
promising path toward efficient, high-frequency VLA control without
compromising performance.

</details>


### [19] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

TL;DR: RF-PGS is a novel framework that reconstructs high-fidelity radio propagation paths from sparse path loss spectra using Planar Gaussians and fully-structured radio radiance, achieving improved accuracy and efficiency for 6G Spatial-CSI modeling.


<details>
  <summary>Details</summary>
Motivation: 6G technologies require large-scale antenna arrays and accurate spatial channel state information, but traditional channel modeling methods face challenges in spatial resolution, efficiency, and scalability. Radiance field-based methods suffer from geometric inaccuracy and costly supervision.

Method: Two-stage approach: 1) Geometry training stage using Planar Gaussians as geometry primitives for dense, surface-aligned scene reconstruction; 2) RF training stage with fully-structured radio radiance and tailored multi-view loss to model radio propagation behavior.

Result: Significantly improves reconstruction accuracy, reduces training costs, and enables efficient representation of wireless channels compared to prior radiance field methods.

Conclusion: RF-PGS offers a practical solution for scalable 6G Spatial-CSI modeling by addressing geometric inaccuracy and supervision costs while maintaining high fidelity in radio propagation path reconstruction.

Abstract: In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [20] [Gaussian Primitive Optimized Deformable Retinal Image Registration](https://arxiv.org/abs/2508.16852)
*Xin Tian,Jiazheng Wang,Yuxi Zhang,Xiang Chen,Renjiu Hu,Gaolei Li,Min Liu,Hang Zhang*

Main category: cs.CV

TL;DR: GPO is a novel deformable retinal image registration framework that uses Gaussian primitives at key vascular features to overcome vanishing gradient issues in homogeneous regions, achieving state-of-the-art performance on the FIRE dataset.


<details>
  <summary>Details</summary>
Motivation: Deformable retinal image registration is challenging due to large homogeneous regions and sparse vascular features, which cause limited gradient signals in standard learning-based approaches.

Method: Extracts keypoints at salient anatomical structures as descriptor-based control nodes modeled as Gaussian primitives. Uses KNN Gaussian interpolation to propagate displacement signals from information-rich nodes to create a globally coherent displacement field. Optimized end-to-end with multi-term loss for keypoint consistency and intensity alignment.

Result: Reduces target registration error from 6.2px to ~2.4px and increases AUC at 25px from 0.770 to 0.938 on FIRE dataset, substantially outperforming existing methods.

Conclusion: GPO effectively addresses vanishing gradient issues in retinal image registration by strategically anchoring nodes in high-gradient regions and using structured message passing, demonstrating significant performance improvements over current state-of-the-art methods.

Abstract: Deformable retinal image registration is notoriously difficult due to large
homogeneous regions and sparse but critical vascular features, which cause
limited gradient signals in standard learning-based frameworks. In this paper,
we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework
that performs structured message passing to overcome these challenges. After an
initial coarse alignment, we extract keypoints at salient anatomical structures
(e.g., major vessels) to serve as a minimal set of descriptor-based control
nodes (DCN). Each node is modelled as a Gaussian primitive with trainable
position, displacement, and radius, thus adapting its spatial influence to
local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation
then blends and propagates displacement signals from these information-rich
nodes to construct a globally coherent displacement field; focusing
interpolation on the top (K) neighbors reduces computational overhead while
preserving local detail. By strategically anchoring nodes in high-gradient
regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal
in textureless areas. The framework is optimized end-to-end via a multi-term
loss that enforces both keypoint consistency and intensity alignment.
Experiments on the FIRE dataset show that GPO reduces the target registration
error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to
0.938, substantially outperforming existing methods. The source code can be
accessed via https://github.com/xintian-99/GPOreg.

</details>


### [21] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

TL;DR: A new benchmark MTMEUR for multimodal emotion understanding and reasoning with 1,451 videos and 5,101 questions, plus a multi-agent framework to improve reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current MLLM research focuses mainly on emotion recognition but neglects emotion reasoning, which is crucial for natural human-machine interactions.

Method: Created MTMEUR benchmark with real-life scenario videos and progressive questions. Proposed multi-agent framework with specialized agents for different reasoning aspects.

Result: Experiments show most existing MLLMs struggle significantly with emotion reasoning tasks on the new benchmark.

Conclusion: The work highlights the gap in emotion reasoning capabilities of current models and provides a benchmark and framework to advance this important area.

Abstract: Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [22] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

TL;DR: Delta-SVD is a training-free compression method that uses SVD to compress DreamBooth fine-tuned models by exploiting the low-rank structure of weight deltas, achieving significant storage reduction with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: Personalized text-to-image models like DreamBooth require storing many subject-specific models, creating substantial storage overhead that limits scalability and practical deployment.

Method: Applies Singular Value Decomposition (SVD) to factorize weight deltas from fine-tuning, followed by energy-based rank truncation to balance compression efficiency and reconstruction fidelity without additional training.

Result: Achieves substantial compression with negligible loss in generation quality (measured by CLIP score, SSIM, and FID) while maintaining plug-and-play compatibility and original model architecture.

Conclusion: Delta-SVD enables scalable and efficient deployment of personalized diffusion models, providing a practical solution for storing and deploying large-scale subject customizations in real-world applications.

Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning
large-scale diffusion backbones, resulting in significant storage overhead when
maintaining many subject-specific models. We present Delta-SVD, a post-hoc,
training-free compression method that targets the parameter weights update
induced by DreamBooth fine-tuning. Our key observation is that these delta
weights exhibit strong low-rank structure due to the sparse and localized
nature of personalization. Delta-SVD first applies Singular Value Decomposition
(SVD) to factorize the weight deltas, followed by an energy-based rank
truncation strategy to balance compression efficiency and reconstruction
fidelity. The resulting compressed models are fully plug-and-play and can be
re-constructed on-the-fly during inference. Notably, the proposed approach is
simple, efficient, and preserves the original model architecture. Experiments
on a multiple subject dataset demonstrate that Delta-SVD achieves substantial
compression with negligible loss in generation quality measured by CLIP score,
SSIM and FID. Our method enables scalable and efficient deployment of
personalized diffusion models, making it a practical solution for real-world
applications that require storing and deploying large-scale subject
customizations.

</details>


### [23] [Do Multimodal LLMs See Sentiment?](https://arxiv.org/abs/2508.16873)
*Neemias B. da Silva,John Harrison,Rodrigo Minetto,Myriam R. Delgado,Bogdan T. Nassu,Thiago H. Silva*

Main category: cs.CV

TL;DR: MLLMsent framework uses Multimodal Large Language Models for visual sentiment analysis through three approaches: direct classification, LLM-based analysis of generated descriptions, and fine-tuning on sentiment-labeled descriptions, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Understanding visual sentiment is crucial for social media interactions, but remains challenging due to complex scene-level semantics that require sophisticated reasoning capabilities.

Method: Three-pronged approach: 1) Direct sentiment classification using MLLMs, 2) Combining MLLMs with pre-trained LLMs for sentiment analysis on generated image descriptions, 3) Fine-tuning LLMs on sentiment-labeled image descriptions.

Result: Achieved SOTA results, outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4% respectively. Cross-dataset testing showed 8.26% improvement over best runner-up without training on new data.

Conclusion: The framework demonstrates strong visual reasoning capabilities for affective computing and establishes new benchmarks for future research in visual sentiment analysis.

Abstract: Understanding how visual content communicates sentiment is critical in an era
where online interaction is increasingly dominated by this kind of media on
social platforms. However, this remains a challenging problem, as sentiment
perception is closely tied to complex, scene-level semantics. In this paper, we
propose an original framework, MLLMsent, to investigate the sentiment reasoning
capabilities of Multimodal Large Language Models (MLLMs) through three
perspectives: (1) using those MLLMs for direct sentiment classification from
images; (2) associating them with pre-trained LLMs for sentiment analysis on
automatically generated image descriptions; and (3) fine-tuning the LLMs on
sentiment-labeled image descriptions. Experiments on a recent and established
benchmark demonstrate that our proposal, particularly the fine-tuned approach,
achieves state-of-the-art results outperforming Lexicon-, CNN-, and
Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,
across different levels of evaluators' agreement and sentiment polarity
categories. Remarkably, in a cross-dataset test, without any training on these
new data, our model still outperforms, by up to 8.26%, the best runner-up,
which has been trained directly on them. These results highlight the potential
of the proposed visual reasoning scheme for advancing affective computing,
while also establishing new benchmarks for future research.

</details>


### [24] [AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception](https://arxiv.org/abs/2508.16881)
*Xilai Li,Huichun Liu,Xiaosong Li,Tao Ye,Zhenyu Kuang,Huafeng Li*

Main category: cs.CV

TL;DR: AWM-Fuse is a novel multi-modality image fusion method that uses global and local text perception to handle adverse weather degradations, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Address the loss of visual information in adverse weather conditions and improve semantic perception by effectively incorporating textual information, which previous methods lacked in categorization and analysis.

Method: Uses global feature perception with BLIP-produced captions to extract scene features and identify degradation types, and local module with ChatGPT descriptions for specific degradation effects. Textual descriptions constrain fusion image generation to align with real semantic labels.

Result: Extensive experiments show AWM-Fuse outperforms current state-of-the-art methods in complex weather conditions and downstream tasks.

Conclusion: The proposed method effectively handles multiple degradations through unified text perception architecture, promoting better generalization and learning of meaningful visual features in adverse weather conditions.

Abstract: Multi-modality image fusion (MMIF) in adverse weather aims to address the
loss of visual information caused by weather-related degradations, providing
clearer scene representations. Although less studies have attempted to
incorporate textual information to improve semantic perception, they often lack
effective categorization and thorough analysis of textual content. In response,
we propose AWM-Fuse, a novel fusion method for adverse weather conditions,
designed to handle multiple degradations through global and local text
perception within a unified, shared weight architecture. In particular, a
global feature perception module leverages BLIP-produced captions to extract
overall scene features and identify primary degradation types, thus promoting
generalization across various adverse weather conditions. Complementing this,
the local module employs detailed scene descriptions produced by ChatGPT to
concentrate on specific degradation effects through concrete textual cues,
thereby capturing finer details. Furthermore, textual descriptions are used to
constrain the generation of fusion images, effectively steering the network
learning process toward better alignment with real semantic labels, thereby
promoting the learning of more meaningful visual features. Extensive
experiments demonstrate that AWM-Fuse outperforms current state-of-the-art
methods in complex weather conditions and downstream tasks. Our code is
available at https://github.com/Feecuin/AWM-Fuse.

</details>


### [25] [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism](https://arxiv.org/abs/2508.16884)
*Yi Zhang,Lingxiao Wei,Bowei Zhang,Ziwei Liu,Kai Yi,Shu Hu*

Main category: cs.CV

TL;DR: SAEViT is a lightweight Vision Transformer that uses sparse attention and convolutional blocks to reduce computational costs while maintaining performance on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Address Vision Transformer's limitations: large model size, high computational cost, and weak local feature modeling ability for real-world applications.

Method: Introduces Sparsely Aggregated Attention (SAA) for adaptive sparse sampling, Channel-Interactive Feed-Forward Network (CIFFN) for inter-channel information exchange, and hierarchical pyramid structure with depth-wise separable convolutional blocks.

Result: Achieves 76.3% and 79.6% Top-1 accuracy on ImageNet-1K with only 0.8 GFLOPs and 1.3 GFLOPs respectively.

Conclusion: SAEViT provides an efficient lightweight solution that balances computation efficiency and performance for various vision tasks.

Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its
strong long-range dependency modelling ability. However, its large model size
with high computational cost and weak local feature modeling ability hinder its
application in real scenarios. To balance computation efficiency and
performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight
ViT based model with convolution blocks, in this paper to achieve efficient
downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated
Attention (SAA) module that performs adaptive sparse sampling based on image
redundancy and recovers the feature map via deconvolution operation, which
significantly reduces the computational complexity of attention operations. In
addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed
to enhance inter-channel information exchange through feature decomposition and
redistribution, mitigating redundancy in traditional feed-forward networks
(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise
separable convolutional blocks (DWSConv) is devised to further strengthen
convolutional features. Extensive experiments on mainstream datasets show that
SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K
classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,
demonstrating a lightweight solution for various fundamental vision tasks.

</details>


### [26] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Proposes a multi-dimensional image quality assessment framework that models quality across technical and aesthetic dimensions, then combines features for final scoring. Also enables flexible training of image restoration models by adjusting dimension weights.


<details>
  <summary>Details</summary>
Motivation: Existing IQA methods focus too much on fitting overall scores, neglecting that humans evaluate image quality from multiple perceptual dimensions before arriving at overall assessment.

Method: MDIQA framework models image quality across 5 technical and 4 aesthetic dimensions in separate branches. Each branch trained under separate dimension guidance, then features combined for final IQA score. Also enables flexible training of image restoration models through dimension weight adjustment.

Result: Extensive experiments demonstrate superior performance compared to existing methods. The framework can be effectively and flexibly applied to image restoration tasks to better align with user preferences.

Conclusion: The multi-dimensional approach better captures human visual perception and provides a flexible framework for both quality assessment and image restoration that can adapt to varying user preferences.

Abstract: Recent advancements in image quality assessment (IQA), driven by
sophisticated deep neural network designs, have significantly improved the
ability to approach human perceptions. However, most existing methods are
obsessed with fitting the overall score, neglecting the fact that humans
typically evaluate image quality from different dimensions before arriving at
an overall quality assessment. To overcome this problem, we propose a
multi-dimensional image quality assessment (MDIQA) framework. Specifically, we
model image quality across various perceptual dimensions, including five
technical and four aesthetic dimensions, to capture the multifaceted nature of
human visual perception within distinct branches. Each branch of our MDIQA is
initially trained under the guidance of a separate dimension, and the
respective features are then amalgamated to generate the final IQA score.
Additionally, when the MDIQA model is ready, we can deploy it for a flexible
training of image restoration (IR) models, enabling the restoration results to
better align with varying user preferences through the adjustment of perceptual
dimension weights. Extensive experiments demonstrate that our MDIQA achieves
superior performance and can be effectively and flexibly applied to image
restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [27] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

TL;DR: SEGS is a training-free framework that addresses the Janus problem in text-to-3D generation by enforcing multi-view consistency through structural energy guidance in diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: Text-to-3D generation suffers from the Janus problem where objects appear correct from front views but have distorted geometry from other angles, caused by viewpoint bias in 2D diffusion priors.

Method: Proposes Structural Energy-Guided Sampling (SEGS) that defines structural energy in a PCA subspace of U-Net features and injects its gradients into the denoising trajectory to steer geometry toward intended viewpoints.

Result: SEGS significantly reduces Janus artifacts, achieving improved geometric alignment and viewpoint consistency without requiring retraining or weight modifications.

Conclusion: SEGS provides an effective plug-and-play solution that seamlessly integrates into existing SDS/VSD pipelines to address viewpoint bias issues in text-to-3D generation.

Abstract: Text-to-3D generation often suffers from the Janus problem, where objects
look correct from the front but collapse into duplicated or distorted geometry
from other angles. We attribute this failure to viewpoint bias in 2D diffusion
priors, which propagates into 3D optimization. To address this, we propose
Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play
framework that enforces multi-view consistency entirely at sampling time. SEGS
defines a structural energy in a PCA subspace of intermediate U-Net features
and injects its gradients into the denoising trajectory, steering geometry
toward the intended viewpoint while preserving appearance fidelity. Integrated
seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,
achieving improved geometric alignment and viewpoint consistency without
retraining or weight modification.

</details>


### [28] [MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition](https://arxiv.org/abs/2508.16922)
*Yudong Hu,Yueju Han,Rui Sun,Jinke Ren*

Main category: cs.CV

TL;DR: MSPCaps is a novel Capsule Network architecture that integrates multi-scale feature learning with efficient capsule routing through a Multi-Scale ResNet Backbone, Patchify Capsule Layer, and Cross-Agreement Routing blocks to achieve superior classification performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing CapsNet variants rely on single high-level feature maps and struggle with multi-scale feature fusion, leading to suboptimal performance. The authors aim to capture rich complementary information from multi-scale features while addressing feature discrepancy issues.

Method: Three key components: 1) Multi-Scale ResNet Backbone (MSRB) extracts diverse multi-scale features, 2) Patchify Capsule Layer partitions features into primary capsules with uniform patch size, 3) Cross-Agreement Routing blocks adaptively route multi-scale capsules by identifying cross-scale prediction pairs with maximum agreement.

Result: MSPCaps achieves remarkable scalability and superior robustness, consistently surpassing baseline methods in classification accuracy. Models range from Tiny (344.3K parameters) to Large (10.9M parameters), demonstrating excellent performance across different scales.

Conclusion: The proposed MSPCaps architecture effectively advances feature representation learning by successfully integrating multi-scale feature learning with efficient capsule routing, offering both high performance and scalability for visual recognition tasks.

Abstract: Capsule Network (CapsNet) has demonstrated significant potential in visual
recognition by capturing spatial relationships and part-whole hierarchies for
learning equivariant feature representations. However, existing CapsNet and
variants often rely on a single high-level feature map, overlooking the rich
complementary information from multi-scale features. Furthermore, conventional
feature fusion strategies (e.g., addition and concatenation) struggle to
reconcile multi-scale feature discrepancies, leading to suboptimal
classification performance. To address these limitations, we propose the
Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that
integrates multi-scale feature learning and efficient capsule routing.
Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet
Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement
Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature
representations from input images, preserving both fine-grained details and
global contextual information. Second, the PatchifyCaps partitions these
multi-scale features into primary capsules using a uniform patch size,
equipping the model with the ability to learn from diverse receptive fields.
Finally, the CAR block adaptively routes the multi-scale capsules by
identifying cross-scale prediction pairs with maximum agreement. Unlike the
simple concatenation of multiple self-routing blocks, CAR ensures that only the
most coherent capsules contribute to the final voting. Our proposed MSPCaps
achieves remarkable scalability and superior robustness, consistently
surpassing multiple baseline methods in terms of classification accuracy, with
configurations ranging from a highly efficient Tiny model (344.3K parameters)
to a powerful Large model (10.9M parameters), highlighting its potential in
advancing feature representation learning.

</details>


### [29] [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR](https://arxiv.org/abs/2508.16927)
*Siqing Yuan,Yulin Wang,Zirui Cao,Yueyan Wang,Zehao Weng,Hui Wang,Lei Xu,Zixian Chen,Lei Chen,Zhong Xue,Dinggang Shen*

Main category: cs.CV

TL;DR: CC-CMR is a gadolinium-free cardiomyopathy screening framework using contrastive learning to align cine CMR and LGE sequences, achieving 94.3% accuracy without contrast agents.


<details>
  <summary>Details</summary>
Motivation: Current CMR screening relies on gadolinium contrast and labor-intensive interpretation, limiting population-scale deployment. There's a need for accurate, contrast-free screening methods.

Method: Contrastive learning and cross-modal alignment framework that encodes fibrosis-specific pathology into cine CMR embeddings using Feature Interaction Module and uncertainty-guided adaptive training.

Result: Achieved 0.943 accuracy (95% CI: 0.886-0.986) on multi-center data from 231 subjects, outperforming state-of-the-art cine-CMR-only models by 4.3%.

Conclusion: CC-CMR demonstrates clinical viability for wide population screening by eliminating gadolinium dependency while maintaining high diagnostic accuracy.

Abstract: Cardiomyopathy, a principal contributor to heart failure and sudden cardiac
mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),
recognized as the diagnostic 'gold standard' through multiparametric protocols,
holds the potential to serve as an accurate screening tool. However, its
reliance on gadolinium contrast and labor-intensive interpretation hinders
population-scale deployment. We propose CC-CMR, a Contrastive Learning and
Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening
using cine CMR sequences. By aligning the latent spaces of cine CMR and Late
Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific
pathology into cine CMR embeddings. A Feature Interaction Module concurrently
optimizes diagnostic precision and cross-modal feature congruence, augmented by
an uncertainty-guided adaptive training mechanism that dynamically calibrates
task-specific objectives to ensure model generalizability. Evaluated on
multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:
0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while
eliminating gadolinium dependency, demonstrating its clinical viability for
wide range of populations and healthcare environments.

</details>


### [30] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: Invert3D enables efficient 3D content personalization through natural language prompts by aligning 3D representations with text embeddings, eliminating the need for computationally expensive retraining procedures.


<details>
  <summary>Details</summary>
Motivation: Current 3D personalization approaches rely on knowledge distillation-based methods that require computationally expensive retraining, creating a need for more efficient personalization techniques.

Method: Develops a camera-conditioned 3D-to-text inverse mechanism that projects 3D contents into a 3D embedding aligned with text embeddings, bridging the gap between 3D content and 2D image personalization techniques.

Result: Extensive experiments demonstrate that Invert3D achieves effective personalization of 3D content through natural language prompts.

Conclusion: The proposed framework enables convenient and efficient 3D content personalization without retraining, making 3D manipulation more accessible through natural language interfaces.

Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [31] [Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.16934)
*Tim Mach,Daniel Rueckert,Alex Berger,Laurin Lux,Ivan Ezhov*

Main category: cs.CV

TL;DR: Novel deep learning framework for cerebral vasculature segmentation in hyperspectral brain images using unsupervised domain adaptation to overcome severe label scarcity.


<details>
  <summary>Details</summary>
Motivation: Address the critical challenge of severe label scarcity that impedes conventional supervised training in biomedical imaging tasks.

Method: Utilizes unsupervised domain adaptation methodology, combining a small amount of expert-annotated ground truth with unlabeled data for training.

Result: Quantitative and qualitative evaluations confirm the method significantly outperforms existing state-of-the-art approaches.

Conclusion: Demonstrates the efficacy of domain adaptation for label-scarce biomedical imaging tasks, particularly in cerebral vasculature segmentation.

Abstract: This work presents a novel deep learning framework for segmenting cerebral
vasculature in hyperspectral brain images. We address the critical challenge of
severe label scarcity, which impedes conventional supervised training. Our
approach utilizes a novel unsupervised domain adaptation methodology, using a
small, expert-annotated ground truth alongside unlabeled data. Quantitative and
qualitative evaluations confirm that our method significantly outperforms
existing state-of-the-art approaches, demonstrating the efficacy of domain
adaptation for label-scarce biomedical imaging tasks.

</details>


### [32] [NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability](https://arxiv.org/abs/2508.16937)
*Krishna Kanth Nakka,Alexandre Alahi*

Main category: cs.CV

TL;DR: NAT (Neuron Attack for Transferability) is a novel adversarial attack method that targets individual neurons instead of entire layers, achieving superior transferability across models and domains with significantly higher fooling rates than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Previous adversarial attack methods focus on layer-level optimization which disproportionately targets a few neurons representing similar concepts, leaving other neurons minimally affected. This limits transferability across different models.

Method: NAT shifts from embedding-level separation to neuron-specific targeting, disrupting the core units of neural networks. It trains generators to maximize embedding separation by targeting specific neurons within the mid-layer embeddings of source models.

Result: Extensive experiments on 41 diverse ImageNet models and 9 fine-grained models show NAT achieves fooling rates surpassing existing baselines by over 14% in cross-model and 4% in cross-domain settings. It also achieves impressive fooling rates within just 10 queries.

Conclusion: Targeting individual neurons provides a more fundamental approach for adversarial transferability, effectively disrupting neural network core units and creating a common basis for transferability across different models.

Abstract: The generation of transferable adversarial perturbations typically involves
training a generator to maximize embedding separation between clean and
adversarial images at a single mid-layer of a source model. In this work, we
build on this approach and introduce Neuron Attack for Transferability (NAT), a
method designed to target specific neuron within the embedding. Our approach is
motivated by the observation that previous layer-level optimizations often
disproportionately focus on a few neurons representing similar concepts,
leaving other neurons within the attacked layer minimally affected. NAT shifts
the focus from embedding-level separation to a more fundamental,
neuron-specific approach. We find that targeting individual neurons effectively
disrupts the core units of the neural network, providing a common basis for
transferability across different models. Through extensive experiments on 41
diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates
that surpass existing baselines by over 14\% in cross-model and 4\% in
cross-domain settings. Furthermore, by leveraging the complementary attacking
capabilities of the trained generators, we achieve impressive fooling rates
within just 10 queries. Our code is available at:
https://krishnakanthnakka.github.io/NAT/

</details>


### [33] [HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis](https://arxiv.org/abs/2508.16942)
*Junhao Wu,Xiuer Gu,Zhiying Li,Yeying Jin,Yunfeng Diao,Zhiyu Li,Zhenbo Song,Xiaomei Zhang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: HieroAction is a vision-language model that provides structured, interpretable assessments of human actions using stepwise reasoning and hierarchical reinforcement learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing action evaluation methods only provide final scores without explanations, limiting practical applicability in domains like sports, healthcare, and robotics where interpretable reasoning is crucial.

Method: Combines Stepwise Action Reasoning (chain of thought process for structured evaluation) and Hierarchical Policy Learning (RL strategy to learn fine-grained sub-action dynamics and align with high-level action quality).

Result: Demonstrates superior performance across multiple benchmark datasets, providing accurate and interpretable assessments.

Conclusion: HieroAction effectively addresses the limitations of existing methods by delivering structured, explainable action evaluations through integrated reasoning and reinforcement learning approaches.

Abstract: Evaluating human actions with clear and detailed feedback is important in
areas such as sports, healthcare, and robotics, where decisions rely not only
on final outcomes but also on interpretable reasoning. However, most existing
methods provide only a final score without explanation or detailed analysis,
limiting their practical applicability. To address this, we introduce
HieroAction, a vision-language model that delivers accurate and structured
assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise
Action Reasoning, a tailored chain of thought process designed specifically for
action assessment, which guides the model to evaluate actions step by step,
from overall recognition through sub action analysis to final scoring, thus
enhancing interpretability and structured understanding; and (2) Hierarchical
Policy Learning, a reinforcement learning strategy that enables the model to
learn fine grained sub action dynamics and align them with high level action
quality, thereby improving scoring precision. The reasoning pathway structures
the evaluation process, while policy learning refines each stage through reward
based optimization. Their integration ensures accurate and interpretable
assessments, as demonstrated by superior performance across multiple benchmark
datasets. Code will be released upon acceptance.

</details>


### [34] [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze](https://arxiv.org/abs/2508.16956)
*Ruicheng Zhang,Puxin Yan,Zeyu Zhang,Yicheng Chang,Hongyi Chen,Zhi Jin*

Main category: cs.CV

TL;DR: RPD-Diff is a novel diffusion model for single-image dehazing that addresses dense and non-uniform haze conditions through physics-guided intermediate state targeting and adaptive patch-specific denoising.


<details>
  <summary>Details</summary>
Motivation: Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack adaptability to spatially varying haze distributions, leading to suboptimal restoration in complex haze scenarios.

Method: Proposes RPD-Diff with Physics-guided Intermediate State Targeting (PIST) strategy that leverages physical priors to reformulate the diffusion Markov chain, and Haze-Aware Denoising Timestep Predictor (HADTP) that dynamically adjusts patch-specific denoising timesteps using transmission map cross-attention.

Result: Extensive experiments across four real-world datasets demonstrate state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality haze-free images with superior detail clarity and color fidelity.

Conclusion: RPD-Diff effectively addresses the limitations of traditional methods by incorporating physics guidance and region-adaptive mechanisms, achieving robust visibility enhancement in complex haze conditions.

Abstract: Single-image dehazing under dense and non-uniform haze conditions remains
challenging due to severe information degradation and spatial heterogeneity.
Traditional diffusion-based dehazing methods struggle with insufficient
generation conditioning and lack of adaptability to spatially varying haze
distributions, which leads to suboptimal restoration. To address these
limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing
Diffusion Model for robust visibility enhancement in complex haze scenarios.
RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST)
strategy, which leverages physical priors to reformulate the diffusion Markov
chain by generation target transitions, mitigating the issue of insufficient
conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising
Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising
timesteps employing a transmission map cross-attention mechanism, adeptly
managing non-uniform haze distributions. Extensive experiments across four
real-world datasets demonstrate that RPD-Diff achieves state-of-the-art
performance in challenging dense and non-uniform haze scenarios, delivering
high-quality, haze-free images with superior detail clarity and color fidelity.

</details>


### [35] [Local Information Matters: A Rethink of Crowd Counting](https://arxiv.org/abs/2508.16970)
*Tianhang Pan,Xiuyi Jia*

Main category: cs.CV

TL;DR: LIMM introduces a crowd counting model that emphasizes local modeling capability through window partitioning and contrastive learning, achieving state-of-the-art performance by focusing on small head regions in images.


<details>
  <summary>Details</summary>
Motivation: Existing crowd counting models use standard backbones designed for general visual tasks with large receptive fields, but fail to address that human heads typically occupy very small portions of images in crowd scenes.

Method: Proposes Local Information Matters Model (LIMM) with two key strategies: 1) window partitioning design applying grid windows to input, and 2) window-wise contrastive learning to enhance local density level distinction. Also includes global attention module for large-sized individuals.

Result: Significant improvement in local modeling capability (8.7% MAE reduction on JHU-Crowd++ high-density subset) without compromising ability to count large-sized individuals. Achieves state-of-the-art performance across multiple public datasets.

Conclusion: Emphasizing local modeling capability through window-based approaches and contrastive learning is an effective design principle for crowd counting, addressing the fundamental characteristic that individuals occupy small image portions.

Abstract: The motivation of this paper originates from rethinking an essential
characteristic of crowd counting: individuals (heads of humans) in the crowd
counting task typically occupy a very small portion of the image. This
characteristic has never been the focus of existing works: they typically use
the same backbone as other visual tasks and pursue a large receptive field.
This drives us to propose a new model design principle of crowd counting:
emphasizing local modeling capability of the model. We follow the principle and
design a crowd counting model named Local Information Matters Model (LIMM). The
main innovation lies in two strategies: a window partitioning design that
applies grid windows to the model input, and a window-wise contrastive learning
design to enhance the model's ability to distinguish between local density
levels. Moreover, a global attention module is applied to the end of the model
to handle the occasionally occurring large-sized individuals. Extensive
experiments on multiple public datasets illustrate that the proposed model
shows a significant improvement in local modeling capability (8.7\% in MAE on
the JHU-Crowd++ high-density subset for example), without compromising its
ability to count large-sized ones, which achieves state-of-the-art performance.
Code is available at: https://github.com/tianhangpan/LIMM.

</details>


### [36] [Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams](https://arxiv.org/abs/2508.16972)
*Minghao Zhou,Rafael Souza,Yaqian Hu,Luming Che*

Main category: cs.CV

TL;DR: The paper introduces RDR framework to improve LVLMs' robustness to visual perturbations in scientific diagrams, proposing new metrics and a specialized dataset.


<details>
  <summary>Details</summary>
Motivation: LVLMs lack robustness to common visual perturbations (noise, blur, occlusions) in real-world scientific documents, which hinders their practical deployment, and existing benchmarks overlook this challenge.

Method: Proposes Robust Diagram Reasoning (RDR) framework with Adaptive Multi-View & Consistency Verification (AMCV) mechanism that generates multiple perturbed diagram versions, performs parallel inference, and uses consistency-based self-correction.

Result: Experiments show state-of-the-art LVLMs like GPT-4V suffer significant performance degradation on perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

Conclusion: The RDR framework effectively addresses the robustness gap in LVLMs for scientific diagram processing and provides new evaluation metrics and dataset for rigorous assessment.

Abstract: Large Language Models (LLMs) and their multimodal variants (LVLMs) hold
immense promise for scientific and engineering applications, particularly in
processing visual information like scientific diagrams. However, their
practical deployment is hindered by a critical lack of robustness to common
visual perturbations such as noise, blur, and occlusions, which are prevalent
in real-world scientific documents. Existing evaluation benchmarks largely
overlook this challenge, leaving the robust reasoning capabilities of LVLMs on
visually degraded scientific diagrams underexplored. To address this, we
introduce the Robust Diagram Reasoning (RDR) framework, a novel approach
designed to enhance and rigorously evaluate LVLMs' performance under such
conditions. At its core, RDR employs an Adaptive Multi-View & Consistency
Verification (AMCV) mechanism, which involves generating multiple perturbed
versions of a diagram, performing parallel inference, and then applying a
consistency-based self-correction loop. We also propose two new metrics,
Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC),
to quantify robustness. Furthermore, we construct SciDiagram-Robust, the first
large-scale scientific diagram question-answering dataset specifically
augmented with diverse, programmatically generated visual perturbations. Our
extensive experiments demonstrate that even state-of-the-art closed-source
LVLMs like GPT-4V exhibit significant performance degradation when faced with
perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

</details>


### [37] [Balanced Sharpness-Aware Minimization for Imbalanced Regression](https://arxiv.org/abs/2508.16973)
*Yahao Liu,Qin Wang,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: BSAM addresses imbalanced regression by reframing it as an imbalanced generalization problem, using a balanced sharpness-aware minimization approach with targeted reweighting to ensure uniform generalization across all observation values.


<details>
  <summary>Details</summary>
Motivation: Real-world regression data often has imbalanced distributions, causing poor performance for rare target values. Traditional regression models struggle with this imbalanced regression problem.

Method: Proposes Balanced Sharpness-Aware Minimization (BSAM) that starts from traditional sharpness-aware minimization and adds a targeted reweighting strategy to homogenize generalization ability across the entire observation space.

Result: Extensive experiments on multiple vision regression tasks (age and depth estimation) show BSAM consistently outperforms existing approaches.

Conclusion: BSAM effectively addresses imbalanced regression by ensuring uniform generalization ability across all target values, providing theoretical generalization guarantees and superior performance on real-world vision tasks.

Abstract: Regression is fundamental in computer vision and is widely used in various
tasks including age estimation, depth estimation, target localization, \etc
However, real-world data often exhibits imbalanced distribution, making
regression models perform poorly especially for target values with rare
observations~(known as the imbalanced regression problem). In this paper, we
reframe imbalanced regression as an imbalanced generalization problem. To
tackle that, we look into the loss sharpness property for measuring the
generalization ability of regression models in the observation space. Namely,
given a certain perturbation on the model parameters, we check how model
performance changes according to the loss values of different target
observations. We propose a simple yet effective approach called Balanced
Sharpness-Aware Minimization~(BSAM) to enforce the uniform generalization
ability of regression models for the entire observation space. In particular,
we start from the traditional sharpness-aware minimization and then introduce a
novel targeted reweighting strategy to homogenize the generalization ability
across the observation space, which guarantees a theoretical generalization
bound. Extensive experiments on multiple vision regression tasks, including age
and depth estimation, demonstrate that our BSAM method consistently outperforms
existing approaches. The code is available
\href{https://github.com/manmanjun/BSAM_for_Imbalanced_Regression}{here}.

</details>


### [38] [Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding](https://arxiv.org/abs/2508.16974)
*Leilei Guo,Antonio Carlos Rivera,Peiyu Tang,Haoxuan Ren,Zheyu Song*

Main category: cs.CV

TL;DR: HCG-LVLM is a hierarchical vision-language model that mimics human coarse-to-fine processing, achieving superior accuracy and reduced hallucination in fine-grained visual tasks.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs exhibit insufficient robustness, proneness to hallucination, and reasoning errors in complex real-world scenarios requiring precise image region localization and fine-grained visual reasoning.

Method: Two-layered architecture: Global Contextual Perception layer for broad understanding and Fine-grained Local Grounding layer with Local Detail Enhancement Module and Semantic Consistency Validator, using adaptive fusion mechanism.

Result: Outperforms state-of-the-art models (Flamingo, BLIP-2, MiniGPT-4) on challenging datasets including GQA, A-OKVQA, and RefCOCO/+/g, achieving superior accuracy and significantly reducing hallucination.

Conclusion: The hierarchical design effectively enhances fine-grained visual-language understanding and precise grounding capabilities, validating the coarse-to-fine cognitive processing approach.

Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have
achieved remarkable progress in natural language processing and multimodal
understanding. Despite their impressive generalization capabilities, current
LVLMs often exhibit insufficient robustness, proneness to hallucination, and
reasoning errors in complex real-world scenarios, particularly when precise
image region localization and fine-grained visual reasoning are required. To
address these limitations, we propose the Hierarchical Contextual Grounding
LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine
cognitive processing. HCG-LVLM employs a two-layered approach: a Global
Contextual Perception layer for initial broad understanding and a Fine-grained
Local Grounding layer. The latter incorporates a Local Detail Enhancement
Module to extract high-resolution features and a Semantic Consistency Validator
to ensure accurate, hallucination-free visual-language alignment. Through an
adaptive fusion mechanism, information from both layers is integrated for
robust and precise outputs. Extensive experiments on challenging datasets,
including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring
Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms
state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model
achieves superior accuracy and significantly reduces hallucination, validating
the effectiveness of its hierarchical design in enhancing fine-grained
visual-language understanding and precise grounding capabilities.

</details>


### [39] [Combating Digitally Altered Images: Deepfake Detection](https://arxiv.org/abs/2508.16975)
*Saksham Kumar,Rhythm Narang*

Main category: cs.CV

TL;DR: A modified Vision Transformer model achieves state-of-the-art Deepfake detection using the OpenForensics Dataset with augmentation and class imbalance handling.


<details>
  <summary>Details</summary>
Motivation: The rise of Deepfake technology creating hyper-realistic manipulated images and videos poses significant challenges to public trust and authorities, requiring robust detection methods.

Method: Modified Vision Transformer (ViT) model trained on OpenForensics Dataset subset with multiple augmentation techniques, oversampling for class imbalance, and stratified train-validation split.

Result: The model demonstrates state-of-the-art performance on the test dataset, achieving high accuracy in meticulously detecting Deepfake images.

Conclusion: The proposed modified Vision Transformer approach provides an effective and robust solution for Deepfake detection, addressing current challenges in identifying manipulated media content.

Abstract: The rise of Deepfake technology to generate hyper-realistic manipulated
images and videos poses a significant challenge to the public and relevant
authorities. This study presents a robust Deepfake detection based on a
modified Vision Transformer(ViT) model, trained to distinguish between real and
Deepfake images. The model has been trained on a subset of the OpenForensics
Dataset with multiple augmentation techniques to increase robustness for
diverse image manipulations. The class imbalance issues are handled by
oversampling and a train-validation split of the dataset in a stratified
manner. Performance is evaluated using the accuracy metric on the training and
testing datasets, followed by a prediction score on a random image of people,
irrespective of their realness. The model demonstrates state-of-the-art results
on the test dataset to meticulously detect Deepfake images.

</details>


### [40] [Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection](https://arxiv.org/abs/2508.16976)
*Bin Pan,Shiyu Shen,Zongbin Wang,Zhenwei Shi,Xia Xu*

Main category: cs.CV

TL;DR: JPS is a parameter-efficient domain generalization method that selectively fine-tunes only a sparse subset of parameters in pre-trained models to preserve their generalization capabilities while adapting to new tasks.


<details>
  <summary>Details</summary>
Motivation: Full fine-tuning of large pre-trained vision models can compromise their intrinsic generalization capabilities. Parameter-efficient adaptation strategies are needed to balance task adaptation with preservation of generalization.

Method: Joint Parameter Selection (JPS) restricts updates to a small, sparse subset of parameters using dual operators to identify parameters with consistent and significant gradients across all source domains.

Result: Extensive benchmark experiments show JPS achieves superior performance compared to state-of-the-art domain generalization methods, demonstrating both efficiency and efficacy.

Conclusion: JPS provides a principled approach to selective fine-tuning that effectively preserves pre-trained models' generalization capabilities while enabling effective domain adaptation.

Abstract: Domain generalization seeks to develop models trained on a limited set of
source domains that are capable of generalizing effectively to unseen target
domains. While the predominant approach leverages large-scale pre-trained
vision models as initialization, recent studies have highlighted that full
fine-tuning can compromise the intrinsic generalization capabilities of these
models. To address this limitation, parameter-efficient adaptation strategies
have emerged, wherein only a subset of model parameters is selectively
fine-tuned, thereby balancing task adaptation with the preservation of
generalization. Motivated by this paradigm, we introduce Joint Parameter
Selection (JPS), a novel method that restricts updates to a small, sparse
subset of parameters, thereby retaining and harnessing the generalization
strength of pre-trained models. Theoretically, we establish a generalization
error bound that explicitly accounts for the sparsity of parameter updates,
thereby providing a principled justification for selective fine-tuning.
Practically, we design a selection mechanism employing dual operators to
identify and update parameters exhibiting consistent and significant gradients
across all source domains. Extensive benchmark experiments demonstrate that JPS
achieves superior performance compared to state-of-the-art domain
generalization methods, substantiating both the efficiency and efficacy of the
proposed approach.

</details>


### [41] [HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching](https://arxiv.org/abs/2508.16984)
*Liang Feng,Shikang Zheng,Jiacheng Liu,Yuqi Lin,Qinming Zhou,Peiliang Cai,Xinyu Wang,Junjie Chen,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: HiCache is a training-free acceleration framework for diffusion models that uses Hermite polynomials and dual-scaling to achieve 6.24x speedup while maintaining or exceeding baseline quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from prohibitive computational costs due to iterative sampling, and existing feature caching methods fail to model the complex dynamics of feature evolution, leading to quality loss.

Method: Uses Hermite polynomials as theoretically optimal basis for Gaussian-correlated feature derivative approximations in Diffusion Transformers, with a dual-scaling mechanism for numerical stability and predictive accuracy.

Result: Achieves 6.24x speedup on FLUX.1-dev while exceeding baseline quality, with strong performance across text-to-image, video generation, and super-resolution tasks.

Conclusion: HiCache provides a fundamental improvement in feature prediction for diffusion models through mathematical alignment with empirical properties, offering significant acceleration without quality degradation.

Abstract: Diffusion models have achieved remarkable success in content generation but
suffer from prohibitive computational costs due to iterative sampling. While
recent feature caching methods tend to accelerate inference through temporal
extrapolation, these methods still suffer from server quality loss due to the
failure in modeling the complex dynamics of feature evolution. To solve this
problem, this paper presents HiCache, a training-free acceleration framework
that fundamentally improves feature prediction by aligning mathematical tools
with empirical properties. Our key insight is that feature derivative
approximations in Diffusion Transformers exhibit multivariate Gaussian
characteristics, motivating the use of Hermite polynomials-the potentially
theoretically optimal basis for Gaussian-correlated processes. Besides, We
further introduce a dual-scaling mechanism that ensures numerical stability
while preserving predictive accuracy. Extensive experiments demonstrate
HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding
baseline quality, maintaining strong performance across text-to-image, video
generation, and super-resolution tasks. Core implementation is provided in the
appendix, with complete code to be released upon acceptance.

</details>


### [42] [An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation](https://arxiv.org/abs/2508.17007)
*Riad Hassan,M. Rubaiyat Hossain Mondal,Sheikh Iqbal Ahamed,Fahad Mostafa,Md Mostafijur Rahman*

Main category: cs.CV

TL;DR: EDLDNet introduces an efficient dual-line decoder segmentation network that balances accuracy and computational efficiency for medical image segmentation, achieving state-of-the-art performance with 84.00% Dice score on Synapse dataset while reducing computational operations by 89.7%.


<details>
  <summary>Details</summary>
Motivation: Current deep learning segmentation methods fail to balance accuracy with computational efficiency - they either prioritize performance at high computational cost or compromise accuracy for efficiency, creating a need for a solution that achieves both.

Method: Proposes EDLDNet with a noisy decoder that incorporates structured perturbation during training for robustness but uses only noise-free decoder at inference. Utilizes Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs), Up-Convolution Blocks (UCBs), and a mutation-based loss function leveraging multi-scale segmentation masks from both decoders.

Result: Outperforms SOTA on four medical imaging datasets. Achieves 84.00% Dice score on Synapse dataset (13.89% improvement over UNet) while reducing MACs by 89.7%. Maintains comparable computational efficiency to recent approaches like EMCAD while achieving higher Dice scores.

Conclusion: EDLDNet demonstrates strong generalization, computational efficiency, and robustness across diverse datasets, establishing it as an effective solution for organ-at-risk segmentation in medical imaging applications.

Abstract: Proper segmentation of organs-at-risk is important for radiation therapy,
surgical planning, and diagnostic decision-making in medical image analysis.
While deep learning-based segmentation architectures have made significant
progress, they often fail to balance segmentation accuracy with computational
efficiency. Most of the current state-of-the-art methods either prioritize
performance at the cost of high computational complexity or compromise accuracy
for efficiency. This paper addresses this gap by introducing an efficient
dual-line decoder segmentation network (EDLDNet). The proposed method features
a noisy decoder, which learns to incorporate structured perturbation at
training time for better model robustness, yet at inference time only the
noise-free decoder is executed, leading to lower computational cost.
Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs),
and Up-Convolution Blocks (UCBs) are further utilized to optimize feature
representation and boost segmentation performance. By leveraging multi-scale
segmentation masks from both decoders, we also utilize a mutation-based loss
function to enhance the model's generalization. Our approach outperforms SOTA
segmentation architectures on four publicly available medical imaging datasets.
EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse
dataset, surpassing baseline model like UNet by 13.89% in Dice score while
significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared
to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice
score but also maintains comparable computational efficiency. The outstanding
performance across diverse datasets establishes EDLDNet's strong
generalization, computational efficiency, and robustness. The source code,
pre-processed data, and pre-trained weights will be available at
https://github.com/riadhassan/EDLDNet .

</details>


### [43] [Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2508.17009)
*Wangyu Wu,Zhenhong Chen,Xiaowen Ma,Wenqiao Zhang,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: CPC is a novel weakly supervised semantic segmentation framework that uses LLMs to create category clusters capturing inter-class relationships and employs contrastive learning for better intra-class consistency and inter-class separation.


<details>
  <summary>Details</summary>
Motivation: Existing WSSS methods focus too much on inter-class separation while neglecting shared semantics among related categories and lack fine-grained discrimination, leading to confusion among visually similar categories.

Method: Uses Large Language Models to derive category clusters encoding inter-class relationships, and introduces class-aware patch-level contrastive loss to enforce intra-class consistency and inter-class separation in a hierarchical design.

Result: CPC surpasses existing state-of-the-art methods on PASCAL VOC 2012 and MS COCO 2014 datasets.

Conclusion: The hierarchical approach leveraging clusters as coarse-grained semantic priors while preserving fine-grained boundaries effectively reduces confusion among visually similar categories in weakly supervised semantic segmentation.

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has
gained attention for its cost-effectiveness. Most existing methods emphasize
inter-class separation, often neglecting the shared semantics among related
categories and lacking fine-grained discrimination. To address this, we propose
Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large
Language Models (LLMs) to derive category clusters that encode intrinsic
inter-class relationships, and further introduces a class-aware patch-level
contrastive loss to enforce intra-class consistency and inter-class separation.
This hierarchical design leverages clusters as coarse-grained semantic priors
while preserving fine-grained boundaries, thereby reducing confusion among
visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014
demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.

</details>


### [44] [Fiducial Marker Splatting for High-Fidelity Robotics Simulations](https://arxiv.org/abs/2508.17012)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.CV

TL;DR: Hybrid framework combining Gaussian Splatting's photorealism with structured fiducial markers for improved robotic simulation in complex environments like greenhouses.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh-based 3D simulations struggle in complex environments with occlusions and repetitive structures, while neural rendering methods lack support for fiducial markers essential for robotic localization.

Method: Novel algorithm for efficiently generating Gaussian Splatting-based fiducial markers (e.g., AprilTags) within cluttered scenes, combining photorealism of GS with structured marker representations.

Result: Outperforms traditional image-fitting techniques in both efficiency and pose-estimation accuracy, demonstrated in challenging greenhouse simulation with dense foliage and occlusions.

Conclusion: The hybrid framework shows significant value for real-world robotic applications, particularly in agricultural settings where complex visual conditions push perception limits.

Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its
traditional reliance on mesh-based representations often struggle in complex
environments, such as densely packed greenhouses featuring occlusions and
repetitive structures. Recent neural rendering methods, like Gaussian Splatting
(GS), achieve remarkable visual realism but lack flexibility to incorporate
fiducial markers, which are essential for robotic localization and control. We
propose a hybrid framework that combines the photorealism of GS with structured
marker representations. Our core contribution is a novel algorithm for
efficiently generating GS-based fiducial markers (e.g., AprilTags) within
cluttered scenes. Experiments show that our approach outperforms traditional
image-fitting techniques in both efficiency and pose-estimation accuracy. We
further demonstrate the framework's potential in a greenhouse simulation. This
agricultural setting serves as a challenging testbed, as its combination of
dense foliage, similar-looking elements, and occlusions pushes the limits of
perception, thereby highlighting the framework's value for real-world
applications.

</details>


### [45] [Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation](https://arxiv.org/abs/2508.17017)
*Konstantina Nikolaidou,George Retsinas,Giorgos Sfikas,Silvia Cascianelli,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: DOG (Dual Orthogonal Guidance) improves diffusion-based handwritten text generation by reducing artifacts and improving readability through orthogonal projection guidance and triangular scheduling.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models for handwritten text generation suffer from memorization, style variability issues, and produce artifacts that reduce readability, especially for challenging styles and out-of-vocabulary words.

Method: Proposes Dual Orthogonal Guidance (DOG) that uses orthogonal projection of negatively perturbed prompts onto positive prompts, combined with triangular scheduling to control guidance strength throughout denoising.

Result: DOG improves content clarity and style variability in state-of-the-art models (DiffusionPen and One-DM), even for out-of-vocabulary words and challenging writing styles.

Conclusion: DOG provides more stable and disentangled guidance than standard CFG, effectively reducing artifacts while maintaining content integrity and enabling diverse yet plausible handwritten text generation.

Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve
impressive results on frequent, in-vocabulary words observed at training time
and on regular styles. However, they are prone to memorizing training samples
and often struggle with style variability and generation clarity. In
particular, standard diffusion models tend to produce artifacts or distortions
that negatively affect the readability of the generated text, especially when
the style is hard to produce. To tackle these issues, we propose a novel
sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an
orthogonal projection of a negatively perturbed prompt onto the original
positive prompt. This approach helps steer the generation away from artifacts
while maintaining the intended content, and encourages more diverse, yet
plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which
relies on unconditional predictions and produces noise at high guidance scales,
DOG introduces a more stable, disentangled direction in the latent space. To
control the strength of the guidance across the denoising process, we apply a
triangular schedule: weak at the start and end of denoising, when the process
is most sensitive, and strongest in the middle steps. Experimental results on
the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both
content clarity and style variability, even for out-of-vocabulary words and
challenging writing styles.

</details>


### [46] [Probabilistic Temporal Masked Attention for Cross-view Online Action Detection](https://arxiv.org/abs/2508.17025)
*Liping Xie,Yang Tan,Shicheng Jing,Huimin Lu,Kanjian Zhang*

Main category: cs.CV

TL;DR: PTMA model uses probabilistic temporal masked attention for cross-view online action detection, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Mainstream OAD models are sensitive to varying video viewpoints, limiting their generalization to unseen sources.

Method: Probabilistic Temporal Masked Attention (PTMA) with GRU-based temporal masked attention cell that leverages probabilistic modeling for latent compressed representations and cross-view feature extraction.

Result: Achieves state-of-the-art performance on DAHLIA, IKEA ASM, and Breakfast datasets under cross-subject, cross-view, and cross-subject-view evaluation protocols.

Conclusion: PTMA effectively addresses viewpoint sensitivity in online action detection through probabilistic modeling and cross-view feature integration.

Abstract: As a critical task in video sequence classification within computer vision,
Online Action Detection (OAD) has garnered significant attention. The
sensitivity of mainstream OAD models to varying video viewpoints often hampers
their generalization when confronted with unseen sources. To address this
limitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)
model, which leverages probabilistic modeling to derive latent compressed
representations of video frames in a cross-view setting. The PTMA model
incorporates a GRU-based temporal masked attention (TMA) cell, which leverages
these representations to effectively query the input video sequence, thereby
enhancing information interaction and facilitating autoregressive frame-level
video analysis. Additionally, multi-view information can be integrated into the
probabilistic modeling to facilitate the extraction of view-invariant features.
Experiments conducted under three evaluation protocols: cross-subject (cs),
cross-view (cv), and cross-subject-view (csv) show that PTMA achieves
state-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.

</details>


### [47] [A Novel Local Focusing Mechanism for Deepfake Detection Generalization](https://arxiv.org/abs/2508.17029)
*Mingliang Li,Lin Yuanbo Wu,Changhong Liu,Hanxi Li*

Main category: cs.CV

TL;DR: Proposes Local Focus Mechanism (LFM) for cross-domain deepfake detection using salience-guided local feature attention with Top-K pooling and regularization techniques to overcome CNN limitations.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods based on deep CNNs show poor generalization across object categories and generation domains due to overfitting to semantic features and loss of local forgery cues through Global Average Pooling.

Method: LFM integrates a Salience Network with Top-K Pooling module to select most informative local patterns, plus Rank-Based Linear Dropout and Random-K Sampling regularization to prevent overfitting.

Result: Achieves 3.7% accuracy improvement and 2.8% average precision increase over state-of-the-art NPR method, with exceptional efficiency of 1789 FPS on single GPU.

Conclusion: Sets new benchmark for cross-domain deepfake detection by effectively addressing CNN limitations through local feature attention and robust regularization techniques.

Abstract: The rapid advancement of deepfake generation techniques has intensified the
need for robust and generalizable detection methods. Existing approaches based
on reconstruction learning typically leverage deep convolutional networks to
extract differential features. However, these methods show poor generalization
across object categories (e.g., from faces to cars) and generation domains
(e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep
CNNs. First, models trained on a specific category tend to overfit to semantic
feature distributions, making them less transferable to other categories,
especially as network depth increases. Second, Global Average Pooling (GAP)
compresses critical local forgery cues into a single vector, thus discarding
discriminative patterns vital for real-fake classification. To address these
issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends
to discriminative local features for differentiating fake from real images. LFM
integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP)
module to select the K most informative local patterns. To mitigate potential
overfitting introduced by Top-K pooling, we introduce two regularization
techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which
enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and
a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel
Relationships (NPR) method, while maintaining exceptional efficiency at 1789
FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for
cross-domain deepfake detection. The source code are available in
https://github.com/lmlpy/LFM.git

</details>


### [48] [F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search](https://arxiv.org/abs/2508.17037)
*Raghul Asokan*

Main category: cs.CV

TL;DR: F4-ITS is a training-free vision-language framework that improves food image-text matching through multi-modal feature fusion and ingredient-based re-ranking, achieving significant performance gains over standard baselines.


<details>
  <summary>Details</summary>
Motivation: The proliferation of digital food content requires robust systems for fine-grained visual understanding and retrieval, particularly for applications like dietary monitoring, smart kitchens, and restaurant automation.

Method: Proposes a training-free VLM-guided framework with uni/bi-directional multi-modal fusion (combining image embeddings with VLM-generated text descriptions) and a novel feature-based re-ranking mechanism using predicted food ingredients.

Result: Achieves ~10% and ~7.7% improvements in top-1 retrieval under dense/sparse caption scenarios, ~28.6% gain in top-k ingredient-level retrieval, and shows smaller models can outperform larger counterparts when augmented with textual fusion.

Conclusion: The F4-ITS framework effectively enhances food image-text search performance through multi-modal feature fusion and ingredient-based re-ranking, making it suitable for resource-constrained settings while maintaining strong retrieval accuracy.

Abstract: The proliferation of digital food content has intensified the need for robust
and accurate systems capable of fine-grained visual understanding and
retrieval. In this work, we address the challenging task of food image-to-text
matching, a critical component in applications such as dietary monitoring,
smart kitchens, and restaurant automation. We propose F4-ITS: Fine-grained
Feature Fusion for Food Image-Text Search, a training-free, vision-language
model (VLM)-guided framework that significantly improves retrieval performance
through enhanced multi-modal feature representations. Our approach introduces
two key contributions: (1) a uni-directional(and bi-directional) multi-modal
fusion strategy that combines image embeddings with VLM-generated textual
descriptions to improve query expressiveness, and (2) a novel feature-based
re-ranking mechanism for top-k retrieval, leveraging predicted food ingredients
to refine results and boost precision. Leveraging open-source image-text
encoders, we demonstrate substantial gains over standard baselines - achieving
~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption
scenarios, and a ~28.6% gain in top-k ingredient-level retrieval. Additionally,
we show that smaller models (e.g., ViT-B/32) can match or outperform larger
counterparts (e.g., ViT-H, ViT-G, ViT-bigG) when augmented with textual fusion,
highlighting the effectiveness of our method in resource-constrained settings.
Code and test datasets will be made publicly available at:
https://github.com/mailcorahul/f4-its

</details>


### [49] [M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments](https://arxiv.org/abs/2508.17044)
*Dmitry Yudin*

Main category: cs.CV

TL;DR: This paper proposes a taxonomy for multimodal 3D mapping methods and introduces M3DMap, a modular system for object-aware 3D map construction in both static and dynamic environments using multimodal data.


<details>
  <summary>Details</summary>
Motivation: There are no universal representations for dynamic 3D scenes that incorporate multimodal data (images, point clouds, text), which poses challenges for robotics and autonomous transportation applications.

Method: The paper proposes a taxonomy classifying methods by scene types, representations, learning methods, and applications. It also presents M3DMap - a modular method with components for neural multimodal object segmentation/tracking, odometry estimation, 3D map construction/updating, and multimodal data retrieval.

Result: The article provides a structured analysis of recent methods using the proposed taxonomy and highlights original implementations of M3DMap modules with advantages for practical tasks like 3D object grounding and mobile manipulation.

Conclusion: Theoretical propositions demonstrate the positive effect of using multimodal data and modern foundational models in 3D mapping methods, providing a comprehensive framework for dynamic 3D scene representation.

Abstract: 3D mapping in dynamic environments poses a challenge for modern researchers
in robotics and autonomous transportation. There are no universal
representations for dynamic 3D scenes that incorporate multimodal data such as
images, point clouds, and text. This article takes a step toward solving this
problem. It proposes a taxonomy of methods for constructing multimodal 3D maps,
classifying contemporary approaches based on scene types and representations,
learning methods, and practical applications. Using this taxonomy, a brief
structured analysis of recent methods is provided. The article also describes
an original modular method called M3DMap, designed for object-aware
construction of multimodal 3D maps for both static and dynamic scenes. It
consists of several interconnected components: a neural multimodal object
segmentation and tracking module; an odometry estimation module, including
trainable algorithms; a module for 3D map construction and updating with
various implementations depending on the desired scene representation; and a
multimodal data retrieval module. The article highlights original
implementations of these modules and their advantages in solving various
practical tasks, from 3D object grounding to mobile manipulation. Additionally,
it presents theoretical propositions demonstrating the positive effect of using
multimodal data and modern foundational models in 3D mapping methods. Details
of the taxonomy and method implementation are available at
https://yuddim.github.io/M3DMap.

</details>


### [50] [Styleclone: Face Stylization with Diffusion Based Data Augmentation](https://arxiv.org/abs/2508.17045)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: StyleClone uses textual inversion and diffusion guidance to augment small style datasets, then trains fast image-to-image networks that outperform diffusion methods in speed and quality for face stylization.


<details>
  <summary>Details</summary>
Motivation: To enable high-quality face stylization in specific styles with limited style images, overcoming the limitations of small datasets and slow diffusion-based methods.

Method: Leverages textual inversion and diffusion-based guided image generation to systematically augment small style datasets, then trains fast image-to-image translation networks on the augmented data.

Result: Outperforms diffusion-based methods in both speed and quality, improves stylization quality, better preserves source image content, and significantly accelerates inference across multiple styles.

Conclusion: The approach effectively addresses the challenge of limited style images through systematic dataset augmentation and enables fast, high-quality face stylization that surpasses diffusion methods.

Abstract: We present StyleClone, a method for training image-to-image translation
networks to stylize faces in a specific style, even with limited style images.
Our approach leverages textual inversion and diffusion-based guided image
generation to augment small style datasets. By systematically generating
diverse style samples guided by both the original style images and real face
images, we significantly enhance the diversity of the style dataset. Using this
augmented dataset, we train fast image-to-image translation networks that
outperform diffusion-based methods in speed and quality. Experiments on
multiple styles demonstrate that our method improves stylization quality,
better preserves source image content, and significantly accelerates inference.
Additionally, we provide a systematic evaluation of the augmentation techniques
and their impact on stylization performance.

</details>


### [51] [PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models](https://arxiv.org/abs/2508.17050)
*Xianjing Cheng,Lintai Wu,Zuowen Wang,Junhui Hou,Jie Wen,Yong Xu*

Main category: cs.CV

TL;DR: PVNet is a diffusion model-based point-voxel interaction framework for LiDAR point cloud upsampling in outdoor scenes without dense supervision, achieving state-of-the-art performance with arbitrary upsampling rates.


<details>
  <summary>Details</summary>
Motivation: LiDAR-scanned data often suffer from extreme sparsity that hinders 3D perception tasks, and existing point cloud upsampling methods focus on individual objects with limited generalization for complex outdoor scenes.

Method: Uses classifier-free guidance-based DDPMs with sparse point cloud as condition and synthesized nearby frames as input. Includes voxel completion module to refine features and point-voxel interaction module to integrate point and voxel features.

Result: Extensive experiments on various benchmarks demonstrate state-of-the-art performance for scene-level point cloud upsampling.

Conclusion: PVNet is the first scene-level point cloud upsampling method supporting arbitrary upsampling rates, effectively addressing LiDAR data sparsity in outdoor environments.

Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on
high-quality point clouds. However, LiDAR-scanned data often suffer from
extreme sparsity, severely hindering downstream 3D perception tasks. Existing
point cloud upsampling methods primarily focus on individual objects, thus
demonstrating limited generalization capability for complex outdoor scenes. To
address this issue, we propose PVNet, a diffusion model-based point-voxel
interaction framework to perform LiDAR point cloud upsampling without dense
supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to
guide the generation, in which we employ a sparse point cloud as the guiding
condition and the synthesized point clouds derived from its nearby frames as
the input. Moreover, we design a voxel completion module to refine and complete
the coarse voxel features for enriching the feature representation. In
addition, we propose a point-voxel interaction module to integrate features
from both points and voxels, which efficiently improves the environmental
perception capability of each upsampled point. To the best of our knowledge,
our approach is the first scene-level point cloud upsampling method supporting
arbitrary upsampling rates. Extensive experiments on various benchmarks
demonstrate that our method achieves state-of-the-art performance. The source
code will be available at https://github.com/chengxianjing/PVNet.

</details>


### [52] [DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054)
*Qingwen Zhang,Xiaomeng Zhu,Yushan Zhang,Yixi Cai,Olov Andersson,Patric Jensfelt*

Main category: cs.CV

TL;DR: DeltaFlow is a lightweight 3D scene flow estimation framework that efficiently captures temporal information across multiple frames with minimal computational cost, achieving state-of-the-art performance with 22% lower error and 2x faster inference.


<details>
  <summary>Details</summary>
Motivation: Previous scene flow methods focus on two consecutive frames, neglecting valuable temporal information. Multi-frame approaches suffer from rapidly escalating computational costs as frame count increases.

Method: Proposes DeltaFlow framework with a Δ scheme for efficient temporal feature extraction, Category-Balanced Loss for underrepresented classes, and Instance Consistency Loss for coherent object motion.

Result: Achieves state-of-the-art performance on Argoverse 2 and Waymo datasets with 22% lower error and 2x faster inference compared to next-best multi-frame method, plus strong cross-domain generalization.

Conclusion: DeltaFlow successfully addresses computational efficiency and accuracy challenges in multi-frame scene flow estimation through innovative temporal feature extraction and specialized loss functions.

Abstract: Previous dominant methods for scene flow estimation focus mainly on input
from two consecutive frames, neglecting valuable information in the temporal
domain. While recent trends shift towards multi-frame reasoning, they suffer
from rapidly escalating computational costs as the number of frames grows. To
leverage temporal information more efficiently, we propose DeltaFlow
($\Delta$Flow), a lightweight 3D framework that captures motion cues via a
$\Delta$ scheme, extracting temporal features with minimal computational cost,
regardless of the number of frames. Additionally, scene flow estimation faces
challenges such as imbalanced object class distributions and motion
inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to
enhance learning across underrepresented classes and an Instance Consistency
Loss to enforce coherent object motion, improving flow accuracy. Extensive
evaluations on the Argoverse 2 and Waymo datasets show that $\Delta$Flow
achieves state-of-the-art performance with up to 22% lower error and $2\times$
faster inference compared to the next-best multi-frame supervised method, while
also demonstrating a strong cross-domain generalization ability. The code is
open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model
weights.

</details>


### [53] [REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework](https://arxiv.org/abs/2508.17061)
*Stefanos Pasios,Nikos Nikolaidis*

Main category: cs.CV

TL;DR: REGEN framework uses dual-stage generative network to enhance game photorealism in real-time, achieving 32x speedup over unpaired image translation methods while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Achieving true photorealism in dynamic game environments at real-time frame rates remains challenging due to tradeoffs between visual quality and performance. Current methods struggle with real-time inference.

Method: Proposes REGEN framework with dual-stage generative network that transforms unpaired image-to-image translation into simpler paired translation task. Uses lightweight method for real-time inference without compromising quality.

Result: Demonstrated on GTA V, achieves visual results comparable to robust unpaired methods while improving inference speed by 32.14 times. Outperforms directly trained lightweight unpaired translation methods.

Conclusion: REGEN successfully bridges the gap between visual quality and performance, enabling real-time photorealism enhancement in games with significant speed improvements while maintaining high visual fidelity.

Abstract: Photorealism is an important aspect of modern video games since it can shape
the player experience and simultaneously impact the immersion, narrative
engagement, and visual fidelity. Although recent hardware technological
breakthroughs, along with state-of-the-art rendering technologies, have
significantly improved the visual realism of video games, achieving true
photorealism in dynamic environments at real-time frame rates still remains a
major challenge due to the tradeoff between visual quality and performance. In
this short paper, we present a novel approach for enhancing the photorealism of
rendered game frames using generative adversarial networks. To this end, we
propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative
Network framework (REGEN), which employs a robust unpaired image-to-image
translation model to produce semantically consistent photorealistic frames that
transform the problem into a simpler paired image-to-image translation task.
This enables training with a lightweight method that can achieve real-time
inference time without compromising visual quality. We demonstrate the
effectiveness of our framework on Grand Theft Auto V, showing that the approach
achieves visual results comparable to the ones produced by the robust unpaired
Im2Im method while improving inference speed by 32.14 times. Our findings also
indicate that the results outperform the photorealism-enhanced frames produced
by directly training a lightweight unpaired Im2Im translation method to
translate the video game frames towards the visual characteristics of
real-world images. Code, pre-trained models, and demos for this work are
available at: https://github.com/stefanos50/REGEN.

</details>


### [54] [SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation](https://arxiv.org/abs/2508.17062)
*Peng Hu,Yu Gu,Liang Luo,Fuji Ren*

Main category: cs.CV

TL;DR: SSG-DiT is a novel framework that improves semantic consistency in controllable video generation by using spatial signal prompting and a dual-branch attention mechanism to better align generated videos with text and image inputs.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models struggle with maintaining semantic consistency and often deviate from the nuanced details specified in prompts, leading to videos that don't precisely match user conditions.

Method: A decoupled two-stage process: 1) Spatial Signal Prompting generates spatially aware visual prompts using pre-trained multi-modal models, 2) SSG-Adapter injects joint conditions (text + visual prompts) into a frozen video DiT backbone via dual-branch attention mechanism.

Result: State-of-the-art performance on VBench benchmark, outperforming existing models in multiple key metrics, particularly in spatial relationship control and overall consistency.

Conclusion: SSG-DiT effectively addresses semantic consistency challenges in controllable video generation through spatial signal guidance and parameter-efficient adaptation, achieving superior alignment with user-specified conditions.

Abstract: Controllable video generation aims to synthesize video content that aligns
precisely with user-provided conditions, such as text descriptions and initial
images. However, a significant challenge persists in this domain: existing
models often struggle to maintain strong semantic consistency, frequently
generating videos that deviate from the nuanced details specified in the
prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided
Diffusion Transformer), a novel and efficient framework for high-fidelity
controllable video generation. Our approach introduces a decoupled two-stage
process. The first stage, Spatial Signal Prompting, generates a spatially aware
visual prompt by leveraging the rich internal representations of a pre-trained
multi-modal model. This prompt, combined with the original text, forms a joint
condition that is then injected into a frozen video DiT backbone via our
lightweight and parameter-efficient SSG-Adapter. This unique design, featuring
a dual-branch attention mechanism, allows the model to simultaneously harness
its powerful generative priors while being precisely steered by external
spatial signals. Extensive experiments demonstrate that SSG-DiT achieves
state-of-the-art performance, outperforming existing models on multiple key
metrics in the VBench benchmark, particularly in spatial relationship control
and overall consistency.

</details>


### [55] [Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry](https://arxiv.org/abs/2508.17081)
*Haoyu Yun,Hamid Krim*

Main category: cs.CV

TL;DR: Proposes integrating Vision Transformer with proximal tools to enable global geometric optimization, overcoming ViT's limitation of only modeling local relationships within individual images.


<details>
  <summary>Details</summary>
Motivation: ViT's optimization is confined to modeling local relationships within images, limiting its ability to capture global geometric relationships between data points.

Method: Integrates ViT with proximal tools where ViT constructs tangent bundle through self-attention (each head = tangent space), then uses proximal iterations to define sections and project data from tangent spaces to base space for global feature alignment.

Result: Experimental results confirm the proposed method outperforms traditional ViT in classification accuracy and data distribution.

Conclusion: The framework successfully enhances ViT's feature representation and classification performance by enabling unified geometric optimization through proximal tools integration.

Abstract: The Vision Transformer (ViT) architecture has become widely recognized in
computer vision, leveraging its self-attention mechanism to achieve remarkable
success across various tasks. Despite its strengths, ViT's optimization remains
confined to modeling local relationships within individual images, limiting its
ability to capture the global geometric relationships between data points. To
address this limitation, this paper proposes a novel framework that integrates
ViT with the proximal tools, enabling a unified geometric optimization approach
to enhance feature representation and classification performance. In this
framework, ViT constructs the tangent bundle of the manifold through its
self-attention mechanism, where each attention head corresponds to a tangent
space, offering geometric representations from diverse local perspectives.
Proximal iterations are then introduced to define sections within the tangent
bundle and project data from tangent spaces onto the base space, achieving
global feature alignment and optimization. Experimental results confirm that
the proposed method outperforms traditional ViT in terms of classification
accuracy and data distribution.

</details>


### [56] [PD-Loss: Proxy-Decidability for Efficient Metric Learning](https://arxiv.org/abs/2508.17082)
*Pedro Silva,Guilherme A. L. Silva,Pablo Coelho,Vander Freitas,Gladston Moreira,David Menotii,Eduardo Luz*

Main category: cs.CV

TL;DR: PD-Loss combines proxy-based efficiency with D-Loss's distribution separability for scalable deep metric learning.


<details>
  <summary>Details</summary>
Motivation: Existing DML methods face trade-offs: pairwise losses have sampling/complexity issues, proxy-based methods lack global distribution optimization, and D-Loss has computational constraints from large mini-batches.

Method: Integrates learnable proxies with the decidability index (d') statistical framework to estimate genuine and impostor distributions through proxies for efficient embedding optimization.

Result: Achieves performance comparable to state-of-the-art methods in fine-grained classification and face verification tasks while being computationally efficient.

Conclusion: PD-Loss offers a scalable, distribution-aware approach to deep metric learning that combines computational efficiency with principled separability optimization, with potential for broader applications.

Abstract: Deep Metric Learning (DML) aims to learn embedding functions that map
semantically similar inputs to proximate points in a metric space while
separating dissimilar ones. Existing methods, such as pairwise losses, are
hindered by complex sampling requirements and slow convergence. In contrast,
proxy-based losses, despite their improved scalability, often fail to optimize
global distribution properties. The Decidability-based Loss (D-Loss) addresses
this by targeting the decidability index (d') to enhance distribution
separability, but its reliance on large mini-batches imposes significant
computational constraints. We introduce Proxy-Decidability Loss (PD-Loss), a
novel objective that integrates learnable proxies with the statistical
framework of d' to optimize embedding spaces efficiently. By estimating genuine
and impostor distributions through proxies, PD-Loss combines the computational
efficiency of proxy-based methods with the principled separability of D-Loss,
offering a scalable approach to distribution-aware DML. Experiments across
various tasks, including fine-grained classification and face verification,
demonstrate that PD-Loss achieves performance comparable to that of
state-of-the-art methods while introducing a new perspective on embedding
optimization, with potential for broader applications.

</details>


### [57] [GRASP: Geospatial pixel Reasoning viA Structured Policy learning](https://arxiv.org/abs/2508.17102)
*Chengjie Jiang,Yunqi Zhou,Jiafeng Yan,Jing Li*

Main category: cs.CV

TL;DR: GRASP is a reinforcement learning framework for geospatial pixel reasoning that uses MLLM-generated bounding boxes and points as prompts for segmentation, achieving state-of-the-art results without mask supervision.


<details>
  <summary>Details</summary>
Motivation: Existing MLLM-based systems require expensive dense pixel supervision and perform poorly on out-of-domain data. The authors aim to create a more efficient and generalizable approach.

Method: A multimodal language model generates bounding boxes and positive points from instructions, which are then used as prompts for a pre-trained segmentation model. The system is optimized purely with reinforcement learning (GRPO) using format and accuracy rewards on boxes/points.

Result: Achieves ~4% improvement in-domain and up to 54% improvement on out-of-domain benchmarks compared to previous state-of-the-art methods.

Conclusion: Complex geospatial segmentation can be effectively learned via reinforcement learning from weak spatial cues, demonstrating robust generalization without expensive mask supervision.

Abstract: Geospatial pixel reasoning is a nascent remote-sensing task that aims to
generate segmentation masks directly from natural-language instructions.
Prevailing MLLM-based systems co-train a language model and a mask decoder with
dense pixel supervision, which is expensive and often weak on out-of-domain
(OOD) data. We introduce GRASP, a structured policy-learning framework. In our
design, a multimodal large language model first emits task-relevant bounding
boxes and positive points from a vision-language instruction. These outputs are
then passed to a pre-trained segmentation model, which consumes them as prompts
to generate the final mask. Instead of supervised fine-tuning, we optimize the
system purely with reinforcement learning: the model is trained solely with
GRPO, guided by format rewards and accuracy rewards computed on boxes and
points (no mask supervision). This leverages strong priors in foundation
models, minimizes trainable parameters, and enables learning from inexpensive
annotations. We additionally curate GRASP-1k, which contains
reasoning-intensive queries, detailed reasoning traces, and fine-grained
segmentation annotations. Evaluations on both in-domain and out-of-domain test
sets show state-of-the-art results: about 4% improvement in-domain and up to
54% on OOD benchmarks. The experiment results evidence our model's robust
generalization and demonstrate that complex geospatial segmentation behaviors
can be learned via RL from weak spatial cues. Code and the dataset will be
released open-source.

</details>


### [58] [SugarcaneShuffleNet: A Very Fast, Lightweight Convolutional Neural Network for Diagnosis of 15 Sugarcane Leaf Diseases](https://arxiv.org/abs/2508.17107)
*Shifat E. Arman,Hasan Muhammad Abdullah,Syed Nazmus Sakib,RM Saiem,Shamima Nasrin Asha,Md Mehedi Hasan,Shahrear Bin Amin,S M Mahin Abrar*

Main category: cs.CV

TL;DR: SugarcaneLD-BD dataset and SugarcaneShuffleNet model provide efficient, lightweight solution for sugarcane disease diagnosis in low-resource regions with 98.02% accuracy and fast inference.


<details>
  <summary>Details</summary>
Motivation: AI plant diagnostic tools often fail in low-resource regions due to poor generalization, high computational requirements, and lack of interpretability, leaving sugarcane farmers vulnerable to leaf diseases.

Method: Created curated SugarcaneLD-BD dataset (638 images, 5 classes), combined with additional datasets. Developed SugarcaneShuffleNet lightweight model optimized for on-device use. Compared against 5 other CNNs with transfer learning and Bayesian optimization. Integrated into SugarcaneAI PWA with Grad-CAM explanations.

Result: SugarcaneShuffleNet achieved 98.02% accuracy, 0.98 F1-score, 4.14ms inference time per image, with only 9.26MB model size. Outperformed other models in efficiency while maintaining comparable accuracy to larger models like MnasNet and EdgeNeXt.

Conclusion: The solution provides a scalable, efficient, and interpretable tool for real-time sugarcane disease diagnosis in resource-constrained environments, offering the best trade-off between accuracy and computational efficiency for field deployment.

Abstract: Despite progress in AI-based plant diagnostics, sugarcane farmers in
low-resource regions remain vulnerable to leaf diseases due to the lack of
scalable, efficient, and interpretable tools. Many deep learning models fail to
generalize under real-world conditions and require substantial computational
resources, limiting their use in resource-constrained regions. In this paper,
we present SugarcaneLD-BD, a curated dataset for sugarcane leaf-disease
classification; SugarcaneShuffleNet, an optimized lightweight model for rapid
on-device diagnosis; and SugarcaneAI, a Progressive Web Application for field
deployment. SugarcaneLD-BD contains 638 curated images across five classes,
including four major sugarcane diseases, collected in Bangladesh under diverse
field conditions and verified by expert pathologists. To enhance diversity, we
combined SugarcaneLD-BD with two additional datasets, yielding a larger and
more representative corpus. Our optimized model, SugarcaneShuffleNet, offers
the best trade-off between speed and accuracy for real-time, on-device
diagnosis. This 9.26 MB model achieved 98.02% accuracy, an F1-score of 0.98,
and an average inference time of 4.14 ms per image. For comparison, we
fine-tuned five other lightweight convolutional neural networks: MnasNet,
EdgeNeXt, EfficientNet-Lite, MobileNet, and SqueezeNet via transfer learning
and Bayesian optimization. MnasNet and EdgeNeXt achieved comparable accuracy to
SugarcaneShuffleNet, but required significantly more parameters, memory, and
computation, limiting their suitability for low-resource deployment. We
integrate SugarcaneShuffleNet into SugarcaneAI, delivering Grad-CAM-based
explanations in the field. Together, these contributions offer a diverse
benchmark, efficient models for low-resource environments, and a practical tool
for sugarcane disease classification. It spans varied lighting, backgrounds and
devices used on-farm

</details>


### [59] [PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science](https://arxiv.org/abs/2508.17117)
*Syed Nazmus Sakib,Nafiul Haque,Mohammad Zabed Hossain,Shifat E. Arman*

Main category: cs.CV

TL;DR: PlantVillageVQA is a large-scale visual question answering dataset for agricultural applications, featuring 193,609 QA pairs across 55,448 images covering 14 crop species and 38 diseases, with expert-verified questions organized by cognitive complexity.


<details>
  <summary>Details</summary>
Motivation: To advance vision-language models for agricultural decision-making and provide a standardized, expert-verified database to improve plant disease identification accuracy.

Method: Created through a two-stage pipeline: (1) template-based QA synthesis from image metadata and (2) multi-stage linguistic re-engineering, followed by iterative expert review for scientific accuracy.

Result: A comprehensive dataset with 193,609 high-quality QA pairs organized into 3 cognitive complexity levels and 9 categories, evaluated using three state-of-the-art models.

Conclusion: The dataset provides a publicly available, standardized resource to enhance diagnostic accuracy for plant disease identification and advance agricultural research.

Abstract: PlantVillageVQA is a large-scale visual question answering (VQA) dataset
derived from the widely used PlantVillage image corpus. It was designed to
advance the development and evaluation of vision-language models for
agricultural decision-making and analysis. The PlantVillageVQA dataset
comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448
images spanning 14 crop species and 38 disease conditions. Questions are
organised into 3 levels of cognitive complexity and 9 distinct categories. Each
question category was phrased manually following expert guidance and generated
via an automated two-stage pipeline: (1) template-based QA synthesis from image
metadata and (2) multi-stage linguistic re-engineering. The dataset was
iteratively reviewed by domain experts for scientific accuracy and relevancy.
The final dataset was evaluated using three state-of-the-art models for quality
assessment. Our objective remains to provide a publicly available, standardised
and expert-verified database to enhance diagnostic accuracy for plant disease
identifications and advance scientific research in the agricultural domain. Our
dataset will be open-sourced at
https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.

</details>


### [60] [CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis](https://arxiv.org/abs/2508.17128)
*Mirza Mumtaz Zahoor,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: A novel hybrid deep learning framework called CE-RS-SBCIT that combines CNNs and Transformers for brain tumor classification from MRI data, achieving over 98% accuracy on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Brain tumors are lethal diseases requiring early detection and accurate classification. Existing deep learning approaches face challenges with computational cost, sensitivity to minor contrast variations, and structural/texture inconsistencies in MRI data.

Method: Hybrid framework integrating residual/spatial learning CNNs with transformer modules. Key innovations: SBCIT (smoothing and boundary-based CNN-integrated Transformer), tailored residual/spatial CNNs, channel enhancement strategy, and novel spatial attention mechanism.

Result: Achieved 98.30% accuracy, 98.08% sensitivity, 98.25% F1-score, and 98.43% precision on challenging MRI datasets from Kaggle and Figshare covering glioma, meningioma, pituitary tumors, and healthy controls.

Conclusion: The proposed CE-RS-SBCIT framework effectively addresses limitations of conventional CNNs and Transformers, demonstrating superior performance in brain tumor classification by leveraging both local fine-grained and global contextual features.

Abstract: Brain tumors remain among the most lethal human diseases, where early
detection and accurate classification are critical for effective diagnosis and
treatment planning. Although deep learning-based computer-aided diagnostic
(CADx) systems have shown remarkable progress. However, conventional
convolutional neural networks (CNNs) and Transformers face persistent
challenges, including high computational cost, sensitivity to minor contrast
variations, structural heterogeneity, and texture inconsistencies in MRI data.
Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integrating
residual and spatial learning-based CNNs with transformer-driven modules. The
proposed framework exploits local fine-grained and global contextual cues
through four core innovations: (i) a smoothing and boundary-based
CNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learning
CNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatial
attention mechanism. The developed SBCIT employs stem convolution and
contextual interaction transformer blocks with systematic smoothing and
boundary operations, enabling efficient global feature modeling. Moreover,
Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,
enrich the representation space, while the CE module amplifies discriminative
channels and mitigates redundancy. Furthermore, the spatial attention mechanism
selectively emphasizes subtle contrast and textural variations across tumor
classes. Extensive evaluation on challenging MRI datasets from Kaggle and
Figshare, encompassing glioma, meningioma, pituitary tumors, and healthy
controls, demonstrates superior performance, achieving 98.30% accuracy, 98.08%
sensitivity, 98.25% F1-score, and 98.43% precision.

</details>


### [61] [Structural Damage Detection Using AI Super Resolution and Visual Language Model](https://arxiv.org/abs/2508.17130)
*Catherine Hoier,Khandaker Mamun Ahmed*

Main category: cs.CV

TL;DR: Novel framework using drones, AI video enhancement (VRT), and visual language model (Gemma3:27b) for automated disaster damage assessment with 84.5% accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional damage assessment methods are labor-intensive, costly, and hazardous, making them impractical for rapid response in resource-limited disaster settings.

Method: Integrated system combining aerial drone footage, Video Restoration Transformer (VRT) for super-resolution, and Gemma3:27b VLM to improve low-resolution footage and classify building damage into four categories with risk levels.

Result: Achieved 84.5% classification accuracy using data from 2023 Turkey earthquakes and 2013 Moore Tornado, demonstrating highly accurate automated damage assessment.

Conclusion: The framework provides cost-effective, accessible disaster damage assessment that enables non-technical users to perform preliminary analyses, improving responsiveness and efficiency of disaster management.

Abstract: Natural disasters pose significant challenges to timely and accurate damage
assessment due to their sudden onset and the extensive areas they affect.
Traditional assessment methods are often labor-intensive, costly, and hazardous
to personnel, making them impractical for rapid response, especially in
resource-limited settings. This study proposes a novel, cost-effective
framework that leverages aerial drone footage, an advanced AI-based video
super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a
27 billion parameter Visual Language Model (VLM). This integrated system is
designed to improve low-resolution disaster footage, identify structural
damage, and classify buildings into four damage categories, ranging from
no/slight damage to total destruction, along with associated risk levels. The
methodology was validated using pre- and post-event drone imagery from the 2023
Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013
Moore Tornado (xBD dataset). The framework achieved a classification accuracy
of 84.5%, demonstrating its ability to provide highly accurate results.
Furthermore, the system's accessibility allows non-technical users to perform
preliminary analyses, thereby improving the responsiveness and efficiency of
disaster management efforts.

</details>


### [62] [Beyond Play and Pause: Turning GPT-4o Spatial Weakness into a Strength for In-Depth Interactive Video Learning](https://arxiv.org/abs/2508.17160)
*Sajad Goudarzi,Samaneh Zamanifard*

Main category: cs.CV

TL;DR: Untwist is an AI system that enables interactive video learning by allowing users to ask questions about specific video regions using bounding boxes, providing context-aware multimodal responses through GPT APIs and Computer Vision integration.


<details>
  <summary>Details</summary>
Motivation: Traditional video-based learning is passive and lacks real-time, region-specific interaction capabilities. Current AI tools only offer transcription and summarization without spatial interaction features.

Method: Integrates GPT APIs with Computer Vision techniques to extract, process, and structure video content. Uses annotated frames instead of raw coordinate data to address GPT-4o's spatial weakness, improving localization accuracy. Includes video pre-processing and real-time interaction architecture.

Result: The system significantly improves accuracy in localizing and interpreting video content, enabling context-aware multimodal responses to user queries about specific video regions.

Conclusion: Untwist transforms passive video consumption into an interactive, AI-driven learning experience that enhances user engagement and comprehension through region-specific interaction capabilities.

Abstract: Traditional video-based learning remains passive, offering limited
opportunities for users to engage dynamically with content. While current
AI-powered tools offer transcription and summarization, they lack real-time,
region-specific interaction capabilities. This paper introduces Untwist, an
AI-driven system that enables interactive video learning by allowing users to
ask questions about the entire video or specific regions using a bounding box,
receiving context-aware, multimodal responses. By integrating GPT APIs with
Computer Vision techniques, Untwist extracts, processes, and structures video
content to enhance comprehension. Our approach addresses GPT-4o spatial
weakness by leveraging annotated frames instead of raw coordinate data,
significantly improving accuracy in localizing and interpreting video content.
This paper describes the system architecture, including video pre-processing
and real-time interaction, and outlines how Untwist can transform passive video
consumption into an interactive, AI-driven learning experience with the
potential to enhance engagement and comprehension.

</details>


### [63] [Development of an isotropic segmentation model for medial temporal lobe subregions on anisotropic MRI atlas using implicit neural representation](https://arxiv.org/abs/2508.17171)
*Yue Li,Pulkit Khandelwal,Rohit Jena,Long Xie,Michael Duong,Amanda E. Denning,Christopher A. Brown,Laura E. M. Wisse,Sandhitsu R. Das,David A. Wolk,Paul A. Yushkevich*

Main category: cs.CV

TL;DR: Using implicit neural representation to combine T1 and T2 MRI advantages for isotropic upsampling of MTL subregion atlas, improving Alzheimer's biomarker accuracy without extra annotation work.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of medial temporal lobe subregions is crucial for Alzheimer's diagnosis, but anisotropic resolution in T2-weighted MRI makes cortical thickness extraction difficult.

Method: Used implicit neural representation to combine T1 and T2 MRI resolution advantages, upsampling MTL subregion atlas from anisotropic to isotropic space, then developed isotropic segmentation model.

Result: Isotropic model showed higher significance in distinguishing mild cognitive impairment from cognitively unimpaired participants, with greater biomarker stability in longitudinal analysis of CU participants.

Conclusion: Improved accuracy of AD imaging biomarkers without increasing annotation workload, enabling more precise quantification of AD-brain atrophy relationship and better disease tracking measures.

Abstract: Imaging biomarkers in magnetic resonance imaging (MRI) are important tools
for diagnosing and tracking Alzheimer's disease (AD). As medial temporal lobe
(MTL) is the earliest region to show AD-related hallmarks, brain atrophy caused
by AD can first be observed in the MTL. Accurate segmentation of MTL subregions
and extraction of imaging biomarkers from them are important. However, due to
imaging limitations, the resolution of T2-weighted (T2w) MRI is anisotropic,
which makes it difficult to accurately extract the thickness of cortical
subregions in the MTL. In this study, we used an implicit neural representation
method to combine the resolution advantages of T1-weighted and T2w MRI to
accurately upsample an MTL subregion atlas set from anisotropic space to
isotropic space, establishing a multi-modality, high-resolution atlas set.
Based on this atlas, we developed an isotropic MTL subregion segmentation
model. In an independent test set, the cortical subregion thickness extracted
using this isotropic model showed higher significance than an anisotropic
method in distinguishing between participants with mild cognitive impairment
and cognitively unimpaired (CU) participants. In longitudinal analysis, the
biomarkers extracted using isotropic method showed greater stability in CU
participants. This study improved the accuracy of AD imaging biomarkers without
increasing the amount of atlas annotation work, which may help to more
accurately quantify the relationship between AD and brain atrophy and provide
more accurate measures for disease tracking.

</details>


### [64] [VROOM - Visual Reconstruction over Onboard Multiview](https://arxiv.org/abs/2508.17172)
*Yajat Yadav,Varun Bharadwaj,Jathin Korrapati,Tanish Baranwal*

Main category: cs.CV

TL;DR: VROOM reconstructs 3D models of Formula 1 circuits using only onboard camera footage, addressing high-speed motion challenges through a pipeline combining DROID-SLAM, AnyCam, and Monst3r with preprocessing techniques.


<details>
  <summary>Details</summary>
Motivation: To enable scalable 4D reconstruction of racing environments using only onboard video footage from racecars, overcoming challenges of high-speed motion and camera frame cuts.

Method: Combines DROID-SLAM, AnyCam, and Monst3r methods with preprocessing techniques including masking, temporal chunking, and resolution scaling to handle dynamic motion and computational constraints.

Result: VROOM successfully partially recovers track and vehicle trajectories in complex Formula 1 environments, demonstrating feasibility of onboard video for 4D reconstruction.

Conclusion: Onboard video footage can be effectively used for scalable 4D reconstruction in real-world racing settings, opening possibilities for circuit modeling and trajectory analysis.

Abstract: We introduce VROOM, a system for reconstructing 3D models of Formula 1
circuits using only onboard camera footage from racecars. Leveraging video data
from the 2023 Monaco Grand Prix, we address video challenges such as high-speed
motion and sharp cuts in camera frames. Our pipeline analyzes different methods
such as DROID-SLAM, AnyCam, and Monst3r and combines preprocessing techniques
such as different methods of masking, temporal chunking, and resolution scaling
to account for dynamic motion and computational constraints. We show that Vroom
is able to partially recover track and vehicle trajectories in complex
environments. These findings indicate the feasibility of using onboard video
for scalable 4D reconstruction in real-world settings. The project page can be
found at https://varun-bharadwaj.github.io/vroom, and our code is available at
https://github.com/yajatyadav/vroom.

</details>


### [65] [Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting](https://arxiv.org/abs/2508.17186)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Cuiqun Chen,Zhuo Zheng,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: Proposes Adversarial Class Prompting (AdvCP) method to address background variation misclassification in weakly-supervised change detection using adversarial prompt mining and sample rectification.


<details>
  <summary>Details</summary>
Motivation: Weakly-supervised change detection methods often misclassify background variations as object changes due to limited image-level supervision, especially in complex remote-sensing scenarios.

Method: Two-phase approach: 1) Adversarial Prompt Mining - uses incorrect one-hot labels to activate erroneous feature mappings and identify background variations likely to be misclassified; 2) Adversarial Sample Rectification - integrates adversarial samples into training via online global prototype built from exponential moving average of current and historical data.

Result: Significant performance enhancements demonstrated on ConvNet, Transformer, and SAM-based baselines. Method shows generalizability to other multi-class weakly-supervised dense prediction scenarios without additional inference cost.

Conclusion: AdvCP effectively addresses co-occurring noise problem in WSCD by identifying and rectifying background variation misclassifications, providing a seamless integration into existing methods with improved performance.

Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object
changes (e.g., objects appearing or disappearing) from background variations
(e.g., environmental changes due to light, weather, or seasonal shifts) in
paired satellite images, relying only on paired image (i.e., image-level)
classification labels. This technique significantly reduces the need for dense
annotations required in fully-supervised change detection. However, as
image-level supervision only indicates whether objects have changed in a scene,
WSCD methods often misclassify background variations as object changes,
especially in complex remote-sensing scenarios. In this work, we propose an
Adversarial Class Prompting (AdvCP) method to address this co-occurring noise
problem, including two phases: a) Adversarial Prompt Mining: After each
training iteration, we introduce adversarial prompting perturbations, using
incorrect one-hot image-level labels to activate erroneous feature mappings.
This process reveals co-occurring adversarial samples under weak supervision,
namely background variation features that are likely to be misclassified as
object changes. b) Adversarial Sample Rectification: We integrate these
adversarially prompt-activated pixel samples into training by constructing an
online global prototype. This prototype is built from an exponentially weighted
moving average of the current batch and all historical training data. Our AdvCP
can be seamlessly integrated into current WSCD methods without adding
additional inference cost. Experiments on ConvNet, Transformer, and Segment
Anything Model (SAM)-based baselines demonstrate significant performance
enhancements. Furthermore, we demonstrate the generalizability of AdvCP to
other multi-class weakly-supervised dense prediction scenarios. Code is
available at https://github.com/zhenghuizhao/AdvCP

</details>


### [66] [MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling](https://arxiv.org/abs/2508.17199)
*Hyeyeon Kim,Sungwoo Han,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CV

TL;DR: A novel multimodal pseudo-labeling method for generating cover images and summaries from text documents, using joint ranking of images and captions to create high-quality datasets.


<details>
  <summary>Details</summary>
Motivation: No existing datasets are available for the cover image generation task that requires producing both summaries and corresponding images from text-only documents, necessitating a low-cost solution for dataset construction.

Method: Collect documents with multiple images and captions, exclude factually inconsistent instances, rank images and captions independently using gold summaries, select images where both image and caption rank first, and remove documents with direct image references in text.

Result: The multimodal pseudo-labeling method constructs more precise datasets and generates higher quality images compared to text-only and image-only pseudo-labeling methods that consider captions and images separately.

Conclusion: The proposed multimodal approach effectively creates high-quality datasets for cover image generation tasks by jointly considering both visual and textual information through ranking-based pseudo-labeling.

Abstract: In this study, we introduce a novel cover image generation task that produces
both a concise summary and a visually corresponding image from a given
text-only document. Because no existing datasets are available for this task,
we propose a multimodal pseudo-labeling method to construct high-quality
datasets at low cost. We first collect documents that contain multiple images
with their captions, and their summaries by excluding factually inconsistent
instances. Our approach selects one image from the multiple images accompanying
the documents. Using the gold summary, we independently rank both the images
and their captions. Then, we annotate a pseudo-label for an image when both the
image and its corresponding caption are ranked first in their respective
rankings. Finally, we remove documents that contain direct image references
within texts. Experimental results demonstrate that the proposed multimodal
pseudo-labeling method constructs more precise datasets and generates higher
quality images than text- and image-only pseudo-labeling methods, which
consider captions and images separately. We release our code at:
https://github.com/HyeyeeonKim/MMCIG

</details>


### [67] [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding](https://arxiv.org/abs/2508.17205)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: A multi-agent framework using mixture-of-experts strategy with large VLMs generating CoT prompts to guide smaller VLMs for highway scene understanding tasks including weather classification, pavement wetness assessment, and traffic congestion detection.


<details>
  <summary>Details</summary>
Motivation: To achieve comprehensive highway scene understanding with robust multi-task reasoning while balancing accuracy and computational efficiency, particularly for deployment in resource-constrained environments and high-risk locations.

Method: Uses a large VLM (e.g., GPT-4o) contextualized with domain knowledge to generate task-specific chain-of-thought prompts, which guide a smaller efficient VLM (e.g., Qwen2.5-VL-7B) for reasoning over short videos and multimodal data including road weather sensors.

Result: Demonstrates consistently strong performance across diverse traffic and environmental conditions, validated on three specialized datasets including a multimodal pavement wetness dataset combining video with sensor data.

Conclusion: The framework enables effective integration with existing traffic camera systems, enhances situational awareness through continuous monitoring of high-risk locations, and delivers timely alerts even in resource-constrained environments.

Abstract: This paper introduces a multi-agent framework for comprehensive highway scene
understanding, designed around a mixture-of-experts strategy. In this
framework, a large generic vision-language model (VLM), such as GPT-4o, is
contextualized with domain knowledge to generates task-specific
chain-of-thought (CoT) prompts. These fine-grained prompts are then used to
guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short
videos, along with complementary modalities as applicable. The framework
simultaneously addresses multiple critical perception tasks, including weather
classification, pavement wetness assessment, and traffic congestion detection,
achieving robust multi-task reasoning while balancing accuracy and
computational efficiency. To support empirical validation, we curated three
specialized datasets aligned with these tasks. Notably, the pavement wetness
dataset is multimodal, combining video streams with road weather sensor data,
highlighting the benefits of multimodal reasoning. Experimental results
demonstrate consistently strong performance across diverse traffic and
environmental conditions. From a deployment perspective, the framework can be
readily integrated with existing traffic camera systems and strategically
applied to high-risk rural locations, such as sharp curves, flood-prone
lowlands, or icy bridges. By continuously monitoring the targeted sites, the
system enhances situational awareness and delivers timely alerts, even in
resource-constrained environments.

</details>


### [68] [Multi-modal Knowledge Decomposition based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology](https://arxiv.org/abs/2508.17213)
*Qibin Zhang,Xinyu Hao,Qiao Chen,Rui Xu,Fengyu Cong,Cheng Lu,Hongming Xu*

Main category: cs.CV

TL;DR: Online distillation approach using Multi-modal Knowledge Decomposition to enhance IHC biomarker prediction from H&E histopathology images when multi-modal data is unavailable during inference.


<details>
  <summary>Details</summary>
Motivation: Simultaneous acquisition of multi-modal data (genomic and pathological) is often challenging due to cost or technical limitations, but IHC biomarker prediction benefits from multi-modal fusion analysis.

Method: Proposes MKD with two teacher models and one student model to extract modality-specific and modality-general features. Uses Similarity-preserving Knowledge Distillation (SKD) and Collaborative Learning for Online Distillation (CLOD) for mutual learning between models.

Result: Superior performance in IHC biomarker prediction using uni-modal data on TCGA-BRCA and in-house QHSU datasets.

Conclusion: The approach effectively leverages multi-modal data during training while enabling accurate inference with pathology slides alone, addressing practical limitations in multi-modal data acquisition.

Abstract: Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data
fusion analysis. However, the simultaneous acquisition of multi-modal data,
such as genomic and pathological information, is often challenging due to cost
or technical limitations. To address this challenge, we propose an online
distillation approach based on Multi-modal Knowledge Decomposition (MKD) to
enhance IHC biomarker prediction in haematoxylin and eosin (H\&E) stained
histopathology images. This method leverages paired genomic-pathology data
during training while enabling inference using either pathology slides alone or
both modalities. Two teacher and one student models are developed to extract
modality-specific and modality-general features by minimizing the MKD loss. To
maintain the internal structural relationships between samples,
Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally,
Collaborative Learning for Online Distillation (CLOD) facilitates mutual
learning between teacher and student models, encouraging diverse and
complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU
datasets demonstrate that our approach achieves superior performance in IHC
biomarker prediction using uni-modal data. Our code is available at
https://github.com/qiyuanzz/MICCAI2025_MKD.

</details>


### [69] [Deep Learning with Self-Attention and Enhanced Preprocessing for Precise Diagnosis of Acute Lymphoblastic Leukemia from Bone Marrow Smears in Hemato-Oncology](https://arxiv.org/abs/2508.17216)
*Md. Maruf,Md. Mahbubul Haque,Bishowjit Paul*

Main category: cs.CV

TL;DR: Deep learning framework using VGG19 with multi-head self-attention and Focal Loss achieves 99.25% accuracy for automated acute lymphoblastic leukemia diagnosis from bone marrow images.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of ALL is crucial for treatment, but conventional methods are complex, time-consuming, and error-prone. Need for automated, efficient diagnostic tools.

Method: Combines preprocessing pipeline with CNN (VGG19 backbone) enhanced with multi-head self-attention block to model long-range dependencies. Uses Focal Loss to address class imbalance.

Result: Enhanced VGG19+MHSA with Focal Loss achieves 99.25% accuracy, outperforming ResNet101 baseline (98.62%).

Conclusion: Attention-augmented CNNs with targeted loss optimization and preprocessing provide highly accurate and efficient automated ALL diagnosis, potentially accelerating clinical workflows.

Abstract: Acute lymphoblastic leukemia (ALL) is a prevalent hematological malignancy in
both pediatric and adult populations. Early and accurate detection with precise
subtyping is essential for guiding therapy. Conventional workflows are complex,
time-consuming, and prone to human error. We present a deep learning framework
for automated ALL diagnosis from bone marrow smear images. The method combines
a robust preprocessing pipeline with convolutional neural networks (CNNs) to
standardize image quality and improve inference efficiency. As a key design, we
insert a multi-head self-attention (MHSA) block into a VGG19 backbone to model
long-range dependencies and contextual relationships among cellular features.
To mitigate class imbalance, we train with Focal Loss. Across evaluated
architectures, the enhanced VGG19+MHSA trained with Focal Loss achieves 99.25%
accuracy, surpassing a strong ResNet101 baseline (98.62%). These results
indicate that attention-augmented CNNs, coupled with targeted loss optimization
and preprocessing, yield more discriminative representations of leukemic cell
morphology. Our approach offers a highly accurate and computationally efficient
tool for automated ALL recognition and subtyping, with potential to accelerate
diagnostic workflows and support reliable decision-making in clinical settings.

</details>


### [70] [4D Visual Pre-training for Robot Learning](https://arxiv.org/abs/2508.17230)
*Chengkai Hou,Yanjie Ze,Yankai Fu,Zeyu Gao,Songbo Hu,Yue Yu,Shanghang Zhang,Huazhe Xu*

Main category: cs.CV

TL;DR: FVP is a 4D visual pre-training framework that uses next-point-cloud-prediction with diffusion models to improve 3D representations for robotics, achieving 28% performance boost on manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing pre-trained visual representations are mostly 2D-based, neglecting the 3D nature of the world, and there's scarcity of large-scale 3D data for universal 3D representation learning.

Method: FVP frames visual pre-training as a next-point-cloud-prediction problem using diffusion models, pre-trained on large public datasets to enhance 3D representations.

Result: Across 12 real-world manipulation tasks, FVP boosts 3D Diffusion Policy's success rate by 28%, achieving state-of-the-art performance and working across various point cloud encoders and datasets.

Conclusion: FVP provides an effective alternative to direct 3D representation learning, significantly improving robotic manipulation performance and demonstrating broad applicability across different models and tasks.

Abstract: General visual representations learned from web-scale datasets for robotics
have achieved great success in recent years, enabling data-efficient robot
learning on manipulation tasks; yet these pre-trained representations are
mostly on 2D images, neglecting the inherent 3D nature of the world. However,
due to the scarcity of large-scale 3D data, it is still hard to extract a
universal 3D representation from web datasets. Instead, we are seeking a
general visual pre-training framework that could improve all 3D representations
as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training
framework for real-world robot learning. FVP frames the visual pre-training
objective as a next-point-cloud-prediction problem, models the prediction model
as a diffusion model, and pre-trains the model on the larger public datasets
directly. Across twelve real-world manipulation tasks, FVP boosts the average
success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP
pre-trained DP3 achieves state-of-the-art performance across imitation learning
methods. Moreover, the efficacy of FVP adapts across various point cloud
encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger
Vision-Language-Action robotic model, enhancing its performance on various
robot tasks. Our project page is available at: https://4d-
visual-pretraining.github.io/.

</details>


### [71] [PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation](https://arxiv.org/abs/2508.17239)
*Xiaoyang Hao,Han Li*

Main category: cs.CV

TL;DR: PersPose introduces Perspective Encoding to incorporate camera intrinsics and Perspective Rotation to center human subjects, achieving state-of-the-art 3D human pose estimation by reducing perspective distortions.


<details>
  <summary>Details</summary>
Motivation: Existing 3D human pose estimation methods use cropped images without camera intrinsics, making relative depth estimation inaccurate. Human subjects appearing away from image center cause perspective distortions that complicate model fitting.

Method: Proposes Perspective Encoding (PE) to encode camera intrinsics of cropped images and Perspective Rotation (PR) to center human subjects in original images, reducing perspective distortions. Combines both in PersPose framework.

Result: Achieves state-of-the-art performance on 3DPW, MPI-INF-3DHP, and Human3.6M datasets. On 3DPW, achieves MPJPE of 60.1 mm (7.54% lower than previous SOTA).

Conclusion: Incorporating camera intrinsics through PE and reducing perspective distortions through PR significantly improves monocular 3D human pose estimation accuracy, demonstrating the importance of considering perspective relationships in cropped images.

Abstract: Monocular 3D human pose estimation (HPE) methods estimate the 3D positions of
joints from individual images. Existing 3D HPE approaches often use the cropped
image alone as input for their models. However, the relative depths of joints
cannot be accurately estimated from cropped images without the corresponding
camera intrinsics, which determine the perspective relationship between 3D
objects and the cropped images. In this work, we introduce Perspective Encoding
(PE) to encode the camera intrinsics of the cropped images. Moreover, since the
human subject can appear anywhere within the original image, the perspective
relationship between the 3D scene and the cropped image differs significantly,
which complicates model fitting. Additionally, the further the human subject
deviates from the image center, the greater the perspective distortions in the
cropped image. To address these issues, we propose Perspective Rotation (PR), a
transformation applied to the original image that centers the human subject,
thereby reducing perspective distortions and alleviating the difficulty of
model fitting. By incorporating PE and PR, we propose a novel 3D HPE framework,
PersPose. Experimental results demonstrate that PersPose achieves
state-of-the-art (SOTA) performance on the 3DPW, MPIINF-3DHP, and Human3.6M
datasets. For example, on the in-the-wild dataset 3DPW, PersPose achieves an
MPJPE of 60.1 mm, 7.54% lower than the previous SOTA approach. Code is
available at: https://github.com/ KenAdamsJoseph/PersPose.

</details>


### [72] [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.17243)
*Zicong Tang,Ziyang Ma,Suqing Wang,Zuchao Li,Lefei Zhang,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.CV

TL;DR: CoViPAL is a layer-wise contextualized visual token pruning method that uses a lightweight Plug-and-Play Pruning Module to remove redundant vision tokens in LVLMs, improving inference efficiency without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large Vision-Language Models process thousands of vision tokens per image, leading to high computational costs and memory overhead during prefilling and decoding stages. Existing pruning methods struggle in shallow layers due to insufficient contextual information.

Method: Proposes CoViPAL with a Plug-and-Play Pruning Module (PPM) that predicts and removes redundant vision tokens before processing by LVLM. The PPM is lightweight, model-agnostic, and operates independently of LVLM architecture.

Result: Extensive experiments show CoViPAL outperforms training-free pruning methods under equal token budgets and surpasses training-based methods with comparable supervision. It improves inference efficiency without compromising accuracy.

Conclusion: CoViPAL provides a scalable and efficient solution for improving inference efficiency in LVLMs by effectively pruning redundant visual tokens while maintaining model performance across various benchmarks.

Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of
text tokens and vision tokens extracted from images or videos. Due to the rich
visual information, a single image can generate thousands of vision tokens,
leading to high computational costs during the prefilling stage and significant
memory overhead during decoding. Existing methods attempt to prune redundant
vision tokens, revealing substantial redundancy in visual representations.
However, these methods often struggle in shallow layers due to the lack of
sufficient contextual information. We argue that many visual tokens are
inherently redundant even in shallow layers and can be safely and effectively
pruned with appropriate contextual signals. In this work, we propose CoViPAL, a
layer-wise contextualized visual token pruning method that employs a
Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision
tokens before they are processed by the LVLM. The PPM is lightweight,
model-agnostic, and operates independently of the LVLM architecture, ensuring
seamless integration with various models. Extensive experiments on multiple
benchmarks demonstrate that CoViPAL outperforms training-free pruning methods
under equal token budgets and surpasses training-based methods with comparable
supervision. CoViPAL offers a scalable and efficient solution to improve
inference efficiency in LVLMs without compromising accuracy.

</details>


### [73] [Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics](https://arxiv.org/abs/2508.17247)
*Lixin Jia,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Dan Ma,Gaobo Yang*

Main category: cs.CV

TL;DR: Proposes Adversarial Interference Simulation (AIS) to protect deepfake forensic watermarks against Multi-Embedding Attacks where additional watermarking destroys original forensic traces.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake proactive forensic methods assume single watermark embedding, but real-world scenarios involve multiple embedding rounds that destroy original forensic watermarks, rendering current defenses ineffective.

Method: Adversarial Interference Simulation (AIS) training paradigm that simulates multi-embedding attacks during fine-tuning with resilience-driven loss to learn sparse and stable watermark representations without modifying network architecture.

Result: Extensive experiments show AIS significantly enhances robustness of various existing methods against Multi-Embedding Attacks, maintaining original watermark extraction ability even after second embedding.

Conclusion: AIS provides a plug-and-play solution to address the critical vulnerability of multi-embedding attacks in deepfake proactive forensics, making watermark-based source tracking reliable in real-world scenarios.

Abstract: With the rapid evolution of deepfake technologies and the wide dissemination
of digital media, personal privacy is facing increasingly serious security
threats. Deepfake proactive forensics, which involves embedding imperceptible
watermarks to enable reliable source tracking, serves as a crucial defense
against these threats. Although existing methods show strong forensic ability,
they rely on an idealized assumption of single watermark embedding, which
proves impractical in real-world scenarios. In this paper, we formally define
and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first
time. When a previously protected image undergoes additional rounds of
watermark embedding, the original forensic watermark can be destroyed or
removed, rendering the entire proactive forensic mechanism ineffective. To
address this vulnerability, we propose a general training paradigm named
Adversarial Interference Simulation (AIS). Rather than modifying the network
architecture, AIS explicitly simulates MEA scenarios during fine-tuning and
introduces a resilience-driven loss function to enforce the learning of sparse
and stable watermark representations. Our method enables the model to maintain
the ability to extract the original watermark correctly even after a second
embedding. Extensive experiments demonstrate that our plug-and-play AIS
training paradigm significantly enhances the robustness of various existing
methods against MEA.

</details>


### [74] [A biological vision inspired framework for machine perception of abutting grating illusory contours](https://arxiv.org/abs/2508.17254)
*Xiao Zhang,Kai-Fu Yang,Xian-Shi Zhang,Hong-Zhi You,Hong-Mei Yan,Yong-Jie Li*

Main category: cs.CV

TL;DR: ICPNet is a novel deep network that improves machine perception of illusory contours to better align with human visual cognition, achieving state-of-the-art performance on abutting grating illusion tasks.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks fail to perceive illusory contours like humans do, creating a misalignment between machine intelligence and human perception patterns that needs to be addressed.

Method: Proposes ICPNet with three key modules: multi-scale feature projection (MFP) for multi-scale representations, feature interaction attention module (FIAM) for feedforward-feedback interaction, and edge fusion module (EFM) with shape constraints inspired by human shape bias.

Result: ICPNet shows significantly higher sensitivity to abutting grating illusory contours than state-of-the-art models, with notable top-1 accuracy improvements across various test subsets on AG-MNIST and AG-Fashion-MNIST datasets.

Conclusion: This work represents a step toward human-level intelligence for DNN-based models by better aligning machine perception with human visual cognition of illusory contours.

Abstract: Higher levels of machine intelligence demand alignment with human perception
and cognition. Deep neural networks (DNN) dominated machine intelligence have
demonstrated exceptional performance across various real-world tasks.
Nevertheless, recent evidence suggests that DNNs fail to perceive illusory
contours like the abutting grating, a discrepancy that misaligns with human
perception patterns. Departing from previous works, we propose a novel deep
network called illusory contour perception network (ICPNet) inspired by the
circuits of the visual cortex. In ICPNet, a multi-scale feature projection
(MFP) module is designed to extract multi-scale representations. To boost the
interaction between feedforward and feedback features, a feature interaction
attention module (FIAM) is introduced. Moreover, drawing inspiration from the
shape bias observed in human perception, an edge detection task conducted via
the edge fusion module (EFM) injects shape constraints that guide the network
to concentrate on the foreground. We assess our method on the existing AG-MNIST
test set and the AG-Fashion-MNIST test sets constructed by this work.
Comprehensive experimental results reveal that ICPNet is significantly more
sensitive to abutting grating illusory contours than state-of-the-art models,
with notable improvements in top-1 accuracy across various subsets. This work
is expected to make a step towards human-level intelligence for DNN-based
models.

</details>


### [75] [SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality](https://arxiv.org/abs/2508.17255)
*Yuzhi Lai,Shenghai Yuan,Peizheng Li,Jun Lou,Andreas Zell*

Main category: cs.CV

TL;DR: SEER-VAR is a novel egocentric vehicle AR framework that dynamically separates cabin and road scenes using depth-guided vision-language grounding, employs dual SLAM branches for context tracking, and uses GPT-based recommendations for context-aware AR overlays.


<details>
  <summary>Details</summary>
Motivation: Existing AR systems assume static or single-view settings, which don't address the dynamic nature of driving environments where cabin and road contexts need separate handling for effective AR experiences.

Method: Uses depth-guided vision-language grounding for semantic scene decomposition, two Context-Aware SLAM Branches (CASB) for tracking egocentric motion in cabin and road contexts, and GPT-based module for generating context-aware AR overlays like dashboard cues and hazard alerts.

Result: Achieves robust spatial alignment and perceptually coherent AR rendering across varied environments. User studies show enhanced scene understanding, overlay relevance, and driver ease. Introduces EgoSLAM-Drive dataset for evaluation.

Conclusion: SEER-VAR provides an effective foundation for future research in LLM-based AR recommendation for egocentric driving, addressing the lack of comparable systems through novel structured prompting and comprehensive evaluation.

Abstract: We present SEER-VAR, a novel framework for egocentric vehicle-based augmented
reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches
(CASB), and LLM-driven recommendation. Unlike existing systems that assume
static or single-view settings, SEER-VAR dynamically separates cabin and road
scenes via depth-guided vision-language grounding. Two SLAM branches track
egocentric motion in each context, while a GPT-based module generates
context-aware overlays such as dashboard cues and hazard alerts. To support
evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring
synchronized egocentric views, 6DoF ground-truth poses, and AR annotations
across diverse driving scenarios. Experiments demonstrate that SEER-VAR
achieves robust spatial alignment and perceptually coherent AR rendering across
varied environments. As one of the first to explore LLM-based AR recommendation
in egocentric driving, we address the lack of comparable systems through
structured prompting and detailed user studies. Results show that SEER-VAR
enhances perceived scene understanding, overlay relevance, and driver ease,
providing an effective foundation for future research in this direction. Code
and dataset will be made open source.

</details>


### [76] [ResLink: A Novel Deep Learning Architecture for Brain Tumor Classification with Area Attention and Residual Connections](https://arxiv.org/abs/2508.17259)
*Sumedha Arya,Nirmal Gaud*

Main category: cs.CV

TL;DR: ResLink - a novel deep learning architecture with area attention mechanisms and residual connections for brain tumor classification from CT scans, achieving 95% accuracy.


<details>
  <summary>Details</summary>
Motivation: Brain tumors pose significant health challenges and early accurate diagnosis is crucial for effective treatment, requiring advanced classification methods for medical imaging.

Method: ResLink integrates area attention mechanisms with residual connections in a multi-stage convolutional pipeline with dropout, regularization, downsampling, and attention-based refinement for classification.

Result: Achieves 95% accuracy on a balanced dataset and demonstrates strong generalizability in brain tumor classification tasks.

Conclusion: ResLink shows potential for improving brain tumor classification and offers a robust, efficient technique for medical imaging applications.

Abstract: Brain tumors show significant health challenges due to their potential to
cause critical neurological functions. Early and accurate diagnosis is crucial
for effective treatment. In this research, we propose ResLink, a novel deep
learning architecture for brain tumor classification using CT scan images.
ResLink integrates novel area attention mechanisms with residual connections to
enhance feature learning and spatial understanding for spatially rich image
classification tasks. The model employs a multi-stage convolutional pipeline,
incorporating dropout, regularization, and downsampling, followed by a final
attention-based refinement for classification. Trained on a balanced dataset,
ResLink achieves a high accuracy of 95% and demonstrates strong
generalizability. This research demonstrates the potential of ResLink in
improving brain tumor classification, offering a robust and efficient technique
for medical imaging applications.

</details>


### [77] [CLIFF: Continual Learning for Incremental Flake Features in 2D Material Identification](https://arxiv.org/abs/2508.17261)
*Sankalp Pandey,Xuan Bac Nguyen,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: CLIFF is a continual learning framework for automated classification of 2D material flakes that addresses appearance shifts across different materials by freezing a backbone model and learning material-specific components for each new material.


<details>
  <summary>Details</summary>
Motivation: Automated layer classification from optical microscopy is challenging due to substantial appearance shifts across different 2D materials, making quantum flake identification difficult for scalable quantum hardware.

Method: Freezes backbone and base head trained on reference material, learns material-specific prompts, embeddings, and delta heads for each new material. Uses prompt pool and cosine-similarity gate to modulate features, plus memory replay with knowledge distillation.

Result: Achieves competitive accuracy with significantly lower forgetting than naive fine-tuning and prompt-based baselines.

Conclusion: First systematic study of continual learning for 2D materials; CLIFF framework effectively handles material appearance shifts while minimizing catastrophic forgetting.

Abstract: Identifying quantum flakes is crucial for scalable quantum hardware; however,
automated layer classification from optical microscopy remains challenging due
to substantial appearance shifts across different materials. In this paper, we
propose a new Continual-Learning Framework for Flake Layer Classification
(CLIFF). To our knowledge, this is the first systematic study of continual
learning in the domain of two-dimensional (2D) materials. Our method enables
the model to differentiate between materials and their physical and optical
properties by freezing a backbone and base head trained on a reference
material. For each new material, it learns a material-specific prompt,
embedding, and a delta head. A prompt pool and a cosine-similarity gate
modulate features and compute material-specific corrections. Additionally, we
incorporate memory replay with knowledge distillation. CLIFF achieves
competitive accuracy with significantly lower forgetting than naive fine-tuning
and a prompt-based baseline.

</details>


### [78] [AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks](https://arxiv.org/abs/2508.17265)
*Zhenyu Liu,Huizhi Liang,Xinrun Li,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: AdaGAT is a novel adversarial distillation method that dynamically adjusts a learnable guide model's training state to enhance student model robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial distillation methods struggle to maintain optimal guide model states during co-training, limiting effective knowledge transfer from teacher to student models.

Method: Proposes Adaptive Guidance Adversarial Training (AdaGAT) with two separate loss functions that allow the guide model to actively participate in backpropagation to achieve optimal state for robustness transfer.

Result: Extensive experiments on CIFAR-10, CIFAR-100, and TinyImageNet using WideResNet-34-10 show enhanced robustness across various adversarial attacks compared to baseline models.

Conclusion: Appropriately adjusting the guide model within a certain accuracy range significantly improves target model robustness, demonstrating AdaGAT's effectiveness in adversarial distillation.

Abstract: Adversarial distillation (AD) is a knowledge distillation technique that
facilitates the transfer of robustness from teacher deep neural network (DNN)
models to lightweight target (student) DNN models, enabling the target models
to perform better than only training the student model independently. Some
previous works focus on using a small, learnable teacher (guide) model to
improve the robustness of a student model. Since a learnable guide model starts
learning from scratch, maintaining its optimal state for effective knowledge
transfer during co-training is challenging. Therefore, we propose a novel
Adaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT,
dynamically adjusts the training state of the guide model to install robustness
to the target model. Specifically, we develop two separate loss functions as
part of the AdaGAT method, allowing the guide model to participate more
actively in backpropagation to achieve its optimal state. We evaluated our
approach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and
TinyImageNet, using the WideResNet-34-10 model as the target model. Our
observations reveal that appropriately adjusting the guide model within a
certain accuracy range enhances the target model's robustness across various
adversarial attacks compared to a variety of baseline models.

</details>


### [79] [Spatial-Temporal Human-Object Interaction Detection](https://arxiv.org/abs/2508.17270)
*Xu Sun,Yunqing He,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: Proposes ST-HOID for video human-object interaction detection with trajectory tracking, introduces new VidOR-HOID dataset, and presents a novel method outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Human-object interaction (HOI) is crucial for human-centric video content understanding, requiring fine-grained detection of interactions and object trajectories in videos.

Method: Novel approach with object trajectory detection module and interaction reasoning module, evaluated on newly constructed VidOR-HOID dataset containing 10,831 spatial-temporal HOI instances.

Result: Experimental results demonstrate the method outperforms baselines generated by state-of-the-art methods from image HOI detection, video visual relation detection, and video HOI recognition.

Conclusion: The proposed ST-HOID framework effectively addresses video-based human-object interaction detection with trajectory tracking, showing superior performance compared to existing methods across multiple domains.

Abstract: In this paper, we propose a new instance-level human-object interaction
detection task on videos called ST-HOID, which aims to distinguish fine-grained
human-object interactions (HOIs) and the trajectories of subjects and objects.
It is motivated by the fact that HOI is crucial for human-centric video content
understanding. To solve ST-HOID, we propose a novel method consisting of an
object trajectory detection module and an interaction reasoning module.
Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID
evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct
extensive experiments to evaluate the effectiveness of our method. The
experimental results demonstrate that our method outperforms the baselines
generated by the state-of-the-art methods of image human-object interaction
detection, video visual relation detection and video human-object interaction
recognition.

</details>


### [80] [Deep Learning-Assisted Detection of Sarcopenia in Cross-Sectional Computed Tomography Imaging](https://arxiv.org/abs/2508.17275)
*Manish Bhardwaj,Huizhi Liang,Ashwin Sivaharan,Sandip Nandhra,Vaclav Snasel,Tamer El-Sayed,Varun Ojha*

Main category: cs.CV

TL;DR: Deep learning models using transfer learning and self-supervised learning can automate sarcopenia assessment from CT scans with high accuracy (93% dice similarity, ±3% error), overcoming manual measurement limitations.


<details>
  <summary>Details</summary>
Motivation: Sarcopenia assessment through manual skeletal muscle area measurement in CT scans is time-consuming and increases clinical workload, limiting timely detection and management of this condition linked to poor surgical outcomes.

Method: Developed deep-learning models using transfer learning and self-supervised learning approaches on labeled and unlabeled CT scan datasets from Freeman Hospital. Models were trained to measure skeletal muscle area at the third lumbar vertebra and generate segmentation masks.

Result: Model predicted skeletal muscle area with average error of ±3 percentage points against manual measurements. Achieved 93% dice similarity coefficient for predicted segmentation masks, demonstrating high accuracy.

Conclusion: The approach provides a pathway to full automation of sarcopenia assessment and detection, addressing class imbalance and limited data issues while offering precise quantitative assessment superior to qualitative detection methods.

Abstract: Sarcopenia is a progressive loss of muscle mass and function linked to poor
surgical outcomes such as prolonged hospital stays, impaired mobility, and
increased mortality. Although it can be assessed through cross-sectional
imaging by measuring skeletal muscle area (SMA), the process is time-consuming
and adds to clinical workloads, limiting timely detection and management;
however, this process could become more efficient and scalable with the
assistance of artificial intelligence applications. This paper presents
high-quality three-dimensional cross-sectional computed tomography (CT) images
of patients with sarcopenia collected at the Freeman Hospital, Newcastle upon
Tyne Hospitals NHS Foundation Trust. Expert clinicians manually annotated the
SMA at the third lumbar vertebra, generating precise segmentation masks. We
develop deep-learning models to measure SMA in CT images and automate this
task. Our methodology employed transfer learning and self-supervised learning
approaches using labelled and unlabeled CT scan datasets. While we developed
qualitative assessment models for detecting sarcopenia, we observed that the
quantitative assessment of SMA is more precise and informative. This approach
also mitigates the issue of class imbalance and limited data availability. Our
model predicted the SMA, on average, with an error of +-3 percentage points
against the manually measured SMA. The average dice similarity coefficient of
the predicted masks was 93%. Our results, therefore, show a pathway to full
automation of sarcopenia assessment and detection.

</details>


### [81] [MTNet: Learning modality-aware representation with transformer for RGBT tracking](https://arxiv.org/abs/2508.17280)
*Ruichao Hou,Boyue Xu,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: MTNet is a transformer-based RGBT tracker with modality-aware feature extraction and transformer fusion that achieves state-of-the-art performance with real-time speed.


<details>
  <summary>Details</summary>
Motivation: Regular fusion paradigms and fixed tracking templates limit feature interaction in RGBT tracking, requiring better modality-specific cue exploration and global dependency capture.

Method: Proposes modality-aware network with channel aggregation/distribution module and spatial similarity perception module, transformer fusion network for global dependencies, trident prediction head for precise localization, and dynamic update strategy for template maintenance.

Result: Achieves satisfactory results compared to state-of-the-art competitors on three RGBT benchmarks while maintaining real-time speed.

Conclusion: The proposed MTNet framework effectively addresses modality-specific feature interaction challenges and demonstrates superior performance in RGBT tracking tasks.

Abstract: The ability to learn robust multi-modality representation has played a
critical role in the development of RGBT tracking. However, the regular fusion
paradigm and the invariable tracking template remain restrictive to the feature
interaction. In this paper, we propose a modality-aware tracker based on
transformer, termed MTNet. Specifically, a modality-aware network is presented
to explore modality-specific cues, which contains both channel aggregation and
distribution module(CADM) and spatial similarity perception module (SSPM). A
transformer fusion network is then applied to capture global dependencies to
reinforce instance representations. To estimate the precise location and tackle
the challenges, such as scale variation and deformation, we design a trident
prediction head and a dynamic update strategy which jointly maintain a reliable
template for facilitating inter-frame communication. Extensive experiments
validate that the proposed method achieves satisfactory results compared with
the state-of-the-art competitors on three RGBT benchmarks while reaching
real-time speed.

</details>


### [82] [Quickly Tuning Foundation Models for Image Segmentation](https://arxiv.org/abs/2508.17283)
*Breenda Das,Lennart Purucker,Timur Carstensen,Frank Hutter*

Main category: cs.CV

TL;DR: QTT-SEG automates SAM fine-tuning using meta-learning to predict optimal configurations, achieving better performance than zero-shot SAM and AutoGluon in minutes.


<details>
  <summary>Details</summary>
Motivation: Foundation models like SAM have strong zero-shot segmentation but underperform on domain-specific tasks, and manual fine-tuning requires significant expertise and effort.

Method: Built on Quick-Tune hyperparameter optimization framework, uses meta-learned cost and performance models to efficiently search over 200 million configurations.

Result: Consistently improves SAM's zero-shot performance, surpasses AutoGluon Multimodal on most binary tasks within 3 minutes, and delivers gains on multiclass datasets.

Conclusion: Meta-learning shows promise for automating model adaptation to specialized segmentation tasks, making fine-tuning more efficient and accessible.

Abstract: Foundation models like SAM (Segment Anything Model) exhibit strong zero-shot
image segmentation performance, but often fall short on domain-specific tasks.
Fine-tuning these models typically requires significant manual effort and
domain expertise. In this work, we introduce QTT-SEG, a meta-learning-driven
approach for automating and accelerating the fine-tuning of SAM for image
segmentation. Built on the Quick-Tune hyperparameter optimization framework,
QTT-SEG predicts high-performing configurations using meta-learned cost and
performance models, efficiently navigating a search space of over 200 million
possibilities. We evaluate QTT-SEG on eight binary and five multiclass
segmentation datasets under tight time constraints. Our results show that
QTT-SEG consistently improves upon SAM's zero-shot performance and surpasses
AutoGluon Multimodal, a strong AutoML baseline, on most binary tasks within
three minutes. On multiclass datasets, QTT-SEG delivers consistent gains as
well. These findings highlight the promise of meta-learning in automating model
adaptation for specialized segmentation tasks. Code available at:
https://github.com/ds-brx/QTT-SEG/

</details>


### [83] [Explain Before You Answer: A Survey on Compositional Visual Reasoning](https://arxiv.org/abs/2508.17298)
*Fucai Ke,Joy Hsu,Zhixi Cai,Zixian Ma,Xin Zheng,Xindi Wu,Sukai Huang,Weiqing Wang,Pari Delir Haghighi,Gholamreza Haffari,Ranjay Krishna,Jiajun Wu,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: A comprehensive survey of compositional visual reasoning literature from 2023-2025, covering 260+ papers, 60+ benchmarks, and tracing the paradigm shift from prompt-enhanced pipelines to unified agentic VLMs.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in dedicated synthesis of compositional visual reasoning literature and provide a systematic review of this rapidly expanding field, as existing surveys focus on monolithic vision-language models or general multimodal reasoning.

Method: Systematic review of 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL) with formalization of core definitions, analysis of five-stage paradigm shift, and cataloging of 60+ benchmarks and metrics.

Result: The survey provides a unified taxonomy, historical roadmap of paradigm shifts (prompt-enhanced pipelines → tool-enhanced LLMs → tool-enhanced VLMs → chain-of-thought reasoning → unified agentic VLMs), and critical analysis of strengths/limitations.

Conclusion: Identifies key insights and open challenges (LLM reasoning limitations, hallucination, deductive reasoning bias, scalable supervision, tool integration, benchmark limitations) and outlines future directions including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols.

Abstract: Compositional visual reasoning has emerged as a key research frontier in
multimodal AI, aiming to endow machines with the human-like ability to
decompose visual scenes, ground intermediate concepts, and perform multi-step
logical inference. While early surveys focus on monolithic vision-language
models or general multimodal reasoning, a dedicated synthesis of the rapidly
expanding compositional visual reasoning literature is still missing. We fill
this gap with a comprehensive survey spanning 2023 to 2025 that systematically
reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We
first formalize core definitions and describe why compositional approaches
offer advantages in cognitive alignment, semantic fidelity, robustness,
interpretability, and data efficiency. Next, we trace a five-stage paradigm
shift: from prompt-enhanced language-centric pipelines, through tool-enhanced
LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and
unified agentic VLMs, highlighting their architectural designs, strengths, and
limitations. We then catalog 60+ benchmarks and corresponding metrics that
probe compositional visual reasoning along dimensions such as grounding
accuracy, chain-of-thought faithfulness, and high-resolution perception.
Drawing on these analyses, we distill key insights, identify open challenges
(e.g., limitations of LLM-based reasoning, hallucination, a bias toward
deductive reasoning, scalable supervision, tool integration, and benchmark
limitations), and outline future directions, including world-model integration,
human-AI collaborative reasoning, and richer evaluation protocols. By offering
a unified taxonomy, historical roadmap, and critical outlook, this survey aims
to serve as a foundational reference and inspire the next generation of
compositional visual reasoning research.

</details>


### [84] [FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising](https://arxiv.org/abs/2508.17299)
*Zhihao Chen,Qi Gao,Zilong Li,Junping Zhang,Yi Zhang,Jun Zhao,Hongming Shan*

Main category: cs.CV

TL;DR: FoundDiff is a foundational diffusion model that provides unified low-dose CT denoising across various dose levels and anatomical regions using a two-stage approach with dose-anatomy perception and adaptive denoising.


<details>
  <summary>Details</summary>
Motivation: Existing DL-based CT denoising methods struggle with diverse noise characteristics and anatomical heterogeneity across different scanning conditions, limiting their clinical generalizability and robustness.

Method: Two-stage strategy: (1) Dose-anatomy perception using DA-CLIP with contrastive learning to quantify dose variations and identify anatomical regions, (2) Adaptive denoising using DA-Diff that integrates learned embeddings via novel dose and anatomy conditional blocks based on Mamba architecture.

Result: Superior denoising performance over state-of-the-art methods on two public LDCT datasets with eight dose levels and three anatomical regions, with remarkable generalization to unseen dose levels.

Conclusion: FoundDiff provides a unified and generalizable solution for low-dose CT denoising that effectively handles diverse clinical scenarios across varying dose levels and anatomical regions.

Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation
exposure while ensuring diagnostically acceptable image quality. Despite
significant advancements driven by deep learning (DL) in recent years, existing
DL-based methods, typically trained on a specific dose level and anatomical
region, struggle to handle diverse noise characteristics and anatomical
heterogeneity during varied scanning conditions, limiting their
generalizability and robustness in clinical scenarios. In this paper, we
propose FoundDiff, a foundational diffusion model for unified and generalizable
LDCT denoising across various dose levels and anatomical regions. FoundDiff
employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive
denoising. First, we develop a dose- and anatomy-aware contrastive language
image pre-training model (DA-CLIP) to achieve robust dose and anatomy
perception by leveraging specialized contrastive learning strategies to learn
continuous representations that quantify ordinal dose variations and identify
salient anatomical regions. Second, we design a dose- and anatomy-aware
diffusion model (DA-Diff) to perform adaptive and generalizable denoising by
synergistically integrating the learned dose and anatomy embeddings from DACLIP
into diffusion process via a novel dose and anatomy conditional block (DACB)
based on Mamba. Extensive experiments on two public LDCT datasets encompassing
eight dose levels and three anatomical regions demonstrate superior denoising
performance of FoundDiff over existing state-of-the-art methods and the
remarkable generalization to unseen dose levels. The codes and models are
available at https://github.com/hao1635/FoundDiff.

</details>


### [85] [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing](https://arxiv.org/abs/2508.17302)
*Peilin Xiong,Junwen Chen,Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: PosBridge is a training-free framework for localized subject-driven image editing that uses positional embedding transplant and Corner Centered Layout to seamlessly integrate custom objects into target scenes with high structural consistency and appearance fidelity.


<details>
  <summary>Details</summary>
Motivation: As generative models scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks for object insertion tasks.

Method: Uses positional embedding transplant to guide diffusion models to replicate structural characteristics of reference objects, combined with Corner Centered Layout that concatenates reference and background images as input to FLUX.1-Fill model during progressive denoising.

Result: Extensive experiments show PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency.

Conclusion: PosBridge demonstrates practical value and potential for broad adoption as an efficient and flexible framework for inserting custom objects into images without requiring training.

Abstract: Localized subject-driven image editing aims to seamlessly integrate
user-specified objects into target scenes. As generative models continue to
scale, training becomes increasingly costly in terms of memory and computation,
highlighting the need for training-free and scalable editing frameworks.To this
end, we propose PosBridge an efficient and flexible framework for inserting
custom objects. A key component of our method is positional embedding
transplant, which guides the diffusion model to faithfully replicate the
structural characteristics of reference objects.Meanwhile, we introduce the
Corner Centered Layout, which concatenates reference images and the background
image as input to the FLUX.1-Fill model. During progressive denoising,
positional embedding transplant is applied to guide the noise distribution in
the target region toward that of the reference object. In this way, Corner
Centered Layout effectively directs the FLUX.1-Fill model to synthesize
identity-consistent content at the desired location. Extensive experiments
demonstrate that PosBridge outperforms mainstream baselines in structural
consistency, appearance fidelity, and computational efficiency, showcasing its
practical value and potential for broad adoption.

</details>


### [86] [First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in the Detail and Minority](https://arxiv.org/abs/2508.17305)
*Songliang Cao,Tianqi Hu,Hao Lu*

Main category: cs.CV

TL;DR: Winning solution for MLCAS 2025 wheat segmentation challenge that focuses on improving stem segmentation through dynamic upsampling, semi-supervised distillation, and test-time scaling.


<details>
  <summary>Details</summary>
Motivation: While baseline models already perform well on wheat segmentation, stems present unique challenges due to fine structure, few pixels, and class imbalance, requiring specialized approaches to achieve competitive advantage.

Method: Three key improvements: 1) SAPA dynamic upsampler for better detail delineation, 2) semi-supervised guided distillation with stem-aware sample selection from unlabeled data, 3) test-time scaling strategy that zooms in and segments images twice.

Result: Achieved first place in the competition, outperforming second place by clear margins.

Conclusion: Focusing on the specific problem nature (stem segmentation challenges) rather than general segmentation tricks was the key to success, demonstrating that domain-specific adaptations can provide significant competitive advantages even with strong baseline models.

Abstract: In this report, we present our solution during the participation of the MLCAS
2025 GWFSS Challenge. This challenge hosts a semantic segmentation competition
specific to wheat plants, which requires to segment three wheat organs
including the head, leaf, and stem, and another background class. In 2025,
participating a segmentation competition is significantly different from that
in previous years where many tricks can play important roles. Nowadays most
segmentation tricks have been well integrated into existing codebases such that
our naive ViT-Adapter baseline has already achieved sufficiently good
performance. Hence, we believe the key to stand out among other competitors is
to focus on the problem nature of wheat per se. By probing visualizations, we
identify the key -- the stem matters. In contrast to heads and leaves, stems
exhibit fine structure and occupy only few pixels, which suffers from fragile
predictions and class imbalance. Building on our baseline, we present three
technical improvements tailored to stems: i) incorporating a dynamic upsampler
SAPA used to enhance detail delineation; ii) leveraging semi-supervised guided
distillation with stem-aware sample selection to mine the treasure beneath
unlabeled data; and iii) applying a test-time scaling strategy to zoom in and
segment twice the image. Despite being simple, the three improvements bring us
to the first place of the competition, outperforming the second place by clear
margins. Code and models will be released at
https://github.com/tiny-smart/gwfss25.

</details>


### [87] [Defending Deepfake via Texture Feature Perturbation](https://arxiv.org/abs/2508.17315)
*Xiao Zhang,Changfang Chen,Tianyi Wang*

Main category: cs.CV

TL;DR: Proactive Deepfake detection using texture-guided perturbations that invisibly insert signals in facial texture regions to distort Deepfake generation and create visible defects in manipulated content.


<details>
  <summary>Details</summary>
Motivation: Existing Deepfake detection methods rely on passive analysis, but high-quality Deepfakes make this unreliable. Proactive defense by inserting invisible signals before image editing offers a more robust solution.

Method: Texture-guided perturbation framework that uses Local Binary Patterns (LBP) to extract texture features, applies localized perturbations to key texture regions with low perceptual saliency, and employs a dual-model attention strategy to optimize perturbations.

Result: Experiments on CelebA-HQ and LFW datasets show promising performance in distorting Deepfake generation and producing obvious visual defects under multiple attack models.

Conclusion: The approach provides an efficient and scalable solution for proactive Deepfake detection by leveraging human visual sensitivity to perturbations and focusing on texture regions for invisible signal insertion.

Abstract: The rapid development of Deepfake technology poses severe challenges to
social trust and information security. While most existing detection methods
primarily rely on passive analyses, due to unresolvable high-quality Deepfake
contents, proactive defense has recently emerged by inserting invisible signals
in advance of image editing. In this paper, we introduce a proactive Deepfake
detection approach based on facial texture features. Since human eyes are more
sensitive to perturbations in smooth regions, we invisibly insert perturbations
within texture regions that have low perceptual saliency, applying localized
perturbations to key texture regions while minimizing unwanted noise in
non-textured areas. Our texture-guided perturbation framework first extracts
preliminary texture features via Local Binary Patterns (LBP), and then
introduces a dual-model attention strategy to generate and optimize texture
perturbations. Experiments on CelebA-HQ and LFW datasets demonstrate the
promising performance of our method in distorting Deepfake generation and
producing obvious visual defects under multiple attack models, providing an
efficient and scalable solution for proactive Deepfake detection.

</details>


### [88] [SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation](https://arxiv.org/abs/2508.17316)
*Zhenyu Jin,Wenjie Li,Zhanyu Ma,Heng Guo*

Main category: cs.CV

TL;DR: SpecGen generates spectral BRDFs from single RGB images, enabling spectral rendering under arbitrary lighting and shapes, achieving 8 dB PSNR improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Synthesizing spectral images across wavelengths is crucial for photorealistic rendering, but existing methods convert RGB to spectral images rather than generating spectral BRDF properties needed for flexible rendering under different conditions.

Method: Proposes Spectral-Spatial Tri-plane Aggregation (SSTA) network that models reflectance responses across wavelengths and incident-outgoing directions, leveraging abundant RGB BRDF data to enhance spectral BRDF generation from limited spectral data.

Result: Method accurately reconstructs spectral BRDFs and surpasses state-of-the-art in hyperspectral image reconstruction, achieving 8 dB improvement in PSNR.

Conclusion: SpecGen enables spectral image rendering under arbitrary illuminations and shapes from single RGB images, addressing the challenge of scarce spectral BRDF data through innovative training strategy and network architecture.

Abstract: Synthesizing spectral images across different wavelengths is essential for
photorealistic rendering. Unlike conventional spectral uplifting methods that
convert RGB images into spectral ones, we introduce SpecGen, a novel method
that generates spectral bidirectional reflectance distribution functions
(BRDFs) from a single RGB image of a sphere. This enables spectral image
rendering under arbitrary illuminations and shapes covered by the corresponding
material. A key challenge in spectral BRDF generation is the scarcity of
measured spectral BRDF data. To address this, we propose the Spectral-Spatial
Tri-plane Aggregation (SSTA) network, which models reflectance responses across
wavelengths and incident-outgoing directions, allowing the training strategy to
leverage abundant RGB BRDF data to enhance spectral BRDF generation.
Experiments show that our method accurately reconstructs spectral BRDFs from
limited spectral data and surpasses state-of-the-art methods in hyperspectral
image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data
will be released upon acceptance.

</details>


### [89] [Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](https://arxiv.org/abs/2508.17334)
*Somraj Gautam,Abhirama Subramanyam Penamakuri,Abhishek Bhandari,Gaurav Harit*

Main category: cs.CV

TL;DR: MMCRICBENCH-3K is a benchmark for evaluating large vision-language models on cricket scorecard VQA, featuring 3K QA pairs across English and Hindi scorecards to test numerical reasoning and cross-lingual generalization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current LVLMs in handling complex numerical reasoning and cross-lingual understanding of semi-structured tabular data like cricket scorecards.

Method: Created a benchmark with 1,463 synthetic cricket scorecard images (ODI, T20, Test formats) and 1,500 English QA pairs, divided into English (MMCRICBENCH-E-1.5K) and Hindi (MMCRICBENCH-H-1.5K) subsets with identical English questions for controlled evaluation.

Result: State-of-the-art LVLMs (GPT-4o, Qwen2.5VL) struggle significantly on both English and Hindi subsets, with performance dropping further on Hindi scorecards despite English questions.

Conclusion: The benchmark reveals critical limitations in LVLMs' structure-aware visual text understanding, numerical reasoning, and cross-lingual generalization capabilities, providing a valuable resource for future research.

Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)
on cricket scorecards, designed to evaluate large vision-language models
(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured
tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated
scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English
QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English
scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi
scorecards, with all questions and answers kept in English to enable controlled
cross-script evaluation. The task demands reasoning over structured numerical
data, multi-image context, and implicit domain knowledge. Empirical results
show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle
on the English subset despite it being their primary training language and
exhibit a further drop in performance on the Hindi subset. This reveals key
limitations in structure-aware visual text understanding, numerical reasoning,
and cross-lingual generalization. The dataset is publicly available via Hugging
Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM
research in this direction.

</details>


### [90] [No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2508.17346)
*Lianrui Mu,Zou Xingze,Jianhong Bai,Jiaqi Hu,Wenjie Zheng,Jiangnan Ye,Jiedong Zhuang,Mudassar Ali,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: HiDA-Net is a novel framework for detecting high-resolution AI-generated images that preserves native-resolution details through feature aggregation and addresses compression artifacts, achieving state-of-the-art performance with over 13% improvement on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detection methods struggle with high-resolution images due to resizing/cropping strategies that lose subtle artifacts and information, creating a gap between low-resolution training data and high-resolution real-world scenarios.

Method: Proposes HiDA-Net with Feature Aggregation Module (FAM) that fuses features from multiple full-resolution local tiles with down-sampled global view, plus Token-wise Forgery Localization (TFL) for spatial sensitivity and JPEG Quality Factor Estimation (QFE) to separate generative artifacts from compression noise.

Result: Achieves state-of-the-art performance with over 13% accuracy increase on Chameleon dataset and 10% improvement on the new HiRes-50K benchmark consisting of 50,568 high-resolution images up to 64 megapixels.

Conclusion: HiDA-Net effectively addresses the challenges of high-resolution AI-generated image detection by preserving native-resolution details and handling compression artifacts, while the new HiRes-50K benchmark facilitates future research in this domain.

Abstract: The rapid growth of high-resolution, meticulously crafted AI-generated images
poses a significant challenge to existing detection methods, which are often
trained and evaluated on low-resolution, automatically generated datasets that
do not align with the complexities of high-resolution scenarios. A common
practice is to resize or center-crop high-resolution images to fit standard
network inputs. However, without full coverage of all pixels, such strategies
risk either obscuring subtle, high-frequency artifacts or discarding
information from uncovered regions, leading to input information loss. In this
paper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net),
a novel framework that ensures no pixel is left behind. We use the Feature
Aggregation Module (FAM), which fuses features from multiple full-resolution
local tiles with a down-sampled global view of the image. These local features
are aggregated and fused with global representations for final prediction,
ensuring that native-resolution details are preserved and utilized for
detection. To enhance robustness against challenges such as localized AI
manipulations and compression, we introduce Token-wise Forgery Localization
(TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor
Estimation (QFE) module to disentangle generative artifacts from compression
noise explicitly. Furthermore, to facilitate future research, we introduce
HiRes-50K, a new challenging benchmark consisting of 50,568 images with up to
64 megapixels. Extensive experiments show that HiDA-Net achieves
state-of-the-art, increasing accuracy by over 13% on the challenging Chameleon
dataset and 10% on our HiRes-50K.

</details>


### [91] [DiCache: Let Diffusion Model Determine Its Own Cache](https://arxiv.org/abs/2508.17356)
*Jiazi Bu,Pengyang Ling,Yujie Zhou,Yibin Wang,Yuhang Zang,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: DiCache is a training-free adaptive caching strategy that uses shallow-layer feature analysis to dynamically determine when and how to cache in diffusion models, achieving better efficiency and visual quality than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing caching-based acceleration methods for diffusion models rely on predefined empirical laws or dataset-level priors, which have limited generalizability and fail on outlier samples due to the dynamic nature of diffusion processes.

Method: DiCache uses two components: (1) Online Probe Profiling Scheme that leverages shallow-layer online probe for real-time caching error estimation, and (2) Dynamic Cache Trajectory Alignment that combines multi-step caches based on shallow-layer feature trajectory to better approximate current features.

Result: Extensive experiments show DiCache achieves higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo, and Flux.

Conclusion: DiCache provides a unified framework for adaptive caching in diffusion models by leveraging shallow-layer feature correlations and trajectory similarities, enabling autonomous caching decisions and better feature approximation without requiring training.

Abstract: Recent years have witnessed the rapid development of acceleration techniques
for diffusion models, especially caching-based acceleration methods. These
studies seek to answer two fundamental questions: "When to cache" and "How to
use cache", typically relying on predefined empirical laws or dataset-level
priors to determine the timing of caching and utilizing handcrafted rules for
leveraging multi-step caches. However, given the highly dynamic nature of the
diffusion process, they often exhibit limited generalizability and fail on
outlier samples. In this paper, a strong correlation is revealed between the
variation patterns of the shallow-layer feature differences in the diffusion
model and those of final model outputs. Moreover, we have observed that the
features from different model layers form similar trajectories. Based on these
observations, we present DiCache, a novel training-free adaptive caching
strategy for accelerating diffusion models at runtime, answering both when and
how to cache within a unified framework. Specifically, DiCache is composed of
two principal components: (1) Online Probe Profiling Scheme leverages a
shallow-layer online probe to obtain a stable prior for the caching error in
real time, enabling the model to autonomously determine caching schedules. (2)
Dynamic Cache Trajectory Alignment combines multi-step caches based on
shallow-layer probe feature trajectory to better approximate the current
feature, facilitating higher visual quality. Extensive experiments validate
DiCache's capability in achieving higher efficiency and improved visual
fidelity over state-of-the-art methods on various leading diffusion models
including WAN 2.1, HunyuanVideo for video generation, and Flux for image
generation.

</details>


### [92] [Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation](https://arxiv.org/abs/2508.17364)
*Guoqing Zhang,Xingtong Ge,Lu Shi,Xin Zhang,Muqing Xue,Wanru Xu,Yigang Cen*

Main category: cs.CV

TL;DR: UniGen framework with CoMoE module and WeaveNet for efficient multi-condition image generation, achieving SOTA performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods train separate control branches for each condition type, leading to redundant model structures and inefficient computational resource usage.

Method: Proposes Condition Modulated Expert (CoMoE) module to aggregate similar patch features and assign to dedicated experts, plus WeaveNet for dynamic connection between backbone and control branches.

Result: Extensive experiments on Subjects-200K and MultiGen-20M datasets show state-of-the-art performance across various conditional image generation tasks.

Conclusion: UniGen framework effectively addresses parameter redundancy and computational inefficiency while enhancing generation efficiency and expressiveness for multi-condition image generation.

Abstract: The image-to-image generation task aims to produce controllable images by
leveraging conditional inputs and prompt instructions. However, existing
methods often train separate control branches for each type of condition,
leading to redundant model structures and inefficient use of computational
resources. To address this, we propose a Unified image-to-image Generation
(UniGen) framework that supports diverse conditional inputs while enhancing
generation efficiency and expressiveness. Specifically, to tackle the widely
existing parameter redundancy and computational inefficiency in controllable
conditional generation architectures, we propose the Condition Modulated Expert
(CoMoE) module. This module aggregates semantically similar patch features and
assigns them to dedicated expert modules for visual representation and
conditional modeling. By enabling independent modeling of foreground features
under different conditions, CoMoE effectively mitigates feature entanglement
and redundant computation in multi-condition scenarios. Furthermore, to bridge
the information gap between the backbone and control branches, we propose
WeaveNet, a dynamic, snake-like connection mechanism that enables effective
interaction between global text-level control from the backbone and
fine-grained control from conditional branches. Extensive experiments on the
Subjects-200K and MultiGen-20M datasets across various conditional image
generation tasks demonstrate that our method consistently achieves
state-of-the-art performance, validating its advantages in both versatility and
effectiveness. The code has been uploaded to
https://github.com/gavin-gqzhang/UniGen.

</details>


### [93] [Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis](https://arxiv.org/abs/2508.17394)
*Nir Mazor,Tom Hope*

Main category: cs.CV

TL;DR: Joint optimization of multimodal retriever with LVLM for medical diagnosis, outperforming standard RAG and achieving competitive results with medical models using general-purpose backbones.


<details>
  <summary>Details</summary>
Motivation: Improve diagnostic accuracy by retrieving relevant visual information from medical literature and hospital records, addressing limitations of standard RAG where error signals don't propagate to retriever.

Method: Develop a model that jointly optimizes multimodal retriever with LVLM for medical diagnosis, using only general-purpose backbones with lightweight fine-tuning.

Result: Competitive performance with medically-pretrained models on clinical multi-label classification and visual QA tasks. Joint optimization significantly improves challenging cases over standard RAG.

Conclusion: While correct diagnosis is frequently achievable with top retrieved images, there's a large performance gap from oracle. Rerankers using frontier LVLMs don't close this gap, leaving room for future improvement.

Abstract: Clinical decision-making often involves interpreting images (e.g., radiology)
for making diagnoses. Retrieving relevant visual information from medical
literature and hospital records could enhance diagnostic accuracy. In this
paper, we develop a model in which a multimodal retriever is jointly optimized
with an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal
is not propagated down to the retriever. We show that using only
general-purpose backbones, with only lightweight fine-tuning, our model is able
to achieve competitive results with medically-pretrained models across clinical
multi-label classification and visual question answering tasks. In a novel
analysis, we additionally find that in many cases different top retrieved
images each lead to different predictions for a given target, and that these
cases are empirically challenging for all models, even for non-retrieval
models. Our joint retrieval optimization significantly improves these
challenging cases over standard RAG. However, oracle analysis reveals that
while the correct diagnosis is frequently achievable using one of the top
retrieved images, in practice there is a large performance gap from the oracle,
and rerankers using frontier LVLMs do not close this gap -- leaving ample room
for improvement by future methods. Code will be made publicly available.

</details>


### [94] [Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches](https://arxiv.org/abs/2508.17397)
*Aoqi Li,Yanghui Song,Jichao Dao,Chengfu Yang*

Main category: cs.CV

TL;DR: Deep learning approach combining VGG19 and ResNet50 for underwater image enhancement, using multi-scale feature analysis and quantitative evaluation with PSNR, UCIQE, and UIQM metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenging problem of image enhancement in complex underwater scenes where visibility and image quality are severely degraded by water properties.

Method: Integrates VGG19 and ResNet50 convolutional neural networks to perform multi-scale and multi-level deep feature analysis of underwater images, creating a unified model that combines their complementary advantages.

Result: Achieves more comprehensive and accurate image enhancement effects, with quantitative evaluation using standard image quality metrics showing improved performance.

Conclusion: Provides practical suggestions for model optimization, multi-model fusion, and hardware selection to improve the practicality and stability of underwater visual enhancement systems for complex environments.

Abstract: This paper addresses the challenging problem of image enhancement in complex
underwater scenes by proposing a solution based on deep learning. The proposed
method skillfully integrates two deep convolutional neural network models,
VGG19 and ResNet50, leveraging their powerful feature extraction capabilities
to perform multi-scale and multi-level deep feature analysis of underwater
images. By constructing a unified model, the complementary advantages of the
two models are effectively integrated, achieving a more comprehensive and
accurate image enhancement effect.To objectively evaluate the enhancement
effect, this paper introduces image quality assessment metrics such as PSNR,
UCIQE, and UIQM to quantitatively compare images before and after enhancement
and deeply analyzes the performance of different models in different
scenarios.Furthermore, to improve the practicality and stability of the
underwater visual enhancement system, this paper also provides practical
suggestions from aspects such as model optimization, multi-model fusion, and
hardware selection, aiming to provide strong technical support for visual
enhancement tasks in complex underwater environments.

</details>


### [95] [MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling](https://arxiv.org/abs/2508.17404)
*Haoyu Wang,Hao Tang,Donglin Di,Zhilu Zhang,Wangmeng Zuo,Feng Gao,Siwei Ma,Shiliang Zhang*

Main category: cs.CV

TL;DR: MoCo is a novel human video generation framework that decouples structure and appearance generation to produce realistic whole-body human motion from text prompts, addressing limitations of existing methods that prioritize appearance over motion coherence.


<details>
  <summary>Details</summary>
Motivation: Existing video generation models focus too much on appearance fidelity, resulting in unrealistic human movements with poor structural coherence. Most datasets only contain facial/upper-body motions or dance videos, limiting generation capabilities to simple movements.

Method: Decouples human video generation into: 1) 3D structure generator for motion sequences from text, 2) appearance synthesis guided by structural sequence. Uses Human-Aware Dynamic Control modules and dense tracking constraints for fine-grained control.

Result: Extensive experiments show MoCo outperforms existing approaches in generating realistic and structurally coherent human videos with complex whole-body motions.

Conclusion: MoCo successfully addresses the challenge of generating consistent human motion from text by separating structure and appearance generation, supported by a new large-scale whole-body human video dataset.

Abstract: Generating human videos with consistent motion from text prompts remains a
significant challenge, particularly for whole-body or long-range motion.
Existing video generation models prioritize appearance fidelity, resulting in
unrealistic or physically implausible human movements with poor structural
coherence. Additionally, most existing human video datasets primarily focus on
facial or upper-body motions, or consist of vertically oriented dance videos,
limiting the scope of corresponding generation methods to simple movements. To
overcome these challenges, we propose MoCo, which decouples the process of
human video generation into two components: structure generation and appearance
generation. Specifically, our method first employs an efficient 3D structure
generator to produce a human motion sequence from a text prompt. The remaining
video appearance is then synthesized under the guidance of the generated
structural sequence. To improve fine-grained control over sparse human
structures, we introduce Human-Aware Dynamic Control modules and integrate
dense tracking constraints during training. Furthermore, recognizing the
limitations of existing datasets, we construct a large-scale whole-body human
video dataset featuring complex and diverse motions. Extensive experiments
demonstrate that MoCo outperforms existing approaches in generating realistic
and structurally coherent human videos.

</details>


### [96] [E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation](https://arxiv.org/abs/2508.17408)
*Bin Huang,Zhong Liu,Huiying Wen,Bingsheng Huang,Xin Chen,Shuo Li*

Main category: cs.CV

TL;DR: E-BayesSAM is an efficient Bayesian adaptation of SAM for medical image segmentation that addresses instability, computational cost, and interpretability issues through Token-wise Variational Bayesian Inference and Self-Optimizing KAN networks.


<details>
  <summary>Details</summary>
Motivation: To overcome three key limitations in Bayesian adaptation of SAM for medical segmentation: instability in fine-tuning, high computational costs from massive parameters, and lack of interpretability in SAM's black-box design.

Method: Combines Token-wise Variational Bayesian Inference (T-VBI) for efficient Bayesian adaptation without training, and Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) with learnable spline activations for improved interpretability and token pruning.

Result: Achieves real-time inference (0.03s/image), superior segmentation accuracy (89.0% DSC), and identifies four critical tokens governing SAM's decisions on five ultrasound datasets.

Conclusion: E-BayesSAM successfully unifies efficiency, reliability, and interpretability, bridging SAM's versatility with clinical needs for safety-critical medical applications.

Abstract: Although the Segment Anything Model (SAM) has advanced medical image
segmentation, its Bayesian adaptation for uncertainty-aware segmentation
remains hindered by three key issues: (1) instability in Bayesian fine-tuning
of large pre-trained SAMs; (2) high computation cost due to SAM's massive
parameters; (3) SAM's black-box design limits interpretability. To overcome
these, we propose E-BayesSAM, an efficient framework combining Token-wise
Variational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and
Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving
interpretability. T-VBI innovatively reinterprets SAM's output tokens as
dynamic probabilistic weights and reparameterizes them as latent variables
without auxiliary training, enabling training-free VBI for uncertainty
estimation. SO-KAN improves token prediction with learnable spline activations
via self-supervised learning, providing insight to prune redundant tokens to
boost efficiency and accuracy. Experiments on five ultrasound datasets
demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image),
(ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\%
vs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four
critical tokens governing SAM's decisions. By unifying efficiency, reliability,
and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs,
advancing deployment in safety-critical medical applications. The source code
is available at https://github.com/mp31192/E-BayesSAM.

</details>


### [97] [Data Leakage in Visual Datasets](https://arxiv.org/abs/2508.17416)
*Patrick Ramos,Ryan Ramos,Noa Garcia*

Main category: cs.CV

TL;DR: Analysis reveals widespread data leakage in visual datasets where evaluation images appear in training data, compromising fair model evaluation across all analyzed datasets.


<details>
  <summary>Details</summary>
Motivation: Large-scale visual datasets are often sourced from the internet where benchmarks are publicly available, creating potential for data leakage that undermines fair model evaluation.

Method: Applied image retrieval techniques to identify and characterize visual leakage according to modality, coverage, and degree across multiple datasets.

Result: Found that all analyzed datasets present some form of leakage, and all types of leakage (from severe to subtle) compromise the reliability of model evaluation in downstream tasks.

Conclusion: Data leakage is a pervasive problem in visual datasets that significantly impacts the fairness and reliability of computer vision model evaluation.

Abstract: We analyze data leakage in visual datasets. Data leakage refers to images in
evaluation benchmarks that have been seen during training, compromising fair
model evaluation. Given that large-scale datasets are often sourced from the
internet, where many computer vision benchmarks are publicly available, our
efforts are focused into identifying and studying this phenomenon. We
characterize visual leakage into different types according to its modality,
coverage, and degree. By applying image retrieval techniques, we unequivocally
show that all the analyzed datasets present some form of leakage, and that all
types of leakage, from severe instances to more subtle cases, compromise the
reliability of model evaluation in downstream tasks.

</details>


### [98] [Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models](https://arxiv.org/abs/2508.17417)
*Xiaojie Yin,Qilong Wang,Qinghua Hu*

Main category: cs.CV

TL;DR: Proposes Constrained Prompt Enhancement (CPE) method to improve visual-textual alignment in VLMs through comprehensive textual prompts and compact visual prompts using TGSSG and CADRS components.


<details>
  <summary>Details</summary>
Motivation: Vision-language models suffer from semantic misalignment due to domain gaps between pre-training and downstream tasks, with existing approaches facing incomplete textual prompts and noisy visual prompts.

Method: Two key components: 1) Topology-Guided Synonymous Semantic Generation (TGSSG) using LLMs to create comprehensive textual prompts via semantic ambiguity entropy and persistent homology analysis; 2) Category-Agnostic Discriminative Region Selection (CADRS) using activation maps to identify discriminative regions and filter noisy visual prompts. Uses set-to-set matching with test-time adaptation and optimal transport.

Result: Improved visual-textual alignment and zero-shot generalization of vision-language models by addressing both textual incompleteness and visual noise issues.

Conclusion: The proposed CPE method effectively enhances prompt quality from both textual and visual perspectives, leading to better alignment and generalization in vision-language models.

Abstract: Vision-language models (VLMs) pre-trained on web-scale data exhibit promising
zero-shot generalization but often suffer from semantic misalignment due to
domain gaps between pre-training and downstream tasks. Existing approaches
primarily focus on text prompting with class-specific descriptions and
visual-text adaptation via aligning cropped image regions with textual
descriptions. However, they still face the issues of incomplete textual prompts
and noisy visual prompts. In this paper, we propose a novel constrained prompt
enhancement (CPE) method to improve visual-textual alignment by constructing
comprehensive textual prompts and compact visual prompts from the semantic
perspective. Specifically, our approach consists of two key components:
Topology-Guided Synonymous Semantic Generation (TGSSG) and Category-Agnostic
Discriminative Region Selection (CADRS). Textually, to address the issue of
incomplete semantic expression in textual prompts, our TGSSG first generates
synonymous semantic set for each category via large language models, and
constructs comprehensive textual prompts based on semantic ambiguity entropy
and persistent homology analysis. Visually, to mitigate the irrelevant visual
noise introduced by random cropping, our CADRS identifies discriminative
regions with activation maps outputted by a pre-trained vision model,
effectively filtering out noisy regions and generating compact visual prompts.
Given the comprehensive set of textual prompts and compact set of visual
prompts, we introduce two set-to-set matching strategies based on test-time
adaptation (TTA) and optimal transport (OT) to achieve effective visual-textual
alignment, and so improve zero-shot generalization of VLMs.

</details>


### [99] [Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search](https://arxiv.org/abs/2508.17427)
*Zhao Zheng,Jingfan Fan,Long Shao,Hong Song,Danni Ai,Tianyu Fu,Deqiang Xiao,Yongtian Wang,Jian Yang*

Main category: cs.CV

TL;DR: A new point cloud registration method using rotation-only branch-and-bound search with geometric maximum overlapping framework, achieving polynomial time complexity and linear space complexity while outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art point cloud registration methods have limitations: graph-based approaches require quadratic space/time complexity for graph construction, while multi-stage branch-and-bound methods suffer from inaccuracy due to local optima between stages.

Method: Decomposes rigid transformation using Chasles' theorem into translation along rotation axis and 2D rigid transformation. Uses BnB search for optimal rotation axis/angle with range maximum query problems. Searches top-k candidate rotation axes via cube mapping, estimates translation through interval stabbing, and solves 2D registration as 1D rotation angle search with sweep line algorithm and segment tree.

Result: Superior accuracy and efficiency demonstrated on 3DMatch, 3DLoMatch, and KITTI datasets compared to state-of-the-art methods, with polynomial time complexity and linear space complexity even in worst-case scenarios.

Conclusion: The proposed geometric maximum overlapping registration framework via rotation-only BnB search provides an effective solution that addresses the limitations of existing methods, offering both computational efficiency and registration accuracy for point cloud alignment tasks.

Abstract: Point cloud registration based on correspondences computes the rigid
transformation that maximizes the number of inliers constrained within the
noise threshold. Current state-of-the-art (SOTA) methods employing spatial
compatibility graphs or branch-and-bound (BnB) search mainly focus on
registration under high outlier ratios. However, graph-based methods require at
least quadratic space and time complexity for graph construction, while
multi-stage BnB search methods often suffer from inaccuracy due to local optima
between decomposed stages. This paper proposes a geometric maximum overlapping
registration framework via rotation-only BnB search. The rigid transformation
is decomposed using Chasles' theorem into a translation along rotation axis and
a 2D rigid transformation. The optimal rotation axis and angle are searched via
BnB, with residual parameters formulated as range maximum query (RMQ) problems.
Firstly, the top-k candidate rotation axes are searched within a hemisphere
parameterized by cube mapping, and the translation along each axis is estimated
through interval stabbing of the correspondences projected onto that axis.
Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D
RMQ of geometric overlapping for axis-aligned rectangles, which is solved
deterministically in polynomial time using sweep line algorithm with segment
tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets
demonstrate superior accuracy and efficiency over SOTA methods, while the time
complexity is polynomial and the space complexity increases linearly with the
number of points, even in the worst case.

</details>


### [100] [FedKLPR: Personalized Federated Learning for Person Re-Identification with Adaptive Pruning](https://arxiv.org/abs/2508.17431)
*Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.CV

TL;DR: FedKLPR is a lightweight federated learning framework for person re-identification that addresses statistical heterogeneity and communication overhead through KL-divergence regularization, weighted aggregation, sparse activation skipping, and cross-round recovery mechanisms.


<details>
  <summary>Details</summary>
Motivation: To enable privacy-preserving person re-identification through federated learning while overcoming challenges of statistical heterogeneity (non-IID data) and high communication costs from frequent model transmission.

Method: Four key components: 1) KL-Divergence Regularization Loss to minimize divergence from global feature distribution, 2) KL-Divergence-Prune Weighted Aggregation for robust aggregation with reduced communication, 3) Sparse Activation Skipping to preserve critical parameters, and 4) Cross-Round Recovery for dynamic pruning control.

Result: Achieves 33%-38% communication reduction on ResNet-50 and 20%-40% on ResNet-34 while maintaining model accuracy within 1% degradation across eight benchmark datasets.

Conclusion: FedKLPR provides an effective solution for communication-efficient federated learning in person re-identification, successfully addressing both statistical heterogeneity and communication overhead challenges while preserving model performance.

Abstract: Person re-identification (Re-ID) is a fundamental task in intelligent
surveillance and public safety. Federated learning (FL) offers a
privacy-preserving solution by enabling collaborative model training without
centralized data collection. However, applying FL to real-world re-ID systems
faces two major challenges: statistical heterogeneity across clients due to
non-IID data distributions, and substantial communication overhead caused by
frequent transmission of large-scale models. To address these issues, we
propose FedKLPR, a lightweight and communication-efficient federated learning
framework for person re-identification. FedKLPR introduces four key components.
First, the KL-Divergence Regularization Loss (KLL) constrains local models by
minimizing the divergence from the global feature distribution, effectively
mitigating the effects of statistical heterogeneity and improving convergence
stability under non-IID conditions. Secondly, KL-Divergence-Prune Weighted
Aggregation (KLPWA) integrates pruning ratio and distributional similarity into
the aggregation process, thereby improving the robustness of the global model
while significantly reducing communication overhead. Furthermore, sparse
Activation Skipping (SAS) mitigates the dilution of critical parameters during
the aggregation of pruned client models by excluding zero-valued weights from
the update process. Finally, Cross-Round Recovery (CRR) introduces a dynamic
pruning control mechanism that halts pruning when necessary, enabling deeper
compression while maintaining model accuracy. Experimental results on eight
benchmark datasets demonstrate that FedKLPR achieves significant communication
reduction. Compared with the state-of-the-art, FedKLPR reduces 33\%-38\%
communication cost on ResNet-50 and 20\%-40\% communication cost on ResNet-34,
while maintaining model accuracy within 1\% degradation.

</details>


### [101] [TinySR: Pruning Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.17434)
*Linwei Dong,Qingnan Fan,Yuhang Yu,Qi Zhang,Jinwei Chen,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: TinySR is a compact diffusion model for real-time image super-resolution that achieves 5.68x speedup and 83% parameter reduction while maintaining quality through architectural optimizations and pruning strategies.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for image super-resolution suffer from high computational overhead due to iterative denoising, making them unsuitable for real-time applications. Even one-step distillation methods remain constrained by large, over-parameterized architectures.

Method: Proposes TinySR with Dynamic Inter-block Activation and Expansion-Corrosion Strategy for depth pruning, VAE compression through channel pruning and attention removal, elimination of time/prompt modules, and pre-caching techniques to accelerate inference.

Result: Achieves up to 5.68x speedup and 83% parameter reduction compared to teacher model TSD-SR while maintaining high perceptual quality in real-world image super-resolution tasks.

Conclusion: TinySR demonstrates that compact diffusion models can achieve real-time performance in image super-resolution without sacrificing quality, making diffusion-based approaches more practical for real-world applications.

Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering
high-quality images from low-resolution inputs that suffer from complex
degradations like noise, blur, and compression. Recently, diffusion models
(DMs) have shown great potential in this area by leveraging strong generative
priors to restore fine details. However, their iterative denoising process
incurs high computational overhead, posing challenges for real-time
applications. Although one-step distillation methods, such as OSEDiff and
TSD-SR, offer faster inference, they remain fundamentally constrained by their
large, over-parameterized model architectures. In this work, we present TinySR,
a compact yet effective diffusion model specifically designed for Real-ISR that
achieves real-time performance while maintaining perceptual quality. We
introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy
to facilitate more effective decision-making in depth pruning. We achieve VAE
compression through channel pruning, attention removal and lightweight SepConv.
We eliminate time- and prompt-related modules and perform pre-caching
techniques to further speed up the model. TinySR significantly reduces
computational cost and model size, achieving up to 5.68x speedup and 83%
parameter reduction compared to its teacher TSD-SR, while still providing high
quality results.

</details>


### [102] [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing](https://arxiv.org/abs/2508.17435)
*Zihan Liang,Jiahao Sun,Haoran Ma*

Main category: cs.CV

TL;DR: RefineEdit-Agent is a training-free intelligent agent framework that uses LLMs and LVLMs for complex, iterative image editing with superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image generation models struggle with fine-grained, iterative editing, granular instruction understanding, context preservation, and lack feedback mechanisms for refinement.

Method: Framework combines LLM planning capabilities with LVLM visual understanding in closed-loop system: LVLM instruction parser, multi-level LLM editing planner, iterative editing module, and LVLM feedback/evaluation loop.

Result: Achieved average score of 3.67 on LongBench-T2I-Edit benchmark (500 images, 9 visual dimensions), significantly outperforming baselines: Direct Re-Prompting (2.29), InstructPix2Pix (2.91), GLIGEN-based Edit (3.16), ControlNet-XL (3.39).

Conclusion: RefineEdit-Agent's agentic design delivers superior edit fidelity and context preservation, validated by ablation studies, human evaluations, and robustness to instruction complexity.

Abstract: Despite the remarkable capabilities of text-to-image (T2I) generation models,
real-world applications often demand fine-grained, iterative image editing that
existing methods struggle to provide. Key challenges include granular
instruction understanding, robust context preservation during modifications,
and the lack of intelligent feedback mechanisms for iterative refinement. This
paper introduces RefineEdit-Agent, a novel, training-free intelligent agent
framework designed to address these limitations by enabling complex, iterative,
and context-aware image editing. RefineEdit-Agent leverages the powerful
planning capabilities of Large Language Models (LLMs) and the advanced visual
understanding and evaluation prowess of Vision-Language Large Models (LVLMs)
within a closed-loop system. Our framework comprises an LVLM-driven instruction
parser and scene understanding module, a multi-level LLM-driven editing planner
for goal decomposition, tool selection, and sequence generation, an iterative
image editing module, and a crucial LVLM-driven feedback and evaluation loop.
To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new
benchmark featuring 500 initial images with complex, multi-turn editing
instructions across nine visual dimensions. Extensive experiments demonstrate
that RefineEdit-Agent significantly outperforms state-of-the-art baselines,
achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for
Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and
3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of
iterative refinement, backbone choices, tool usage, and robustness to
instruction complexity further validate the efficacy of our agentic design in
delivering superior edit fidelity and context preservation.

</details>


### [103] [Disentangled Geometry and Appearance for Efficient Multi-View Surface Reconstruction and Rendering](https://arxiv.org/abs/2508.17436)
*Qitong Zhang,Jieqing Feng*

Main category: cs.CV

TL;DR: Efficient mesh-based neural rendering method that eliminates separate mesh extraction, achieves state-of-the-art speed (4.84min training, 0.023s rendering) with competitive quality, and enables practical mesh/texture editing.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of neural rendering methods that require additional mesh extraction steps, which cause poor-quality surfaces with mesh aliasing and restrict downstream applications.

Method: Uses explicit mesh representation with differentiable rasterization. Introduces disentangled geometry/appearance model without deep networks, neural deformation field for global geometric context, novel regularization for geometric features, and view-invariant diffuse term baked into vertices.

Result: Achieves state-of-the-art training speed (4.84 minutes) and rendering speed (0.023 seconds) with reconstruction quality competitive with top methods. Enables practical mesh and texture editing applications.

Conclusion: The method combines efficiency, competitive quality, and broad applicability, making it a valuable contribution to multi-view surface reconstruction and rendering with practical editing capabilities.

Abstract: This paper addresses the limitations of neural rendering-based multi-view
surface reconstruction methods, which require an additional mesh extraction
step that is inconvenient and would produce poor-quality surfaces with mesh
aliasing, restricting downstream applications. Building on the explicit mesh
representation and differentiable rasterization framework, this work proposes
an efficient solution that preserves the high efficiency of this framework
while significantly improving reconstruction quality and versatility.
Specifically, we introduce a disentangled geometry and appearance model that
does not rely on deep networks, enhancing learning and broadening
applicability. A neural deformation field is constructed to incorporate global
geometric context, enhancing geometry learning, while a novel regularization
constrains geometric features passed to a neural shader to ensure its accuracy
and boost shading. For appearance, a view-invariant diffuse term is separated
and baked into mesh vertices, further improving rendering efficiency.
Experimental results demonstrate that the proposed method achieves
state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds,
with reconstruction quality that is competitive with top-performing methods.
Moreover, the method enables practical applications such as mesh and texture
editing, showcasing its versatility and application potential. This combination
of efficiency, competitive quality, and broad applicability makes our approach
a valuable contribution to multi-view surface reconstruction and rendering.

</details>


### [104] [Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels](https://arxiv.org/abs/2508.17437)
*Long Le,Ryan Lucas,Chen Wang,Chuhao Chen,Dinesh Jayaraman,Eric Eaton,Lingjie Liu*

Main category: cs.CV

TL;DR: PIXIE is a fast neural network that predicts 3D physical material properties from visual features using supervised learning, outperforming optimization methods and enabling realistic physics simulation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for inferring physical properties from 3D scenes rely on slow per-scene optimization, limiting generalizability and real-world application.

Method: Trains a generalizable neural network using supervised losses to predict physical properties from 3D visual features, coupled with Gaussian Splatting for scene representation and leveraging pretrained features like CLIP.

Result: PIXIE is 1.46-4.39x better and orders of magnitude faster than test-time optimization methods, with zero-shot generalization to real-world scenes using only synthetic training data.

Conclusion: PIXIE enables fast, accurate inference of physical material properties for realistic physics simulation, advancing interactive virtual world creation with strong generalization capabilities.

Abstract: Inferring the physical properties of 3D scenes from visual information is a
critical yet challenging task for creating interactive and realistic virtual
worlds. While humans intuitively grasp material characteristics such as
elasticity or stiffness, existing methods often rely on slow, per-scene
optimization, limiting their generalizability and application. To address this
problem, we introduce PIXIE, a novel method that trains a generalizable neural
network to predict physical properties across multiple scenes from 3D visual
features purely using supervised losses. Once trained, our feed-forward network
can perform fast inference of plausible material fields, which coupled with a
learned static scene representation like Gaussian Splatting enables realistic
physics simulation under external forces. To facilitate this research, we also
collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and
physic material annotations. Extensive evaluations demonstrate that PIXIE is
about 1.46-4.39x better and orders of magnitude faster than test-time
optimization methods. By leveraging pretrained visual features like CLIP, our
method can also zero-shot generalize to real-world scenes despite only ever
been trained on synthetic data. https://pixie-3d.github.io/

</details>


### [105] [Investigating Domain Gaps for Indoor 3D Object Detection](https://arxiv.org/abs/2508.17439)
*Zijing Zhao,Zhu Xu,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: This paper introduces a comprehensive benchmark for domain adaptive indoor 3D object detection, addressing the challenge of adapting detectors across different indoor point cloud datasets with varying distributions.


<details>
  <summary>Details</summary>
Motivation: Existing 3D object detection research has been limited to datasets with identical training and testing distributions, but real-world applications require detectors that can generalize across different data sources with varying collection methods and characteristics.

Method: The authors create a comprehensive benchmark using ScanNet, SUN RGB-D, 3D Front datasets, plus new large-scale synthetic datasets (ProcTHOR-OD and ProcFront) generated by a 3D simulator. They conduct experiments across different adaptation scenarios including synthetic-to-real, point cloud quality, layout, and instance feature adaptation.

Result: The research analyzes the impact of different domain gaps on 3D object detectors and provides baseline approaches to improve adaptation performance across datasets.

Conclusion: The work establishes foundational benchmarks for domain adaptive indoor 3D object detection and aims to inspire future development of detectors with stronger cross-domain generalization capabilities.

Abstract: As a fundamental task for indoor scene understanding, 3D object detection has
been extensively studied, and the accuracy on indoor point cloud data has been
substantially improved. However, existing researches have been conducted on
limited datasets, where the training and testing sets share the same
distribution. In this paper, we consider the task of adapting indoor 3D object
detectors from one dataset to another, presenting a comprehensive benchmark
with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed
large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.
Since indoor point cloud datasets are collected and constructed in different
ways, the object detectors are likely to overfit to specific factors within
each dataset, such as point cloud quality, bounding box layout and instance
features. We conduct experiments across datasets on different adaptation
scenarios including synthetic-to-real adaptation, point cloud quality
adaptation, layout adaptation and instance feature adaptation, analyzing the
impact of different domain gaps on 3D object detectors. We also introduce
several approaches to improve adaptation performances, providing baselines for
domain adaptive indoor 3D object detection, hoping that future works may
propose detectors with stronger generalization ability across domains. Our
project homepage can be found in
https://jeremyzhao1998.github.io/DAVoteNet-release/.

</details>


### [106] [Multi-Level LVLM Guidance for Untrimmed Video Action Recognition](https://arxiv.org/abs/2508.17442)
*Liyang Peng,Sihan Zhu,Yunjie Guo*

Main category: cs.CV

TL;DR: ECVT is a novel video transformer that uses large vision-language models to generate multi-granularity semantic descriptions for better action recognition and localization in untrimmed videos, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with capturing fine-grained actions, long-term temporal dependencies, and high-level semantic information from low-level visual features in complex, untrimmed videos.

Method: Dual-branch architecture with Video Encoding Branch for spatio-temporal features and Cross-Modal Guidance Branch using LVLM for multi-granularity semantic descriptions. Includes adaptive gating, cross-modal attention, and event graph module for temporal context calibration.

Result: Achieves state-of-the-art performance with average mAP of 40.5% on ActivityNet v1.3 and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

Conclusion: ECVT effectively bridges the semantic gap in video understanding by leveraging LVLMs' advanced semantic capabilities, significantly enhancing temporal structure and event logic comprehension in videos.

Abstract: Action recognition and localization in complex, untrimmed videos remain a
formidable challenge in computer vision, largely due to the limitations of
existing methods in capturing fine-grained actions, long-term temporal
dependencies, and high-level semantic information from low-level visual
features. This paper introduces the Event-Contextualized Video Transformer
(ECVT), a novel architecture that leverages the advanced semantic understanding
capabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT
employs a dual-branch design, comprising a Video Encoding Branch for
spatio-temporal feature extraction and a Cross-Modal Guidance Branch. The
latter utilizes an LVLM to generate multi-granularity semantic descriptions,
including Global Event Prompting for macro-level narrative and Temporal
Sub-event Prompting for fine-grained action details. These multi-level textual
cues are integrated into the video encoder's learning process through
sophisticated mechanisms such as adaptive gating for high-level semantic
fusion, cross-modal attention for fine-grained feature refinement, and an event
graph module for temporal context calibration. Trained end-to-end with a
comprehensive loss function incorporating semantic consistency and temporal
calibration terms, ECVT significantly enhances the model's ability to
understand video temporal structures and event logic. Extensive experiments on
ActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves
state-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3
and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

</details>


### [107] [A Synthetic Dataset for Manometry Recognition in Robotic Applications](https://arxiv.org/abs/2508.17468)
*Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Joao Manoel Herrera Pinheiro,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.CV

TL;DR: Hybrid data synthesis pipeline combining procedural rendering and AI video generation overcomes data scarcity in industrial object detection, achieving superior performance with 1:1 real-synthetic data mixture.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and high acquisition costs for training robust object detection models in hazardous industrial environments like offshore oil platforms, where collecting real-world data is impractical and expensive.

Method: Proposes hybrid data synthesis pipeline using BlenderProc for photorealistic images with precise annotations and domain randomization, integrated with NVIDIA's Cosmos-Predict2 for physically plausible video sequences with temporal diversity and rare viewpoints.

Result: YOLO-based detection network trained on composite dataset (real + synthetic data) achieves superior performance compared to real-only training. 1:1 real-synthetic mixture yielded highest accuracy, surpassing real-only baseline.

Conclusion: Synthetic-first approach is viable, efficient, cost-effective, and safe alternative for developing reliable perception systems in safety-critical, resource-constrained industrial applications.

Abstract: This work addresses the challenges of data scarcity and high acquisition
costs for training robust object detection models in complex industrial
environments, such as offshore oil platforms. The practical and economic
barriers to collecting real-world data in these hazardous settings often hamper
the development of autonomous inspection systems. To overcome this, in this
work we propose and validate a hybrid data synthesis pipeline that combines
procedural rendering with AI-driven video generation. Our methodology leverages
BlenderProc to create photorealistic images with precise annotations and
controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2
world-foundation model to synthesize physically plausible video sequences with
temporal diversity, capturing rare viewpoints and adverse conditions. We
demonstrate that a YOLO-based detection network trained on a composite dataset,
blending real images with our synthetic data, achieves superior performance
compared to models trained exclusively on real-world data. Notably, a 1:1
mixture of real and synthetic data yielded the highest accuracy, surpassing the
real-only baseline. These findings highlight the viability of a synthetic-first
approach as an efficient, cost-effective, and safe alternative for developing
reliable perception systems in safety-critical and resource-constrained
industrial applications.

</details>


### [108] [T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation](https://arxiv.org/abs/2508.17472)
*Kaiyue Sun,Rongyao Fang,Chengqi Duan,Xian Liu,Xihui Liu*

Main category: cs.CV

TL;DR: T2I-ReasonBench is a new benchmark for evaluating text-to-image models' reasoning capabilities across four dimensions with a two-stage evaluation protocol.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the reasoning abilities of text-to-image generation models, which is crucial for understanding their capabilities beyond simple image generation.

Method: Developed a benchmark with four reasoning dimensions (Idiom Interpretation, Textual Image Design, Entity-Reasoning, Scientific-Reasoning) and a two-stage evaluation protocol measuring both reasoning accuracy and image quality.

Result: The paper benchmarks various T2I generation models and provides comprehensive performance analysis, though specific results are not detailed in the abstract.

Conclusion: T2I-ReasonBench serves as a valuable evaluation framework for assessing reasoning capabilities in text-to-image models, enabling systematic comparison and analysis of different models' performance.

Abstract: We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of
text-to-image (T2I) models. It consists of four dimensions: Idiom
Interpretation, Textual Image Design, Entity-Reasoning and
Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the
reasoning accuracy and image quality. We benchmark various T2I generation
models, and provide comprehensive analysis on their performances.

</details>


### [109] [GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis](https://arxiv.org/abs/2508.17478)
*Xuhao Shan,Ruiquan Ge,Jikui Liu,Linglong Wu,Chi Zhang,Siqi Liu,Wenjian Qin,Wenwen Min,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: GraphMMP is a two-stage multimodal prognosis model using graph neural networks with mutual information-based feature graphs and Mamba-based global fusion, achieving state-of-the-art performance on liver prognosis and METABRIC datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling complex interactions between heterogeneous medical data modalities while capturing both local and global dependencies across different data types in multimodal medical analysis.

Method: Two-stage multimodal prognosis model based on graph neural networks that constructs feature graphs using mutual information and features a global fusion module built on Mamba architecture.

Result: GraphMMP surpasses existing methods on liver prognosis and METABRIC study datasets, demonstrating superior performance in multimodal medical prognosis tasks.

Conclusion: The proposed GraphMMP model effectively addresses multimodal medical data analysis challenges and significantly boosts prognosis performance through its graph-based approach with mutual information and Mamba-based fusion.

Abstract: In the field of multimodal medical data analysis, leveraging diverse types of
data and understanding their hidden relationships continues to be a research
focus. The main challenges lie in effectively modeling the complex interactions
between heterogeneous data modalities with distinct characteristics while
capturing both local and global dependencies across modalities. To address
these challenges, this paper presents a two-stage multimodal prognosis model,
GraphMMP, which is based on graph neural networks. The proposed model
constructs feature graphs using mutual information and features a global fusion
module built on Mamba, which significantly boosts prognosis performance.
Empirical results show that GraphMMP surpasses existing methods on datasets
related to liver prognosis and the METABRIC study, demonstrating its
effectiveness in multimodal medical prognosis tasks.

</details>


### [110] [Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning](https://arxiv.org/abs/2508.17488)
*Zhiwen Chen,Jinjian Wu,Zhiyu Zhu,Yifan Zhang,Guangming Shi,Junhui Hou*

Main category: cs.CV

TL;DR: A sensitivity-aware regularized tuning framework for optimizing multi-modal trackers by incorporating parameter sensitivities to improve plasticity-stability trade-off during fine-tuning of pre-trained RGB models.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning approaches for multi-modal trackers suffer from either excessive freedom or over-restriction, leading to suboptimal plasticity-stability trade-off when adapting pre-trained RGB models to multi-modal contexts.

Method: Proposes a sensitivity-aware regularized tuning framework that analyzes tangent space of pre-trained weights to measure prior sensitivities (for preserving generalization) and explores transfer sensitivities during tuning (for adaptability and stability), incorporating these as regularization terms.

Result: Extensive experiments show superior performance surpassing state-of-the-art techniques across various multi-modal tracking tasks, with significant enhancement in transferability across modalities.

Conclusion: The proposed sensitivity-aware regularization framework effectively addresses the plasticity-stability dilemma in multi-modal tracker optimization, demonstrating improved transfer performance while maintaining source domain capabilities.

Abstract: This paper tackles the critical challenge of optimizing multi-modal trackers
by effectively adapting the pre-trained models for RGB data. Existing
fine-tuning paradigms oscillate between excessive freedom and over-restriction,
both leading to a suboptimal plasticity-stability trade-off. To mitigate this
dilemma, we propose a novel sensitivity-aware regularized tuning framework,
which delicately refines the learning process by incorporating intrinsic
parameter sensitivities. Through a comprehensive investigation from pre-trained
to multi-modal contexts, we identify that parameters sensitive to pivotal
foundational patterns and cross-domain shifts are primary drivers of this
issue. Specifically, we first analyze the tangent space of pre-trained weights
to measure and orient prior sensitivities, dedicated to preserving
generalization. Then, we further explore transfer sensitivities during the
tuning phase, emphasizing adaptability and stability. By incorporating these
sensitivities as regularization terms, our method significantly enhances the
transferability across modalities. Extensive experiments showcase the superior
performance of the proposed method, surpassing current state-of-the-art
techniques across various multi-modal tracking. The source code and models will
be publicly available at https://github.com/zhiwen-xdu/SRTrack.

</details>


### [111] [Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice](https://arxiv.org/abs/2508.17502)
*Hugo Bohy,Minh Tran,Kevin El Haddad,Thierry Dutoit,Mohammad Soleymani*

Main category: cs.CV

TL;DR: Social-MAE is an audiovisual Masked Autoencoder pre-trained on social interaction data that achieves state-of-the-art results on emotion recognition and laughter detection tasks through self-supervised learning.


<details>
  <summary>Details</summary>
Motivation: Human social behaviors are inherently multimodal, requiring powerful audiovisual models for perception. Existing models need better adaptation to social interaction contexts.

Method: Extended CAV-MAE to handle more frames, pre-trained on VoxCeleb2 social interaction dataset using self-supervised masked autoencoding approach.

Result: Achieved SOTA on multimodal emotion recognition and laughter recognition, competitive results on apparent personality estimation.

Conclusion: In-domain self-supervised pre-training on social data is highly effective for social behavior perception tasks.

Abstract: Human social behaviors are inherently multimodal necessitating the
development of powerful audiovisual models for their perception. In this paper,
we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on
an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE),
which is pre-trained on audiovisual social data. Specifically, we modify
CAV-MAE to receive a larger number of frames as input and pre-train it on a
large dataset of human social interaction (VoxCeleb2) in a self-supervised
manner. We demonstrate the effectiveness of this model by finetuning and
evaluating the model on different social and affective downstream tasks,
namely, emotion recognition, laughter detection and apparent personality
estimation. The model achieves state-of-the-art results on multimodal emotion
recognition and laughter recognition and competitive results for apparent
personality estimation, demonstrating the effectiveness of in-domain
self-supervised pre-training. Code and model weight are available here
https://github.com/HuBohy/SocialMAE.

</details>


### [112] [DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers](https://arxiv.org/abs/2508.17509)
*Michael Podsiadly,Brendon K Lay*

Main category: cs.CV

TL;DR: Combines DINO and Barlow Twins self-supervised learning methods to create a hybrid model that achieves comparable performance with fewer labels and computational resources.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of individual self-supervised learning methods - DINO's sensitivity to augmentations and Barlow Twins' large batch size requirements - by leveraging their complementary strengths for more efficient training.

Method: Combines DINO's self-distillation strategy with Barlow Twins' redundancy-reduction objective, trained on MS COCO dataset using only 10% labeled data for linear probing evaluation.

Result: The hybrid model achieves comparable loss and classification accuracy to standalone DINO while maintaining strong feature representations, with attention visualizations showing improved semantic segmentation capability.

Conclusion: The combined approach provides a scalable, label-efficient alternative for training Vision Transformers in resource-constrained environments, offering the benefits of both methods while mitigating their individual limitations.

Abstract: Training AI models to understand images without costly labeled data remains a
challenge. We combine two techniques--DINO (teacher-student learning) and
Barlow Twins (redundancy reduction)--to create a model that learns better with
fewer labels and less compute. While both DINO and Barlow Twins have
independently demonstrated strong performance in self-supervised learning, each
comes with limitations--DINO may be sensitive to certain augmentations, and
Barlow Twins often requires batch sizes too large to fit on consumer hardware.
By combining the redundancy-reduction objective of Barlow Twins with the
self-distillation strategy of DINO, we aim to leverage their complementary
strengths. We train a hybrid model on the MS COCO dataset using only 10\% of
labeled data for linear probing, and evaluate its performance against
standalone DINO and Barlow Twins implementations. Preliminary results show that
the combined approach achieves comparable loss and classification accuracy to
DINO while maintaining strong feature representations. Attention visualizations
further suggest improved semantic segmentation capability in the hybrid model.
This combined method offers a scalable, label-efficient alternative for
training ViTs in resource-constrained environments.

</details>


### [113] [OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation](https://arxiv.org/abs/2508.17524)
*Xingxin He,Aurora Rofena,Ruimin Feng,Haozhe Liao,Zhaoye Zhou,Albert Jang,Fang Liu*

Main category: cs.CV

TL;DR: OmniMRI is a unified vision-language foundation model that integrates the entire MRI workflow (acquisition, reconstruction, segmentation, detection, diagnosis, reporting) into a single architecture using large-scale multimodal training.


<details>
  <summary>Details</summary>
Motivation: Current MRI workflows are fragmented into separate stages and lack integration of imaging data with clinical language information that radiologists use in practice. Existing deep learning approaches are anatomy-specific and lack generalizability.

Method: Multi-stage training on large-scale heterogeneous data (60 datasets, 220k+ MRI volumes, 19M+ slices) including self-supervised vision pretraining, vision-language alignment, multimodal pretraining, and multi-task instruction tuning.

Result: Qualitative results show OmniMRI can perform diverse tasks including MRI reconstruction, anatomical/pathological segmentation, abnormality detection, diagnostic suggestion, and radiology report generation within a single architecture.

Conclusion: OmniMRI demonstrates potential to consolidate fragmented MRI pipelines into a scalable, generalist framework that unifies imaging and clinical language for comprehensive end-to-end MRI interpretation.

Abstract: Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but
remains constrained by fragmented, multi-stage workflows encompassing
acquisition, reconstruction, segmentation, detection, diagnosis, and reporting.
While deep learning has achieved progress in individual tasks, existing
approaches are often anatomy- or application-specific and lack generalizability
across diverse clinical settings. Moreover, current pipelines rarely integrate
imaging data with complementary language information that radiologists rely on
in routine practice. Here, we introduce OmniMRI, a unified vision-language
foundation model designed to generalize across the entire MRI workflow. OmniMRI
is trained on a large-scale, heterogeneous corpus curated from 60 public
datasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating
image-only data, paired vision-text data, and instruction-response data. Its
multi-stage training paradigm, comprising self-supervised vision pretraining,
vision-language alignment, multimodal pretraining, and multi-task instruction
tuning, progressively equips the model with transferable visual
representations, cross-modal reasoning, and robust instruction-following
capabilities. Qualitative results demonstrate OmniMRI's ability to perform
diverse tasks within a single architecture, including MRI reconstruction,
anatomical and pathological segmentation, abnormality detection, diagnostic
suggestion, and radiology report generation. These findings highlight OmniMRI's
potential to consolidate fragmented pipelines into a scalable, generalist
framework, paving the way toward foundation models that unify imaging and
clinical language for comprehensive, end-to-end MRI interpretation.

</details>


### [114] [Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks](https://arxiv.org/abs/2508.17537)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: Polynomial approximation for camera velocity estimation from asynchronous point tracks, with minimal solvers developed for low-degree problems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating both translational and angular camera velocity from asynchronous point tracks, which is relevant for rolling shutter and event cameras where the original problem is non-polynomial.

Method: Propose a polynomial approximation to handle the non-polynomial original problem, classify the resulting minimal problems, determine their algebraic degrees, and develop minimal solvers for problems with low degrees.

Result: Developed minimal solvers for several low-degree problems and evaluated them on both synthetic and real datasets, with code to be made publicly available.

Conclusion: The polynomial approximation approach enables effective estimation of camera velocity from asynchronous point tracks, providing practical solutions for rolling shutter and event camera applications.

Abstract: We address the problem of estimating both translational and angular velocity
of a camera from asynchronous point tracks, a formulation relevant to rolling
shutter and event cameras. Since the original problem is non-polynomial, we
propose a polynomial approximation, classify the resulting minimal problems,
and determine their algebraic degrees. Furthermore, we develop minimal solvers
for several problems with low degrees and evaluate them on synthetic and real
datasets. The code will be made publicly available.

</details>


### [115] [Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection](https://arxiv.org/abs/2508.17567)
*Daniel Frees,Moritz Bolling,Aditri Bhagirath*

Main category: cs.CV

TL;DR: This paper investigates optimal CNN architectures for medical imaging tasks and compares RadImageNet vs ImageNet pre-training, finding that specific architectures with ResNet50 backbones perform best but RadImageNet doesn't provide superior performance over ImageNet for the studied tasks.


<details>
  <summary>Details</summary>
Motivation: Medical imaging data scarcity limits model efficacy, and while transfer learning helps, it's unclear which CNN architectures work best and whether medical-specific pre-training (RadImageNet) outperforms general pre-training (ImageNet) for downstream medical tasks.

Method: Comprehensive investigation of CNN architectures for breast lesion malignancy and ACL tear detection, comparing RadImageNet and ImageNet pre-training with statistical analysis. Tested 1D convolutional classifiers with skip connections, ResNet50 backbones, and partial unfreezing strategies.

Result: Best models achieved AUCs of 0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy detection, competitive with previous works. No evidence found that RadImageNet pre-training provides superior performance over ImageNet for these specific medical tasks.

Conclusion: Optimal medical classification performance comes from specific architectural choices (1D conv with skip connections, ResNet50 backbones, partial unfreezing), but medical-specific pre-training (RadImageNet) doesn't necessarily outperform general pre-training (ImageNet) for the studied tasks.

Abstract: Modern computer vision models have proven to be highly useful for medical
imaging classification and segmentation tasks, but the scarcity of medical
imaging data often limits the efficacy of models trained from scratch. Transfer
learning has emerged as a pivotal solution to this, enabling the fine-tuning of
high-performance models on small data. Mei et al. (2022) found that
pre-training CNNs on a large dataset of radiologist-labeled images
(RadImageNet) enhanced model performance on downstream tasks compared to
ImageNet pretraining. The present work extends Mei et al. (2022) by conducting
a comprehensive investigation to determine optimal CNN architectures for breast
lesion malignancy detection and ACL tear detection, as well as performing
statistical analysis to compare the effect of RadImageNet and ImageNet
pre-training on downstream model performance. Our findings suggest that
1-dimensional convolutional classifiers with skip connections, ResNet50
pre-trained backbones, and partial backbone unfreezing yields optimal
downstream medical classification performance. Our best models achieve AUCs of
0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy
detection, competitive with the results reported by Mei et al. (2022) and
surpassing other previous works. We do not find evidence confirming RadImageNet
pre-training to provide superior downstream performance for ACL tear and breast
lesion classification tasks.

</details>


### [116] [MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation](https://arxiv.org/abs/2508.17568)
*Liane Makatura,Benjamin Jones,Siyuan Bian,Wojciech Matusik*

Main category: cs.CV

TL;DR: A framework with MetaDSL language, MetaDB repository, and MetaBench benchmarks for metamaterial design and analysis, enabling structure reconstruction, inverse design, and performance prediction.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in metamaterial design caused by geometric complexity and non-trivial mapping from architecture to behavior, providing integrated tools for design and understanding.

Method: Developed three components: (i) MetaDSL - a domain-specific language for metamaterial designs, (ii) MetaDB - a curated repository with 150,000+ parameterized programs and derivatives, (iii) MetaBench - benchmark suites for testing vision-language metamaterial assistants.

Result: Created a comprehensive framework with baselines from fine-tuned vision-language models and an interactive CAD-like interface. The repository includes geometry, renderings, and simulated elastic properties.

Conclusion: The framework provides a strong foundation for integrated design and understanding of structure-representation-property relationships in metamaterials, advancing the field through standardized tools and benchmarks.

Abstract: Metamaterials are micro-architected structures whose geometry imparts highly
tunable-often counter-intuitive-bulk properties. Yet their design is difficult
because of geometric complexity and a non-trivial mapping from architecture to
behaviour. We address these challenges with three complementary contributions.
(i) MetaDSL: a compact, semantically rich domain-specific language that
captures diverse metamaterial designs in a form that is both human-readable and
machine-parsable. (ii) MetaDB: a curated repository of more than 150,000
parameterized MetaDSL programs together with their
derivatives-three-dimensional geometry, multi-view renderings, and simulated
elastic properties. (iii) MetaBench: benchmark suites that test three core
capabilities of vision-language metamaterial assistants-structure
reconstruction, property-driven inverse design, and performance prediction. We
establish baselines by fine-tuning state-of-the-art vision-language models and
deploy an omni-model within an interactive, CAD-like interface. Case studies
show that our framework provides a strong first step toward integrated design
and understanding of structure-representation-property relationships.

</details>


### [117] [IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data](https://arxiv.org/abs/2508.17579)
*Meida Chen,Luis Leal,Yue Hu,Rong Liu,Butian Xiong,Andrew Feng,Jiuyi Xu,Yangming Shi*

Main category: cs.CV

TL;DR: IDU pipeline enables efficient incremental updates of 3D military training environments using minimal new imagery and AI-generated assets, reducing time and costs compared to full rebuilds.


<details>
  <summary>Details</summary>
Motivation: Military organizations need to maintain up-to-date 3D virtual environments for training, but frequent full-scale updates are time-consuming and expensive due to the dynamic nature of battlefield conditions where objects appear and vanish.

Method: Incremental Dynamic Update (IDU) pipeline: 1) Camera pose estimation to align new images with existing 3D model, 2) Change detection to identify modifications, 3) 3D generative AI to create new assets, 4) Human-guided integration of single objects at a time into existing 3D Gaussian Splatting models.

Result: Experimental results show the IDU pipeline significantly reduces update time and labor requirements compared to traditional full-scale reconstruction methods.

Conclusion: The IDU pipeline provides a cost-effective and targeted solution for maintaining current 3D models in rapidly evolving military scenarios, offering substantial efficiency improvements over conventional approaches.

Abstract: For simulation and training purposes, military organizations have made
substantial investments in developing high-resolution 3D virtual environments
through extensive imaging and 3D scanning. However, the dynamic nature of
battlefield conditions-where objects may appear or vanish over time-makes
frequent full-scale updates both time-consuming and costly. In response, we
introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently
updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with
only a small set of newly acquired images. Our approach starts with camera pose
estimation to align new images with the existing 3D model, followed by change
detection to pinpoint modifications in the scene. A 3D generative AI model is
then used to create high-quality 3D assets of the new elements, which are
seamlessly integrated into the existing 3D model. The IDU pipeline incorporates
human guidance to ensure high accuracy in object identification and placement,
with each update focusing on a single new object at a time. Experimental
results confirm that our proposed IDU pipeline significantly reduces update
time and labor, offering a cost-effective and targeted solution for maintaining
up-to-date 3D models in rapidly evolving military scenarios.

</details>


### [118] [HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](https://arxiv.org/abs/2508.17588)
*Quanjian Song,Xinyu Wang,Donghao Zhou,Jingyu Lin,Cunjian Chen,Yue Ma,Xiu Li*

Main category: cs.CV

TL;DR: HERO is a training-free hierarchical acceleration framework that speeds up diffusion-based world models by 1.73× with minimal quality loss, using patch-wise refresh for shallow layers and linear extrapolation for deeper layers.


<details>
  <summary>Details</summary>
Motivation: Generation-driven world models using diffusion models suffer from slow inference due to their iterative nature, and existing acceleration techniques cause quality degradation when applied to world models.

Method: HERO uses hierarchical strategies: (1) patch-wise refresh mechanism with sampling and frequency-aware tracking for shallow layers with high temporal variability, and (2) linear extrapolation scheme that bypasses attention and feed-forward computations for deeper stable layers.

Result: HERO achieves 1.73× speedup with minimal quality degradation, significantly outperforming existing diffusion acceleration methods.

Conclusion: The hierarchical approach effectively accelerates world model inference by leveraging the different characteristics of shallow and deep layers, providing substantial speed improvements while maintaining quality.

Abstract: Generation-driven world models create immersive virtual environments but
suffer slow inference due to the iterative nature of diffusion models. While
recent advances have improved diffusion model efficiency, directly applying
these techniques to world models introduces limitations such as quality
degradation. In this paper, we present HERO, a training-free hierarchical
acceleration framework tailored for efficient world models. Owing to the
multi-modal nature of world models, we identify a feature coupling phenomenon,
wherein shallow layers exhibit high temporal variability, while deeper layers
yield more stable feature representations. Motivated by this, HERO adopts
hierarchical strategies to accelerate inference: (i) In shallow layers, a
patch-wise refresh mechanism efficiently selects tokens for recomputation. With
patch-wise sampling and frequency-aware tracking, it avoids extra metric
computation and remain compatible with FlashAttention. (ii) In deeper layers, a
linear extrapolation scheme directly estimates intermediate features. This
completely bypasses the computations in attention modules and feed-forward
networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with
minimal quality degradation, significantly outperforming existing diffusion
acceleration methods.

</details>


### [119] [TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints](https://arxiv.org/abs/2508.17595)
*Vinh-Thuan Ly,Hoang M. Truong,Xuan-Huong Nguyen*

Main category: cs.CV

TL;DR: TinyGiantVLM is a lightweight two-stage VLM framework that addresses spatial reasoning challenges in warehouse environments using RGB+depth inputs and MoE fusion, achieving strong performance on industrial spatial tasks.


<details>
  <summary>Details</summary>
Motivation: Existing VLMs struggle with fine-grained spatial reasoning in 3D warehouse environments, particularly with understanding object arrangements and multimodal cues in industrial settings.

Method: Two-stage framework with global+region feature encoding from RGB/depth modalities, MoE fusion module for dynamic spatial representation combination, and two-phase training (free-form answers then normalized evaluation).

Result: 64M-parameter base model achieved 5th place on AI City Challenge 2025 Track 3 with 66.8861 score; 80M-parameter variant with expanded MoE showed improved spatial reasoning performance.

Conclusion: The framework effectively bridges visual perception and spatial understanding in industrial environments, demonstrating that lightweight modular architectures can handle complex spatial reasoning tasks.

Abstract: Reasoning about fine-grained spatial relationships in warehouse-scale
environments poses a significant challenge for existing vision-language models
(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and
multimodal cues in real-world industrial settings. In this paper, we present
TinyGiantVLM, a lightweight and modular two-stage framework designed for
physical spatial reasoning, distinguishing itself from traditional geographic
reasoning in complex logistics scenes. Our approach encodes both global and
region-level features from RGB and depth modalities using pretrained visual
backbones. To effectively handle the complexity of high-modality inputs and
diverse question types, we incorporate a Mixture-of-Experts (MoE) fusion
module, which dynamically combines spatial representations to support
downstream reasoning tasks and improve convergence. Training is conducted in a
two-phase strategy: the first phase focuses on generating free-form answers to
enhance spatial reasoning ability, while the second phase uses normalized
answers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our
64M-parameter base model achieved 5th place on the leaderboard with a score of
66.8861, demonstrating strong performance in bridging visual perception and
spatial understanding in industrial environments. We further present an
80M-parameter variant with expanded MoE capacity, which demonstrates improved
performance on spatial reasoning tasks.

</details>


### [120] [HotSpotter - Patterned Species Instance Recognition](https://arxiv.org/abs/2508.17605)
*Jonathan P. Crall,Charles V. Stewart,Tanya Y. Berger-Wolf,Daniel I. Rubenstein,Siva R. Sundaresan*

Main category: cs.CV

TL;DR: HotSpotter is a fast, accurate algorithm for individual animal identification across multiple species using keypoint matching and competitive scoring mechanisms.


<details>
  <summary>Details</summary>
Motivation: To develop a species-agnostic algorithm for identifying individual animals from images that is both faster and more accurate than existing methods, enabling large-scale wildlife monitoring and conservation efforts.

Method: Two approaches: 1) Sequential matching of query images against database images using keypoint/hotspot extraction and scoring, 2) Fast nearest neighbor search with competitive scoring derived from Local Naive Bayes Nearest Neighbor algorithm for instance recognition.

Result: Successfully tested on databases of over 1000 images across multiple species (zebras, giraffes, leopards, lionfish), achieving higher accuracy than published methods with query matching in just a few seconds.

Conclusion: HotSpotter provides an efficient and accurate solution for individual animal identification that works across multiple species, making it valuable for wildlife research and conservation applications requiring large-scale image analysis.

Abstract: We present HotSpotter, a fast, accurate algorithm for identifying individual
animals against a labeled database. It is not species specific and has been
applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We
describe two approaches, both based on extracting and matching keypoints or
"hotspots". The first tests each new query image sequentially against each
database image, generating a score for each database image in isolation, and
ranking the results. The second, building on recent techniques for instance
recognition, matches the query image against the database using a fast nearest
neighbor search. It uses a competitive scoring mechanism derived from the Local
Naive Bayes Nearest Neighbor algorithm recently proposed for category
recognition. We demonstrate results on databases of more than 1000 images,
producing more accurate matches than published methods and matching each query
image in just a few seconds.

</details>


### [121] [A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores](https://arxiv.org/abs/2508.17613)
*Nur Amirah Abd Hamid,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: A weighted Vision Transformer multi-task learning framework that jointly predicts ADAS-Cog global score and 13 sub-scores from baseline MRI scans, with sub-score-specific loss weighting to improve predictive accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus only on predicting the global ADAS-Cog score while overlooking the predictive value of its 13 sub-scores, which reflect distinct cognitive domains and may have varying influence on the global score.

Method: Proposed a weighted ViT-based multi-task learning framework that integrates Vision Transformer as feature extractor and systematically investigates sub-score-specific loss weighting strategies for joint prediction of global score and sub-scores at Month 24.

Result: Weighting strategies are group-dependent: strong weighting improves performance for MCI subjects with more heterogeneous MRI patterns, while moderate weighting is more effective for CN subjects with lower variability. Uniform weighting underutilizes key sub-scores.

Conclusion: The framework provides a flexible, interpretable approach to AD prognosis using end-to-end MRI-based learning, with sub-score-specific weighting enhancing both predictive accuracy and clinical interpretability.

Abstract: Prognostic modeling is essential for forecasting future clinical scores and
enabling early detection of Alzheimers disease (AD). While most existing
methods focus on predicting the ADAS-Cog global score, they often overlook the
predictive value of its 13 sub-scores, which reflect distinct cognitive
domains. Some sub-scores may exert greater influence on determining global
scores. Assigning higher loss weights to these clinically meaningful sub-scores
can guide the model to focus on more relevant cognitive domains, enhancing both
predictive accuracy and interpretability. In this study, we propose a weighted
Vision Transformer (ViT)-based multi-task learning (MTL) framework to jointly
predict the ADAS-Cog global score using baseline MRI scans and its 13
sub-scores at Month 24. Our framework integrates ViT as a feature extractor and
systematically investigates the impact of sub-score-specific loss weighting on
model performance. Results show that our proposed weighting strategies are
group-dependent: strong weighting improves performance for MCI subjects with
more heterogeneous MRI patterns, while moderate weighting is more effective for
CN subjects with lower variability. Our findings suggest that uniform weighting
underutilizes key sub-scores and limits generalization. The proposed framework
offers a flexible, interpretable approach to AD prognosis using end-to-end
MRI-based learning. (Github repo link will be provided after review)

</details>


### [122] [JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on](https://arxiv.org/abs/2508.17614)
*Aowen Wang,Wei Li,Hao Luo,Mengxing Ao,Chenyu Zhu,Xinyang Li,Fan Wang*

Main category: cs.CV

TL;DR: JCo-MVTON is a mask-free virtual try-on system using multi-modal diffusion transformers that integrates reference person and target garment images directly into the denoising process, achieving state-of-the-art performance with strong real-world generalization.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional virtual try-on systems that rely heavily on human body masks, have limited fine-grained control over garment attributes, and poor generalization to real-world scenarios.

Method: Uses Multi-Modal Diffusion Transformer (MM-DiT) backbone with dedicated conditional pathways that fuse reference person and target garment features within self-attention layers. Includes refined positional encodings and attention masks for spatial alignment. Employs bidirectional generation strategy with mask-based model for reference images and self-supervised "Try-Off" model for garment recovery, followed by manual curation.

Result: Achieves state-of-the-art performance on DressCode benchmark, significantly outperforming existing methods in both quantitative metrics and human evaluations. Shows strong generalization in real-world applications, surpassing commercial systems.

Conclusion: JCo-MVTON successfully addresses key limitations of virtual try-on systems through its multi-modal diffusion transformer approach and innovative dataset construction strategy, demonstrating superior performance and real-world applicability.

Abstract: Virtual try-on systems have long been hindered by heavy reliance on human
body masks, limited fine-grained control over garment attributes, and poor
generalization to real-world, in-the-wild scenarios. In this paper, we propose
JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free
Virtual Try-On), a novel framework that overcomes these limitations by
integrating diffusion-based image generation with multi-modal conditional
fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our
approach directly incorporates diverse control signals -- such as the reference
person image and the target garment image -- into the denoising process through
dedicated conditional pathways that fuse features within the self-attention
layers. This fusion is further enhanced with refined positional encodings and
attention masks, enabling precise spatial alignment and improved garment-person
integration. To address data scarcity and quality, we introduce a bidirectional
generation strategy for dataset construction: one pipeline uses a mask-based
model to generate realistic reference images, while a symmetric ``Try-Off''
model, trained in a self-supervised manner, recovers the corresponding garment
images. The synthesized dataset undergoes rigorous manual curation, allowing
iterative improvement in visual fidelity and diversity. Experiments demonstrate
that JCo-MVTON achieves state-of-the-art performance on public benchmarks
including DressCode, significantly outperforming existing methods in both
quantitative metrics and human evaluations. Moreover, it shows strong
generalization in real-world applications, surpassing commercial systems.

</details>


### [123] [Improving Interpretability in Alzheimer's Prediction via Joint Learning of ADAS-Cog Scores](https://arxiv.org/abs/2508.17619)
*Nur Amirah Abd Hamid,Mohd Shahrizal Rusli,Muhammad Thaqif Iman Mohd Taufek,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: Multi-task learning framework using Vision Transformer and Swin Transformer to jointly predict global ADAS-Cog score and its 13 sub-scores from baseline MRI and longitudinal clinical data, showing improved global prediction but revealing model instability due to clinical feature dominance.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus only on global ADAS-Cog score prediction and overlook the predictive value of domain-specific sub-scores, which could provide better insights into cognitive decline patterns in Alzheimer's disease.

Method: Proposed multi-task learning framework using Vision Transformer and Swin Transformer architectures to extract imaging features from baseline MRI, fused with longitudinal clinical scores from baseline and Month 6 to jointly predict global score and 13 sub-scores at Month 24.

Result: Incorporating sub-score learning improves global score prediction. Analysis revealed Q1 (Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) dominate global score prediction, but some influential sub-scores show high prediction errors due to clinical feature dominance over MRI features.

Conclusion: Sub-score informed modeling provides value for AD prediction, but highlights need for improved multimodal fusion and adaptive loss weighting to achieve balanced learning and build more interpretable, clinically robust frameworks.

Abstract: Accurate prediction of clinical scores is critical for early detection and
prognosis of Alzheimers disease (AD). While existing approaches primarily focus
on forecasting the ADAS-Cog global score, they often overlook the predictive
value of its sub-scores (13 items), which capture domain-specific cognitive
decline. In this study, we propose a multi task learning (MTL) framework that
jointly predicts the global ADAS-Cog score and its sub-scores (13 items) at
Month 24 using baseline MRI and longitudinal clinical scores from baseline and
Month 6. The main goal is to examine how each sub scores particularly those
associated with MRI features contribute to the prediction of the global score,
an aspect largely neglected in prior MTL studies. We employ Vision Transformer
(ViT) and Swin Transformer architectures to extract imaging features, which are
fused with longitudinal clinical inputs to model cognitive progression. Our
results show that incorporating sub-score learning improves global score
prediction. Subscore level analysis reveals that a small subset especially Q1
(Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) consistently
dominates the predicted global score. However, some of these influential
sub-scores exhibit high prediction errors, pointing to model instability.
Further analysis suggests that this is caused by clinical feature dominance,
where the model prioritizes easily predictable clinical scores over more
complex MRI derived features. These findings emphasize the need for improved
multimodal fusion and adaptive loss weighting to achieve more balanced
learning. Our study demonstrates the value of sub score informed modeling and
provides insights into building more interpretable and clinically robust AD
prediction frameworks. (Github repo provided)

</details>


### [124] [Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes](https://arxiv.org/abs/2508.17634)
*Ryan Faulkner,Ian Reid,Simon Ratcliffe,Tat-Jun Chin*

Main category: cs.CV

TL;DR: Novel open-set segmentation approach for outdoor LiDAR point clouds using Mamba architecture and reconstruction-based methods, improving performance on existing methods and handling outlier objects effectively.


<details>
  <summary>Details</summary>
Motivation: Outdoor LiDAR scanning produces large-scale point clouds used in robotics, automotive, and surveillance applications where outlier objects from outside training data inevitably appear, requiring robust open-set segmentation.

Method: Combines object defect-detection learnings with Mamba architecture's long-range dependency handling and scalability. Uses reconstruction-based approach for outdoor scene open-set segmentation with Mamba-based architecture.

Result: Approach improves performance when applied to both the authors' own method and existing methods. Mamba-based architecture is competitive with voxel-convolution methods on challenging large-scale point clouds.

Conclusion: The proposed reconstruction-based approach using Mamba architecture effectively addresses open-set segmentation in outdoor LiDAR scenes, demonstrating improved performance and competitiveness with existing methods on large-scale point cloud data.

Abstract: LiDAR scanning in outdoor scenes acquires accurate distance measurements over
wide areas, producing large-scale point clouds. Application examples for this
data include robotics, automotive vehicles, and land surveillance. During such
applications, outlier objects from outside the training data will inevitably
appear. Our research contributes a novel approach to open-set segmentation,
leveraging the learnings of object defect-detection research. We also draw on
the Mamba architecture's strong performance in utilising long-range
dependencies and scalability to large data. Combining both, we create a
reconstruction based approach for the task of outdoor scene open-set
segmentation. We show that our approach improves performance not only when
applied to our our own open-set segmentation method, but also when applied to
existing methods. Furthermore we contribute a Mamba based architecture which is
competitive with existing voxel-convolution based methods on challenging,
large-scale pointclouds.

</details>


### [125] [Wound3DAssist: A Practical Framework for 3D Wound Assessment](https://arxiv.org/abs/2508.17635)
*Remi Chierchia,Rodrigo Santa Cruz,Léo Lebrat,Yulia Arzhaeva,Mohammad Ali Armin,Jeremy Oorloff,Chuong Nguyen,Olivier Salvado,Clinton Fookes,David Ahmedt-Aristizabal*

Main category: cs.CV

TL;DR: Wound3DAssist is a 3D wound assessment framework using monocular smartphone videos that overcomes limitations of 2D methods by providing accurate 3D models, automatic measurements, and tissue analysis in under 20 minutes.


<details>
  <summary>Details</summary>
Motivation: Current 2D digital wound assessment methods suffer from perspective distortion, limited field of view, inability to capture depth, and are subjective and time-consuming, especially for complex anatomical regions.

Method: A practical framework that generates accurate 3D models from short handheld smartphone videos, integrating 3D reconstruction, wound segmentation, tissue classification, and periwound analysis into a modular workflow.

Result: The framework achieves millimeter-level accuracy, supports high-quality wound bed visualization, provides reliable tissue composition analysis, and completes full assessments in under 20 minutes across digital models, silicone phantoms, and real patients.

Conclusion: Wound3DAssist demonstrates feasibility for real-world clinical use by providing view-independent, robust 3D wound assessment that overcomes the limitations of traditional 2D methods while maintaining practical assessment times.

Abstract: Managing chronic wounds remains a major healthcare challenge, with clinical
assessment often relying on subjective and time-consuming manual documentation
methods. Although 2D digital videometry frameworks aided the measurement
process, these approaches struggle with perspective distortion, a limited field
of view, and an inability to capture wound depth, especially in anatomically
complex or curved regions. To overcome these limitations, we present
Wound3DAssist, a practical framework for 3D wound assessment using monocular
consumer-grade videos. Our framework generates accurate 3D models from short
handheld smartphone video recordings, enabling non-contact, automatic
measurements that are view-independent and robust to camera motion. We
integrate 3D reconstruction, wound segmentation, tissue classification, and
periwound analysis into a modular workflow. We evaluate Wound3DAssist across
digital models with known geometry, silicone phantoms, and real patients.
Results show that the framework supports high-quality wound bed visualization,
millimeter-level accuracy, and reliable tissue composition analysis. Full
assessments are completed in under 20 minutes, demonstrating feasibility for
real-world clinical use.

</details>


### [126] [Few-Shot Pattern Detection via Template Matching and Regression](https://arxiv.org/abs/2508.17636)
*Eunchan Jo,Dahyun Kang,Sanghyun Kim,Yunseon Choi,Minsu Cho*

Main category: cs.CV

TL;DR: Proposes TMR, a simple yet effective few-shot pattern detector using template matching and regression, outperforming state-of-the-art methods on multiple benchmarks including a new RPINE dataset.


<details>
  <summary>Details</summary>
Motivation: Previous few-shot object counting and detection methods fail to localize non-object patterns and lose structural information by collapsing exemplars into prototypes, limiting their applicability to broader pattern detection tasks.

Method: Revisits classic template matching and regression with a minimalistic structure using learnable convolutional/projection layers on a frozen backbone to preserve spatial layout of exemplars.

Result: Outperforms state-of-the-art methods on RPINE, FSCD-147, and FSCD-LVIS benchmarks, demonstrating strong generalization in cross-dataset evaluation.

Conclusion: Template matching and regression with preserved spatial information provides an effective solution for few-shot pattern detection across diverse pattern types beyond object categories.

Abstract: We address the problem of few-shot pattern detection, which aims to detect
all instances of a given pattern, typically represented by a few exemplars,
from an input image. Although similar problems have been studied in few-shot
object counting and detection (FSCD), previous methods and their benchmarks
have narrowed patterns of interest to object categories and often fail to
localize non-object patterns. In this work, we propose a simple yet effective
detector based on template matching and regression, dubbed TMR. While previous
FSCD methods typically represent target exemplars as spatially collapsed
prototypes and lose structural information, we revisit classic template
matching and regression. It effectively preserves and leverages the spatial
layout of exemplars through a minimalistic structure with a small number of
learnable convolutional or projection layers on top of a frozen backbone We
also introduce a new dataset, dubbed RPINE, which covers a wider range of
patterns than existing object-centric datasets. Our method outperforms the
state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and
FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.

</details>


### [127] [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning](https://arxiv.org/abs/2508.17638)
*Xinyu Wei,Guoli Yang,Jialu Zhou,Mingyue Yang,Leqian Li,Kedi Zhang,Chunping Qiu*

Main category: cs.CV

TL;DR: DEHVF is an efficient vision-language fine-tuning method that dynamically fuses hierarchical visual features into LLMs to avoid sequence expansion while maintaining cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: Current LVLMs suffer from increased input sequence length when concatenating visual features with text tokens, causing computational overhead. Existing fusion methods neglect hierarchical semantic representations and fine-grained visual information from shallower layers.

Method: Proposes DEHVF with lightweight hierarchical visual fuser that dynamically selects and fuses hierarchical visual features based on LLM layer representations. Projects and aligns fused features before embedding into corresponding FFN layers of LLMs.

Result: Achieves higher accuracy than existing PEFT baselines on VL benchmarks including ScienceQA visual question answering and COCO Captions image captioning, while maintaining efficient training and inference.

Conclusion: DEHVF effectively addresses sequence expansion issues by dynamically fusing multi-layer visual information, enabling precise cross-modal alignment at same semantic granularity with minimal parameter fine-tuning.

Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects
visual features and then concatenates them with text tokens to form a unified
sequence input for Large Language Models (LLMs). However, this paradigm leads
to a significant increase in the length of the input sequence, resulting in
substantial computational overhead. Existing methods attempt to fuse visual
information into the intermediate layers of LLMs, which alleviate the sequence
length issue but often neglect the hierarchical semantic representations within
the model and the fine-grained visual information available in the shallower
visual encoding layers. To address this limitation, we propose DEHVF, an
efficient vision-language fine-tuning method based on dynamic embedding and
fusion of hierarchical visual features. Its core lies in leveraging the
inherent hierarchical representation characteristics of visual encoders and
language models. Through a lightweight hierarchical visual fuser, it
dynamically selects and fuses hierarchical features corresponding to semantic
granularity based on the internal representations of each layer in LLMs. The
fused layer-related visual features are then projected and aligned before being
directly embedded into the Feed-Forward Network (FFN) of the corresponding
layer in LLMs. This approach not only avoids sequence expansion but also
dynamically fuses multi-layer visual information. By fine-tuning only a small
number of parameters, DEHVF achieves precise alignment and complementarity of
cross-modal information at the same semantic granularity. We conducted
experiments across various VL benchmarks, including visual question answering
on ScienceQA and image captioning on COCO Captions. The results demonstrate
that DEHVF achieves higher accuracy than existing parameter-efficient
fine-tuning (PEFT) baselines while maintaining efficient training and
inference.

</details>


### [128] [HyTver: A Novel Loss Function for Longitudinal Multiple Sclerosis Lesion Segmentation](https://arxiv.org/abs/2508.17639)
*Dayan Perera,Ting Fung Fung,Vishnu Monn*

Main category: cs.CV

TL;DR: Proposes HyTver, a novel hybrid loss function for longitudinal MS lesion segmentation that addresses data imbalance issues while maintaining performance across multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Longitudinal MS lesion segmentation faces input/output imbalance challenges. Existing loss functions (Dice, Cross-Entropy) are inadequate - either too computationally complex or perform poorly on non-region-based metrics.

Method: Developed HyTver, a hybrid loss function designed to mitigate imbalance problems without the computational complexity of hyperparameter exponents, while maintaining performance across different metric types.

Result: Achieved Dice score of 0.659 while ensuring distance-based metrics remain comparable to other popular loss functions. Demonstrated stability when used on pre-trained models.

Conclusion: HyTver provides an effective solution for MS lesion segmentation imbalance, offering good segmentation performance while maintaining balanced metrics across different evaluation dimensions.

Abstract: Longitudinal Multiple Sclerosis Lesion Segmentation is a particularly
challenging problem that involves both input and output imbalance in the data
and segmentation. Therefore in order to develop models that are practical, one
of the solutions is to develop better loss functions. Most models naively use
either Dice loss or Cross-Entropy loss or their combination without too much
consideration. However, one must select an appropriate loss function as the
imbalance can be mitigated by selecting a proper loss function. In order to
solve the imbalance problem, multiple loss functions were proposed that claimed
to solve it. They come with problems of their own which include being too
computationally complex due to hyperparameters as exponents or having
detrimental performance in metrics other than region-based ones. We propose a
novel hybrid loss called HyTver that achieves good segmentation performance
while maintaining performance in other metrics. We achieve a Dice score of
0.659 while also ensuring that the distance-based metrics are comparable to
other popular functions. In addition, we also evaluate the stability of the
loss functions when used on a pre- trained model and perform extensive
comparisons with other popular loss functions

</details>


### [129] [FloraSyntropy-Net: Scalable Deep Learning with Novel FloraSyntropy Archive for Large-Scale Plant Disease Diagnosis](https://arxiv.org/abs/2508.17653)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FloraSyntropy-Net: A federated learning framework with memetic algorithm optimization that achieves 96.38% accuracy on plant disease detection and demonstrates exceptional generalization (99.84% on unrelated pest dataset).


<details>
  <summary>Details</summary>
Motivation: Address the lack of generalization in existing AI solutions for plant disease diagnosis, which are typically constrained to specific species and fail to perform across diverse agricultural settings.

Method: Propose FloraSyntropy-Net - a federated learning framework integrating Memetic Algorithm for optimal base model selection (DenseNet201), a novel Deep Block for enhanced feature representation, and client-cloning strategy for scalable privacy-preserving training.

Result: Achieves state-of-the-art 96.38% accuracy on FloraSyntropy benchmark (178,922 images, 35 plant species, 97 disease classes) and exceptional 99.84% accuracy on unrelated Pest dataset, demonstrating superior generalization.

Conclusion: Provides both a valuable new dataset resource and a robust, highly generalizable framework that advances practical large-scale agricultural AI applications with strong privacy preservation.

Abstract: Early diagnosis of plant diseases is critical for global food safety, yet
most AI solutions lack the generalization required for real-world agricultural
diversity. These models are typically constrained to specific species, failing
to perform accurately across the broad spectrum of cultivated plants. To
address this gap, we first introduce the FloraSyntropy Archive, a large-scale
dataset of 178,922 images across 35 plant species, annotated with 97 distinct
disease classes. We establish a benchmark by evaluating numerous existing
models on this archive, revealing a significant performance gap. We then
propose FloraSyntropy-Net, a novel federated learning framework (FL) that
integrates a Memetic Algorithm (MAO) for optimal base model selection
(DenseNet201), a novel Deep Block for enhanced feature representation, and a
client-cloning strategy for scalable, privacy-preserving training.
FloraSyntropy-Net achieves a state-of-the-art accuracy of 96.38% on the
FloraSyntropy benchmark. Crucially, to validate its generalization capability,
we test the model on the unrelated multiclass Pest dataset, where it
demonstrates exceptional adaptability, achieving 99.84% accuracy. This work
provides not only a valuable new resource but also a robust and highly
generalizable framework that advances the field towards practical, large-scale
agricultural AI applications.

</details>


### [130] [Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark](https://arxiv.org/abs/2508.17658)
*Yaolei Qi,Yikai Yang,Wenbo Peng,Shumei Miao,Yutao Hu,Guanyu Yang*

Main category: cs.CV

TL;DR: A novel point cloud-based approach for tubular structure completion that addresses discontinuity issues in medical imaging, featuring a new dataset and TSRNet with superior performance over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation algorithms struggle with structural discontinuities in tubular structures like coronary arteries, particularly in severe clinical cases such as stenosis and occlusions, which compromises diagnostic accuracy.

Method: Proposes TSRNet (Tubular Structure Reconnection Network) with detail-preserved feature extractor, multiple dense refinement strategy, and global-to-local loss function. Establishes PC-CAC dataset from real clinical data.

Result: Outperforms state-of-the-art approaches across multiple evaluation metrics on PC-CAC and two additional public datasets (PC-ImageCAS and PC-PTR), setting new benchmark for point cloud-based tubular structure reconstruction.

Conclusion: The method effectively reconnects discontinuous tubular structures while maintaining structural integrity, providing a valuable solution for medical imaging and computer-assisted diagnosis with publicly available benchmark.

Abstract: Complex tubular structures are essential in medical imaging and
computer-assisted diagnosis, where their integrity enhances anatomical
visualization and lesion detection. However, existing segmentation algorithms
struggle with structural discontinuities, particularly in severe clinical cases
such as coronary artery stenosis and vessel occlusions, which leads to
undesired discontinuity and compromising downstream diagnostic accuracy.
Therefore, it is imperative to reconnect discontinuous structures to ensure
their completeness. In this study, we explore the tubular structure completion
based on point cloud for the first time and establish a Point Cloud-based
Coronary Artery Completion (PC-CAC) dataset, which is derived from real
clinical data. This dataset provides a novel benchmark for tubular structure
completion. Additionally, we propose TSRNet, a Tubular Structure Reconnection
Network that integrates a detail-preservated feature extractor, a multiple
dense refinement strategy, and a global-to-local loss function to ensure
accurate reconnection while maintaining structural integrity. Comprehensive
experiments on our PC-CAC and two additional public datasets (PC-ImageCAS and
PC-PTR) demonstrate that our method consistently outperforms state-of-the-art
approaches across multiple evaluation metrics, setting a new benchmark for
point cloud-based tubular structure reconstruction. Our benchmark is available
at https://github.com/YaoleiQi/PCCAC.

</details>


### [131] [M^3-GloDets: Multi-Region and Multi-Scale Analysis of Fine-Grained Diseased Glomerular Detection](https://arxiv.org/abs/2508.17666)
*Tianyu Shi,Xinzi He,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: M^3-GloDet framework systematically evaluates glomeruli detection models across regions, scales, and classes, finding optimal performance with intermediate patch sizes and moderate magnifications.


<details>
  <summary>Details</summary>
Motivation: Current computer vision research focuses mainly on normal or globally sclerotic glomeruli, leaving diseased glomerular subtypes understudied despite their clinical importance and morphological complexity.

Method: Developed M^3-GloDet framework to evaluate benchmark and state-of-the-art detection models using diverse region-of-interest sizes and imaging resolutions on a multi-class diseased glomerular dataset.

Result: Intermediate patch sizes provided the best balance between context and efficiency, while moderate magnifications enhanced generalization by reducing overfitting.

Conclusion: The systematic evaluation provides actionable insights for refining automated detection strategies and clinical workflows in digital renal pathology, advancing understanding of model strengths and limitations.

Abstract: Accurate detection of diseased glomeruli is fundamental to progress in renal
pathology and underpins the delivery of reliable clinical diagnoses. Although
recent advances in computer vision have produced increasingly sophisticated
detection algorithms, the majority of research efforts have focused on normal
glomeruli or instances of global sclerosis, leaving the wider spectrum of
diseased glomerular subtypes comparatively understudied. This disparity is not
without consequence; the nuanced and highly variable morphological
characteristics that define these disease variants frequently elude even the
most advanced computational models. Moreover, ongoing debate surrounds the
choice of optimal imaging magnifications and region-of-view dimensions for
fine-grained glomerular analysis, adding further complexity to the pursuit of
accurate classification and robust segmentation.
  To bridge these gaps, we present M^3-GloDet, a systematic framework designed
to enable thorough evaluation of detection models across a broad continuum of
regions, scales, and classes. Within this framework, we evaluate both
long-standing benchmark architectures and recently introduced state-of-the-art
models that have achieved notable performance, using an experimental design
that reflects the diversity of region-of-interest sizes and imaging resolutions
encountered in routine digital renal pathology. As the results, we found that
intermediate patch sizes offered the best balance between context and
efficiency. Additionally, moderate magnifications enhanced generalization by
reducing overfitting. Through systematic comparison of these approaches on a
multi-class diseased glomerular dataset, our aim is to advance the
understanding of model strengths and limitations, and to offer actionable
insights for the refinement of automated detection strategies and clinical
workflows in the digital pathology domain.

</details>


### [132] [Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection](https://arxiv.org/abs/2508.17667)
*Runhe Lai,Xinhua Lu,Kanghao Chen,Qichao Chen,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: A novel vision-language model framework for medical OOD detection that uses cross-scale visual fusion and hard pseudo-OOD generation to better identify unknown diseases resembling known ones.


<details>
  <summary>Details</summary>
Motivation: To improve trustworthy medical diagnosis by detecting out-of-distribution (OOD) samples that represent unknown diseases, reducing misdiagnosis risks when unknown diseases resemble known ones.

Method: Proposes a vision-language model framework with cross-scale visual fusion to integrate hierarchical visual information from multiple scales, and cross-scale hard pseudo-OOD sample generation to enhance detection capabilities.

Result: Experimental evaluations on three public medical datasets show superior OOD detection performance compared to existing methods.

Conclusion: The proposed framework effectively improves discrimination of challenging unknown diseases in medical images through multi-scale visual integration and pseudo-OOD generation strategies.

Abstract: In trustworthy medical diagnosis systems, integrating out-of-distribution
(OOD) detection aims to identify unknown diseases in samples, thereby
mitigating the risk of misdiagnosis. In this study, we propose a novel OOD
detection framework based on vision-language models (VLMs), which integrates
hierarchical visual information to cope with challenging unknown diseases that
resemble known diseases. Specifically, a cross-scale visual fusion strategy is
proposed to couple visual embeddings from multiple scales. This enriches the
detailed representation of medical images and thus improves the discrimination
of unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation
strategy is proposed to benefit OOD detection maximally. Experimental
evaluations on three public medical datasets support that the proposed
framework achieves superior OOD detection performance compared to existing
methods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.

</details>


### [133] [Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing](https://arxiv.org/abs/2508.17686)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: LGTTP is a language-guided temporal token pruning method that reduces computational complexity in VLMs for long-form videos by adaptively pruning tokens based on temporal cues from queries, achieving 65% computation reduction while maintaining 97-99% performance.


<details>
  <summary>Details</summary>
Motivation: Vision Language Models struggle with long-form videos due to quadratic attention complexity, requiring more efficient methods to handle temporal information without sacrificing performance.

Method: Language-Guided Temporal Token Pruning (LGTTP) leverages query temporal cues to adaptively prune video tokens, preserving higher token density in relevant segments while being model-agnostic and integrable with existing frameworks.

Result: Achieves 65% computation reduction while preserving 97-99% original performance. Improves HIT@1 by +9.5% on QVHighlights and retains 99.6% of R@1 on Charades-STA. Excels with temporal markers and general video tasks.

Conclusion: LGTTP effectively addresses computational bottlenecks in VLMs for long videos through adaptive token pruning, maintaining near-original performance while significantly reducing computational overhead across various video understanding tasks.

Abstract: Vision Language Models (VLMs) struggle with long-form videos due to the
quadratic complexity of attention mechanisms. We propose Language-Guided
Temporal Token Pruning (LGTTP), which leverages temporal cues from queries to
adaptively prune video tokens, preserving contextual continuity while reducing
computational overhead. Unlike uniform pruning or keyframe selection, LGTTP
retains higher token density in temporally relevant segments. Our
model-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a
65% reduction in computation while preserving 97-99% of the original
performance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on
Charades-STA, it retains 99.6% of R@1. It excels on queries with explicit
temporal markers and remains effective across general video understanding
tasks.

</details>


### [134] [Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset](https://arxiv.org/abs/2508.17699)
*Z. Rafati,M. Hoseyni,J. Khoramdel,A. Nikoofard*

Main category: cs.CV

TL;DR: This paper quantitatively compares 9 CAM methods for brain hemorrhage detection, finding HiResCAM and AblationCAM perform best for localization and segmentation respectively, establishing a benchmark for XAI in medical imaging.


<details>
  <summary>Details</summary>
Motivation: To increase transparency and clinical trust in deep learning models for medical imaging by investigating explainable AI (XAI) techniques, specifically Class Activation Mapping methods for brain hemorrhage diagnosis.

Method: Developed a pipeline to extract pixel-level segmentation from classification models using 9 CAM algorithms applied across multiple network stages. Quantitative evaluation on Hemorica dataset using Dice, IoU, and pixel-wise overlap metrics.

Result: Best localization at stage 5 of EfficientNetV2S, with HiResCAM achieving highest bounding-box alignment and AblationCAM achieving best pixel-level performance (Dice: 0.57, IoU: 0.40) despite models being trained only for classification.

Conclusion: Establishes a reproducible benchmark for CAM methods in brain hemorrhage detection and demonstrates the potential of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.

Abstract: Explainable Artificial Intelligence (XAI) has become an essential component
of medical imaging research, aiming to increase transparency and clinical trust
in deep learning models. This study investigates brain hemorrhage diagnosis
with a focus on explainability through Class Activation Mapping (CAM)
techniques. A pipeline was developed to extract pixellevel segmentation and
detection annotations from classification models using nine state-of-the-art
CAM algorithms, applied across multiple network stages, and quantitatively
evaluated on the Hemorica dataset, which uniquely provides both slice-level
labels and high-quality segmentation masks. Metrics including Dice, IoU, and
pixel-wise overlap were employed to benchmark CAM variants. Results show that
the strongest localization performance occurred at stage 5 of EfficientNetV2S,
with HiResCAM yielding the highest bounding-box alignment and AblationCAM
achieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong
accuracy given that models were trained solely for classification without
segmentation supervision. To the best of current knowledge, this is among the f
irst works to quantitatively compare CAM methods for brain hemorrhage
detection, establishing a reproducible benchmark and underscoring the potential
of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.

</details>


### [135] [CATformer: Contrastive Adversarial Transformer for Image Super-Resolution](https://arxiv.org/abs/2508.17708)
*Qinyi Tian,Spence Cox,Laura E. Dalton*

Main category: cs.CV

TL;DR: CATformer is a novel neural network that combines diffusion-inspired feature refinement with adversarial and contrastive learning for super-resolution, outperforming existing transformer-based and diffusion methods in both efficiency and visual quality.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between transformer-, diffusion-, and GAN-based methods in super-resolution by integrating their strengths into a unified architecture for practical applications.

Method: Uses a dual-branch architecture with a primary diffusion-inspired transformer for progressive latent refinement and an auxiliary transformer branch for noise robustness through learned latent contrasts, fused and decoded using Residual-in-Residual Dense Blocks.

Result: Extensive experiments show CATformer outperforms recent transformer-based and diffusion-inspired methods in both efficiency and visual image quality on benchmark datasets.

Conclusion: The work successfully bridges performance gaps between different super-resolution approaches and lays a foundation for practical applications of diffusion-inspired transformers in image enhancement.

Abstract: Super-resolution remains a promising technique to enhance the quality of
low-resolution images. This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning. CATformer employs a
dual-branch architecture combining a primary diffusion-inspired transformer,
which progressively refines latent representations, with an auxiliary
transformer branch designed to enhance robustness to noise through learned
latent contrasts. These complementary representations are fused and decoded
using deep Residual-in-Residual Dense Blocks for enhanced reconstruction
quality. Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality. This work bridges the performance gap
among transformer-, diffusion-, and GAN-based methods, laying a foundation for
practical applications of diffusion-inspired transformers in super-resolution.

</details>


### [136] [NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction](https://arxiv.org/abs/2508.17712)
*Soham Dasgupta,Shanthika Naik,Preet Savalia,Sujay Kumar Ingle,Avinash Sharma*

Main category: cs.CV

TL;DR: NGD: Neural Gradient-based Deformation method for high-quality dynamic garment reconstruction from monocular video using adaptive remeshing and dynamic texture maps


<details>
  <summary>Details</summary>
Motivation: Existing methods have limitations - implicit representations provide smooth geometry without high-frequency details, while template methods using vertex displacement cause artifacts. Need better approach for complex garment dynamics.

Method: Proposes Neural Gradient-based Deformation (NGD) method with adaptive remeshing strategy for evolving surfaces like wrinkles and pleats, plus dynamic texture maps for lighting/shadow effects

Result: Significant improvements over state-of-the-art methods in both qualitative and quantitative evaluations, achieving high-quality garment reconstructions

Conclusion: NGD successfully addresses limitations of previous approaches by combining neural gradient-based deformation with adaptive remeshing and dynamic textures for superior dynamic garment reconstruction

Abstract: Dynamic garment reconstruction from monocular video is an important yet
challenging task due to the complex dynamics and unconstrained nature of the
garments. Recent advancements in neural rendering have enabled high-quality
geometric reconstruction with image/video supervision. However, implicit
representation methods that use volume rendering often provide smooth geometry
and fail to model high-frequency details. While template reconstruction methods
model explicit geometry, they use vertex displacement for deformation, which
results in artifacts. Addressing these limitations, we propose NGD, a Neural
Gradient-based Deformation method to reconstruct dynamically evolving textured
garments from monocular videos. Additionally, we propose a novel adaptive
remeshing strategy for modelling dynamically evolving surfaces like wrinkles
and pleats of the skirt, leading to high-quality reconstruction. Finally, we
learn dynamic texture maps to capture per-frame lighting and shadow effects. We
provide extensive qualitative and quantitative evaluations to demonstrate
significant improvements over existing SOTA methods and provide high-quality
garment reconstructions.

</details>


### [137] [F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model](https://arxiv.org/abs/2508.17714)
*Hanbo Bi,Zhiqiang Yuan,Zexi Jia,Jiapei Zhang,Chongyang Li,Peixiang Luo,Ying Deng,Xiaoyue Duan,Jinchao Zhang*

Main category: cs.CV

TL;DR: This paper introduces Fine-grained Fragment Retrieval (FFR) task for retrieving semantically coherent multimodal fragments from long conversations, creates the MLDR dataset, and proposes F2RVLM model with two-stage training and difficulty-aware curriculum sampling that outperforms existing VLMs.


<details>
  <summary>Details</summary>
Motivation: Traditional dialogue retrieval fails to meet users' needs for revisiting semantically coherent content scattered across long multimodal conversations, requiring a more fine-grained approach.

Method: Propose F2RVLM - a generative retrieval model trained in two stages: supervised fine-tuning for fragment-level retrieval knowledge, and GRPO-based reinforcement learning with multi-objective rewards. Use difficulty-aware curriculum sampling to handle varying intra-fragment complexity.

Result: F2RVLM outperforms popular Vision-Language Models in both in-domain and real-domain settings, demonstrating superior retrieval performance on the constructed MLDR dataset and WeChat-based test set.

Conclusion: The proposed FFR task and F2RVLM model effectively address the challenge of retrieving semantically coherent multimodal fragments from long-form conversations, showing significant improvements over existing approaches.

Abstract: Traditional dialogue retrieval aims to select the most appropriate utterance
or image from recent dialogue history. However, they often fail to meet users'
actual needs for revisiting semantically coherent content scattered across
long-form conversations. To fill this gap, we define the Fine-grained Fragment
Retrieval (FFR) task, requiring models to locate query-relevant fragments,
comprising both utterances and images, from multimodal long-form dialogues. As
a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue
retrieval dataset to date, averaging 25.45 turns per dialogue, with each
naturally spanning three distinct topics. To evaluate generalization in
real-world scenarios, we curate and annotate a WeChat-based test set comprising
real-world multimodal dialogues with an average of 75.38 turns. Building on
these resources, we explore existing generation-based Vision-Language Models
(VLMs) on FFR and observe that they often retrieve incoherent utterance-image
fragments. While optimized for generating responses from visual-textual inputs,
these models lack explicit supervision to ensure semantic coherence within
retrieved fragments. To this end, we propose F2RVLM, a generative retrieval
model trained in a two-stage paradigm: (1) supervised fine-tuning to inject
fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning
with multi-objective rewards promoting semantic precision, relevance, and
contextual coherence. To handle varying intra-fragment complexity, from locally
dense to sparsely distributed, we introduce difficulty-aware curriculum
sampling that ranks training instances by model-predicted difficulty and
gradually exposes the model to harder samples. This boosts reasoning ability in
long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain
and real-domain settings, demonstrating superior retrieval performance.

</details>


### [138] [Instant Preference Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.17718)
*Yang Li,Songlin Yang,Xiaoxuan Han,Wei Wang,Jing Dong,Yueming Lyu,Ziyu Xue*

Main category: cs.CV

TL;DR: Training-free framework using MLLMs for instant preference-aligned text-to-image generation that supports multi-round interactive refinement without additional training.


<details>
  <summary>Details</summary>
Motivation: Existing T2I methods rely on static preferences or fine-tuning, limiting adaptability to evolving user intents. Need for real-time, training-free preference alignment.

Method: Decouples into preference understanding (MLLM extracts global signals from reference images) and preference-guided generation (keyword control + cross-attention modulation). Uses structured instruction design for prompt enrichment.

Result: Outperforms prior approaches on Viper dataset and collected benchmark in both quantitative metrics and human evaluations. Supports broader and more fine-grained preference coverage.

Conclusion: Opens new possibilities for dialog-based generation and MLLM-diffusion integration, enabling real-time context-aware image generation with precise alignment across global and local elements.

Abstract: Text-to-image (T2I) generation has greatly enhanced creative expression, yet
achieving preference-aligned generation in a real-time and training-free manner
remains challenging. Previous methods often rely on static, pre-collected
preferences or fine-tuning, limiting adaptability to evolving and nuanced user
intents. In this paper, we highlight the need for instant preference-aligned
T2I generation and propose a training-free framework grounded in multimodal
large language model (MLLM) priors. Our framework decouples the task into two
components: preference understanding and preference-guided generation. For
preference understanding, we leverage MLLMs to automatically extract global
preference signals from a reference image and enrich a given prompt using
structured instruction design. Our approach supports broader and more
fine-grained coverage of user preferences than existing methods. For
preference-guided generation, we integrate global keyword-based control and
local region-aware cross-attention modulation to steer the diffusion model
without additional training, enabling precise alignment across both global
attributes and local elements. The entire framework supports multi-round
interactive refinement, facilitating real-time and context-aware image
generation. Extensive experiments on the Viper dataset and our collected
benchmark demonstrate that our method outperforms prior approaches in both
quantitative metrics and human evaluations, and opens up new possibilities for
dialog-based generation and MLLM-diffusion integration.

</details>


### [139] [Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework](https://arxiv.org/abs/2508.17726)
*Koichiro Kamide,Shunsuke Sakai,Shun Maeda,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: A unified framework for human action anomaly detection that uses contrastive learning and generative motion augmentation to enable few-shot learning across action categories, eliminating the need for separate models per category.


<details>
  <summary>Details</summary>
Motivation: Existing methods require separate training for each action category with large normal samples, which hinders scalability and limits real-world applicability where data is scarce or novel categories frequently appear.

Method: Constructs a category-agnostic representation space via contrastive learning, enabling anomaly detection by comparing test samples with a small support set. Uses generative motion augmentation with diffusion-based foundation models to create diverse training samples.

Result: Extensive experiments on HumanAct12 dataset demonstrate state-of-the-art effectiveness under both seen and unseen category settings, with improved training efficiency and model scalability for few-shot HAAD.

Conclusion: The proposed framework successfully addresses scalability limitations of existing methods and enables effective few-shot human action anomaly detection across categories through unified representation learning and generative augmentation.

Abstract: Human Action Anomaly Detection (HAAD) aims to identify anomalous actions
given only normal action data during training. Existing methods typically
follow a one-model-per-category paradigm, requiring separate training for each
action category and a large number of normal samples. These constraints hinder
scalability and limit applicability in real-world scenarios, where data is
often scarce or novel categories frequently appear. To address these
limitations, we propose a unified framework for HAAD that is compatible with
few-shot scenarios. Our method constructs a category-agnostic representation
space via contrastive learning, enabling AD by comparing test samples with a
given small set of normal examples (referred to as the support set). To improve
inter-category generalization and intra-category robustness, we introduce a
generative motion augmentation strategy harnessing a diffusion-based foundation
model for creating diverse and realistic training samples. Notably, to the best
of our knowledge, our work is the first to introduce such a strategy
specifically tailored to enhance contrastive learning for action AD. Extensive
experiments on the HumanAct12 dataset demonstrate the state-of-the-art
effectiveness of our approach under both seen and unseen category settings,
regarding training efficiency and model scalability for few-shot HAAD.

</details>


### [140] [Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning](https://arxiv.org/abs/2508.17728)
*Nisreen Albzour,Sarah S. Lam*

Main category: cs.CV

TL;DR: Deep learning framework using U-Net segmentation and classification for cervical cancer detection from Pap smear images, showing marginal performance improvement with segmentation.


<details>
  <summary>Details</summary>
Motivation: Cervical cancer is a major global health issue causing high mortality among women. Manual Pap smear examination is time-consuming and error-prone, necessitating automated solutions for early detection.

Method: Proposed a deep learning framework integrating U-Net for segmentation and a classification model. Used Herlev Pap Smear Dataset. Compared performance between models trained on segmented vs non-segmented images.

Result: Segmented images marginally improved precision (0.41% higher) and F1-score (1.30% higher), indicating slightly more balanced classification performance. Segmentation impact on classification was found to be limited.

Conclusion: While segmentation aids feature extraction, its effect on classification performance is minimal. The framework serves as a supplemental tool to assist pathologists in early cervical cancer diagnosis.

Abstract: Cervical cancer remains a significant global health concern and a leading
cause of cancer-related deaths among women. Early detection through Pap smear
tests is essential to reduce mortality rates; however, the manual examination
is time consuming and prone to human error. This study proposes a deep learning
framework that integrates U-Net for segmentation and a classification model to
enhance diagnostic performance. The Herlev Pap Smear Dataset, a publicly
available cervical cell dataset, was utilized for training and evaluation. The
impact of segmentation on classification performance was evaluated by comparing
the model trained on segmented images and another trained on non-segmented
images. Experimental results showed that the use of segmented images marginally
improved the model performance on precision (about 0.41 percent higher) and
F1-score (about 1.30 percent higher), which suggests a slightly more balanced
classification performance. While segmentation helps in feature extraction, the
results showed that its impact on classification performance appears to be
limited. The proposed framework offers a supplemental tool for clinical
applications, which may aid pathologists in early diagnosis.

</details>


### [141] [CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp Segmentation](https://arxiv.org/abs/2508.17729)
*Feng Jiang,Zongfei Zhang,Xin Xu*

Main category: cs.CV

TL;DR: CMFDNet is a novel polyp segmentation architecture that addresses three key challenges: shape/size variation, indistinct boundaries, and small polyp detection through three specialized modules (CMD, MSA, FD).


<details>
  <summary>Details</summary>
Motivation: Existing polyp segmentation methods struggle with significant shape/size variations, indistinct boundaries between polyps and adjacent tissues, and the tendency to overlook small polyps during segmentation.

Method: CMFDNet uses three innovative modules: CMD module (cross-scanning decoder to reduce blurry boundaries), MSA module (multi-branch parallel structure for diverse geometry recognition), and FD module (establishes decoder feature dependencies to improve small polyp detection).

Result: Outperforms six state-of-the-art methods, achieving mDice score improvements of 1.83% on ETIS dataset and 1.55% on ColonDB dataset compared to the best existing method.

Conclusion: CMFDNet effectively addresses key limitations in polyp segmentation through its specialized modules, demonstrating superior performance particularly in handling boundary clarity and small polyp detection across multiple datasets.

Abstract: Automated colonic polyp segmentation is crucial for assisting doctors in
screening of precancerous polyps and diagnosis of colorectal neoplasms.
Although existing methods have achieved promising results, polyp segmentation
remains hindered by the following limitations,including: (1) significant
variation in polyp shapes and sizes, (2) indistinct boundaries between polyps
and adjacent tissues, and (3) small-sized polyps are easily overlooked during
the segmentation process. Driven by these practical difficulties, an innovative
architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD
module. The CMD module, serving as an innovative decoder, introduces a
cross-scanning method to reduce blurry boundaries. The MSA module adopts a
multi-branch parallel structure to enhance the recognition ability for polyps
with diverse geometries and scale distributions. The FD module establishes
dependencies among all decoder features to alleviate the under-detection of
polyps with small-scale features. Experimental results show that CMFDNet
outperforms six SOTA methods used for comparison, especially on ETIS and
ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and
1.55%, respectively.

</details>


### [142] [DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning](https://arxiv.org/abs/2508.17746)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: DroneKey is a novel framework for 3D drone pose estimation that addresses challenges in detecting drone propellers as keypoints through transformer-based detection with gated feature combination and pose-adaptive loss functions, achieving state-of-the-art performance in both 2D keypoint detection (99.68% AP) and 3D pose estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with drone keypoint detection due to the high visual similarity and diverse poses of drone propellers, which serve as critical keypoints for 3D pose estimation in anti-drone systems.

Method: Combines 2D keypoint detector and 3D pose estimator using transformer encoder layers with two key-representations (intermediate and compact) combined via gated sum, and introduces pose-adaptive Mahalanobis distance in loss function for stable predictions across extreme poses.

Result: Achieved 99.68% AP in keypoint detection, outperforming existing methods. For 3D pose estimation: MAE-angle of 10.62°, RMSE of 0.221m, and MAE-absolute of 0.076m. Real-time processing at 44 FPS. Ablation studies confirmed improved stability and accuracy.

Conclusion: DroneKey effectively addresses drone-specific keypoint detection challenges through innovative feature combination and loss design, achieving high accuracy and real-time performance. The publicly released datasets and code support further research in anti-drone systems.

Abstract: Estimating the 3D pose of a drone is important for anti-drone systems, but
existing methods struggle with the unique challenges of drone keypoint
detection. Drone propellers serve as keypoints but are difficult to detect due
to their high visual similarity and diversity of poses. To address these
challenges, we propose DroneKey, a framework that combines a 2D keypoint
detector and a 3D pose estimator specifically designed for drones. In the
keypoint detection stage, we extract two key-representations (intermediate and
compact) from each transformer encoder layer and optimally combine them using a
gated sum. We also introduce a pose-adaptive Mahalanobis distance in the loss
function to ensure stable keypoint predictions across extreme poses. We built
new datasets of drone 2D keypoints and 3D pose to train and evaluate our
method, which have been publicly released. Experiments show that our method
achieves an AP of 99.68% (OKS) in keypoint detection, outperforming existing
methods. Ablation studies confirm that the pose-adaptive Mahalanobis loss
function improves keypoint prediction stability and accuracy. Additionally,
improvements in the encoder design enable real-time processing at 44 FPS. For
3D pose estimation, our method achieved an MAE-angle of 10.62{\deg}, an RMSE of
0.221m, and an MAE-absolute of 0.076m, demonstrating high accuracy and
reliability. The code and dataset are available at
https://github.com/kkanuseobin/DroneKey.

</details>


### [143] [From Global to Local: Social Bias Transfer in CLIP](https://arxiv.org/abs/2508.17750)
*Ryan Ramos,Yusuke Hirota,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: This paper analyzes how social biases from CLIP pre-training transfer to downstream tasks, finding inconsistent bias transfer patterns and convergence of representation spaces during adaptation.


<details>
  <summary>Details</summary>
Motivation: To understand how social biases learned during CLIP pre-training propagate to downstream applications like visual question answering and image captioning, and whether bias transfer occurs consistently.

Method: Comprehensive empirical analysis examining: 1) pre-training bias variation between global and local data views, 2) correlations between pre-training and downstream biases across varying bias levels, and 3) investigation of representation space convergence during downstream adaptation.

Result: Bias measurement is highly dependent on data subsets, consistent trends in bias transfer are difficult to discover, and representation spaces of different pre-trained CLIPs converge when adapted for downstream tasks.

Conclusion: The work provides insights into bias behavior and highlights the need for better bias mitigation practices, as current methods show inconsistent bias transfer patterns.

Abstract: The recycling of contrastive language-image pre-trained (CLIP) models as
backbones for a large number of downstream tasks calls for a thorough analysis
of their transferability implications, especially their well-documented
reproduction of social biases and human stereotypes. How do such biases,
learned during pre-training, propagate to downstream applications like visual
question answering or image captioning? Do they transfer at all?
  We investigate this phenomenon, referred to as bias transfer in prior
literature, through a comprehensive empirical analysis. Firstly, we examine how
pre-training bias varies between global and local views of data, finding that
bias measurement is highly dependent on the subset of data on which it is
computed. Secondly, we analyze correlations between biases in the pre-trained
models and the downstream tasks across varying levels of pre-training bias,
finding difficulty in discovering consistent trends in bias transfer. Finally,
we explore why this inconsistency occurs, showing that under the current
paradigm, representation spaces of different pre-trained CLIPs tend to converge
when adapted for downstream tasks. We hope this work offers valuable insights
into bias behavior and informs future research to promote better bias
mitigation practices.

</details>


### [144] [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
*Mingyue Yang,Dianxi Shi,Jialu Zhou,Xinyu Wei,Leqian Li,Shaowu Yang,Chunping Qiu*

Main category: cs.CV

TL;DR: CEIDM is a diffusion-based T2I method with dual controls for entities and their interactions, using LLM-based relationship mining, interactive action clustering, and an entity control network to generate high-quality images with realistic entity interactions.


<details>
  <summary>Details</summary>
Motivation: Text-to-Image generation struggles with controlling complex entities and their intricate interactions in diffusion models, leading to unrealistic or illogical image outputs.

Method: 1) LLM-based entity interactive relationship mining using chain of thought; 2) Interactive action clustering and offset method with global/local bidirectional offsets; 3) Entity control network with semantic-guided masks and multi-scale convolutional/dynamic networks.

Result: CEIDM outperforms existing state-of-the-art methods in both entity control and interaction control, generating images with more realistic logic and accurate interactive relationships.

Conclusion: The proposed dual-control approach effectively addresses the challenge of controlling entities and their interactions in T2I generation, producing higher quality and more logically consistent images than existing methods.

Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their
intricate interactions pose a significant challenge for T2I method based on
diffusion model: how to effectively control entity and their interactions to
produce high-quality images. To address this, we propose CEIDM, a image
generation method based on diffusion model with dual controls for entity and
interaction. First, we propose an entity interactive relationships mining
approach based on Large Language Models (LLMs), extracting reasonable and rich
implicit interactive relationships through chain of thought to guide diffusion
models to generate high-quality images that are closer to realistic logic and
have more reasonable interactive relationships. Furthermore, We propose an
interactive action clustering and offset method to cluster and offset the
interactive action features contained in each text prompts. By constructing
global and local bidirectional offsets, we enhance semantic understanding and
detail supplementation of original actions, making the model's understanding of
the concept of interactive "actions" more accurate and generating images with
more accurate interactive actions. Finally, we design an entity control network
which generates masks with entity semantic guidance, then leveraging
multi-scale convolutional network to enhance entity feature and dynamic network
to fuse feature. It effectively controls entities and significantly improves
image quality. Experiments show that the proposed CEIDM method is better than
the most representative existing methods in both entity control and their
interaction control.

</details>


### [145] [Robust Anomaly Detection in Industrial Environments via Meta-Learning](https://arxiv.org/abs/2508.17789)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: RAD is a robust anomaly detection framework that combines Normalizing Flows with Meta-Learning to handle noisy training labels in industrial settings, achieving strong performance even with 50% mislabeled data.


<details>
  <summary>Details</summary>
Motivation: Conventional anomaly detection methods struggle with mislabeled training samples, which are common in real-world industrial environments where perfect data curation is challenging.

Method: Integrates Normalizing Flows with Model-Agnostic Meta-Learning using bi-level optimization, uncertainty-guided adaptive L2 regularization, multiscale feature processing, and precise likelihood estimation for anomaly scoring.

Result: Achieved I-AUROC scores of 95.4% on MVTec-AD and 94.6% on KSDD2 under clean conditions, maintaining robust performance above 86.8% and 92.1% even with 50% mislabeled training samples.

Conclusion: RAD demonstrates exceptional resilience to label noise and effectively detects subtle anomalies across diverse industrial scenarios, making it a practical solution for real-world applications with imperfect data.

Abstract: Anomaly detection is fundamental for ensuring quality control and operational
efficiency in industrial environments, yet conventional approaches face
significant challenges when training data contains mislabeled samples-a common
occurrence in real-world scenarios. This paper presents RAD, a robust anomaly
detection framework that integrates Normalizing Flows with Model-Agnostic
Meta-Learning to address the critical challenge of label noise in industrial
settings. Our approach employs a bi-level optimization strategy where
meta-learning enables rapid adaptation to varying noise conditions, while
uncertainty quantification guides adaptive L2 regularization to maintain model
stability. The framework incorporates multiscale feature processing through
pretrained feature extractors and leverages the precise likelihood estimation
capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive
evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance,
achieving I-AUROC scores of 95.4% and 94.6% respectively under clean
conditions, while maintaining robust detection capabilities above 86.8% and
92.1% even when 50% of training samples are mislabeled. The results highlight
RAD's exceptional resilience to noisy training conditions and its ability to
detect subtle anomalies across diverse industrial scenarios, making it a
practical solution for real-world anomaly detection applications where perfect
data curation is challenging.

</details>


### [146] [Sketchpose: Learning to Segment Cells with Partial Annotations](https://arxiv.org/abs/2508.17798)
*Clément Cazorla,Nathanaël Munier,Renaud Morin,Pierre Weiss*

Main category: cs.CV

TL;DR: A method for cell segmentation that uses distance maps but works with partially annotated objects, enabling frugal learning and transfer learning while maintaining segmentation quality.


<details>
  <summary>Details</summary>
Motivation: Current cell segmentation networks require fully annotated datasets, which is a serious limitation for generating training sets and performing transfer learning.

Method: Proposes a method that still relies on distance maps but can handle partially annotated objects, embedded in a user-friendly Napari plugin.

Result: The approach leads to substantial savings in time and resources without sacrificing segmentation quality in frugal learning, transfer learning, and regular learning contexts.

Conclusion: The method overcomes the limitation of requiring fully annotated datasets while maintaining the accuracy benefits of distance map-based segmentation approaches.

Abstract: The most popular networks used for cell segmentation (e.g. Cellpose,
Stardist, HoverNet,...) rely on a prediction of a distance map. It yields
unprecedented accuracy but hinges on fully annotated datasets. This is a
serious limitation to generate training sets and perform transfer learning. In
this paper, we propose a method that still relies on the distance map and
handles partially annotated objects. We evaluate the performance of the
proposed approach in the contexts of frugal learning, transfer learning and
regular learning on regular databases. Our experiments show that it can lead to
substantial savings in time and resources without sacrificing segmentation
quality. The proposed algorithm is embedded in a user-friendly Napari plugin.

</details>


### [147] [PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models](https://arxiv.org/abs/2508.17807)
*Kai Zhao,Wubang Yuan,Alex Lingyu Hung,Dan Zeng*

Main category: cs.CV

TL;DR: Simple position-based reweighting method to fix recency bias in visual token pruning for VLMs, improving performance with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from recency bias where later visual tokens (image bottom) get inflated attention scores, leading to suboptimal pruning that disproportionately retains bottom image tokens.

Method: Propose Position-reweighted Visual Token Pruning - a plug-and-play reweighting mechanism that adjusts attention scores based on spatial positions to counteract recency bias.

Result: Extensive experiments show improved performance of visual token pruning with minimal computational overhead.

Conclusion: The method effectively alleviates recency bias in visual token pruning and can be seamlessly integrated into existing frameworks without architectural changes or extra training.

Abstract: Vision-Language Models (VLMs) typically process a significantly larger number
of visual tokens compared to text tokens due to the inherent redundancy in
visual signals. Visual token pruning is a promising direction to reduce the
computational cost of VLMs by eliminating redundant visual tokens. The
text-visual attention score is a widely adopted criterion for visual token
pruning as it reflects the relevance of visual tokens to the text input.
However, many sequence models exhibit a recency bias, where tokens appearing
later in the sequence exert a disproportionately large influence on the model's
output. In VLMs, this bias manifests as inflated attention scores for tokens
corresponding to the lower regions of the image, leading to suboptimal pruning
that disproportionately retains tokens from the image bottom. In this paper, we
present an extremely simple yet effective approach to alleviate the recency
bias in visual token pruning. We propose a straightforward reweighting
mechanism that adjusts the attention scores of visual tokens according to their
spatial positions in the image. Our method, termed Position-reweighted Visual
Token Pruning, is a plug-and-play solution that can be seamlessly incorporated
into existing visual token pruning frameworks without any changes to the model
architecture or extra training. Extensive experiments on LVLMs demonstrate that
our method improves the performance of visual token pruning with minimal
computational overhead.

</details>


### [148] [UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization](https://arxiv.org/abs/2508.17816)
*Xingyu Ai,Shaoyu Wang,Zhiyuan Jia,Ao Xu,Hongming Shan,Jianhua Ma,Qiegen Liu*

Main category: cs.CV

TL;DR: UniSino is a foundation model that standardizes CT sinograms in the projection domain to address undersampling and noise artifacts, achieving superior reconstruction quality and generalization across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Conventional CT sinogram correction methods lack generalizability across heterogeneous artifact types and rely on manually designed algorithms with fixed parameters, leading to compromised diagnostic accuracy from undersampling and noise artifacts.

Method: Proposes UniSino, a foundation model that operates directly in the projection domain rather than image domain, incorporating physical characteristics of sinograms in its training framework to enhance generalization across multiple undersampling scenarios and subtasks.

Result: Experimental results demonstrate superior reconstruction quality in both single and mixed undersampling cases, showing exceptional robustness and generalization across four benchmark datasets.

Conclusion: UniSino provides a universal solution for CT sinogram standardization with strong generalization capabilities, addressing limitations of conventional methods and enabling robust performance in diverse undersampling scenarios for improved CT imaging.

Abstract: During raw-data acquisition in CT imaging, diverse factors can degrade the
collected sinograms, with undersampling and noise leading to severe artifacts
and noise in reconstructed images and compromising diagnostic accuracy.
Conventional correction methods rely on manually designed algorithms or fixed
empirical parameters, but these approaches often lack generalizability across
heterogeneous artifact types. To address these limitations, we propose UniSino,
a foundation model for universal CT sinogram standardization. Unlike existing
foundational models that operate in image domain, UniSino directly standardizes
data in the projection domain, which enables stronger generalization across
diverse undersampling scenarios. Its training framework incorporates the
physical characteristics of sinograms, enhancing generalization and enabling
robust performance across multiple subtasks spanning four benchmark datasets.
Experimental results demonstrate thatUniSino achieves superior reconstruction
quality both single and mixed undersampling case, demonstrating exceptional
robustness and generalization in sinogram enhancement for CT imaging. The code
is available at: https://github.com/yqx7150/UniSino.

</details>


### [149] [TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration](https://arxiv.org/abs/2508.17817)
*Meiqi Gong,Hao Zhang,Xunpeng Yi,Linfeng Tang,Jiayi Ma*

Main category: cs.CV

TL;DR: First video fusion framework with temporal modeling and visual-semantic collaboration to ensure visual fidelity, semantic accuracy, and temporal consistency across frames.


<details>
  <summary>Details</summary>
Motivation: Existing methods use static frame-based image fusion for videos, neglecting temporal dependencies and causing inconsistent results across frames.

Method: Visual-semantic interaction module with Dinov2 and VGG19 for distillation, temporal cooperative module for degradation enhancement, temporal-enhanced mechanism with temporal loss, and new evaluation metrics.

Result: Extensive experiments on public video datasets demonstrate superior performance compared to existing methods.

Conclusion: Proposed framework effectively addresses temporal consistency issues in video fusion through novel temporal modeling and visual-semantic collaboration techniques.

Abstract: Existing multi-modal fusion methods typically apply static frame-based image
fusion techniques directly to video fusion tasks, neglecting inherent temporal
dependencies and leading to inconsistent results across frames. To address this
limitation, we propose the first video fusion framework that explicitly
incorporates temporal modeling with visual-semantic collaboration to
simultaneously ensure visual fidelity, semantic accuracy, and temporal
consistency. First, we introduce a visual-semantic interaction module
consisting of a semantic branch and a visual branch, with Dinov2 and VGG19
employed for targeted distillation, allowing simultaneous enhancement of both
the visual and semantic representations. Second, we pioneer integrate the video
degradation enhancement task into the video fusion pipeline by constructing a
temporal cooperative module, which leverages temporal dependencies to
facilitate weak information recovery. Third, to ensure temporal consistency, we
embed a temporal-enhanced mechanism into the network and devise a temporal loss
to guide the optimization process. Finally, we introduce two innovative
evaluation metrics tailored for video fusion, aimed at assessing the temporal
consistency of the generated fused videos. Extensive experimental results on
public video datasets demonstrate the superiority of our method. Our code is
released at https://github.com/Meiqi-Gong/TemCoCo.

</details>


### [150] [A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection](https://arxiv.org/abs/2508.17827)
*Muhammad Aqeel,Danijel Skocaj,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: CoZAD is a zero-shot anomaly detection framework that combines soft confident learning with meta-learning and contrastive representation to address data scarcity in industrial and medical applications without needing labeled anomaly data.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and high annotation costs in industrial and medical anomaly detection, particularly in evolving manufacturing and healthcare environments where traditional supervised methods are impractical.

Method: Integrates soft confident learning (confidence-based weighting instead of discarding uncertain samples), meta-learning (MAML framework), and contrastive feature representation. Uses IQR-based thresholding for data uncertainty and covariance regularization for model uncertainty.

Result: State-of-the-art performance across 10 datasets: outperforms existing methods on 6/7 industrial benchmarks, achieves 99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD, and 96.3% P-AUROC on MVTec-AD for pixel-level localization.

Conclusion: CoZAD provides effective zero-shot anomaly detection without vision-language alignments or model ensembles, making it suitable for resource-constrained environments requiring rapid deployment across industrial and medical domains.

Abstract: Industrial and medical anomaly detection faces critical challenges from data
scarcity and prohibitive annotation costs, particularly in evolving
manufacturing and healthcare settings. To address this, we propose CoZAD, a
novel zero-shot anomaly detection framework that integrates soft confident
learning with meta-learning and contrastive feature representation. Unlike
traditional confident learning that discards uncertain samples, our method
assigns confidence-based weights to all training data, preserving boundary
information while emphasizing prototypical normal patterns. The framework
quantifies data uncertainty through IQR-based thresholding and model
uncertainty via covariance based regularization within a Model-Agnostic
Meta-Learning. Contrastive learning creates discriminative feature spaces where
normal patterns form compact clusters, enabling rapid domain adaptation.
Comprehensive evaluation across 10 datasets spanning industrial and medical
domains demonstrates state-of-the-art performance, outperforming existing
methods on 6 out of 7 industrial benchmarks with notable improvements on
texture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and
pixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates
dependence on vision-language alignments or model ensembles, making it valuable
for resourceconstrained environments requiring rapid deployment.

</details>


### [151] [HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation](https://arxiv.org/abs/2508.17832)
*Xiping Wang,Yuxi Wang,Mengqi Zhou,Junsong Fan,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: HLG introduces a hierarchical coarse-to-fine approach for 3D indoor scene generation, addressing fine-grained object placement issues through layout alignment and optimization networks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with fine-grained object placements in 3D scene generation, limiting realism and utility for virtual reality, interior design, and embodied AI applications.

Method: Hierarchical Layout Generation (HLG) with fine-grained layout alignment module that uses vertical/horizontal decoupling, and trainable layout optimization network to fix placement errors and intersections.

Result: Superior performance in generating realistic indoor scenes compared to existing methods, demonstrated through extensive experiments.

Conclusion: HLG advances 3D scene generation field and enables more detailed environments for various applications; code will be released to support future research.

Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior
design, embodied intelligence, and scene understanding. While existing methods
have made progress in coarse-scale furniture arrangement, they struggle to
capture fine-grained object placements, limiting the realism and utility of
generated environments. This gap hinders immersive virtual experiences and
detailed scene comprehension for embodied AI applications. To address these
issues, we propose Hierarchical Layout Generation (HLG), a novel method for
fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine
hierarchical approach, refining scene layouts from large-scale furniture
placement to intricate object arrangements. Specifically, our fine-grained
layout alignment module constructs a hierarchical layout through vertical and
horizontal decoupling, effectively decomposing complex 3D indoor scenes into
multiple levels of granularity. Additionally, our trainable layout optimization
network addresses placement issues, such as incorrect positioning, orientation
errors, and object intersections, ensuring structurally coherent and physically
plausible scene generation. We demonstrate the effectiveness of our approach
through extensive experiments, showing superior performance in generating
realistic indoor scenes compared to existing methods. This work advances the
field of scene generation and opens new possibilities for applications
requiring detailed 3D environments. We will release our code upon publication
to encourage future research.

</details>


### [152] [SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection](https://arxiv.org/abs/2508.17843)
*Weiqi Yan,Lvhai Chen,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: SCOUT introduces a semi-supervised framework for camouflaged object detection that uses adaptive data selection and text fusion to better utilize unlabeled data, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Pixel-level annotation is costly and hinders COD development. Previous semi-supervised methods don't effectively utilize unlabeled data, leaving room for improvement.

Method: Proposes SCOUT with two modules: Adaptive Data Augment and Selection (ADAS) for valuable data selection through adversarial augment/sampling, and Text Fusion Module (TFM) that combines camouflage knowledge with text-visual interaction. Also built RefTextCOD dataset.

Result: Extensive experiments show the method surpasses previous semi-supervised COD methods and achieves state-of-the-art performance.

Conclusion: SCOUT effectively addresses the annotation cost problem in COD through innovative data selection and text fusion techniques, demonstrating superior performance over existing approaches.

Abstract: The difficulty of pixel-level annotation has significantly hindered the
development of the Camouflaged Object Detection (COD) field. To save on
annotation costs, previous works leverage the semi-supervised COD framework
that relies on a small number of labeled data and a large volume of unlabeled
data. We argue that there is still significant room for improvement in the
effective utilization of unlabeled data. To this end, we introduce a
Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive
Data Selection (SCOUT). It includes an Adaptive Data Augment and Selection
(ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable
data for annotation through an adversarial augment and sampling strategy. The
TFM module further leverages the selected valuable data by combining
camouflage-related knowledge and text-visual interaction. To adapt to this
work, we build a new dataset, namely RefTextCOD. Extensive experiments show
that the proposed method surpasses previous semi-supervised methods in the COD
field and achieves state-of-the-art performance. Our code will be released at
https://github.com/Heartfirey/SCOUT.

</details>


### [153] [Diffusion-Based Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2508.17844)
*Maham Nazir,Muhammad Aqeel,Francesco Setti*

Main category: cs.CV

TL;DR: DiffAug: A framework combining text-guided diffusion generation with automatic segmentation validation to synthesize rare medical abnormalities, achieving 8-10% Dice improvement and 28% false negative reduction.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation models struggle with rare abnormalities due to scarce annotated pathological data, limiting their performance in detecting critical conditions like small polyps and flat lesions.

Method: Uses latent diffusion models conditioned on medical text descriptions and spatial masks to synthesize abnormalities via inpainting on normal images, with dynamic quality validation through a latent-space segmentation network for spatial accuracy.

Result: Achieves state-of-the-art performance on three medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2) with 8-10% Dice improvements over baselines and reduces false negative rates by up to 28% for challenging cases.

Conclusion: The proposed framework effectively addresses data scarcity for rare abnormalities through text-guided diffusion generation and automated validation, significantly improving segmentation performance for early detection in screening applications.

Abstract: Medical image segmentation models struggle with rare abnormalities due to
scarce annotated pathological data. We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge. Our proposed approach uses latent
diffusion models conditioned on medical text descriptions and spatial masks to
synthesize abnormalities via inpainting on normal images. Generated samples
undergo dynamic quality validation through a latentspace segmentation network
that ensures accurate localization while enabling single-step inference. The
text prompts, derived from medical literature, guide the generation of diverse
abnormality types without requiring manual annotation. Our validation mechanism
filters synthetic samples based on spatial accuracy, maintaining quality while
operating efficiently through direct latent estimation. Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.

</details>


### [154] [Alternating Training-based Label Smoothing Enhances Prompt Generalization](https://arxiv.org/abs/2508.17846)
*Yang Chen,Yanbin Wei,Ke Jin,Yi Kong,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: ATLaS method combines label smoothing with prompt tuning through alternating training with one-hot and soft labels, improving generalization performance of vision-language models.


<details>
  <summary>Details</summary>
Motivation: Prompt tuning is parameter-efficient but has limited generalization. Label smoothing improves generalization but weakens prompt tuning performance when applied directly.

Method: Alternating Training-based Label Smoothing (ATLaS) with two types of offline soft labels: Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL) for inter-class and instance-class relationships.

Result: Extensive experiments show ATLaS consistently enhances generalization performance of prompt tuning and is compatible with existing prompt tuning methods.

Conclusion: ATLaS effectively integrates label smoothing with prompt tuning, providing better generalization while maintaining compatibility with prevalent methods.

Abstract: Recent advances in pre-trained vision-language models have demonstrated
remarkable zero-shot generalization capabilities. To further enhance these
models' adaptability to various downstream tasks, prompt tuning has emerged as
a parameter-efficient fine-tuning method. However, despite its efficiency, the
generalization ability of prompt remains limited. In contrast, label smoothing
(LS) has been widely recognized as an effective regularization technique that
prevents models from becoming over-confident and improves their generalization.
This inspires us to explore the integration of LS with prompt tuning. However,
we have observed that the vanilla LS even weakens the generalization ability of
prompt tuning. To address this issue, we propose the Alternating Training-based
Label Smoothing (ATLaS) method, which alternately trains with standard one-hot
labels and soft labels generated by LS to supervise the prompt tuning.
Moreover, we introduce two types of efficient offline soft labels, including
Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide
inter-class or instance-class relationships for prompt tuning. The theoretical
properties of the proposed ATLaS method are analyzed. Extensive experiments
demonstrate that the proposed ATLaS method, combined with CSL and ISL,
consistently enhances the generalization performance of prompt tuning.
Moreover, the proposed ATLaS method exhibits high compatibility with prevalent
prompt tuning methods, enabling seamless integration into existing methods.

</details>


### [155] [Box-Level Class-Balanced Sampling for Active Object Detection](https://arxiv.org/abs/2508.17849)
*Jingyi Liao,Xun Xu,Chuan-Sheng Foo,Lile Cai*

Main category: cs.CV

TL;DR: Class-balanced active learning for object detection that addresses class imbalance in pseudo labels through balanced sampling and improved pseudo labeling.


<details>
  <summary>Details</summary>
Motivation: Active learning for object detection suffers from class imbalance in pseudo labels, where early-stage models perform well only on majority classes, leading to biased training data.

Method: Proposes class-balanced sampling to select more objects from minority classes for labeling, and task-aware soft pseudo labeling to improve pseudo label accuracy.

Result: Achieves state-of-the-art performance on public benchmarking datasets by creating more balanced training data through ground truth labels and improved pseudo labels.

Conclusion: The proposed class-balanced sampling and task-aware soft pseudo labeling effectively address class imbalance in box-level active learning for object detection, leading to better model performance.

Abstract: Training deep object detectors demands expensive bounding box annotation.
Active learning (AL) is a promising technique to alleviate the annotation
burden. Performing AL at box-level for object detection, i.e., selecting the
most informative boxes to label and supplementing the sparsely-labelled image
with pseudo labels, has been shown to be more cost-effective than selecting and
labelling the entire image. In box-level AL for object detection, we observe
that models at early stage can only perform well on majority classes, making
the pseudo labels severely class-imbalanced. We propose a class-balanced
sampling strategy to select more objects from minority classes for labelling,
so as to make the final training data, \ie, ground truth labels obtained by AL
and pseudo labels, more class-balanced to train a better model. We also propose
a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo
labels. We evaluate our method on public benchmarking datasets and show that
our method achieves state-of-the-art performance.

</details>


### [156] [VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)
*Pengfei Jiang,Hanjun Li,Linglan Zhao,Fei Chao,Ke Yan,Shouhong Ding,Rongrong Ji*

Main category: cs.CV

TL;DR: VISA introduces group-wise visual token selection and aggregation to compress visual tokens in MLLMs while preserving more visual information, achieving better performance-speed trade-off than previous methods.


<details>
  <summary>Details</summary>
Motivation: Address inefficient inference caused by excessive visual tokens in multimodal large language models (MLLMs) while maintaining visual information quality.

Method: Uses graph-based visual token aggregation (VTA) to form semantic similarity graphs and aggregate information from removed tokens. Implements group-wise token selection (GTS) to divide tokens into kept/removed groups guided by text tokens, enabling progressive visual information aggregation.

Result: Outperforms previous methods consistently across LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA benchmarks, achieving superior trade-off between model performance and inference speed.

Conclusion: VISA effectively compresses visual tokens while preserving more information than previous pruning approaches, demonstrating practical efficiency improvements for MLLMs.

Abstract: In this study, we introduce a novel method called group-wise \textbf{VI}sual
token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue
of inefficient inference stemming from excessive visual tokens in multimoal
large language models (MLLMs). Compared with previous token pruning approaches,
our method can preserve more visual information while compressing visual
tokens. We first propose a graph-based visual token aggregation (VTA) module.
VTA treats each visual token as a node, forming a graph based on semantic
similarity among visual tokens. It then aggregates information from removed
tokens into kept tokens based on this graph, producing a more compact visual
token representation. Additionally, we introduce a group-wise token selection
strategy (GTS) to divide visual tokens into kept and removed ones, guided by
text tokens from the final layers of each group. This strategy progressively
aggregates visual information, enhancing the stability of the visual
information extraction process. We conduct comprehensive experiments on
LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate
the efficacy of VISA. Our method consistently outperforms previous methods,
achieving a superior trade-off between model performance and inference speed.
The code is available at https://github.com/mobiushy/VISA.

</details>


### [157] [AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering](https://arxiv.org/abs/2508.17860)
*Kang Zeng,Guojin Zhong,Jintao Cheng,Jin Yuan,Zhiyong Li*

Main category: cs.CV

TL;DR: Proposes Adaptive Visual Anchoring strategy to compress redundant visual information in Multi-Image VQA, improving accuracy and efficiency through adaptive compression and collaborative decoding.


<details>
  <summary>Details</summary>
Motivation: Existing MVQA methods lack flexibility in controlling compressed visual tokens and produce discrete visual fragments, hindering MLLMs' holistic image comprehension due to visual redundancy from multiple images.

Method: Adaptive Visual Anchoring strategy that can be integrated into existing MLLMs for adaptive compression, plus a collaborative decoding mechanism to balance global and compressed visual inputs.

Result: Extensive experiments show consistent performance improvements across various MLLMs, validating the method's effectiveness.

Conclusion: The proposed strategy effectively addresses visual redundancy in MVQA, offering significant accuracy improvements while maintaining flexibility and holistic image comprehension.

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has driven
significant progress in Visual Question Answering (VQA), evolving from Single
to Multi Image VQA (MVQA). However, the increased number of images in MVQA
inevitably introduces substantial visual redundancy that is irrelevant to
question answering, negatively impacting both accuracy and efficiency. To
address this issue, existing methods lack flexibility in controlling the number
of compressed visual tokens and tend to produce discrete visual fragments,
which hinder MLLMs' ability to comprehend images holistically. In this paper,
we propose a straightforward yet universal Adaptive Visual Anchoring strategy,
which can be seamlessly integrated into existing MLLMs, offering significant
accuracy improvements through adaptive compression. Meanwhile, to balance the
results derived from both global and compressed visual input, we further
introduce a novel collaborative decoding mechanism, enabling optimal
performance. Extensive experiments validate the effectiveness of our method,
demonstrating consistent performance improvements across various MLLMs. The
code will be publicly available.

</details>


### [158] [Camera Pose Refinement via 3D Gaussian Splatting](https://arxiv.org/abs/2508.17876)
*Lulu Hao,Lipu Zhou,Zhenzhong Wei,Xu Wang*

Main category: cs.CV

TL;DR: GS-SMC is a novel camera pose refinement framework that uses 3D Gaussian Splatting to render novel views and employs epipolar geometric constraints for iterative optimization, achieving state-of-the-art performance without requiring scene-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing camera pose refinement methods require reconstructing scenes for different descriptors or retraining networks per scene, while geometry-free methods lack accuracy. The goal is to create a lightweight solution that works across diverse scenes without additional training.

Method: Leverages existing 3DGS models to render novel views, then uses an iterative optimization approach with epipolar geometric constraints between query and rendered images. Flexible feature extractors and matchers can be used to establish these constraints.

Result: Achieves 53.3% and 56.9% reductions in median translation and rotation errors on 7-Scenes dataset, and 40.7% and 53.2% reductions on Cambridge Landmarks dataset, outperforming state-of-the-art methods.

Conclusion: GS-SMC provides an effective and lightweight camera pose refinement solution that works across diverse scenes without requiring scene-specific training or fine-tuning, demonstrating superior accuracy through geometric constraints.

Abstract: Camera pose refinement aims at improving the accuracy of initial pose
estimation for applications in 3D computer vision. Most refinement approaches
rely on 2D-3D correspondences with specific descriptors or dedicated networks,
requiring reconstructing the scene again for a different descriptor or fully
retraining the network for each scene. Some recent methods instead infer pose
from feature similarity, but their lack of geometry constraints results in less
accuracy. To overcome these limitations, we propose a novel camera pose
refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as
GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing
3DGS model to render novel views, providing a lightweight solution that can be
directly applied to diverse scenes without additional training or fine-tuning.
Specifically, we introduce an iterative optimization approach, which refines
the camera pose using epipolar geometric constraints among the query and
multiple rendered images. Our method allows flexibly choosing feature
extractors and matchers to establish these constraints. Extensive empirical
evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate
that our method outperforms state-of-the-art camera pose refinement approaches,
achieving 53.3% and 56.9% reductions in median translation and rotation errors
on 7-Scenes, and 40.7% and 53.2% on Cambridge.

</details>


### [159] [Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection](https://arxiv.org/abs/2508.17877)
*Dabbrata Das,Mahshar Yahan,Md Tareq Zaman,Md Rishadul Bayesh*

Main category: cs.CV

TL;DR: Hybrid framework combining fine-tuned Vision Transformer with edge-based processing for AI-generated image detection, achieving 97.75% accuracy on CIFAKE dataset.


<details>
  <summary>Details</summary>
Motivation: Address limitations of conventional deep learning methods that overlook subtle structural inconsistencies and require substantial computational resources for detecting AI-generated images.

Method: Fine-tuned Vision Transformer combined with novel edge-based module that computes variance from edge-difference maps before/after smoothing, exploiting smoother textures and weaker edges in AI-generated content.

Result: Achieves 97.75% accuracy and 97.77% F1-score on CIFAKE dataset, surpassing state-of-the-art models across multiple benchmarks including Artistic and Custom Curated datasets.

Conclusion: Proposed framework is lightweight, interpretable, and effective for both still images and video frames, making it suitable for real-world applications in automated content verification and digital forensics.

Abstract: The rapid advancement of generative models has led to a growing prevalence of
highly realistic AI-generated images, posing significant challenges for digital
forensics and content authentication. Conventional detection methods mainly
rely on deep learning models that extract global features, which often overlook
subtle structural inconsistencies and demand substantial computational
resources. To address these limitations, we propose a hybrid detection
framework that combines a fine-tuned Vision Transformer (ViT) with a novel
edge-based image processing module. The edge-based module computes variance
from edge-difference maps generated before and after smoothing, exploiting the
observation that AI-generated images typically exhibit smoother textures,
weaker edges, and reduced noise compared to real images. When applied as a
post-processing step on ViT predictions, this module enhances sensitivity to
fine-grained structural cues while maintaining computational efficiency.
Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets
demonstrate that the proposed framework achieves superior detection performance
across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on
CIFAKE, surpassing widely adopted state-of-the-art models. These results
establish the proposed method as a lightweight, interpretable, and effective
solution for both still images and video frames, making it highly suitable for
real-world applications in automated content verification and digital
forensics.

</details>


### [160] [ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement](https://arxiv.org/abs/2508.17885)
*Raul Balmez,Alexandru Brateanu,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ISALux is a transformer-based low-light image enhancement method that integrates illumination and semantic priors using a novel self-attention block and MoE-based feed-forward network with LoRA adaptations to prevent overfitting.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of low-light image enhancement by integrating both illumination and semantic information, and to overcome overfitting issues caused by distinct light patterns in benchmarking datasets.

Method: Uses Hybrid Illumination and Semantics-Aware Multi-Headed Self-Attention (HISA-MSA) with two self-attention modules for independent processing of illumination and semantic features, plus a Mixture of Experts-based Feed-Forward Network with gating mechanism and low-rank matrix adaptations (LoRA).

Result: Extensive evaluations show ISALux is competitive with state-of-the-art methods across multiple specialized datasets, with ablation studies confirming the contribution of each component.

Conclusion: ISALux effectively integrates illumination and semantic priors for low-light image enhancement, demonstrating competitive performance while addressing overfitting through innovative architectural components.

Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.
Our architecture includes an original self-attention block, Hybrid Illumination
and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates
illumination and semantic segmentation maps for en- hanced feature extraction.
ISALux employs two self-attention modules to independently process illumination
and semantic features, selectively enriching each other to regulate luminance
and high- light structural variations in real-world scenarios. A Mixture of
Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,
with a gating mechanism conditionally activating the top K experts for
specialized processing. To address overfitting in LLIE methods caused by
distinct light patterns in benchmarking datasets, we enhance the HISA-MSA
module with low-rank matrix adaptations (LoRA). Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model. Code will be released upon publication.

</details>


### [161] [UniAPO: Unified Multimodal Automated Prompt Optimization](https://arxiv.org/abs/2508.17890)
*Qipeng Zhu,Yanzhe Chen,Huasong Zhong,Yan Li,Jie Chen,Zhixin Zhang,Junping Zhang,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniAPO is a unified multimodal automated prompt optimization framework that addresses visual token inflation and lack of process-level supervision in multimodal tasks through EM-inspired optimization and short-long term memory mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing automatic prompt optimization methods are effective for text-only inputs but face challenges in multimodal tasks due to visual token inflation (long visual sequences restricting context capacity) and lack of process-level supervision (focusing only on outcome-level feedback).

Method: UniAPO uses an EM-inspired optimization process that decouples feedback modeling and prompt refinement. It introduces a short-long term memory mechanism where historical feedback mitigates context limitations and historical prompts provide directional guidance for optimization.

Result: UniAPO achieves consistent performance gains across text, image, and video benchmarks, demonstrating effectiveness as a unified framework for efficient and transferable prompt optimization in multimodal scenarios.

Conclusion: UniAPO establishes the first framework specifically tailored for multimodal automated prompt optimization, successfully addressing the core challenges of visual token inflation and process-level supervision through its innovative memory mechanisms and stable optimization approach.

Abstract: Prompting is fundamental to unlocking the full potential of large language
models. To automate and enhance this process, automatic prompt optimization
(APO) has been developed, demonstrating effectiveness primarily in text-only
input scenarios. However, extending existing APO methods to multimodal tasks,
such as video-language generation introduces two core challenges: (i) visual
token inflation, where long visual token sequences restrict context capacity
and result in insufficient feedback signals; (ii) a lack of process-level
supervision, as existing methods focus on outcome-level supervision and
overlook intermediate supervision, limiting prompt optimization. We present
UniAPO: Unified Multimodal Automated Prompt Optimization, the first framework
tailored for multimodal APO. UniAPO adopts an EM-inspired optimization process
that decouples feedback modeling and prompt refinement, making the optimization
more stable and goal-driven. To further address the aforementioned challenges,
we introduce a short-long term memory mechanism: historical feedback mitigates
context limitations, while historical prompts provide directional guidance for
effective prompt optimization. UniAPO achieves consistent gains across text,
image, and video benchmarks, establishing a unified framework for efficient and
transferable prompt optimization.

</details>


### [162] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: Lightweight visual speech recognition architectures that reduce hardware costs while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Deep VSR systems have high computation costs that limit practical deployment in resource-constrained scenarios, preventing wider adoption.

Method: Develop lightweight end-to-end architectures using efficient image classification models and lightweight block designs in temporal convolution network backbones, following the two-network paradigm (visual feature extraction + sequence classification).

Result: Created several unified models with low resource requirements but strong recognition performance, validated on the largest public English words database.

Conclusion: The developed lightweight architectures effectively address the hardware cost issue while maintaining recognition performance, making VSR more practical for real-world applications.

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input
sequence using only the video data. Practical applications of such systems
include medical assistance as well as human-machine interactions. A VSR system
is typically employed in a complementary role in cases where the audio is
corrupt or not available. In order to accurately predict the spoken words,
these architectures often rely on deep neural networks in order to extract
meaningful representations from the input sequence. While deep architectures
achieve impressive recognition performance, relying on such models incurs
significant computation costs which translates into increased resource demands
in terms of hardware requirements and results in limited applicability in
real-world scenarios where resources might be constrained. This factor prevents
wider adoption and deployment of speech recognition systems in more practical
applications. In this work, we aim to alleviate this issue by developing
architectures for VSR that have low hardware costs. Following the standard
two-network design paradigm, where one network handles visual feature
extraction and another one utilizes the extracted features to classify the
entire sequence, we develop lightweight end-to-end architectures by first
benchmarking efficient models from the image classification literature, and
then adopting lightweight block designs in a temporal convolution network
backbone. We create several unified models with low resource requirements but
strong recognition performance. Experiments on the largest public database for
English words demonstrate the effectiveness and practicality of our developed
models. Code and trained models will be made publicly available.

</details>


### [163] [EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images](https://arxiv.org/abs/2508.17916)
*Xinning Yao,Bo Liu,Bojian Li,Jingjing Wang,Jinghua Yue,Fugen Zhou*

Main category: cs.CV

TL;DR: EndoUFM is an unsupervised monocular depth estimation framework for endoscopic surgery that integrates dual foundation models with adaptive fine-tuning and novel architectural components to overcome domain adaptation challenges in surgical environments.


<details>
  <summary>Details</summary>
Motivation: Existing monocular depth estimation techniques perform poorly in surgical environments due to varying illumination and complex textures. Visual foundation models trained on natural images have domain adaptability limitations and semantic perception deficiencies when applied to endoscopy.

Method: The framework integrates dual foundation models with a novel adaptive fine-tuning strategy using Random Vector Low-Rank Adaptation (RVLoRA), a Residual block based on Depthwise Separable Convolution (Res-DSC) for fine-grained feature capture, and a mask-guided smoothness loss for depth consistency within anatomical tissues.

Result: Extensive experiments on SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm state-of-the-art performance while maintaining efficient model size.

Conclusion: EndoUFM enhances surgeons' spatial perception during minimally invasive procedures, improving surgical precision and safety with important implications for augmented reality and navigation systems.

Abstract: Depth estimation is a foundational component for 3D reconstruction in
minimally invasive endoscopic surgeries. However, existing monocular depth
estimation techniques often exhibit limited performance to the varying
illumination and complex textures of the surgical environment. While powerful
visual foundation models offer a promising solution, their training on natural
images leads to significant domain adaptability limitations and semantic
perception deficiencies when applied to endoscopy. In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors. The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features. Furthermore, we design a
mask-guided smoothness loss to enforce depth consistency within anatomical
tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size. This work contributes to augmenting
surgeons' spatial perception during minimally invasive procedures, thereby
enhancing surgical precision and safety, with crucial implications for
augmented reality and navigation systems.

</details>


### [164] [Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation](https://arxiv.org/abs/2508.17924)
*Konstantin Egorov,Stepan Botman,Pavel Blinov,Galina Zubkova,Anton Ivaschenko,Alexander Kolsanov,Andrey Savchenko*

Main category: cs.CV

TL;DR: A large-scale multi-view video dataset for remote photoplethysmography (rPPG) with 3600 recordings from 600 subjects, captured under varied conditions with multiple cameras and paired with comprehensive physiological data.


<details>
  <summary>Details</summary>
Motivation: Existing rPPG datasets are limited by small size, privacy concerns with facial videos, and lack of diversity in conditions, hindering progress in remote health monitoring.

Method: Created a comprehensive dataset with 3600 synchronized video recordings from 600 subjects using multiple consumer-grade cameras at different angles, paired with 100 Hz PPG signals and extended health metrics including ECG, blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress levels.

Result: Trained an efficient rPPG model and compared its quality with existing approaches in cross-dataset scenarios, demonstrating improved performance.

Conclusion: The public release of this large-scale dataset and model should significantly accelerate progress in developing AI medical assistants for remote health monitoring.

Abstract: Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical
issues of existing publicly available datasets: small size, privacy concerns
with facial videos, and lack of diversity in conditions. The paper introduces a
novel comprehensive large-scale multi-view video dataset for rPPG and health
biomarkers estimation. Our dataset comprises 3600 synchronized video recordings
from 600 subjects, captured under varied conditions (resting and post-exercise)
using multiple consumer-grade cameras at different angles. To enable multimodal
analysis of physiological states, each recording is paired with a 100 Hz PPG
signal and extended health metrics, such as electrocardiogram, arterial blood
pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and
stress level. Using this data, we train an efficient rPPG model and compare its
quality with existing approaches in cross-dataset scenarios. The public release
of our dataset and model should significantly speed up the progress in the
development of AI medical assistants.

</details>


### [165] [See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops](https://arxiv.org/abs/2508.17932)
*Zixuan Dong,Baoyun Peng,Yufei Wang,Lin Liu,Xinxin Dong,Yunlong Cao,Xiaodong Wang*

Main category: cs.CV

TL;DR: CAVIA is a training-free framework that coordinates reasoning and visual attention for long-form video question answering, achieving state-of-the-art performance on multiple benchmarks through dynamic guidance of visual extraction based on reasoning needs.


<details>
  <summary>Details</summary>
Motivation: Current video QA systems decouple reasoning from perception, leading to either information loss through premature visual abstraction or computational inefficiency through exhaustive processing. Different queries require different visual evidence from the same video content.

Method: CAVIA creates a closed-loop system with three innovations: (1) hierarchical reasoning and guided localization to precise frames, (2) cross-modal semantic bridging for targeted extraction, and (3) confidence-driven iterative synthesis.

Result: Achieves state-of-the-art performance: EgoSchema (65.7%, +5.3%), NExT-QA (76.1%, +2.6%), and IntentQA (73.8%, +6.9%)

Conclusion: Dynamic reasoning-perception coordination provides a scalable paradigm for video understanding, demonstrating that adaptive visual extraction guided by reasoning requirements significantly improves performance.

Abstract: Human video comprehension demonstrates dynamic coordination between reasoning
and visual attention, adaptively focusing on query-relevant details. However,
current long-form video question answering systems employ rigid pipelines that
decouple reasoning from perception, leading to either information loss through
premature visual abstraction or computational inefficiency through exhaustive
processing. The core limitation lies in the inability to adapt visual
extraction to specific reasoning requirements, different queries demand
fundamentally different visual evidence from the same video content. In this
work, we present CAVIA, a training-free framework that revolutionizes video
understanding through reasoning, perception coordination. Unlike conventional
approaches where visual processing operates independently of reasoning, CAVIA
creates a closed-loop system where reasoning continuously guides visual
extraction based on identified information gaps. CAVIA introduces three
innovations: (1) hierarchical reasoning, guided localization to precise frames;
(2) cross-modal semantic bridging for targeted extraction; (3)
confidence-driven iterative synthesis. CAVIA achieves state-of-the-art
performance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA
(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic
reasoning-perception coordination provides a scalable paradigm for video
understanding.

</details>


### [166] [Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets](https://arxiv.org/abs/2508.17961)
*Tina Dorosti,Johannes Thalhammer,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 2D U-Nets outperform 2.5D and 3D approaches for sparse-sampling streak artifact correction in CT scans across parallel, fan, and cone beam geometries.


<details>
  <summary>Details</summary>
Motivation: To investigate how different beam geometries and input data dimensions affect sparse-sampling artifact correction in clinical CT scans, aiming to incorporate volumetric context to improve model performance.

Method: Used 22 subjects' CT scans, simulated sparse sampling with Astra toolbox for parallel/fan/cone beam geometries. Trained 2D and 3D U-Nets on 14 subjects, tested on 8. Compared 2D (512x512), 2.5D (64x64x3 from three orthogonal planes), and 3D (64x64x64 blocks) input dimensions.

Result: 2D U-Net trained on axial 2D slices achieved the best performance across all beam geometries, with superior MSE and SSIM values compared to 2.5D and 3D approaches.

Conclusion: Despite efforts to incorporate volumetric context, traditional 2D U-Net processing of axial slices remains most effective for sparse-sampling artifact correction in CT imaging.

Abstract: This study aims to investigate the effect of various beam geometries and
dimensions of input data on the sparse-sampling streak artifact correction task
with U-Nets for clinical CT scans as a means of incorporating the volumetric
context into artifact reduction tasks to improve model performance. A total of
22 subjects were retrospectively selected (01.2016-12.2018) from the Technical
University of Munich's research hospital, TUM Klinikum rechts der Isar.
Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,
fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and
3D U-Nets were trained and validated on 14, and tested on 8 subjects,
respectively. For the dimensionality study, in addition to the 512x512 2D CT
images, the CT scans were further pre-processed to generate a so-called '2.5D',
and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data
refers to individual 64-voxel blocks. An axial, coronal, and sagittal cut
through the center of each block resulted in three 64x64 2D patches that were
rearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance
was assessed with the mean squared error (MSE) and structural similarity index
measure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices
results in the best MSE and SSIM values, outperforming the 2.5D and 3D input
data dimensions.

</details>


### [167] [SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization](https://arxiv.org/abs/2508.17972)
*Junyuan Deng,Heng Li,Tao Xie,Weiqiang Ren,Qian Zhang,Ping Tan,Xiaoyang Guo*

Main category: cs.CV

TL;DR: SAIL-Recon is a feed-forward Transformer that enhances scene regression methods to handle large-scale Structure-from-Motion by incorporating visual localization capabilities and neural scene representations.


<details>
  <summary>Details</summary>
Motivation: Existing scene regression methods like VGGT perform well with extreme viewpoint changes but struggle with large numbers of input images, limiting their scalability for large-scale scenes.

Method: The method first computes a neural scene representation from anchor images, then fine-tunes a regression network to reconstruct all input images conditioned on this representation, combining scene regression with visual localization.

Result: SAIL-Recon achieves state-of-the-art results on camera pose estimation and novel view synthesis benchmarks (TUM-RGBD, CO3Dv2, Tanks & Temples) while scaling efficiently to large-scale scenes.

Conclusion: The approach successfully addresses scalability limitations of scene regression methods and demonstrates superior performance across multiple benchmarks, with code and models made publicly available.

Abstract: Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)
problem by directly regressing camera poses and 3D scene structures from input
images. They demonstrate impressive performance in handling images under
extreme viewpoint changes. However, these methods struggle to handle a large
number of input images. To address this problem, we introduce SAIL-Recon, a
feed-forward Transformer for large scale SfM, by augmenting the scene
regression network with visual localization capabilities. Specifically, our
method first computes a neural scene representation from a subset of anchor
images. The regression network is then fine-tuned to reconstruct all input
images conditioned on this neural scene representation. Comprehensive
experiments show that our method not only scales efficiently to large-scale
scenes, but also achieves state-of-the-art results on both camera pose
estimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and
Tanks & Temples. We will publish our model and code. Code and models are
publicly available at: https://hkust-sail.github.io/ sail-recon/.

</details>


### [168] [Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving](https://arxiv.org/abs/2508.17975)
*Md Shahi Amran Hossain,Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.CV

TL;DR: Novel hybrid computer vision architecture combining YOLOv8 and 5-layer CNN for robust object detection in autonomous driving, achieving >90% accuracy in drifted environments using synthetic training data.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in autonomous driving by improving object detection accuracy under challenging conditions like adverse weather and low lighting that cause data drift and degrade model performance.

Method: Developed a dual-mode hybrid framework using YOLOv8 for fast detection and a five-layer CNN for verification, trained with thousands of synthetic road environment images to enhance robustness against data drift.

Result: The system achieved over 90% detection accuracy when tested with drift-augmented road images, demonstrating significant improvement in performance under challenging conditions.

Conclusion: The hybrid model architecture effectively enhances road safety by providing robust object detection in drifted environments, showing the value of combining different computer vision approaches for autonomous driving applications.

Abstract: The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.

</details>


### [169] [Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization](https://arxiv.org/abs/2508.17976)
*Keyang Zhang,Chenqi Kong,Hui Liu,Bo Ding,Xinghao Jiang,Haoliang Li*

Main category: cs.CV

TL;DR: A Propose-Rectify framework that combines MLLM semantic reasoning with forensic analysis for precise image manipulation detection and localization.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with perceiving subtle forensic artifacts needed for accurate manipulation localization despite their strong semantic understanding capabilities.

Method: Two-stage framework: 1) Forensic-adapted LLaVA generates initial proposals using semantic reasoning, 2) Forensics Rectification Module validates proposals with multi-scale forensic feature analysis and Enhanced Segmentation Module incorporates forensic cues into SAM for precise boundary delineation.

Result: State-of-the-art performance across diverse datasets with exceptional robustness and generalization capabilities.

Conclusion: The framework effectively bridges semantic reasoning with forensic-specific analysis, ensuring comprehensive detection accuracy and precise localization of manipulated regions.

Abstract: The increasing sophistication of image manipulation techniques demands robust
forensic solutions that can both reliably detect alterations and precisely
localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show
promise by leveraging world knowledge and semantic understanding for
context-aware detection, yet they struggle with perceiving subtle, low-level
forensic artifacts crucial for accurate manipulation localization. This paper
presents a novel Propose-Rectify framework that effectively bridges semantic
reasoning with forensic-specific analysis. In the proposal stage, our approach
utilizes a forensic-adapted LLaVA model to generate initial manipulation
analysis and preliminary localization of suspicious regions based on semantic
understanding and contextual reasoning. In the rectification stage, we
introduce a Forensics Rectification Module that systematically validates and
refines these initial proposals through multi-scale forensic feature analysis,
integrating technical evidence from several specialized filters. Additionally,
we present an Enhanced Segmentation Module that incorporates critical forensic
cues into SAM's encoded image embeddings, thereby overcoming inherent semantic
biases to achieve precise delineation of manipulated regions. By
synergistically combining advanced multimodal reasoning with established
forensic methodologies, our framework ensures that initial semantic proposals
are systematically validated and enhanced through concrete technical evidence,
resulting in comprehensive detection accuracy and localization precision.
Extensive experimental validation demonstrates state-of-the-art performance
across diverse datasets with exceptional robustness and generalization
capabilities.

</details>


### [170] [Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection](https://arxiv.org/abs/2508.18007)
*Xinyue Liu,Jianyuan Wang,Biao Leng,Shuo Zhang*

Main category: cs.CV

TL;DR: Proposes Cross-Domain Distillation framework for Fully Unsupervised Anomaly Detection using teacher-student knowledge distillation to handle training data containing anomalies.


<details>
  <summary>Details</summary>
Motivation: Fully Unsupervised Anomaly Detection (FUAD) aims to detect anomalies without any labels, even when training data contains anomalous samples, which traditional methods struggle with.

Method: Uses Cross-Domain Distillation with Domain-Specific Training (dividing data into domains with lower anomaly ratios) and Cross-Domain Knowledge Aggregation where pseudo-normal features guide a global student model.

Result: Achieves significant performance improvements on noisy MVTec AD and VisA datasets compared to baseline methods.

Conclusion: The proposed CDD framework effectively addresses the challenge of learning normal representations when training data contains anomalies, validating its effectiveness for FUAD settings.

Abstract: Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of
Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any
labels even when the training set may contain anomalous samples. To achieve
FUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based
on teacher-student framework into the FUAD setting. However, due to the
presence of anomalies in the training data, traditional KD methods risk
enabling the student to learn the teacher's representation of anomalies under
FUAD setting, thereby resulting in poor anomaly detection performance. To
address this issue, we propose a novel Cross-Domain Distillation (CDD)
framework based on the widely studied reverse distillation (RD) paradigm.
Specifically, we design a Domain-Specific Training, which divides the training
set into multiple domains with lower anomaly ratios and train a domain-specific
student for each. Cross-Domain Knowledge Aggregation is then performed, where
pseudo-normal features generated by domain-specific students collaboratively
guide a global student to learn generalized normal representations across all
samples. Experimental results on noisy versions of the MVTec AD and VisA
datasets demonstrate that our method achieves significant performance
improvements over the baseline, validating its effectiveness under FUAD
setting.

</details>


### [171] [Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria](https://arxiv.org/abs/2508.18012)
*Sochukwuma Nwokoye,Desmond Moru*

Main category: cs.CV

TL;DR: Neural network system for identifying Nigerian currency to assist visually impaired individuals with cash transactions, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: To help visually impaired individuals differentiate various forms of cash and streamline commercial transactions using artificial intelligence.

Method: Built a custom dataset of 3,468 images and trained an SSD (Single Shot MultiBox Detector) neural network model for cash recognition.

Result: The system achieved over 90% Mean Average Precision score in accurately identifying Nigerian currency.

Conclusion: The system has strong potential to contribute to assistive technology and improve quality of life for visually impaired people in Nigeria and beyond.

Abstract: Neural networks in assistive technology for visually impaired leverage
artificial intelligence's capacity to recognize patterns in complex data. They
are used for converting visual data into auditory or tactile representations,
helping the visually impaired understand their surroundings. The primary aim of
this research is to explore the potential of artificial neural networks to
facilitate the differentiation of various forms of cash for individuals with
visual impairments. In this study, we built a custom dataset of 3,468 images,
which was subsequently used to train an SSD neural network model. The proposed
system can accurately identify Nigerian cash, thereby streamlining commercial
transactions. The performance of the system in terms of accuracy was assessed,
and the Mean Average Precision score was over 90%. We believe that our system
has the potential to make a substantial contribution to the field of assistive
technology while also improving the quality of life of visually challenged
persons in Nigeria and beyond.

</details>


### [172] [Towards Continual Visual Anomaly Detection in the Medical Domain](https://arxiv.org/abs/2508.18013)
*Manuel Barusco,Francesco Borsatti,Nicola Beda,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: First study applying continual learning to visual anomaly detection in medical imaging, using PatchCoreCL model on BMAD dataset with excellent performance retention (<1% forgetting).


<details>
  <summary>Details</summary>
Motivation: Medical imaging data evolves over time, causing model performance degradation. Continual learning provides framework to adapt models incrementally while preserving knowledge, but this approach hasn't been explored for visual anomaly detection in medical field.

Method: Used PatchCoreCL (continual learning version of PatchCore model) and evaluated on BMAD dataset - real-world medical imaging data with image-level and pixel-level annotations.

Result: PatchCoreCL achieved performance comparable to task-specific models with forgetting value less than 1%, demonstrating effective knowledge retention.

Conclusion: Continual learning is feasible and promising for adaptive visual anomaly detection in medical imaging, enabling models to evolve with changing data distributions while maintaining performance.

Abstract: Visual Anomaly Detection (VAD) seeks to identify abnormal images and
precisely localize the corresponding anomalous regions, relying solely on
normal data during training. This approach has proven essential in domains such
as manufacturing and, more recently, in the medical field, where accurate and
explainable detection is critical. Despite its importance, the impact of
evolving input data distributions over time has received limited attention,
even though such changes can significantly degrade model performance. In
particular, given the dynamic and evolving nature of medical imaging data,
Continual Learning (CL) provides a natural and effective framework to
incrementally adapt models while preserving previously acquired knowledge. This
study explores for the first time the application of VAD models in a CL
scenario for the medical field. In this work, we utilize a CL version of the
well-established PatchCore model, called PatchCoreCL, and evaluate its
performance using BMAD, a real-world medical imaging dataset with both
image-level and pixel-level annotations. Our results demonstrate that
PatchCoreCL is an effective solution, achieving performance comparable to the
task-specific models, with a forgetting value less than a 1%, highlighting the
feasibility and potential of CL for adaptive VAD in medical imaging.

</details>


### [173] [FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction](https://arxiv.org/abs/2508.18031)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: A novel framework using generative models (CycleGANs, cGANs) for craniofacial reconstruction from 2D X-ray images, addressing limitations of traditional methods and achieving realistic face generation for forensic identification.


<details>
  <summary>Details</summary>
Motivation: Traditional craniofacial reconstruction methods require expert knowledge and are time-consuming, while existing probabilistic models fail to capture cross-domain skull-face attributes. There's a need for automated, efficient forensic identification tools.

Method: Used various generative models (CycleGANs, cGANs) fine-tuned to generate realistic images across skull and face domains from 2D X-ray inputs. Proposed a retrieval framework where generated faces are matched against real face databases.

Result: First use of 2D X-rays as skull representation for generative craniofacial reconstruction. Evaluated using FID, IS, and SSIM scores showing quality generation. Experimental results demonstrate effectiveness for forensic applications.

Conclusion: The proposed framework provides an effective automated tool for forensic science, enabling craniofacial reconstruction from 2D X-rays with promising results for victim identification when other methods fail.

Abstract: Craniofacial reconstruction in forensics is one of the processes to identify
victims of crime and natural disasters. Identifying an individual from their
remains plays a crucial role when all other identification methods fail.
Traditional methods for this task, such as clay-based craniofacial
reconstruction, require expert domain knowledge and are a time-consuming
process. At the same time, other probabilistic generative models like the
statistical shape model or the Basel face model fail to capture the skull and
face cross-domain attributes. Looking at these limitations, we propose a
generic framework for craniofacial reconstruction from 2D X-ray images. Here,
we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune
the generator and discriminator parts to generate more realistic images in two
distinct domains, which are the skull and face of an individual. This is the
first time where 2D X-rays are being used as a representation of the skull by
generative models for craniofacial reconstruction. We have evaluated the
quality of generated faces using FID, IS, and SSIM scores. Finally, we have
proposed a retrieval framework where the query is the generated face image and
the gallery is the database of real faces. By experimental results, we have
found that this can be an effective tool for forensic science.

</details>


### [174] [Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation](https://arxiv.org/abs/2508.18032)
*Yaqi Li,Peng Chen,Mingyang Han,Bu Pi,Haoxiang Shi,Runzhou Zhao,Yang Yao,Xuan Zhang,Jun Song*

Main category: cs.CV

TL;DR: Visual-CoG proposes stage-aware rewards throughout image generation pipeline to address limitations of final-only guidance in autoregressive text-to-image models, achieving significant improvements on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing autoregressive T2I models struggle with multi-attribute and ambiguous prompts, and current reinforcement learning approaches only provide rewards at the final generation stage, making it difficult to identify which stages contribute positively to outcomes.

Method: Proposes Visual-Chain of Guidance (Visual-CoG) paradigm with three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. Also constructs VisCog-Bench benchmark.

Result: Achieves improvements of 15% on GenEval, 5% on T2I-CompBench, and 19% on the proposed VisCog-Bench, demonstrating superior performance compared to existing methods.

Conclusion: The Visual-CoG paradigm with stage-aware rewards effectively addresses limitations of final-only guidance in text-to-image generation, significantly improving performance on complex and ambiguous prompts across multiple benchmarks.

Abstract: Despite the promising progress of recent autoregressive models in
text-to-image (T2I) generation, their ability to handle multi-attribute and
ambiguous prompts remains limited. To address these limitations, existing works
have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and
employed reinforcement learning (RL) to improve reasoning capabilities.
However, most models provide reward signals only at the end of the generation
stage. This monolithic final-only guidance makes it difficult to identify which
stages contribute positively to the final outcome and may lead to suboptimal
policies. To tackle this issue, we propose a Visual-Chain of Guidance
(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process
refining, and outcome evaluation, with stage-aware rewards providing immediate
guidance throughout the image generation pipeline. We further construct a
visual cognition benchmark, VisCog-Bench, which comprises four subtasks to
evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on
GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,
5%, and 19%, respectively, demonstrating the superior performance of the
proposed Visual-CoG. We will release all the resources soon.

</details>


### [175] [ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation](https://arxiv.org/abs/2508.18050)
*Jianwen Tan,Huiyao Zhang,Rui Xiong,Han Zhou,Hongfei Wang,Ye Li*

Main category: cs.CV

TL;DR: ArgusCogito is a zero-shot chain-of-thought framework for camouflaged object segmentation that uses cross-modal reasoning and omnidirectional attention inspired by the Hundred-eyed Giant perceptual strategy, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for camouflaged object segmentation suffer from shallow feature representation, inadequate reasoning mechanisms, and weak cross-modal integration, leading to incomplete target separation and imprecise segmentation results.

Method: Three-stage cognitive framework: 1) Conjecture - global reasoning with cross-modal fusion (RGB, depth, semantic maps) for holistic understanding; 2) Focus - omnidirectional attention-driven scanning for precise localization; 3) Sculpting - iterative generation of dense point prompts for high-fidelity mask generation.

Result: Achieves state-of-the-art performance on four challenging COS benchmarks and three Medical Image Segmentation benchmarks, demonstrating exceptional efficacy, superior generalization capability, and robustness.

Conclusion: The cognitive-inspired framework with cross-modal synergy and omnidirectional reasoning effectively addresses the challenges of camouflaged object segmentation, providing a powerful zero-shot solution that outperforms existing methods.

Abstract: Camouflaged Object Segmentation (COS) poses a significant challenge due to
the intrinsic high similarity between targets and backgrounds, demanding models
capable of profound holistic understanding beyond superficial cues. Prevailing
methods, often limited by shallow feature representation, inadequate reasoning
mechanisms, and weak cross-modal integration, struggle to achieve this depth of
cognition, resulting in prevalent issues like incomplete target separation and
imprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed
Giant-emphasizing holistic observation, omnidirectional focus, and intensive
scrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought
framework underpinned by cross-modal synergy and omnidirectional reasoning
within Vision-Language Models (VLMs). ArgusCogito orchestrates three
cognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive
prior through global reasoning with cross-modal fusion (RGB, depth, semantic
maps), enabling holistic scene understanding and enhanced target-background
disambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning
and focused reasoning, guided by semantic priors from Conjecture, enabling
precise target localization and region-of-interest refinement. (3) Sculpting:
Progressively sculpts high-fidelity segmentation masks by integrating
cross-modal information and iteratively generating dense positive/negative
point prompts within focused regions, emulating Argus' intensive scrutiny.
Extensive evaluations on four challenging COS benchmarks and three Medical
Image Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves
state-of-the-art (SOTA) performance, validating the framework's exceptional
efficacy, superior generalization capability, and robustness.

</details>


### [176] [Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images](https://arxiv.org/abs/2508.18067)
*Kaiyu Li,Xiangyong Cao,Ruixun Liu,Shihong Wang,Zixuan Jiang,Zhi Wang,Deyu Meng*

Main category: cs.CV

TL;DR: SegEarth-OV is the first annotation-free open-vocabulary segmentation framework for remote sensing images, featuring SimFeatUp for high-resolution detail restoration and Global Bias Alleviation for enhanced semantic fidelity, with AlignEarth enabling cross-modal knowledge transfer to SAR images.


<details>
  <summary>Details</summary>
Motivation: Remote sensing image segmentation faces challenges with new object categories and expensive manual annotations. Existing natural image frameworks struggle with RS data's scale variations and fine details, requiring costly adaptations.

Method: Proposes SegEarth-OV framework with SimFeatUp upsampler for high-resolution detail restoration, Global Bias Alleviation operation for local semantic enhancement, and AlignEarth for cross-modal knowledge transfer from optical to SAR encoders without building new foundation models.

Result: Extensive experiments on optical and SAR datasets show dramatic improvements over state-of-the-art methods, validating the framework's effectiveness for annotation-free open-vocabulary segmentation.

Conclusion: SegEarth-OV establishes a robust foundation for annotation-free and open-world Earth observation, successfully addressing the unique challenges of remote sensing image segmentation across diverse sensor types.

Abstract: Semantic segmentation of remote sensing (RS) images is pivotal for
comprehensive Earth observation, but the demand for interpreting new object
categories, coupled with the high expense of manual annotation, poses
significant challenges. Although open-vocabulary semantic segmentation (OVSS)
offers a promising solution, existing frameworks designed for natural images
are insufficient for the unique complexities of RS data. They struggle with
vast scale variations and fine-grained details, and their adaptation often
relies on extensive, costly annotations. To address this critical gap, this
paper introduces SegEarth-OV, the first framework for annotation-free
open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,
a universal upsampler that robustly restores high-resolution spatial details
from coarse features, correcting distorted target shapes without any
task-specific post-training. We also present a simple yet effective Global Bias
Alleviation operation to subtract the inherent global context from patch
features, significantly enhancing local semantic fidelity. These components
empower SegEarth-OV to effectively harness the rich semantics of pre-trained
VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the
framework's universality to other challenging RS modalities like SAR images,
where large-scale VLMs are unavailable and expensive to create, we introduce
AlignEarth, which is a distillation-based strategy and can efficiently transfer
semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the
need to build SAR foundation models from scratch and enabling universal OVSS
across diverse sensor types. Extensive experiments on both optical and SAR
datasets validate that SegEarth-OV can achieve dramatic improvements over the
SOTA methods, establishing a robust foundation for annotation-free and
open-world Earth observation.

</details>


### [177] [EventTracer: Fast Path Tracing-based Event Stream Rendering](https://arxiv.org/abs/2508.18071)
*Zhenyang Li,Xiaoyang Bai,Jinfan Lu,Pengfei Shen,Edmund Y. Lam,Yifan Peng*

Main category: cs.CV

TL;DR: EventTracer is a path tracing-based rendering pipeline that efficiently simulates high-fidelity event sequences from 3D scenes using low SPP path tracing and a lightweight event spiking network with BiLIF units and EMD loss.


<details>
  <summary>Details</summary>
Motivation: Existing event stream simulation methods work with costly noiseless RGB frames and achieve only 100-300 FPS temporal resolution, far lower than real-world event data. There's a need for efficient, high-fidelity event simulation without expensive hardware setups.

Method: Uses low sample-per-pixel path tracing for efficient rendering, trains a lightweight event spiking network with bipolar leaky integrate-and-fired units and bidirectional earth mover distance loss to denoise RGB videos into realistic event sequences.

Result: EventTracer runs at about 4 minutes per second of 720p video, captures better scene details, and shows greater similarity to real-world event data than other simulators in downstream tasks.

Conclusion: EventTracer is a promising tool for creating large-scale event-RGB datasets at low cost, narrowing the sim-to-real gap in event-based vision, and boosting applications in robotics, autonomous driving, and VR/AR.

Abstract: Simulating event streams from 3D scenes has become a common practice in
event-based vision research, as it meets the demand for large-scale, high
temporal frequency data without setting up expensive hardware devices or
undertaking extensive data collections. Yet existing methods in this direction
typically work with noiseless RGB frames that are costly to render, and
therefore they can only achieve a temporal resolution equivalent to 100-300
FPS, far lower than that of real-world event data. In this work, we propose
EventTracer, a path tracing-based rendering pipeline that simulates
high-fidelity event sequences from complex 3D scenes in an efficient and
physics-aware manner. Specifically, we speed up the rendering process via low
sample-per-pixel (SPP) path tracing, and train a lightweight event spiking
network to denoise the resulting RGB videos into realistic event sequences. To
capture the physical properties of event streams, the network is equipped with
a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a
bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at
a speed of about 4 minutes per second of 720p video, and it inherits the merit
of accurate spatiotemporal modeling from its path tracing backbone. We show in
two downstream tasks that EventTracer captures better scene details and
demonstrates a greater similarity to real-world event data than other event
simulators, which establishes it as a promising tool for creating large-scale
event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based
vision, and boosting various application scenarios such as robotics, autonomous
driving, and VRAR.

</details>


### [178] [Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering](https://arxiv.org/abs/2508.18075)
*Chun Liu,Chen Zhang,Zhuo Li,Zheng Li,Wei Yang*

Main category: cs.CV

TL;DR: A prototype learning and clustering method for open-set few-shot HSI classification that not only rejects unknown classes but also discovers and clusters them into distinct categories.


<details>
  <summary>Details</summary>
Motivation: Current open-set HSI classification methods only reject unknown class samples without identifying or discovering the unknown classes themselves, leaving potential valuable information unexplored.

Method: Uses prototype learning with few labeled samples to infer unknown class prototypes while distinguishing known classes. Clusters rejected unknown samples based on distance to inferred prototypes.

Result: Extensive experiments on four benchmark HSI datasets show competitive performance compared to state-of-the-art methods in open-set few-shot classification tasks.

Conclusion: The proposed method successfully addresses the limitation of existing approaches by not only rejecting unknown samples but also discovering and clustering them into meaningful unknown classes.

Abstract: Open-set few-shot hyperspectral image (HSI) classification aims to classify
image pixels by using few labeled pixels per class, where the pixels to be
classified may be not all from the classes that have been seen. To address the
open-set HSI classification challenge, current methods focus mainly on
distinguishing the unknown class samples from the known class samples and
rejecting them to increase the accuracy of identifying known class samples.
They fails to further identify or discovery the unknow classes among the
samples. This paper proposes a prototype learning and clustering method for
discoverying unknown classes in HSIs under the few-shot environment. Using few
labeled samples, it strives to develop the ability of infering the prototypes
of unknown classes while distinguishing unknown classes from known classes.
Once the unknown class samples are rejected by the learned known class
classifier, the proposed method can further cluster the unknown class samples
into different classes according to their distance to the inferred unknown
class prototypes. Compared to existing state-of-the-art methods, extensive
experiments on four benchmark HSI datasets demonstrate that our proposed method
exhibits competitive performance in open-set few-shot HSI classification tasks.
All the codes are available at \href{https://github.com/KOBEN-ff/OpenFUCD-main}
{https://github.com/KOBEN-ff/OpenFUCD-main}

</details>


### [179] [Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2508.18095)
*Zhicong Tang,Tiankai Hang,Shuyang Gu,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: Unifies Score-based Generative Models and Schrödinger Bridge through three reparameterization techniques (IPMM, IPTM, IPFM) that accelerate and stabilize training, with novel SGM-based initialization strategies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between Score-based Generative Models (SGMs/Diffusion models) and Schrödinger Bridge problems, enabling more efficient training and improved performance by leveraging advantages of both approaches.

Method: Proposes three iterative reparameterization techniques: IPMM (Mean-Matching), IPTM (Terminus-Matching), and IPFM (Flow-Matching). Introduces initialization strategies using pre-trained SGMs to train SB-based models effectively.

Result: Significant acceleration and stabilization of SB-based model training. Improved performance of both SB-based models and SGMs through the unified approach. Extensive experiments demonstrate effectiveness.

Conclusion: This work successfully unifies SGMs and Schrödinger Bridge problems, providing efficient training methods and paving the way for future generative model research.

Abstract: This paper aims to unify Score-based Generative Models (SGMs), also known as
Diffusion models, and the Schr\"odinger Bridge (SB) problem through three
reparameterization techniques: Iterative Proportional Mean-Matching (IPMM),
Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional
Flow-Matching (IPFM). These techniques significantly accelerate and stabilize
the training of SB-based models. Furthermore, the paper introduces novel
initialization strategies that use pre-trained SGMs to effectively train
SB-based models. By using SGMs as initialization, we leverage the advantages of
both SB-based models and SGMs, ensuring efficient training of SB-based models
and further improving the performance of SGMs. Extensive experiments
demonstrate the significant effectiveness and improvements of the proposed
methods. We believe this work contributes to and paves the way for future
research on generative models.

</details>


### [180] [BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines](https://arxiv.org/abs/2508.18136)
*Nico Klar,Nizam Gifary,Felix P. G. Ziegler,Frank Sehnke,Anton Kaifel,Eric Price,Aamir Ahmad*

Main category: cs.CV

TL;DR: BirdRecorder is an AI-based anti-collision system that uses SSD detection, hardware acceleration, and tracking algorithms to protect endangered birds from wind turbine collisions in real-time.


<details>
  <summary>Details</summary>
Motivation: Address the conflict between renewable energy expansion (wind power) and wildlife conservation by preventing bird-turbine collisions, particularly for endangered species like the red kite.

Method: Integrates robotics, telemetry, and AI algorithms with Single Shot Detector (SSD) for detection, specialized hardware acceleration, and tracking algorithms for real-time image processing within 800m range.

Result: Outperforms existing approaches in both accuracy and efficiency, achieving high detection precision while maintaining necessary speed for real-time decision-making.

Conclusion: BirdRecorder successfully bridges renewable energy expansion with wildlife conservation, enabling sustainable coexistence of technology and nature through effective bird protection.

Abstract: The urgent need for renewable energy expansion, particularly wind power, is
hindered by conflicts with wildlife conservation. To address this, we developed
BirdRecorder, an advanced AI-based anti-collision system to protect endangered
birds, especially the red kite (Milvus milvus). Integrating robotics,
telemetry, and high-performance AI algorithms, BirdRecorder aims to detect,
track, and classify avian species within a range of 800 m to minimize
bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and
software architectures to enable real-time image processing. Leveraging Single
Shot Detector (SSD) for detection, combined with specialized hardware
acceleration and tracking algorithms, our system achieves high detection
precision while maintaining the speed necessary for real-time decision-making.
By combining these components, BirdRecorder outperforms existing approaches in
both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the
BirdRecorder system. By bridging the gap between renewable energy expansion and
wildlife conservation, BirdRecorder contributes to a more sustainable
coexistence of technology and nature.

</details>


### [181] [Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability](https://arxiv.org/abs/2508.18154)
*Syamantak Sarkar,Revoti P. Bora,Bhupender Kaushal,Sudhish N George,Kiran Raja*

Main category: cs.CV

TL;DR: Evaluation of Class Activation Maps (CAMs) robustness to noise perturbations across architectures and datasets, proposing a new robustness metric measuring consistency and responsiveness.


<details>
  <summary>Details</summary>
Motivation: CAMs are important for visualizing deep learning model regions, but their robustness to different noise types remains underexplored and needs systematic evaluation.

Method: Evaluated various CAM methods under different noise perturbations across multiple architectures and datasets. Proposed a robustness metric with two properties: consistency (stability under non-class-changing perturbations) and responsiveness (sensitivity to prediction changes).

Result: Found considerable variability in noise sensitivity for different CAMs. The proposed metric was empirically evaluated across models, perturbations, and datasets with complementary statistical tests.

Conclusion: The study highlights the need for robust CAM evaluation and provides a practical metric to assess CAM robustness, demonstrating its applicability across different scenarios.

Abstract: Class Activation Maps (CAMs) are one of the important methods for visualizing
regions used by deep learning models. Yet their robustness to different noise
remains underexplored. In this work, we evaluate and report the resilience of
various CAM methods for different noise perturbations across multiple
architectures and datasets. By analyzing the influence of different noise types
on CAM explanations, we assess the susceptibility to noise and the extent to
which dataset characteristics may impact explanation stability. The findings
highlight considerable variability in noise sensitivity for various CAMs. We
propose a robustness metric for CAMs that captures two key properties:
consistency and responsiveness. Consistency reflects the ability of CAMs to
remain stable under input perturbations that do not alter the predicted class,
while responsiveness measures the sensitivity of CAMs to changes in the
prediction caused by such perturbations. The metric is evaluated empirically
across models, different perturbations, and datasets along with complementary
statistical tests to exemplify the applicability of our proposed approach.

</details>


### [182] [SpotEdit: Evaluating Visually-Guided Image Editing Methods](https://arxiv.org/abs/2508.18159)
*Sara Ghazanfari,Wei-An Lin,Haitong Tian,Ersin Yumer*

Main category: cs.CV

TL;DR: SpotEdit is a comprehensive benchmark for visually-guided image editing that evaluates diverse generative models and reveals significant performance gaps, including addressing the critical issue of hallucination where models incorrectly perceive visual cues.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations for visually-guided image editing are too simple and insufficient for real-world editing challenges, creating a need for a more comprehensive benchmark that can systematically assess different generative models.

Method: The authors developed SpotEdit benchmark that systematically evaluates diverse diffusion, autoregressive, and hybrid generative models across various editing tasks, with a dedicated component specifically designed to assess hallucination issues.

Result: The benchmark uncovered substantial performance disparities between different generative models and revealed that leading models like GPT-4o often hallucinate visual cues and erroneously perform editing tasks.

Conclusion: SpotEdit provides a comprehensive evaluation framework that addresses critical challenges in visually-guided image editing, particularly highlighting the hallucination problem, and the benchmark is publicly released for community use.

Abstract: Visually-guided image editing, where edits are conditioned on both visual
cues and textual prompts, has emerged as a powerful paradigm for fine-grained,
controllable content generation. Although recent generative models have shown
remarkable capabilities, existing evaluations remain simple and insufficiently
representative of real-world editing challenges. We present SpotEdit, a
comprehensive benchmark designed to systematically assess visually-guided image
editing methods across diverse diffusion, autoregressive, and hybrid generative
models, uncovering substantial performance disparities. To address a critical
yet underexplored challenge, our benchmark includes a dedicated component on
hallucination, highlighting how leading models, such as GPT-4o, often
hallucinate the existence of a visual cue and erroneously perform the editing
task. Our code and benchmark are publicly released at
https://github.com/SaraGhazanfari/SpotEdit.

</details>


### [183] [Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance](https://arxiv.org/abs/2508.18177)
*Xiangxiang Wang,Xuanyu Wang,YiJia Luo,Yongbin Yu,Manping Fan,Jingtao Zhang,Liyong Ren*

Main category: cs.CV

TL;DR: Dual innovation framework combining cross-modal quantization for VLMs and scene-aware multi-agent system for visually impaired assistance, reducing memory from 38GB to 16GB with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To develop efficient vision-language models and assistive technology for visually impaired users, addressing high memory requirements and enabling comprehensive real-time assistance.

Method: Cross-modal differentiated quantization framework for VLMs + scene-aware vectorized memory multi-agent system with perception-memory-reasoning workflows and persistent storage.

Result: Quantized 19B-parameter model shows only 2.05% performance drop on MMBench, maintains 63.7 accuracy on OCR-VQA, reduces memory from 38GB to 16GB, and achieves 2.83-3.52s response latency.

Conclusion: The framework advances computational efficiency and assistive technology, providing visually impaired users with real-time scene perception, text recognition, and navigation assistance.

Abstract: This study proposes the dual technological innovation framework, including a
cross-modal differ entiated quantization framework for vision-language models
(VLMs) and a scene-aware vectorized
  memory multi-agent system for visually impaired assistance. The modular
framework was developed
  implementing differentiated processing strategies, effectively reducing
memory requirements from
  38GB to 16GB while maintaining model performance. The multi-agent
architecture combines
  scene classification, vectorized memory, and multimodal interaction, enabling
persistent storage
  and efficient retrieval of scene memories. Through
perception-memory-reasoning workflows, the
  system provides environmental information beyond the current view using
historical memories.
  Experiments show the quantized 19B-parameter model only experiences a 2.05%
performance drop
  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),
outperforming smaller
  models with equivalent memory requirements like the Molmo-7B series. The
system maintains
  response latency between 2.83-3.52 seconds from scene analysis to initial
speech output, substantially
  faster than non-streaming methods. This research advances computational
efficiency and assistive
  technology, offering visually impaired users comprehensive real-time
assistance in scene perception,
  text recognition, and navigation.

</details>


### [184] [Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning](https://arxiv.org/abs/2508.18186)
*Le Zhang,Fuping Wu,Arun Thirunavukarasu,Kevin Bronik,Thomas Nichols,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: A method using coarse noisy annotations from both positive and negative classes to train CNNs for semantic segmentation, outperforming state-of-the-art methods especially when coarse annotations are limited.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labeling for segmentation is time-consuming and requires expert annotators, while coarse annotations are quicker, cheaper, and easier to produce even by non-experts.

Method: Two coupled CNNs learn true segmentation label distributions from purely noisy coarse annotations, with complementary label learning to estimate negative label distribution and maintain high fidelity with noisy training data characteristics.

Result: Outperforms state-of-the-art methods on MNIST-based toy dataset, Cityscapes multi-class segmentation, and retinal medical images, particularly when coarse annotations are scarce compared to dense annotations.

Conclusion: Coarse noisy annotations from both target and background classes can effectively train segmentation models, providing a practical solution to reduce annotation costs while maintaining high performance.

Abstract: Large annotated datasets are vital for training segmentation models, but
pixel-level labeling is time-consuming, error-prone, and often requires scarce
expert annotators, especially in medical imaging. In contrast, coarse
annotations are quicker, cheaper, and easier to produce, even by non-experts.
In this paper, we propose to use coarse drawings from both positive (target)
and negative (background) classes in the image, even with noisy pixels, to
train a convolutional neural network (CNN) for semantic segmentation. We
present a method for learning the true segmentation label distributions from
purely noisy coarse annotations using two coupled CNNs. The separation of the
two CNNs is achieved by high fidelity with the characters of the noisy training
annotations. We propose to add a complementary label learning that encourages
estimating negative label distribution. To illustrate the properties of our
method, we first use a toy segmentation dataset based on MNIST. We then present
the quantitative results of experiments using publicly available datasets:
Cityscapes dataset for multi-class segmentation, and retinal images for medical
applications. In all experiments, our method outperforms state-of-the-art
methods, particularly in the cases where the ratio of coarse annotations is
small compared to the given dense annotations.

</details>


### [185] [BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding](https://arxiv.org/abs/2508.18187)
*Xuan-Bac Nguyen,Thanh-Dat Truong,Pawan Sinha,Khoa Luu*

Main category: cs.CV

TL;DR: A new continual learning approach called BRAIN addresses memory decay in brain signals by mitigating compounding bias and preventing catastrophic forgetting, achieving state-of-the-art performance in vision-brain understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Memory decay causes brain signals to become weaker and less consistent over time, leading to compounding bias that degrades performance of vision-brain understanding models.

Method: Proposes Bias-Mitigation Continual Learning (BRAIN) with De-bias Contrastive Learning loss function and Angular-based Forgetting Mitigation to address bias and prevent knowledge loss.

Result: Achieves State-of-the-Art performance across various benchmarks, outperforming both prior methods and non-continual learning approaches.

Conclusion: The BRAIN approach effectively addresses the challenges of inconsistent brain signals over time through bias mitigation and continual learning techniques.

Abstract: Memory decay makes it harder for the human brain to recognize visual objects
and retain details. Consequently, recorded brain signals become weaker,
uncertain, and contain poor visual context over time. This paper presents one
of the first vision-learning approaches to address this problem. First, we
statistically and experimentally demonstrate the existence of inconsistency in
brain signals and its impact on the Vision-Brain Understanding (VBU) model. Our
findings show that brain signal representations shift over recording sessions,
leading to compounding bias, which poses challenges for model learning and
degrades performance. Then, we propose a new Bias-Mitigation Continual Learning
(BRAIN) approach to address these limitations. In this approach, the model is
trained in a continual learning setup and mitigates the growing bias from each
learning step. A new loss function named De-bias Contrastive Learning is also
introduced to address the bias problem. In addition, to prevent catastrophic
forgetting, where the model loses knowledge from previous sessions, the new
Angular-based Forgetting Mitigation approach is introduced to preserve learned
knowledge in the model. Finally, the empirical experiments demonstrate that our
approach achieves State-of-the-Art (SOTA) performance across various
benchmarks, surpassing prior and non-continual learning methods.

</details>


### [186] [Explain and Monitor Deep Learning Models for Computer Vision using Obz AI](https://arxiv.org/abs/2508.18188)
*Neo Christopher Chung,Jakub Binda*

Main category: cs.CV

TL;DR: Obz AI is a comprehensive software ecosystem that bridges the gap between explainable AI (XAI) techniques and practical computer vision deployments by providing integrated explainability, observability, and monitoring capabilities for vision AI systems.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in computer vision are often treated as black boxes with limited transparency, and despite advancements in XAI, explainability remains underutilized in practical deployments due to lack of integrated software solutions connecting XAI with knowledge management and monitoring frameworks.

Method: Developed Obz AI - a comprehensive software ecosystem with seamless integration pipeline from Python client library to full-stack analytics dashboard, enabling incorporation of advanced XAI methodologies, feature extraction for outlier detection, and real-time AI model monitoring.

Result: Obz AI facilitates state-of-the-art explainability and observability for vision AI systems, making deep model decision-making mechanisms interpretable and promoting responsible deployment of computer vision systems.

Conclusion: Obz AI successfully closes the gap between XAI research and practical CV deployments by providing an integrated solution that enhances transparency, interpretability, and continuous monitoring capabilities for vision AI systems.

Abstract: Deep learning has transformed computer vision (CV), achieving outstanding
performance in classification, segmentation, and related tasks. Such AI-based
CV systems are becoming prevalent, with applications spanning from medical
imaging to surveillance. State of the art models such as convolutional neural
networks (CNNs) and vision transformers (ViTs) are often regarded as ``black
boxes,'' offering limited transparency into their decision-making processes.
Despite a recent advancement in explainable AI (XAI), explainability remains
underutilized in practical CV deployments. A primary obstacle is the absence of
integrated software solutions that connect XAI techniques with robust knowledge
management and monitoring frameworks. To close this gap, we have developed Obz
AI, a comprehensive software ecosystem designed to facilitate state-of-the-art
explainability and observability for vision AI systems. Obz AI provides a
seamless integration pipeline, from a Python client library to a full-stack
analytics dashboard. With Obz AI, a machine learning engineer can easily
incorporate advanced XAI methodologies, extract and analyze features for
outlier detection, and continuously monitor AI models in real time. By making
the decision-making mechanisms of deep models interpretable, Obz AI promotes
observability and responsible deployment of computer vision systems.

</details>


### [187] [Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance](https://arxiv.org/abs/2508.18213)
*Ayce Idil Aytekin,Helge Rhodin,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: A diffusion-based framework for 3D object reconstruction from monocular RGB images using hand-object interaction as geometric guidance, producing high-quality geometry with physical plausibility.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of prior methods that require extensive post-processing or produce low-quality reconstructions by leveraging hand-object interaction as geometric guidance for more accurate and robust 3D reconstruction.

Method: Uses a latent diffusion model conditioned on inpainted object appearance with inference-time guidance. Applies supervision to velocity field while optimizing hand and object transformations using multi-modal geometric cues (normal/depth alignment, silhouette consistency, 2D keypoint reprojection), SDF supervision, and contact/non-intersection constraints.

Result: Produces accurate, robust, and coherent reconstructions under occlusion while generalizing well to in-the-wild scenarios with high-quality object geometry.

Conclusion: The proposed diffusion-based framework successfully leverages hand-object interaction as geometric guidance to achieve superior 3D object reconstruction from monocular images without extensive post-processing.

Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry
of hand-held objects from monocular RGB images by leveraging hand-object
interaction as geometric guidance. Our method conditions a latent diffusion
model on an inpainted object appearance and uses inference-time guidance to
optimize the object reconstruction, while simultaneously ensuring plausible
hand-object interactions. Unlike prior methods that rely on extensive
post-processing or produce low-quality reconstructions, our approach directly
generates high-quality object geometry during the diffusion process by
introducing guidance with an optimization-in-the-loop design. Specifically, we
guide the diffusion model by applying supervision to the velocity field while
simultaneously optimizing the transformations of both the hand and the object
being reconstructed. This optimization is driven by multi-modal geometric cues,
including normal and depth alignment, silhouette consistency, and 2D keypoint
reprojection. We further incorporate signed distance field supervision and
enforce contact and non-intersection constraints to ensure physical
plausibility of hand-object interaction. Our method yields accurate, robust and
coherent reconstructions under occlusion while generalizing well to in-the-wild
scenarios.

</details>


### [188] [GM-Skip: Metric-Guided Transformer Block Skipping for Efficient Vision-Language Models](https://arxiv.org/abs/2508.18227)
*Lianming Huang,Haibo Hu,Qiao Li,Xin He,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: GM-Skip is a framework that accelerates Vision-Language Model inference by adaptively skipping redundant Transformer blocks while maintaining output quality, achieving up to 45.4% latency reduction in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Transformer-based VLMs have high computational costs that hinder deployment in latency-sensitive applications like autonomous driving, requiring efficient inference methods.

Method: Uses greedy metric-guided block selection with metric feedback (accuracy, CIDEr) to identify redundant layers, reverse-order deletion to preserve early foundational blocks, and a tunable sparsity-performance trade-off objective.

Result: Improves single-object classification accuracy on COCO Person category from 19.1% to 87.3% while skipping >40% of blocks. Achieves 45.4% latency reduction in autonomous vehicle deployment with Autoware.Universe.

Conclusion: GM-Skip effectively accelerates VLM inference while preserving performance, demonstrating practical value for real-world latency-sensitive applications.

Abstract: Transformer-based Vision-Language Models (VLMs) have achieved impressive
performance on tasks such as image captioning, object recognition, and visual
reasoning, but their high computational cost hinders deployment in
latency-sensitive applications like autonomous driving. We introduce GM-Skip, a
flexible and metric-adaptive framework for Transformer block skipping that
accelerates VLM inference while preserving output quality. GM-Skip features a
greedy, metric-guided block selection strategy that uses metric feedback (e.g.,
accuracy, CIDEr) to identify redundant layers, along with a reverse-order
deletion mechanism that preserves early foundational blocks to avoid
performance collapse. To support diverse deployment needs, it incorporates a
tunable trade-off between sparsity and performance via a score-sparsity balance
objective. Experiments across multiple tasks and datasets, including COCO and
CODA, show that GM-Skip consistently improves inference speed while maintaining
task performance. On the COCO dataset, GM-Skip improves single-object
classification accuracy on the Person category from 19.1 percent to 87.3
percent while skipping more than 40 percent of Transformer blocks. In
real-world deployment, it achieves up to 45.4 percent latency reduction on
single-object detection when integrated into an autonomous vehicle running
Autoware.Universe, validating the effectiveness of its skip configurations and
confirming its practical value in accelerating real-world inference.

</details>


### [189] [Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation](https://arxiv.org/abs/2508.18235)
*Ashwath Vaithinathan Aravindan,Abha Jha,Matthew Salaway,Atharva Sandeep Bhide,Duygu Nur Yaldiz*

Main category: cs.CV

TL;DR: SKD-CAG is a novel defense method that selectively removes backdoor triggers from text-to-image diffusion models while preserving generation quality, achieving near-perfect removal accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-to-image diffusion models are vulnerable to backdoor attacks where adversaries inject imperceptible textual triggers, but existing defenses focus on classification models rather than generative models.

Method: Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG) uses knowledge distillation to guide the model in correcting poisoned prompts while maintaining image quality, leveraging cross-attention mechanisms to neutralize backdoor influences at the attention level.

Result: Achieves 100% removal accuracy for pixel backdoors and 93% for style-based attacks without sacrificing robustness or image fidelity.

Conclusion: Targeted unlearning through SKD-CAG is an effective defense strategy for securing generative models against backdoor attacks.

Abstract: Text-to-image diffusion models have revolutionized generative AI, but their
vulnerability to backdoor attacks poses significant security risks. Adversaries
can inject imperceptible textual triggers into training data, causing models to
generate manipulated outputs. Although text-based backdoor defenses in
classification models are well-explored, generative models lack effective
mitigation techniques against. We address this by selectively erasing the
model's learned associations between adversarial text triggers and poisoned
outputs, while preserving overall generation quality. Our approach,
Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG), uses
knowledge distillation to guide the model in correcting responses to poisoned
prompts while maintaining image quality by exploiting the fact that the
backdoored model still produces clean outputs in the absence of triggers. Using
the cross-attention mechanism, SKD-CAG neutralizes backdoor influences at the
attention level, ensuring the targeted removal of adversarial effects.
Extensive experiments show that our method outperforms existing approaches,
achieving removal accuracy 100\% for pixel backdoors and 93\% for style-based
attacks, without sacrificing robustness or image fidelity. Our findings
highlight targeted unlearning as a promising defense to secure generative
models. Code and model weights can be found at
https://github.com/Mystic-Slice/Sealing-The-Backdoor .

</details>


### [190] [Interpretable Evaluation of AI-Generated Content with Language-Grounded Sparse Encoders](https://arxiv.org/abs/2508.18236)
*Yiming Tang,Arash Lagzian,Srinivas Anumasa,Qiran Zou,Trang Nguyen,Ehsan Adeli,Ching-Yu Cheng,Yilun Du,Dianbo Liu*

Main category: cs.CV

TL;DR: LanSE introduces interpretable evaluation metrics for AI-generated images by identifying visual patterns and describing them in natural language, providing fine-grained assessment across four quality dimensions.


<details>
  <summary>Details</summary>
Motivation: Current evaluation metrics for AI-generated content provide only coarse-grained assessments, failing to identify specific strengths and weaknesses needed for model selection, development, and scientific understanding.

Method: Language-Grounded Sparse Encoders (LanSE) architecture that identifies interpretable visual patterns and automatically describes them in natural language, validated through large-scale human evaluation (11,000+ annotations) and LMM-based analysis.

Result: LanSE achieves >93% accuracy in detecting interpretable visual patterns in synthetic images, reveals nuanced model differences (e.g., FLUX's superior physical plausibility, SDXL-medium's strong content diversity), and aligns with human judgments.

Conclusion: LanSE bridges interpretability with practical evaluation needs, offering a powerful tool for model selection, quality control, and model improvement, addressing the need for public confidence and safety in AI-generated content.

Abstract: While the quality of AI-generated contents, such as synthetic images, has
become remarkably high, current evaluation metrics provide only coarse-grained
assessments, failing to identify specific strengths and weaknesses that
researchers and practitioners need for model selection and development, further
limiting the scientific understanding and commercial deployment of these
generative models. To address this, we introduce Language-Grounded Sparse
Encoders (LanSE), a novel architecture that creates interpretable evaluation
metrics by identifying interpretable visual patterns and automatically
describing them in natural language. Through large-scale human evaluation (more
than 11,000 annotations) and large multimodal model (LMM) based analysis, LanSE
demonstrates reliable capabilities to detect interpretable visual patterns in
synthetic images with more than 93\% accuracy in natural images. LanSE further
provides a fine-grained evaluation framework that quantifies four key
dimensions of generation quality, prompt match, visual realism, physical
plausibility, and content diversity. LanSE reveals nuanced model differences
invisible to existing metrics, for instance, FLUX's superior physical
plausibility and SDXL-medium's strong content diversity, while aligning with
human judgments. By bridging interpretability with practical evaluation needs,
LanSE offers all users of generative AI models a powerful tool for model
selection, quality control of synthetic content, and model improvement. These
capabilities directly address the need for public confidence and safety in
AI-generated content, both critical for the future of generative AI
applications.

</details>


### [191] [PriorFormer: A Transformer for Real-time Monocular 3D Human Pose Estimation with Versatile Geometric Priors](https://arxiv.org/abs/2508.18238)
*Mohamed Adjel,Vincent Bonnet*

Main category: cs.CV

TL;DR: A lightweight Transformer-based model that converts 2D human joint sequences to 3D poses using geometric priors, working in both calibrated and uncalibrated camera settings with high efficiency.


<details>
  <summary>Details</summary>
Motivation: To create a versatile 3D pose estimation system that can operate in various deployment scenarios from lab environments to in-the-wild videos without requiring complete calibration data.

Method: Transformer-based architecture with masking mechanism to handle missing geometric priors (segment lengths and camera intrinsics), trained on AMASS dataset with synthetic 2D data from random camera poses.

Result: Achieved 36mm average 3D joint position accuracy, outperforming expert models even with complete priors, and runs efficiently (380μs on GPU, 1800μs on CPU).

Conclusion: The proposed versatile model significantly improves state-of-the-art accuracy while being computationally efficient enough for deployment on embedded and low-power devices.

Abstract: This paper proposes a new lightweight Transformer-based lifter that maps
short sequences of human 2D joint positions to 3D poses using a single camera.
The proposed model takes as input geometric priors including segment lengths
and camera intrinsics and is designed to operate in both calibrated and
uncalibrated settings. To this end, a masking mechanism enables the model to
ignore missing priors during training and inference. This yields a single
versatile network that can adapt to different deployment scenarios, from fully
calibrated lab environments to in-the-wild monocular videos without
calibration. The model was trained using 3D keypoints from AMASS dataset with
corresponding 2D synthetic data generated by sampling random camera poses and
intrinsics. It was then compared to an expert model trained, only on complete
priors, and the validation was done by conducting an ablation study. Results
show that both, camera and segment length priors, improve performance and that
the versatile model outperforms the expert, even when all priors are available,
and maintains high accuracy when priors are missing. Overall the average 3D
joint center positions estimation accuracy was as low as 36mm improving state
of the art by half a centimeter and at a much lower computational cost. Indeed,
the proposed model runs in 380$\mu$s on GPU and 1800$\mu$s on CPU, making it
suitable for deployment on embedded platforms and low-power devices.

</details>


### [192] [GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations](https://arxiv.org/abs/2508.18242)
*Fadi Khatib,Dror Moran,Guy Trostianetsky,Yoni Kasten,Meirav Galun,Ronen Basri*

Main category: cs.CV

TL;DR: GSVisLoc is a visual localization method that uses 3D Gaussian Splatting scene representations to estimate camera pose from query images through robust feature matching without requiring scene modifications or retraining.


<details>
  <summary>Details</summary>
Motivation: To enable visual localization using 3DGS scene representations without the need for modifications, retraining, or additional reference images, leveraging the explicit 3D structure for accurate camera pose estimation.

Method: Three-step approach: 1) Coarse matching between downsampled/encoded 3D Gaussian features and image patch features, 2) Fine matching for refinement, 3) Pose refinement for final accurate camera position and orientation estimation.

Result: Competitive localization performance on standard indoor and outdoor benchmarks, outperforming existing 3DGS-based baselines, with effective generalization to novel scenes without additional training.

Conclusion: GSVisLoc successfully demonstrates that 3D Gaussian Splatting representations can be effectively leveraged for visual localization through robust feature matching, providing accurate pose estimation while maintaining generalization capabilities across diverse scenes.

Abstract: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian
Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a
query image, our goal is to estimate the camera's position and orientation. We
accomplish this by robustly matching scene features to image features. Scene
features are produced by downsampling and encoding the 3D Gaussians while image
features are obtained by encoding image patches. Our algorithm proceeds in
three steps, starting with coarse matching, then fine matching, and finally by
applying pose refinement for an accurate final estimate. Importantly, our
method leverages the explicit 3DGS scene representation for visual localization
without requiring modifications, retraining, or additional reference images. We
evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive
localization performance on standard benchmarks while outperforming existing
3DGS-based baselines. Moreover, our approach generalizes effectively to novel
scenes without additional training.

</details>


### [193] [MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs](https://arxiv.org/abs/2508.18264)
*Sixun Dong,Juhua Hu,Mian Zhang,Ming Yin,Yanjie Fu,Qi Qian*

Main category: cs.CV

TL;DR: MMTok is a multimodal token pruning method that uses both vision and text tokens to select informative vision tokens through coverage optimization, achieving significant speedup while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: Existing vision token pruning methods use only unimodal information (vision or text) and ignore the multimodal nature of vision-language tasks, lacking a generic criterion for different modalities.

Method: Formulates token selection as a maximum coverage problem, optimizes a subset of vision tokens to cover both text tokens and original vision tokens, and uses a VLM agent to improve text token quality for guiding vision pruning.

Result: Achieves 1.87x speedup while maintaining 98.7% original performance on LLaVA-NeXT-13B, and preserves 87.7% performance with only 4 vision tokens on LLaVA-1.5-7B.

Conclusion: Vision and text information are complementary, and combining multimodal information with coverage criterion effectively improves token selection efficiency while preserving performance.

Abstract: Vision-Language Models (VLMs) demonstrate impressive performance in
understanding visual content with language instruction by converting visual
input to vision tokens. However, redundancy in vision tokens results in the
degenerated inference efficiency of VLMs. While many algorithms have been
proposed to reduce the number of vision tokens, most of them apply only
unimodal information (i.e., vision/text) for pruning and ignore the inherent
multimodal property of vision-language tasks. Moreover, it lacks a generic
criterion that can be applied to different modalities. To mitigate this
limitation, in this work, we propose to leverage both vision and text tokens to
select informative vision tokens by the criterion of coverage. We first
formulate the subset selection problem as a maximum coverage problem.
Afterward, a subset of vision tokens is optimized to cover the text tokens and
the original set of vision tokens, simultaneously. Finally, a VLM agent can be
adopted to further improve the quality of text tokens for guiding vision
pruning. The proposed method MMTok is extensively evaluated on benchmark
datasets with different VLMs. The comparison illustrates that vision and text
information are complementary, and combining multimodal information can surpass
the unimodal baseline with a clear margin. Moreover, under the maximum coverage
criterion on the POPE dataset, our method achieves a 1.87x speedup while
maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,
with only four vision tokens, it still preserves 87.7% of the original
performance on LLaVA-1.5-7B. These results highlight the effectiveness of
coverage in token selection.

</details>


### [194] [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265)
*Weiyun Wang,Zhangwei Gao,Lixin Gu,Hengjun Pu,Long Cui,Xingguang Wei,Zhaoyang Liu,Linglin Jing,Shenglong Ye,Jie Shao,Zhaokai Wang,Zhe Chen,Hongjie Zhang,Ganlin Yang,Haomin Wang,Qi Wei,Jinhui Yin,Wenhao Li,Erfei Cui,Guanzhou Chen,Zichen Ding,Changyao Tian,Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Yuchen Duan,Xuehui Wang,Songze Li,Xiangyu Zhao,Haodong Duan,Nianchen Deng,Bin Fu,Yinan He,Yi Wang,Conghui He,Botian Shi,Junjun He,Yingtong Xiong,Han Lv,Lijun Wu,Wenqi Shao,Kaipeng Zhang,Huipeng Deng,Biqing Qi,Jiaye Ge,Qipeng Guo,Wenwei Zhang,Wanli Ouyang,Limin Wang,Min Dou,Xizhou Zhu,Tong Lu,Dahua Lin,Jifeng Dai,Bowen Zhou,Weijie Su,Kai Chen,Yu Qiao,Wenhai Wang,Gen Luo*

Main category: cs.CV

TL;DR: InternVL 3.5 is an open-source multimodal model family that introduces Cascade RL for enhanced reasoning, Visual Resolution Router for efficiency, and achieves state-of-the-art performance with 16% reasoning improvement and 4x speedup over previous version.


<details>
  <summary>Details</summary>
Motivation: To advance open-source multimodal models by improving versatility, reasoning capability, and inference efficiency while narrowing the performance gap with commercial models like GPT-5.

Method: Uses Cascade Reinforcement Learning (offline + online RL) for reasoning enhancement, Visual Resolution Router for dynamic resolution adjustment, and Decoupled Vision-Language Deployment for GPU load balancing.

Result: Achieves +16.0% overall reasoning performance gain, 4.05x inference speedup, state-of-the-art results across multimodal tasks, and supports novel capabilities like GUI interaction and embodied agency.

Conclusion: InternVL 3.5 significantly advances open-source multimodal AI with improved reasoning, efficiency, and novel capabilities, making it competitive with leading commercial models while remaining publicly accessible.

Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that
significantly advances versatility, reasoning capability, and inference
efficiency along the InternVL series. A key innovation is the Cascade
Reinforcement Learning (Cascade RL) framework, which enhances reasoning through
a two-stage process: offline RL for stable convergence and online RL for
refined alignment. This coarse-to-fine training strategy leads to substantial
improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To
optimize efficiency, we propose a Visual Resolution Router (ViR) that
dynamically adjusts the resolution of visual tokens without compromising
performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)
strategy separates the vision encoder and language model across different GPUs,
effectively balancing computational load. These contributions collectively
enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning
performance and a 4.05$\times$ inference speedup compared to its predecessor,
i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as
GUI interaction and embodied agency. Notably, our largest model, i.e.,
InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs
across general multimodal, reasoning, text, and agentic tasks -- narrowing the
performance gap with leading commercial models like GPT-5. All models and code
are publicly released.

</details>


### [195] [ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models](https://arxiv.org/abs/2508.18271)
*Haitang Feng,Jie Liu,Jie Tang,Gangshan Wu,Beiqi Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: ObjFiller-3D uses video editing models instead of 2D image inpainting for more consistent 3D object completion, achieving superior reconstruction quality compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Multi-view 2D image inpainting for 3D completion suffers from inconsistencies across views, leading to blurred textures, spatial discontinuities, and visual artifacts that hinder accurate and realistic 3D object completion.

Method: Leverages state-of-the-art video editing models to fill masked regions of 3D objects, analyzes the representation gap between 3D and videos, adapts video inpainting for 3D scenes, and introduces reference-based 3D inpainting.

Result: Achieves PSNR of 26.6 vs. NeRFiller's 15.9 and LPIPS of 0.19 vs. Instant3dit's 0.25, producing more faithful and fine-grained reconstructions across diverse datasets.

Conclusion: ObjFiller-3D demonstrates strong potential for practical deployment in real-world 3D editing applications and significantly outperforms previous methods in reconstruction quality and consistency.

Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the
inherent inconsistencies across different inpainted views can result in blurred
textures, spatial discontinuities, and distracting visual artifacts. These
inconsistencies pose significant challenges when striving for accurate and
realistic 3D object completion, particularly in applications that demand high
fidelity and structural coherence. To overcome these limitations, we propose
ObjFiller-3D, a novel method designed for the completion and editing of
high-quality and consistent 3D objects. Instead of employing a conventional 2D
image inpainting model, our approach leverages a curated selection of
state-of-the-art video editing model to fill in the masked regions of 3D
objects. We analyze the representation gap between 3D and videos, and propose
an adaptation of a video inpainting model for 3D scene inpainting. In addition,
we introduce a reference-based 3D inpainting method to further enhance the
quality of reconstruction. Experiments across diverse datasets show that
compared to previous methods, ObjFiller-3D produces more faithful and
fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of
0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for
practical deployment in real-world 3D editing applications. Project page:
https://objfiller3d.github.io/ Code:
https://github.com/objfiller3d/ObjFiller-3D .

</details>
