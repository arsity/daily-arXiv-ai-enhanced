{"id": "2509.09720", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "ASOS is a dataset of 50 common supermarket items with high-quality 3D meshes for robotics and computer vision benchmarking, featuring accessible real-world objects instead of synthetic models.", "motivation": "To provide a cost-effective, accessible dataset of real-world supermarket items for benchmarking in robotics and computer vision, addressing limitations of existing datasets that use synthetic models or specialized objects with limited availability.", "method": "Used structure-from-motion techniques with high-resolution imaging to generate watertight 3D textured meshes of 50 readily available supermarket items across 10 categories, sourced from a major Australian supermarket chain.", "result": "Created a comprehensive dataset with diverse shapes, sizes, and weights that provides high-quality 3D models of common household items for research applications.", "conclusion": "ASOS offers valuable benchmarking capabilities for object detection, pose estimation, and robotics applications due to its emphasis on real-world accessibility and practical applicability."}}
{"id": "2509.09721", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "A multimodal RAG framework for post-disaster housing damage assessment that combines visual building damage analysis with text retrieval from policies/posts, achieving 9.6% improvement in retrieval accuracy.", "motivation": "Accurate housing damage evaluations after natural disasters are crucial for insurance claims processing and resource planning, requiring effective multimodal analysis of visual damage and policy documentation.", "method": "Two-branch multimodal encoder with ResNet+Transformer for images and BERT for text, cross-modal interaction via multi-head attention, modal attention gating for generation, and end-to-end training with multi-task loss optimization.", "result": "Superior performance in retrieval accuracy and damage severity classification, with Top-1 retrieval accuracy improved by 9.6%.", "conclusion": "The MM-RAG framework effectively bridges visual and textual modalities for comprehensive disaster damage assessment, demonstrating significant improvements in accuracy for insurance and resource planning applications."}}
{"id": "2509.09722", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "Novel ensemble framework using Gemini 2.0 Flash with multiple augmented document variants and custom Needleman-Wunsch aligner improves transcription accuracy by 4% on historical documents.", "motivation": "To stabilize text extraction from noisy historical documents by addressing transcription inconsistencies and improving accuracy through ensemble methods.", "method": "Transcribe multiple augmented variants of each document image using Gemini 2.0 Flash, then fuse outputs with a custom Needleman-Wunsch style aligner to produce consensus transcription with confidence scores.", "result": "4 percentage point accuracy improvement over single-shot baseline on new dataset of 622 Pennsylvania death records; padding and blurring most effective for accuracy, grid warp best for confidence separation.", "conclusion": "The approach is simple, scalable, and immediately deployable to other document collections and transcription models for improved historical document processing."}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "MITS is the first large-scale multimodal benchmark dataset for Intelligent Traffic Surveillance, containing 170,400 real-world images with comprehensive annotations and 5M QA pairs, which significantly improves LMM performance in ITS applications.", "motivation": "General-domain large multimodal models perform poorly in Intelligent Traffic Surveillance due to lack of dedicated multimodal datasets, creating a need for domain-specific training data.", "method": "Created MITS dataset with 170,400 real ITS images annotated with 8 main categories and 24 subcategories, plus generated high-quality captions and 5M instruction-following QA pairs covering five critical ITS tasks.", "result": "Fine-tuning on MITS dramatically improved LMM performance: LLaVA-1.5 from 0.494 to 0.905 (+83.2%), LLaVA-1.6 from 0.678 to 0.921 (+35.8%), Qwen2-VL from 0.584 to 0.926 (+58.6%), Qwen2.5-VL from 0.732 to 0.930 (+27.0%).", "conclusion": "MITS effectively bridges the gap in ITS multimodal data, enabling development of ITS-specific applications and significantly advancing both ITS and LMM research through open-source release."}}
{"id": "2509.09732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Gro\u00dfmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "Tree-based reasoning for VLMs underperforms standard zero-shot prompting despite achieving high accuracy in understanding tree structure, though image descriptions can enhance both methods.", "motivation": "To investigate whether structured, tree-based reasoning can enhance vision language model performance on fine-grained tasks and large hierarchical label spaces.", "method": "Introduced a framework that decomposes classification into interpretable decisions using decision trees, evaluated on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets, and explored enhancing tree prompts with LLM-generated classes and image descriptions.", "result": "Tree-based reasoning consistently underperformed standard zero-shot prompting, though the model achieved 98.2% accuracy in understanding tree knowledge. Adding image descriptions enhanced performance for both tree-based and zero-shot methods.", "conclusion": "Structured reasoning has limitations in visual classification, but the findings offer insights for designing more interpretable VLM systems."}}
{"id": "2509.09737", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "PSI is a system that learns controllable world models from data through a 3-step cycle: probabilistic prediction, structure extraction, and integration, enabling improved video understanding and generation capabilities.", "motivation": "To create richly controllable and flexibly promptable world models that can extract meaningful structures from data and use them for improved prediction and understanding tasks.", "method": "Three-step cycle: 1) Build probabilistic graphical model (Psi) as random-access autoregressive sequence model, 2) Extract low-dimensional structures via causal inference, 3) Integrate structures as new token types for continual training.", "result": "Trained on 1.4T video tokens; achieved state-of-the-art optical flow, self-supervised depth, and object segmentation; enabled various video prediction and understanding tasks with improved performance.", "conclusion": "PSI provides a framework for learning world models that continuously improve through structure extraction and integration, creating universal prompting capabilities similar to LLMs for video data."}}
{"id": "2509.09742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "First analysis of video data leakage in federated learning via gradient inversion attacks, showing feature extractors provide some protection but leakage is still possible, with super-resolution techniques enhancing attack effectiveness.", "motivation": "Federated learning's privacy protection is threatened by gradient inversion attacks that can reconstruct private training data from shared gradients. While known for images/text/tabular data, video data vulnerability remains unexamined.", "method": "Evaluated two video classification approaches: pre-trained feature extractors vs raw video frame processing. Tested gradient inversion attacks across scenarios with zero, one, or multiple reference frames, using super-resolution to enhance reconstructed videos.", "result": "Feature extractors offer greater resilience against attacks but leakage still occurs if classifier lacks complexity. Super-resolution techniques successfully enhance reconstructed video quality. Attacks remain viable even with limited reference frames.", "conclusion": "Video data leakage in federated learning is a viable threat that warrants further investigation, as current protection methods are insufficient against determined gradient inversion attacks."}}
{"id": "2509.09750", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "Semi-supervised co-training framework combining Faster R-CNN and YOLO for object detection in retail environments, with ensemble classification and metaheuristic optimization, reducing annotation costs while maintaining high accuracy.", "motivation": "Address challenges in densely packed retail environments where limited labeled data, occlusion, overlapping objects, and frequent product/layout changes make traditional object detection difficult and annotation costly.", "method": "Co-training framework with Faster R-CNN (ResNet backbone) for precise localization and YOLO (Darknet backbone) for global context, mutual pseudo-label exchange, ensemble classification (XGBoost, Random Forest, SVM), and metaheuristic hyperparameter optimization.", "result": "Strong performance on SKU-110k dataset, demonstrating improved accuracy in occluded and overlapping object scenarios, reduced annotation costs, and effective adaptation to retail environment changes.", "conclusion": "The framework offers scalable and practical solution for real-world retail applications including automated inventory tracking, product monitoring, and checkout systems, with minimized manual labeling requirements."}}
{"id": "2509.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "Token Purging (PG) is a backpropagation-free test-time adaptation method for 3D point cloud classification that removes domain-shifted tokens before attention layers, achieving superior accuracy and efficiency over existing methods.", "motivation": "To address performance degradation caused by distribution shifts in 3D point cloud classification without requiring iterative updates or backpropagation during test-time adaptation.", "method": "Proposes Token Purging (PG) that operates at token level to remove tokens highly affected by domain shifts before they reach attention layers. Two variants: PG-SP (uses source statistics) and PG-SF (fully source-free using CLS-token-driven adaptation).", "result": "PG-SP achieves +10.3% higher accuracy than state-of-the-art backpropagation-free methods. PG-SF sets new benchmarks for source-free adaptation. PG is 12.4x faster and 5.5x more memory efficient than baseline.", "conclusion": "Token Purging provides an effective, efficient solution for test-time adaptation in 3D point cloud classification, suitable for real-world deployment due to its speed and memory efficiency advantages."}}
{"id": "2509.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "A novel cross-view localization method that directly matches ground-to-aerial image features using depth priors, avoiding BEV transformation issues and supporting both metric and relative depth with scale-aware alignment.", "motivation": "Previous methods transform ground images to bird's-eye view, causing information loss from perspective distortion and height compression, which degrades alignment quality with aerial imagery.", "method": "Directly establishes correspondences between ground and aerial images, lifts matched keypoints to BEV using monocular depth prior, and employs scale-aware Procrustes alignment for pose estimation with optional scale recovery for relative depth.", "result": "Achieves superior localization performance under challenging conditions (cross-area generalization, unknown orientation) with only weak pose supervision, learns accurate feature correspondences, and works with various depth models without per-model finetuning.", "conclusion": "The method provides accurate, interpretable cross-view localization with flexibility for real-world deployment, overcoming limitations of traditional BEV transformation approaches through direct feature matching and robust depth handling."}}
{"id": "2509.09808", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. L\u00f3pez"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "KidsVisionCheck is a free mobile app that uses AI and smartphone cameras to perform pediatric vision screening via red-eye reflex analysis, achieving 90% accuracy without specialist equipment.", "motivation": "To make pediatric vision screening more accessible worldwide by recreating the clinical Bruckner test using smartphone technology and AI, enabling early detection of visual impairments in children.", "method": "Deep neural networks trained on ophthalmologist-labeled red-eye reflex images from children, using smartphone-captured pupil images to perform vision screening.", "result": "The model achieved 90% accuracy on unseen test data, providing reliable performance for detecting visual impairments without requiring specialist equipment.", "conclusion": "This work represents a significant step toward accessible pediatric vision screening and early intervention for vision abnormalities globally using mobile technology."}}
{"id": "2509.09828", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "DGFusion: A depth-guided multimodal fusion method that uses depth information to dynamically adapt sensor fusion for semantic perception in autonomous vehicles, achieving state-of-the-art performance on challenging datasets.", "motivation": "Current sensor fusion approaches treat sensor data uniformly across spatial extent, which hinders performance in challenging conditions. There's a need for condition-aware fusion that adapts to spatially varying sensor reliability.", "method": "Proposes DGFusion network that treats multimodal segmentation as multi-task problem. Uses lidar measurements as input and depth ground truth. Includes auxiliary depth head to learn depth-aware features, encodes them into local depth tokens, and uses attentive cross-modal fusion with global condition token. Also proposes robust loss for handling sparse and noisy lidar data.", "result": "Achieves state-of-the-art panoptic and semantic segmentation performance on challenging MUSES and DELIVER datasets.", "conclusion": "Depth-guided fusion with spatially varying local depth tokens and global conditioning effectively adapts sensor fusion to varying reliability across scenes, significantly improving semantic perception performance in autonomous driving scenarios."}}
{"id": "2509.09841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "Patch-based rosacea detection using ResNet-18 with localized facial patches achieves competitive accuracy while preserving patient privacy.", "motivation": "Rosacea requires precise early detection for effective treatment, and traditional full-image methods may compromise patient privacy and lack focus on clinically relevant regions.", "method": "Extract various image patches from facial images in different sizes, shapes, and locations. Use ResNet-18 deep learning framework to evaluate how localized visual information influences model performance through investigation studies.", "result": "Patch-based strategies achieve competitive or superior accuracy and sensitivity compared to full-image methods, while guiding the model to focus on clinically relevant regions and enhancing robustness and interpretability.", "conclusion": "The proposed patch-based strategies offer practical insights for improving automated dermatological diagnostics by preserving patient privacy and enhancing model performance through localized analysis."}}
{"id": "2509.09844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "Privacy-preserving rosacea detection using synthetic data and clinical priors with masked facial regions to focus on diagnostically relevant areas while protecting identity.", "motivation": "Rosacea is underdiagnosed and automated detection faces challenges due to diffuse symptoms, limited labeled datasets, and privacy concerns with facial images.", "method": "Uses clinical priors to create redness-informed masks focusing on central facial areas (cheeks, nose, forehead), then trains ResNet-18 on masked synthetic images.", "result": "Achieves superior performance over full-face baselines with notable gains in accuracy, recall and F1 score on real-world test data.", "conclusion": "Synthetic data combined with clinical priors enables accurate and ethical dermatological AI systems suitable for privacy-sensitive telemedicine applications."}}
{"id": "2509.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "Comprehensive ablation study of ULW framework for laparoscopic image desmoking, evaluating individual components including learnable Wiener filter and loss function terms.", "motivation": "To rigorously assess the effectiveness and necessity of individual components within the ULW framework for laparoscopic image desmoking.", "method": "Systematic ablation study removing the learnable Wiener filter and selectively using individual loss terms (MSE, SSIM loss, perceptual loss) from the compound loss function. Benchmarking on paired laparoscopic images dataset.", "result": "Evaluation using quantitative metrics (SSIM, PSNR, MSE, CIEDE-2000) alongside qualitative visual comparisons.", "conclusion": "The study provides comprehensive analysis of component contributions to the ULW framework's overall performance in laparoscopic image desmoking."}}
{"id": "2509.09859", "categories": ["cs.CV", "cs.LG", "68W99"], "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "WAVE-DETR is a multi-modal drone detector that fuses RGB visual and acoustic signals using Deformable DETR and Wav2Vec2 architectures, achieving significant performance improvements in drone detection across all size categories.", "motivation": "To create a robust UAV detection system that works effectively under challenging environmental conditions by leveraging both visual and acoustic information, overcoming limitations of single-modal approaches.", "method": "Combines Deformable DETR for visual processing with Wav2Vec2 for acoustic feature extraction. Tests four fusion configurations: gated mechanism, linear layer, MLP, and cross attention. Uses Drone-vs-Bird dataset and new ARDrone dataset with 7,500+ synchronized image-audio segments.", "result": "Gated fusion approach performed best, improving mAP by 11.1-15.3% for small drones across IoU thresholds 0.5-0.9. Also enhanced mAP for medium and large drones with overall gains of 3.27-5.84% across all drone sizes.", "conclusion": "Multi-modal fusion of visual and acoustic signals significantly improves drone detection performance, with gated fusion being the most effective approach for enhancing detection accuracy across various drone sizes and environmental conditions."}}
{"id": "2509.09869", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "Surrogate supervision decouples input domain from supervision domain to improve robustness of deep learning-based deformable image registration against input variations like artifacts, FOV mismatch, and modality differences.", "motivation": "Deep learning registration methods achieve strong accuracy but remain sensitive to variations in input image characteristics such as artifacts, field-of-view mismatch, or modality differences, limiting their generalizability.", "method": "Introduces surrogate supervision that applies estimated spatial transformations to surrogate images, allowing training on heterogeneous inputs while ensuring supervision is computed in domains where similarity is well defined.", "result": "Demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences across three applications, while maintaining high performance on well-curated data.", "conclusion": "Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity, enabling broader applicability in diverse biomedical imaging scenarios."}}
{"id": "2509.09911", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "A framework combining convolutional autoencoder and Vision Transformer improves dental age estimation accuracy and provides transparency, revealing data-centric limitations in tooth morphology variability.", "motivation": "Deep learning's black box nature limits adoption in high-stakes forensic applications like dental age estimation, requiring both performance improvement and interpretability.", "method": "Combines convolutional autoencoder (AE) with Vision Transformer (ViT) to enhance classification accuracy and provide multi-faceted diagnostic insights through latent space analysis and image reconstructions.", "result": "Improved classification accuracy from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Analysis revealed performance gap is data-centric due to high intra-class morphological variability in tooth 38 dataset.", "conclusion": "The framework provides both accuracy enhancement and interpretability, demonstrating insufficiency of single interpretability modes and offering robust support for expert decision-making in forensic age estimation."}}
{"id": "2509.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "SCoDA is a novel Source-Free Domain Adaptation method that uses self-supervised pre-training and geometric manifold alignment to outperform existing SFDA approaches without requiring supervised source data.", "motivation": "Existing SFDA methods discard crucial geometric information about the latent manifold when using cosine similarity over L2-normalized features, and rely on supervised pre-training which may not be available.", "method": "Initializes with self-supervised pre-trained teacher model, uses geometric manifold alignment with Space Similarity Loss, and employs EMA updates to prevent catastrophic forgetting. Combines instance-level feature matching with geometric preservation.", "result": "Extensive experiments show SCoDA significantly outperforms state-of-the-art SFDA methods on benchmark datasets.", "conclusion": "Self-supervised pre-training combined with geometric manifold alignment provides an effective approach for source-free domain adaptation that preserves crucial structural information and avoids reliance on supervised source data."}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edg\u00fc", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "Zero-shot cell tracking framework using SAM2 foundation model for unsupervised microscopy image analysis without manual labeling or dataset-specific training.", "motivation": "Existing deep learning methods require costly manual labeling and have limited generalizability across diverse microscopy datasets due to data diversity.", "method": "Integrate Segment Anything 2 (SAM2) foundation model into tracking pipeline as a fully-unsupervised approach that doesn't depend on training datasets.", "result": "Achieves competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos without dataset-specific adaptation or fine-tuning.", "conclusion": "Proposed framework overcomes limitations of manual labeling and poor generalization by leveraging foundation models for zero-shot cell tracking across diverse microscopy data."}}
{"id": "2509.09946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "A method to extend 2D multi-camera tracking systems into 3D space using depth information and point-cloud reconstruction without requiring complete system overhaul.", "motivation": "Existing MTMC systems are built for 2D tracking, and transitioning to 3D tracking would require rebuilding all components from scratch, which is impractical for current systems.", "method": "Utilizes depth information to reconstruct targets in point-cloud space, performs clustering and yaw refinement for 3D box recovery, and enhances data association using target's local ID consistency for global ID assignment across frames.", "result": "Achieved 3rd place on the 2025 AI City Challenge's 3D MTMC dataset leaderboard.", "conclusion": "The approach successfully enables existing 2D multi-camera tracking systems to operate in 3D space without complete system replacement, demonstrating practical viability for real-world surveillance applications."}}
{"id": "2509.09958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "A zero-shot workflow using visual-language verification with general-purpose VLMs achieves competitive REC performance without task-specific training, outperforming trained baselines.", "motivation": "To demonstrate that referring expression comprehension can be effectively solved without REC-specific training through proper workflow design rather than task-specific pretraining.", "method": "Reformulates REC as box-wise visual-language verification using proposals from a generic detector (YOLO-World), where a general-purpose VLM answers True/False queries for each region independently.", "result": "Surpasses zero-shot GroundingDINO baseline and exceeds reported results for trained GroundingDINO and GroundingDINO+CRG on RefCOCO, RefCOCO+, and RefCOCOg datasets.", "conclusion": "Workflow design, not task-specific pretraining, is key to strong zero-shot REC performance, with verification outperforming selection-based prompting and supporting abstention and multiple matches."}}
{"id": "2509.09961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "Proposes Random Projected Copy-and-Paste (RPCP) augmentation to address extreme pixel imbalance in wheat disease/insect damage segmentation by extracting rare patches, applying transformations, and blending them naturally into backgrounds.", "motivation": "Extreme pixel-level imbalance in wheat disease segmentation (insect damage occupies tiny fraction of pixels) causes overfitting to common classes and insufficient learning of rare classes, impairing overall performance.", "method": "Extract rare insect-damage patches from training images, apply random geometric transformations, paste them in appropriate regions avoiding overlaps, and use random projection filter to refine local features for natural blending.", "result": "Substantially improves segmentation performance on insect damage class while maintaining or slightly enhancing accuracy on other categories.", "conclusion": "Targeted augmentation effectively mitigates extreme pixel imbalance, offering a straightforward yet effective solution for agricultural segmentation problems."}}
{"id": "2509.09962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "A new HMM framework that combines uncertain identities with tracking to improve long-term multi-object tracking performance, particularly for applications like livestock monitoring where sporadic identifications are available.", "motivation": "Existing MOT approaches suffer from identity switches over time, making them unsuitable for long-term tracking needed in applications like livestock behavior analysis where videos span several minutes.", "method": "Proposes a Hidden Markov Model (HMM) framework that incorporates both uncertain identities and tracking information, leveraging sporadic identifications from sources like feeders in livestock settings.", "result": "Improves F1 score of ByteTrack on a 10-minute pig tracking dataset with 21 identifications, shows robustness to identification uncertainty, and validates performance on MOT17 and MOT20 benchmarks with both ByteTrack and FairMOT.", "conclusion": "The HMM framework effectively addresses long-term MOT challenges by utilizing sporadic identity information, demonstrating improved tracking performance and robustness across different scenarios and datasets."}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "Survey paper on event camera fusion with traditional frame-based capture for video restoration and 3D reconstruction tasks, covering deep learning approaches for temporal/spatial enhancement and compiling available datasets.", "motivation": "Event cameras offer low latency, low power consumption, and high capture rates, but their fusion with traditional frame-based systems can significantly benefit video restoration and 3D reconstruction tasks.", "method": "Systematic review of deep learning contributions to image/video enhancement, focusing on temporal enhancement (frame interpolation, motion deblurring) and spatial enhancement (super-resolution, low-light/HDR enhancement, artifact reduction), plus 3D reconstruction evolution.", "result": "Comprehensive survey covering diverse topics with in-depth discussions on recent works for improving visual quality under challenging conditions, along with compilation of openly available datasets for reproducible research.", "conclusion": "The survey consolidates recent progress to inspire further research into leveraging event camera systems combined with deep learning for advanced visual media restoration and enhancement."}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack is the first transformer-based ANN-SNN hybrid tracker that uses ISTA adapters to effectively fuse RGB and event data for superior visual object tracking performance and energy efficiency.", "motivation": "Existing artificial neural networks struggle to exploit the sparse, asynchronous nature of event streams in RGB-Event tracking, and hybrid ANN-SNN architectures face challenges in effectively fusing features across heterogeneous paradigms.", "method": "A two-branch model with vision transformer for RGB spatial context and spiking transformer for event spatio-temporal dynamics, featuring model-based ISTA adapters for bidirectional feature interaction derived from sparse representation theory, plus temporal downsampling attention for feature alignment.", "result": "State-of-the-art performance on RGB-Event tracking benchmarks (FE240hz, VisEvent, COESOT, FELT) while maintaining high energy efficiency.", "conclusion": "ISTASTrack demonstrates the effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking, successfully bridging modality and paradigm gaps between RGB and event data processing."}}
{"id": "2509.09988", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "Proposed solar flare prediction model using multiple deep state space models with FLARE loss to handle class imbalance, achieving superior performance on standard metrics.", "motivation": "Accurate solar flare prediction is crucial for infrastructure protection, but current methods struggle with severe class imbalance across flare classes.", "method": "Multiple deep state space models with frequency & local-boundary-aware reliability loss (FLARE loss) to address class imbalance issues.", "result": "Outperformed baseline approaches in both Gandin-Murphy-Gerrity score and true skill statistic metrics on multi-wavelength solar image dataset covering 11-year solar cycle.", "conclusion": "The proposed method effectively addresses class imbalance in solar flare prediction and demonstrates improved performance and reliability compared to existing approaches."}}
{"id": "2509.10005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "TUNI is a novel RGB-thermal semantic segmentation model that integrates feature extraction and cross-modal fusion in a unified encoder, achieving competitive performance with fewer parameters and real-time inference speed.", "motivation": "Existing RGB-T models use separate encoders pre-trained on RGB images, leading to limited thermal feature extraction, suboptimal cross-modal fusion, and redundant architecture that compromises real-time efficiency.", "method": "Proposes TUNI with a unified RGB-T encoder using stacked blocks for simultaneous multi-modal feature extraction and fusion. Uses large-scale pre-training with RGB and pseudo-thermal data, slims down thermal branch, and introduces RGB-T local module with adaptive cosine similarity for selective feature emphasis.", "result": "Achieves competitive performance with state-of-the-art models on FMB, PST900 and CART datasets, with fewer parameters and lower computational cost. Achieves 27 FPS inference speed on Jetson Orin NX.", "conclusion": "TUNI successfully addresses limitations of previous RGB-T models by integrating feature extraction and fusion in a unified architecture, demonstrating both performance efficiency and real-time deployment capability for autonomous platforms."}}
{"id": "2509.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "Novel few-part-shot font generation model that creates complete fonts using only partial character shapes as input, improving efficiency over traditional few-shot methods.", "motivation": "To address the inefficiency of conventional few-shot font generation that requires complete character shapes, and to understand how partial design elements influence overall character structure.", "method": "Proposes a model that uses partial shapes (design elements) as input to generate entire fonts, focusing on how partial details affect complete character structures.", "result": "The approach enables font generation with only partial input shapes, improving creation efficiency while providing insights into design element influence.", "conclusion": "This method offers a more efficient font generation process and valuable understanding of how partial design details shape complete character structures."}}
{"id": "2509.10021", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas K\u00fchne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "An efficient VIO pipeline optimized for micro/nano-UAVs using quantized feature tracking methods (SuperPoint, PX4FLOW, ORB) on RISC-V SoCs, achieving 3.65x RMSE reduction with real-time performance.", "motivation": "Bridge the gap between high-accuracy VIO systems requiring powerful hardware and lightweight implementations suitable for microcontrollers on resource-constrained micro/nano-UAVs.", "method": "Developed optimized VIO pipeline with state-of-the-art feature detection/tracking (SuperPoint, PX4FLOW, ORB), quantized for RISC-V SoCs, using rigid body motion model to reduce estimation errors in planar motion.", "result": "Achieved average 3.65x RMSE reduction over baseline using ORB tracker on GAP9 SoC. PX4FLOW showed on-par accuracy with ORB at lower runtime for speeds below 24 pixels/frame.", "conclusion": "The pipeline successfully enables high-accuracy VIO on ultra-low-power systems, making it suitable for real-time applications on micro/nano-UAVs with significant performance improvements."}}
{"id": "2509.10024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "MLANet: Hierarchical Multi-Level Attention Network for 3D face reconstruction from single in-the-wild images using CNN with attention mechanisms and semi-supervised training.", "motivation": "Lack of ground-truth labeled datasets and complexity of real-world environments pose challenges for 3D face reconstruction from 2D images, despite its wide applications.", "method": "Uses hierarchical backbone network with multi-level attention mechanisms during 2D feature extraction. Employs semi-supervised training with 3DMM parameters and differentiable renderer for end-to-end training.", "result": "Extensive experiments on AFLW2000-3D and MICC Florence datasets show effectiveness in 3D face reconstruction and alignment tasks, evaluated both quantitatively and qualitatively.", "conclusion": "Proposed MLANet effectively addresses 3D face reconstruction challenges from single in-the-wild images through hierarchical attention mechanisms and semi-supervised training strategy."}}
{"id": "2509.10026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT is a multilingual visual question answering framework that combines visual chain-of-thought reasoning with multi-aspect reward optimization, achieving state-of-the-art performance on multiple benchmarks and surpassing proprietary models like GPT-4o and Gemini.", "motivation": "Existing multilingual visual question answering approaches rely primarily on textual chain-of-thought reasoning and provide limited support for multilingual multimodal reasoning, constraining real-world deployment.", "method": "LaV-CoT uses a multi-stage reasoning pipeline with text summary, language identification, spatial captioning, and logical reasoning. It employs automated data curation and two-stage training with supervised fine-tuning and language-aware group relative policy optimization guided by multi-aspect rewards.", "result": "Achieves up to 9.5% accuracy improvements over similar-sized open-source baselines, surpasses 2x larger models by 2.6%, and outperforms proprietary models like GPT-4o and Gemini-2.5-flash on multiple datasets.", "conclusion": "LaV-CoT demonstrates superior multilingual multimodal reasoning capabilities and effectiveness for real-world industrial deployment through comprehensive evaluations and online A/B testing."}}
{"id": "2509.10058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "Training-free framework using LLM to disambiguate color terms and refine text embeddings in CIELAB space for accurate color alignment in text-to-image generation.", "motivation": "Current diffusion models struggle with nuanced and compound color terms, producing images misaligned with human intent, especially for applications like fashion and design.", "method": "Uses LLM to resolve ambiguous color terms, then refines text embeddings based on spatial relationships in CIELAB color space for precise color blending.", "result": "Improves color alignment without compromising image quality, working without additional training or reference images.", "conclusion": "Bridges gap between text semantics and visual generation by systematically handling color ambiguity through LLM-guided embedding refinement."}}
{"id": "2509.10059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "AVI-Math is the first benchmark for evaluating multimodal mathematical reasoning in aerial vehicle imagery, covering geometry, logic, and algebra with 3,773 UAV-captured questions. Current VLMs struggle significantly with these reasoning tasks despite success on other benchmarks.", "motivation": "Current vision-language models lack adequate testing for mathematical reasoning in UAV-based remote sensing applications, which is critical for precise distance/area computations, trajectory estimations, and spatial analysis.", "method": "Created AVI-Math benchmark with 3,773 high-quality UAV-captured questions across 6 mathematical subjects and 20 topics, collected at varying altitudes and angles. Evaluated 14 prominent VLMs and explored Chain-of-Thought prompting and fine-tuning techniques.", "result": "Despite success on previous multimodal benchmarks, current VLMs struggle significantly with mathematical reasoning tasks in AVI-Math, exposing substantial limitations in their reasoning capabilities.", "conclusion": "The benchmark reveals critical gaps in VLM mathematical reasoning for UAV applications. Chain-of-Thought prompting and fine-tuning show promise for improvement, providing valuable insights for advancing trustworthy UAV-based VLMs in real-world scenarios."}}
{"id": "2509.10080", "categories": ["cs.CV", "I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "BEVTraj is a novel trajectory prediction framework that uses real-time sensor data in bird's-eye view space without relying on pre-built HD maps, achieving comparable performance to map-based models with greater flexibility.", "motivation": "Overcome limitations of pre-built HD maps (limited coverage, inability to adapt to transient changes) and local map construction modules (only recognize predefined elements, may miss critical details or introduce errors).", "method": "Operates directly in BEV space using real-time sensor data, leverages deformable attention to extract context from dense BEV features, and introduces Sparse Goal Candidate Proposal (SGCP) module for end-to-end prediction without post-processing.", "result": "Achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating dependency on pre-built maps.", "conclusion": "BEVTraj provides an effective alternative to map-dependent approaches, demonstrating that high-quality trajectory prediction can be achieved using real-time sensor data alone without sacrificing performance."}}
{"id": "2509.10093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10093", "abs": "https://arxiv.org/abs/2509.10093", "authors": ["Laura Bragagnolo", "Matteo Terreran", "Leonardo Barcellona", "Stefano Ghidoni"], "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing", "comment": "ICIAP 2025", "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.", "AI": {"tldr": "A novel multi-view training framework improves multi-human parsing performance in occlusion scenarios by 4.20% over baseline models.", "motivation": "Existing multi-human parsing models struggle with overlapping/occluded bodies, but different viewpoints can provide separation information that helps resolve these ambiguities.", "method": "Proposes a training framework using multi-view information with weak supervision on human instances and a multi-view consistency loss. Uses semi-automatic annotation strategy to generate instance segmentation masks from multi-view RGB+D data and 3D human skeletons.", "result": "Achieves up to 4.20% relative improvement on human parsing over baseline models in occlusion scenarios.", "conclusion": "Multi-view information and consistency constraints effectively enhance multi-human parsing performance, particularly for challenging occlusion cases where single-view approaches fail."}}
{"id": "2509.10105", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10105", "abs": "https://arxiv.org/abs/2509.10105", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "title": "VARCO-VISION-2.0 Technical Report", "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.", "AI": {"tldr": "VARCO-VISION-2.0 is an open-weight bilingual vision-language model for Korean and English with improved capabilities over its predecessor, supporting multi-image understanding, layout-aware OCR, and achieving strong benchmark performance.", "motivation": "To advance bilingual vision-language models for Korean and English with enhanced multimodal capabilities, improved safety, and practical deployment options including lightweight versions for on-device use.", "method": "Four-stage curriculum training with memory-efficient techniques, supporting multi-image understanding for documents/charts/tables, layout-aware OCR with spatial location prediction, and preference optimization for safety.", "result": "Achieves enhanced multimodal alignment, preserves core language abilities, strong spatial grounding, competitive bilingual performance, and 8th place on OpenCompass VLM leaderboard among comparable-scale models.", "conclusion": "VARCO-VISION-2.0 advances bilingual VLM development with practical applications, offering both full-scale (14B) and lightweight (1.7B) variants for different deployment scenarios."}}
{"id": "2509.10114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10114", "abs": "https://arxiv.org/abs/2509.10114", "authors": ["MohammadAli Hamidi", "Hadi Amirpour", "Luigi Atzori", "Christian Timmerer"], "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss", "comment": null, "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.", "AI": {"tldr": "Lightweight face image quality assessment method using ensemble of MobileNetV3-Small and ShuffleNetV2 with correlation-aware loss, achieving high accuracy with low computational cost.", "motivation": "Existing FIQA methods are either not face-specific or computationally intensive, limiting practical deployment in real-world face recognition systems.", "method": "Ensemble of two compact CNNs (MobileNetV3-Small and ShuffleNetV2) with prediction-level fusion via averaging, using MSECorrLoss that combines MSE with Pearson correlation regularizer.", "result": "Achieves SRCC of 0.9829 and PLCC of 0.9894 on VQualA benchmark while meeting efficiency constraints.", "conclusion": "Proposed method provides an effective balance between accuracy and computational efficiency, making it suitable for real-world face image quality assessment applications."}}
{"id": "2509.10122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.", "AI": {"tldr": "RCOD is a one-step diffusion framework for real-world image super-resolution that enables explicit control over fidelity-realism trade-offs through latent domain grouping and degradation-aware sampling.", "motivation": "One-step diffusion methods for super-resolution lack flexible control mechanisms to balance fidelity and realism across diverse scenarios, unlike multi-step methods that can adjust sampling steps.", "method": "Proposes RCOD framework with latent domain grouping strategy for explicit fidelity-realism control, degradation-aware sampling strategy, and visual prompt injection module replacing text prompts with degradation-aware visual tokens.", "result": "Achieves superior fidelity and perceptual quality while maintaining computational efficiency, outperforming state-of-the-art OSD methods in both quantitative metrics and visual qualities.", "conclusion": "RCOD provides flexible realism control capabilities during inference with minimal training modifications, demonstrating effective trade-off management between fidelity and realism in real-world image super-resolution."}}
{"id": "2509.10134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10134", "abs": "https://arxiv.org/abs/2509.10134", "authors": ["Rini Smita Thakur", "Rajeev Ranjan Dwivedi", "Vinod K Kurmi"], "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment", "comment": "Accepted in BMVC 2025", "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.", "AI": {"tldr": "Grad-CL is a source-free domain adaptation framework for optic disc/cup segmentation that uses gradient-guided pseudolabel refinement and cosine similarity contrastive learning to improve cross-domain performance without accessing source data.", "motivation": "Segmentation models trained on one dataset perform poorly on target data from different imaging protocols/conditions. There's a need for domain adaptation that doesn't require original source data access.", "method": "Two-stage approach: 1) Gradient-guided pseudolabel refinement using class-specific feature extraction and uncertainty quantification, 2) Cosine similarity-based contrastive learning to enforce inter-class separability between optic cup and disc features.", "result": "Outperforms state-of-the-art unsupervised and source-free domain adaptation methods on cross-domain fundus imaging datasets, achieving superior segmentation accuracy and improved boundary delineation.", "conclusion": "Grad-CL provides an effective source-free domain adaptation solution for medical image segmentation that maintains performance across different imaging conditions without requiring access to original training data."}}
{"id": "2509.10140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.", "AI": {"tldr": "VQBridge is a novel projector that enables 100% codebook usage in vector quantization networks, achieving state-of-the-art reconstruction and significantly improving image generation performance when integrated with LlamaGen.", "motivation": "Vector quantization training faces instability due to straight-through estimation bias, one-step-behind updates, and sparse codebook gradients, leading to suboptimal reconstruction and low codebook usage.", "method": "Proposed VQBridge, a robust projector based on map function method that optimizes code vectors through compress-process-recover pipeline, combined with learning annealing to achieve full codebook usage.", "result": "Achieves 100% codebook usage even with 262k-codebook, state-of-the-art reconstruction performance, consistent improvement with larger codebooks/higher channels/longer training, and effective across VQ variants. Integrated with LlamaGen, it surpasses VAR by 0.5 and DiT by 0.2 rFID.", "conclusion": "VQBridge enables stable and effective codebook training, demonstrating the importance of high-quality tokenizers for strong autoregressive image generation and providing a scalable solution for full codebook utilization."}}
{"id": "2509.10156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10156", "abs": "https://arxiv.org/abs/2509.10156", "authors": ["Goker Erdogan", "Nikhil Parthasarathy", "Catalin Ionescu", "Drew Hudson", "Alexander Lerchner", "Andrew Zisserman", "Mehdi Sajjadi", "Joao Carreira"], "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing", "comment": "ICCV 2025", "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.", "AI": {"tldr": "LayerLock is a self-supervised learning method that progressively freezes ViT layers during training to accelerate masked autoencoding and enable effective latent prediction without representation collapse.", "motivation": "The authors observed that during video masked-autoencoding training, ViT layers converge sequentially by depth (shallow layers first, deep layers later), and sought to exploit this pattern to improve training efficiency and enable scalable latent prediction.", "method": "Progressive layer freezing according to an explicit schedule based on layer convergence order, applied to large masked autoencoding models up to 4B parameters.", "result": "LayerLock surpasses non-latent masked prediction methods on the 4DS perception suite benchmark, demonstrating accelerated training and effective latent prediction without representation collapse.", "conclusion": "Progressive layer freezing based on natural convergence patterns provides a simple yet effective approach for scalable self-supervised visual representation learning that outperforms standard methods."}}
{"id": "2509.10241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.", "AI": {"tldr": "Appearance embeddings improve photometric fidelity but not geometric accuracy in 3D reconstruction for space robotics. Convex splatting provides more compact representations than Gaussian splatting.", "motivation": "To systematically compare implicit and explicit novel view synthesis methods for space-based 3D object reconstruction and evaluate the role of appearance embeddings in geometric accuracy for space robotics applications.", "method": "Used the SPEED+ dataset to compare K-Planes, Gaussian Splatting, and Convex Splatting methods, analyzing how appearance embeddings affect both photometric fidelity and geometric accuracy.", "result": "Appearance embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Convex splatting achieves more compact and clutter-free representations than Gaussian splatting.", "conclusion": "Appearance embeddings have limited benefits for geometry-centric tasks in space scenarios. Convex splatting offers advantages for safety-critical applications like interaction and collision avoidance due to its efficient representation."}}
{"id": "2509.10250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.", "AI": {"tldr": "GAMMA is a novel training framework that improves AI-generated image detection by reducing domain bias and enhancing semantic alignment through diverse manipulation strategies and multi-task supervision.", "motivation": "Existing AI-generated image detectors struggle with generalization to unseen generative models due to reliance on generation-specific artifacts like stylistic priors and compression patterns.", "method": "Proposes GAMMA framework with diverse manipulation strategies (inpainting-based manipulation, semantics-preserving perturbations), multi-task supervision with dual segmentation heads and classification head, and reverse cross-attention mechanism for bias correction.", "result": "Achieves state-of-the-art generalization performance on GenImage benchmark with 5.8% accuracy improvement and maintains strong robustness on newly released generative models like GPT-4o.", "conclusion": "GAMMA effectively addresses generalization limitations in AI-generated image detection by reducing domain bias and improving semantic alignment through innovative manipulation strategies and multi-task learning."}}
{"id": "2509.10257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "\u017diga \u0160piclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.", "AI": {"tldr": "Comparative analysis of three fetal brain MRI super-resolution methods (NiftyMIC, SVRTK, NeSVoR) shows NeSVoR has highest reconstruction success rate (>90%) across healthy and pathological cases, with diagnostic classification unaffected despite volumetric differences between methods.", "motivation": "Fetal brain MRI suffers from low resolution, motion artifacts, and inadequate 3D anatomy capture. Super-resolution reconstruction methods aim to address these limitations, but their comparative performance in pathological cases and impact on downstream analysis remain underexplored.", "method": "Applied three state-of-the-art SRR methods (NiftyMIC, SVRTK, NeSVoR) to 140 fetal brain MRI scans including healthy controls and pathological cases with ventriculomegaly. Each reconstruction was segmented using BoUNTi algorithm to extract volumes of nine brain structures.", "result": "NeSVoR demonstrated the highest and most consistent reconstruction success rate (>90%) across both healthy and pathological groups. Significant differences in volumetric estimates were observed between SRR methods, but classification performance for ventriculomegaly was not affected by the choice of SRR method.", "conclusion": "NeSVoR shows superior robustness for fetal brain MRI super-resolution reconstruction, and diagnostic performance remains resilient despite volumetric variability introduced by different SRR methods."}}
{"id": "2509.10259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.", "AI": {"tldr": "Proposes Mask Consistency Regularization (MCR) to address mask hallucination and mask-shape bias in object removal tasks using diffusion models, through dilation and reshape mask perturbations during training.", "motivation": "Current diffusion models for object removal suffer from mask hallucination (generating irrelevant content) and mask-shape bias (filling mask area with objects that mimic mask shape rather than surrounding context).", "method": "Introduces Mask Consistency Regularization (MCR) with two mask perturbations: dilation masks to align output with surrounding content, and reshaped masks to break mask-shape bias, enforcing consistency between perturbed branches and original mask outputs.", "result": "MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal tasks with more robust and contextually coherent inpainting results.", "conclusion": "The proposed MCR training strategy effectively addresses key challenges in object removal by enforcing mask consistency through strategic perturbations, resulting in superior inpainting quality and reduced artifacts."}}
{"id": "2509.10260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "MagicMirror is a comprehensive framework for assessing physical artifacts in text-to-image generation, featuring a detailed artifact taxonomy, large-scale human-annotated dataset (MagicData340K), a trained VLM assessor (MagicAssessor), and an automated benchmark (MagicBench) that reveals significant artifacts even in top-tier models.", "motivation": "Current text-to-image models suffer from persistent physical artifacts (anatomical and structural flaws) that degrade perceptual quality and limit applications, but lack a systematic evaluation framework for these diverse and complex artifacts.", "method": "1) Established detailed artifact taxonomy 2) Created MagicData340K - 340K human-annotated images with fine-grained artifact labels 3) Trained MagicAssessor VLM using novel data sampling and multi-level reward system with GRPO 4) Built MagicBench automated benchmark for T2I model evaluation", "result": "Evaluation with MagicBench revealed that even top-tier models like GPT-image-1 consistently suffer from significant artifacts, demonstrating the prevalence and severity of artifact issues in current T2I generation.", "conclusion": "Artifact reduction represents a critical frontier for future T2I development, and MagicMirror provides the necessary comprehensive framework for systematic assessment and improvement of generated image quality."}}
{"id": "2509.10266", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10266", "abs": "https://arxiv.org/abs/2509.10266", "authors": ["Wenfang Wu", "Tingting Yuan", "Yupeng Li", "Daling Wang", "Xiaoming Fu"], "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion", "comment": null, "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.", "AI": {"tldr": "SignClip improves sign language translation by fusing manual (hand gestures) and non-manual (lip movements) cues with hierarchical contrastive learning, achieving state-of-the-art results on benchmark datasets.", "motivation": "Most sign language translation approaches focus only on manual signals and overlook non-manual cues like mouthing, which are crucial for disambiguating visually similar signs and conveying essential linguistic information.", "method": "Proposes SignClip framework that fuses spatial gesture and lip movement features, and introduces hierarchical contrastive learning with multi-level alignment objectives for semantic consistency across sign-lip and visual-text modalities.", "result": "Outperforms previous state-of-the-art model SpaMo on PHOENIX14T dataset: BLEU-4 improved from 24.32 to 24.71, ROUGE improved from 46.57 to 48.38 in gloss-free setting.", "conclusion": "Incorporating non-manual cues like mouthing significantly improves sign language translation accuracy, demonstrating the importance of multi-modal fusion for better SLT performance."}}
{"id": "2509.10278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10278", "abs": "https://arxiv.org/abs/2509.10278", "authors": ["Vidit Vidit", "Pavel Korshunov", "Amir Mohammadi", "Christophe Ecabert", "Ketan Kotwal", "S\u00e9bastien Marcel"], "title": "Detecting Text Manipulation in Images using Vision Language Models", "comment": "Accepted in Synthetic Realities and Biometric Security Workshop\n  BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/", "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.", "AI": {"tldr": "Analysis of VLMs for text manipulation detection shows open-source models are improving but still lag behind closed-source models like GPT-4o, with specialized image manipulation VLMs struggling to generalize to text detection tasks.", "motivation": "While Large Vision Language Models have proven effective for image manipulation detection, there's a significant research gap in understanding their capabilities for text manipulation detection, particularly for real-world misuse scenarios.", "method": "Benchmarked both closed-source and open-source VLMs on various text manipulation datasets, including in-the-wild scene texts and fantasy ID cards that simulate challenging real-world misuse cases. Also evaluated image manipulation detection-specific VLMs for text manipulation tasks.", "result": "Open-source VLMs are making progress but still underperform compared to closed-source models like GPT-4o. Specialized image manipulation VLMs demonstrate poor generalization when applied to text manipulation detection tasks.", "conclusion": "There's a need for continued development of VLMs specifically for text manipulation detection, as current models show limitations in generalization and performance compared to closed-source alternatives, particularly for challenging real-world scenarios."}}
{"id": "2509.10282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10282", "abs": "https://arxiv.org/abs/2509.10282", "authors": ["Gang Li", "Tianjiao Chen", "Mingle Zhou", "Min Li", "Delong Han", "Jin Wan"], "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection", "comment": "Page 14, 5 pictures", "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.", "AI": {"tldr": "MCL-AD is a novel zero-shot 3D anomaly detection framework that leverages multimodal collaboration across point clouds, RGB images, and text semantics to achieve superior performance without labeled training data.", "motivation": "Existing methods focus only on point clouds and neglect rich semantic cues from complementary modalities like RGB images and text priors, which limits performance in zero-shot 3D anomaly detection scenarios.", "method": "Proposes Multimodal Prompt Learning Mechanism (MPLM) with object-agnostic decoupled text prompts and multimodal contrastive loss, plus Collaborative Modulation Mechanism (CMM) to jointly modulate RGB image-guided and point cloud-guided branches.", "result": "Extensive experiments demonstrate state-of-the-art performance in zero-shot 3D anomaly detection.", "conclusion": "MCL-AD framework effectively leverages multimodal collaboration learning across point clouds, RGB images, and text semantics to achieve superior zero-shot 3D anomaly detection performance."}}
{"id": "2509.10298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10298", "abs": "https://arxiv.org/abs/2509.10298", "authors": ["Laith Nayal", "Mahmoud Mousatat", "Bader Rasheed"], "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks", "comment": "8 pages, 2 tables", "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.", "AI": {"tldr": "Lipschitz-guided stochastic depth method with depth-dependent drop probabilities improves robustness while maintaining accuracy and reducing computation in Vision Transformers.", "motivation": "Deep neural networks and Vision Transformers achieve state-of-the-art performance but are highly vulnerable to adversarial perturbations, with standard defenses often being computationally expensive or lacking formal guarantees.", "method": "Propose a Lipschitz-guided stochastic depth (DropPath) method where drop probabilities increase with depth to control the effective Lipschitz constant of the network, regularizing deeper layers.", "result": "Experiments on CIFAR-10 with ViT-Tiny show maintained near-baseline clean accuracy, enhanced robustness under FGSM, PGD-20, and AutoAttack attacks, and significant FLOPs reduction compared to baseline and linear DropPath schedules.", "conclusion": "The depth-dependent drop probability schedule effectively improves adversarial robustness while preserving clean accuracy and reducing computational cost in Vision Transformers."}}
{"id": "2509.10310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10310", "abs": "https://arxiv.org/abs/2509.10310", "authors": ["Evan Murphy", "Marco Viola", "Vladimir A. Krylov"], "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments", "comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)", "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.", "AI": {"tldr": "Probabilistic framework using energy maps for precise geolocation of street furniture in urban environments, integrating GIS data with stochastic optimization.", "motivation": "Effective monitoring and maintenance of public infrastructure requires precise geolocation of street furniture in complex urban environments, which is challenging for local authorities and private stakeholders.", "method": "Proposes a probabilistic framework based on energy maps that encode spatial likelihood of object locations, using map-based geopositioned format to integrate external geospatial information (GIS layers, road maps, placement constraints). Introduces stochastic birth-and-death optimization algorithm to infer most probable asset configurations.", "result": "Evaluated using realistic simulation based on geolocated dataset of street lighting infrastructure in Dublin city centre, demonstrating potential for scalable and accurate urban asset mapping.", "conclusion": "The framework shows promise for improving contextual awareness and localization accuracy in urban asset management, with implementation made publicly available on GitHub."}}
{"id": "2509.10312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "ClusCa accelerates diffusion transformers by performing spatial clustering to reduce token computation by over 90%, achieving up to 4.96x speedup without quality loss.", "motivation": "Diffusion transformers suffer from high computational costs due to iterative denoising, and existing feature caching methods only leverage temporal similarity while ignoring spatial similarity.", "method": "Cluster-Driven Feature Caching (ClusCa) performs spatial clustering on tokens in each timestep, computes only one token per cluster, and propagates the information to all other tokens in the cluster.", "result": "Achieves 4.96x acceleration on FLUX with 99.49% ImageReward (surpassing original by 0.51%), works on DiT, FLUX and HunyuanVideo for both image and video generation without training requirements.", "conclusion": "ClusCa provides an effective orthogonal approach to existing feature caching methods by exploiting spatial similarity, significantly reducing computational costs while maintaining or improving output quality."}}
{"id": "2509.10334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "I-Segmenter is the first fully integer-only Vision Transformer framework for semantic segmentation that achieves near-FP32 accuracy while significantly reducing model size and enabling faster inference.", "motivation": "Vision Transformers for semantic segmentation have high memory and computational costs that limit deployment on resource-constrained devices, and they are fragile under low-precision quantization due to error accumulation in encoder-decoder pipelines.", "method": "Built on Segmenter architecture, replaces floating-point operations with integer-only counterparts, introduces \u03bb-ShiftGELU activation to handle long-tailed distributions, removes L2 normalization, and replaces bilinear interpolation with nearest neighbor upsampling.", "result": "Achieves accuracy within 5.1% of FP32 baseline on average, reduces model size by up to 3.8x, enables 1.2x faster inference, and delivers competitive accuracy even with one-shot PTQ using a single calibration image.", "conclusion": "I-Segmenter provides a practical integer-only solution for efficient ViT-based segmentation deployment on resource-constrained devices while maintaining competitive performance."}}
{"id": "2509.10341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "GARD is a novel deep learning method for OCT image denoising that uses gamma-based diffusion models instead of Gaussian noise assumptions, with a noise-reduced fidelity term to prevent noise reintroduction and accelerated inference.", "motivation": "OCT images suffer from speckle noise that obscures fine details and hinders accurate diagnosis. Existing denoising methods struggle to balance noise reduction with preservation of crucial anatomical structures.", "method": "GARD uses a Denoising Diffusion Gamma Model to better reflect speckle statistics, incorporates a Noise-Reduced Fidelity Term using pre-processed images to guide denoising, and adapts the Denoising Diffusion Implicit Model framework for faster inference.", "result": "GARD significantly outperforms traditional denoising methods and state-of-the-art deep learning models in PSNR, SSIM, and MSE metrics. Qualitative results show sharper edges and better preservation of fine anatomical details.", "conclusion": "GARD provides an effective solution for OCT image despeckling by accurately modeling speckle statistics and preventing noise reintroduction, resulting in superior image quality and structural preservation compared to existing methods."}}
{"id": "2509.10344", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10344", "abs": "https://arxiv.org/abs/2509.10344", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C. Dvornek"], "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography", "comment": "Accepted by MICCAI 2025", "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.", "AI": {"tldr": "GLAM is a foundation visual language model for mammography that uses geometry-guided global and local alignment to better capture multi-view relationships in breast cancer screening, outperforming existing methods.", "motivation": "Current mammography VLMs adapted from natural images ignore domain-specific multi-view relationships that radiologists use, losing critical geometric context and resulting in suboptimal predictions.", "method": "Proposes GLAM with geometry guidance for multi-view mammography pretraining, using joint global and local, visual-visual, and visual-language contrastive learning to capture cross-view alignments and fine-grained local features.", "result": "Pretrained on the large EMBED mammography dataset, GLAM outperforms baseline models across multiple datasets under different settings.", "conclusion": "The geometry-guided approach effectively addresses domain-specific characteristics of mammography, particularly multi-view relationships, leading to improved performance in breast cancer screening applications."}}
{"id": "2509.10345", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10345", "abs": "https://arxiv.org/abs/2509.10345", "authors": ["Georgios Pantazopoulos", "Eda B. \u00d6zyi\u011fit"], "title": "Towards Understanding Visual Grounding in Visual Language Models", "comment": null, "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.", "AI": {"tldr": "Survey paper reviewing visual grounding in vision language models, covering importance, core components, applications, benchmarks, and future research directions.", "motivation": "To provide a comprehensive overview of visual grounding capabilities in modern VLMs and their wide-ranging applications across domains like referring expression comprehension, visual question answering, and environment control.", "method": "Systematic review and analysis of representative works in visual grounding research, examining core components of grounded model development, practical applications, evaluation metrics, and interrelations with multimodal reasoning.", "result": "Outlines the contemporary paradigm for developing grounded VLMs, examines their diverse applications, discusses evaluation benchmarks, and analyzes relationships between visual grounding, chain-of-thought, and reasoning capabilities.", "conclusion": "Identifies challenges in visual grounding and suggests promising future research directions to advance the field of grounded multimodal understanding and generation."}}
{"id": "2509.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.", "AI": {"tldr": "Attention Attack disrupts text-based image editing by using auto-generated captions to break cross-attention alignment between images and text prompts, without needing to know the editing method or prompt.", "motivation": "Text-based image editing methods are vulnerable to adversarial attacks that target the visual-textual alignment, but existing attacks require knowledge of the editing prompt or method.", "method": "Proposes Attention Attack that uses automatically generated captions of source images as proxy prompts to disrupt cross-attention mechanisms, breaking image-text alignment without knowing the actual edit prompt.", "result": "Experiments on TEDBench++ show the attack significantly degrades editing performance while remaining imperceptible. Two new evaluation metrics (Caption Similarity and semantic IoU) are introduced to better measure attack effectiveness.", "conclusion": "The Attention Attack successfully compromises text-based image editing systems by targeting cross-attention mechanisms, demonstrating vulnerabilities in current editing methods and providing improved evaluation strategies for immunization success."}}
{"id": "2509.10366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10366", "abs": "https://arxiv.org/abs/2509.10366", "authors": ["Fabien Allemand", "Attilio Fiandrotti", "Sumanta Chaudhuri", "Alaa Eddine Mazouz"], "title": "Efficient Learned Image Compression Through Knowledge Distillation", "comment": "19 pages, 21 figures", "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .", "AI": {"tldr": "Knowledge distillation reduces neural network image compression resource requirements while maintaining performance across different architectures and quality/bitrate tradeoffs.", "motivation": "Neural network-based image compression outperforms conventional codecs but requires significant processing power, making it unsuitable for real-time use on resource-constrained platforms.", "method": "Leveraging knowledge distillation where smaller neural networks are trained on outputs of larger, more complex models to achieve better performance than independent training.", "result": "Knowledge distillation effectively reduces processing and energy requirements for image compression across various architecture sizes and different quality/bitrate tradeoffs.", "conclusion": "Knowledge distillation is a viable approach to make neural image compression more practical for resource-constrained platforms, with potential for extension to transformer-based models and exploration of different teacher models and loss functions."}}
{"id": "2509.10388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10388", "abs": "https://arxiv.org/abs/2509.10388", "authors": ["Zeqing Leo Yuan", "Mani Ramanagopal", "Aswin C. Sankaranarayanan", "Srinivasa G. Narasimhan"], "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition", "comment": null, "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.", "AI": {"tldr": "Training-free intrinsic image decomposition using visible-thermal image pairs, leveraging thermal absorption to derive ordinal constraints for self-supervised shading and reflectance recovery.", "motivation": "Lack of extensive ground-truth data for real-world intrinsic image decomposition, with existing methods relying on synthetic data or sparse annotations for limited scenes.", "method": "Uses visible-thermal image pairs and the principle that absorbed light becomes heat detected by thermal cameras. Relates ordinalities between visible/thermal intensities to shading/reflectance ordinalities to densely self-supervise neural network optimization.", "result": "Superior performance over recent learning-based models in quantitative evaluations with known reflectance/shading under natural/artificial lighting, and qualitative experiments across diverse outdoor scenes.", "conclusion": "Provides a scalable path to curating real-world ordinal supervision previously infeasible via manual labeling, demonstrating effective training-free intrinsic decomposition."}}
{"id": "2509.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10407", "abs": "https://arxiv.org/abs/2509.10407", "authors": ["Xiem HoangVan", "Dang BuiDinh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards", "comment": null, "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.", "AI": {"tldr": "This paper presents a comprehensive survey on compressed video quality enhancement (CVQE) methods, addressing limitations in existing surveys by introducing a novel taxonomy, unified benchmarking framework, and systematic analysis of performance-complexity trade-offs.", "motivation": "Existing CVQE surveys lack systematic classification linking methods to specific standards and artifacts, insufficient comparative analysis across architectural paradigms, and underdeveloped benchmarking practices, creating gaps in the field.", "method": "The paper introduces three key contributions: 1) a novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization; 2) a unified benchmarking framework with modern compression protocols and standard test sequences; 3) systematic analysis of trade-offs between reconstruction performance and computational complexity.", "result": "The survey establishes a foundation for consistent assessment and informed model selection in CVQE research and deployment, providing comprehensive analysis of state-of-the-art methods and highlighting promising future research directions.", "conclusion": "This comprehensive review addresses critical gaps in CVQE literature by providing systematic classification, fair benchmarking standards, and analysis of performance-complexity trade-offs, serving as a valuable resource for researchers and practitioners in video quality enhancement."}}
{"id": "2509.10408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10408", "abs": "https://arxiv.org/abs/2509.10408", "authors": ["Iacopo Curti", "Pierluigi Zama Ramirez", "Alioscia Petrelli", "Luigi Di Stefano"], "title": "Multimodal SAM-adapter for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.", "AI": {"tldr": "MM SAM-adapter extends Segment Anything Model for multimodal semantic segmentation using adapter network to fuse auxiliary sensor data with RGB features, achieving state-of-the-art performance on challenging benchmarks.", "motivation": "Current semantic segmentation methods are vulnerable to challenging conditions like poor lighting, occlusions, and adverse weather. Multimodal approaches that integrate auxiliary sensor data can provide complementary information to enhance robustness.", "method": "Proposes an adapter network that injects fused multimodal features (e.g., LiDAR, infrared) into SAM's RGB features, enabling selective incorporation of auxiliary modalities only when they provide additional cues while retaining RGB generalization.", "result": "Achieves state-of-the-art performance on three challenging benchmarks (DeLiVER, FMB, MUSES). Outperforms competing methods in both RGB-easy and RGB-hard subsets, demonstrating effectiveness in both favorable and adverse conditions.", "conclusion": "The framework provides balanced and efficient use of multimodal information for robust scene understanding, highlighting the effectiveness of multimodal adaptation in semantic segmentation."}}
{"id": "2509.10441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen is a new method that replaces VAE decoders with a one-step generator to enable arbitrary resolution image generation from fixed-size latents, reducing 4K generation time from over 100 seconds to under 10 seconds.", "motivation": "Current diffusion models have quadratic computational complexity with resolution, making high-resolution image generation (like 4K) slow and resource-intensive, taking over 100 seconds.", "method": "Proposes InfGen which treats the fixed latent from diffusion models as content representation and uses a compact one-step generator to decode arbitrary resolution images, replacing the traditional VAE decoder.", "result": "InfGen reduces 4K image generation time to under 10 seconds and enables arbitrary high-resolution generation without retraining diffusion models.", "conclusion": "InfGen provides an efficient solution for arbitrary resolution image generation that can be applied to any model using the same latent space, significantly reducing computational complexity and generation time."}}
{"id": "2509.10453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10453", "abs": "https://arxiv.org/abs/2509.10453", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets", "comment": null, "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.", "AI": {"tldr": "Self-supervised learning model for Alzheimer's prediction using 3D brain MRI with temporal order prediction and contrastive learning, outperforming supervised methods on most tasks while handling variable-length inputs and time intervals.", "motivation": "Address limitations of existing deep learning models for Alzheimer's prediction including lack of labeled data, poor generalization across datasets, and inflexibility to varying numbers of input scans and time intervals.", "method": "Adapted three state-of-the-art temporal self-supervised learning approaches for 3D brain MRI analysis with novel extensions for variable-length inputs and robust spatial features. Pre-trained on aggregated dataset of 3,161 patients from four public datasets.", "result": "SSL model with temporal order prediction and contrastive learning outperformed supervised learning on 6 out of 7 downstream tasks including diagnosis classification, conversion detection, and future conversion prediction.", "conclusion": "The SSL approach demonstrates superior adaptability and generalizability across tasks and varying input configurations, showing robust performance across clinical applications for Alzheimer's disease prediction."}}
